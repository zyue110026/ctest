	k8s.io/kubernetes/build/pause/windows/wincat		coverage: 0.0% of statements
=== RUN   TestCtestServerOverride

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:21]: matched config: {test_fixture.json [ETCD default override] env [] {               }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:36:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:36:39 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:28]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:21]: matched config: {test_fixture.json [ETCD servers and override set] env [] {  ETCDServers ETCDServersOverrides            }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:36:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:36:39 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"ETCDServers","ETCDServersOverride":"ETCDServersOverrides","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:28]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestServerOverride (0.01s)
=== RUN   TestCtestStorageOptions

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:97]: matched config: {test_fixture.json [storage options supplied] env [] {             StorageBackend StorageMediaType 1s}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:36:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:36:39 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"1s","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"StorageBackend","StorageMediaType":"StorageMediaType"})[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:104]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:97]: matched config: {test_fixture.json [storage options not supplied] env [] {               }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:36:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:36:39 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:104]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestStorageOptions (0.01s)
=== RUN   TestCtestTLSFlags

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:188]: matched config: {test_fixture.json [mTLS enabled] env [] {  https://127.0.0.1:2379  CAKey CACert CACertPath APIServerKey APIServerCert APIServerCertPath APIServerKeyPath ETCDKey ETCDCert   }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:36:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:36:39 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"APIServerCert","APIServerCertPath":"APIServerCertPath","APIServerKey":"APIServerKey","APIServerKeyPath":"APIServerKeyPath","CACert":"CACert","CACertPath":"CACertPath","CAKey":"CAKey","CompactionInterval":"","ETCDCert":"ETCDCert","ETCDKey":"ETCDKey","ETCDServers":"https://127.0.0.1:2379","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:195]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:188]: matched config: {test_fixture.json [mTLS disabled] env [] {               }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:36:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:36:39 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:36:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:195]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestTLSFlags (0.01s)
=== RUN   TestCtestEncryptionProviderFlag
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_set
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_not_set
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_large_payload
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_malformed_base64
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
--- PASS: TestCtestEncryptionProviderFlag (1.56s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_set (0.70s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_not_set (0.17s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_large_payload (0.47s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_malformed_base64 (0.22s)
=== RUN   TestCtestKMSIntegration
[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_kms_test.go:142]: Loading hard‑coded volume config
[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_kms_test.go:150]: got default config: {test_fixture.json [CLOUD_KMS_INTEGRATION set] volumes [pods deployments statefulsets daemonsets replicasets] {kmssocket {&HostPathVolumeSource{Path:/var/run/kmsplugin,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:36:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:36:41 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/cluster/gce/gci	2.560s
	k8s.io/kubernetes/cluster/gce/gci/mounter		coverage: 0.0% of statements
# k8s.io/kubernetes/cmd/kube-controller-manager/app
# [k8s.io/kubernetes/cmd/kube-controller-manager/app]
cmd/kube-controller-manager/app/ctest_core_test.go:139:3: fmt.Printf format %d has arg i of wrong type string
=== RUN   TestCtestMigrate
[DEBUG-CTEST 2026-02-16 15:36:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:38]: Starting TestCtestMigrate with edge cases
[DEBUG-CTEST 2026-02-16 15:36:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:77]: Total migrations to run: 11
Running migration 0: v3-v3-up
=== RUN   TestCtestMigrate/v3-v3-up
  I0216 15:36:40.277646   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:36:40.278144   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:36:40.278151   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server v3-v3-up-0: [/usr/local/bin/etcd-3.0.17 --name v3-v3-up-0 --initial-cluster v3-v3-up-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-v3-v3-up-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 1: oldest-newest-up
=== RUN   TestCtestMigrate/oldest-newest-up
  I0216 15:37:00.343092   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:37:00.347841   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:37:00.347852   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server oldest-newest-up-0: [/usr/local/bin/etcd-3.0.17 --name oldest-newest-up-0 --initial-cluster oldest-newest-up-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-oldest-newest-up-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 2: v3-v3-up-with-additional-client-url
=== RUN   TestCtestMigrate/v3-v3-up-with-additional-client-url
  I0216 15:37:20.394571   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:37:20.399636   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:37:20.399670   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server v3-v3-up-with-additional-client-url-0: [/usr/local/bin/etcd-3.0.17 --name v3-v3-up-with-additional-client-url-0 --initial-cluster v3-v3-up-with-additional-client-url-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-v3-v3-up-with-additional-client-url-0 --listen-client-urls http://127.0.0.1:2379,http://10.128.0.1:2379 --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 3: ha-v3-v3-up
=== RUN   TestCtestMigrate/ha-v3-v3-up
  I0216 15:37:40.427930   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:37:40.428366   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:37:40.427933   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:37:40.431348   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:37:40.431355   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
  I0216 15:37:40.431457   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:37:40.431462   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
  I0216 15:37:40.432003   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:37:40.432355   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server ha-v3-v3-up-2: [/usr/local/bin/etcd-3.0.17 --name ha-v3-v3-up-2 --initial-cluster ha-v3-v3-up-0=https://127.0.0.1:2380,ha-v3-v3-up-1=https://127.0.0.1:12380,ha-v3-v3-up-2=https://127.0.0.1:22380 --debug --data-dir /tmp/etcd-data-dir-ha-v3-v3-up-2 --listen-client-urls  --advertise-client-urls http://127.0.0.1:22379 --listen-peer-urls https://127.0.0.1:22380 --initial-advertise-peer-urls https://127.0.0.1:22380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
Starting server ha-v3-v3-up-0: [/usr/local/bin/etcd-3.0.17 --name ha-v3-v3-up-0 --initial-cluster ha-v3-v3-up-0=https://127.0.0.1:2380,ha-v3-v3-up-1=https://127.0.0.1:12380,ha-v3-v3-up-2=https://127.0.0.1:22380 --debug --data-dir /tmp/etcd-data-dir-ha-v3-v3-up-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
Starting server ha-v3-v3-up-1: [/usr/local/bin/etcd-3.0.17 --name ha-v3-v3-up-1 --initial-cluster ha-v3-v3-up-0=https://127.0.0.1:2380,ha-v3-v3-up-1=https://127.0.0.1:12380,ha-v3-v3-up-2=https://127.0.0.1:22380 --debug --data-dir /tmp/etcd-data-dir-ha-v3-v3-up-1 --listen-client-urls  --advertise-client-urls http://127.0.0.1:12379 --listen-peer-urls https://127.0.0.1:12380 --initial-advertise-peer-urls https://127.0.0.1:12380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 4: v3-v3-down
=== RUN   TestCtestMigrate/v3-v3-down
  I0216 15:38:00.677063   87332 migrator.go:66] Starting migration to 3.1.12/etcd3
  I0216 15:38:00.685611   87332 migrator.go:87] Converging current version '3.1.12/etcd3' to target version '3.1.12/etcd3'
  I0216 15:38:00.685622   87332 migrator.go:91] current version '3.1.12/etcd3' equals or is one minor version previous of target version '3.1.12/etcd3' - migration complete
Starting server v3-v3-down-0: [/usr/local/bin/etcd-3.1.12 --name v3-v3-down-0 --initial-cluster v3-v3-down-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-v3-v3-down-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.1.12: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 5: zero-member-count
=== RUN   TestCtestMigrate/zero-member-count
Running migration 6: negative-member-count
=== RUN   TestCtestMigrate/negative-member-count
Running migration 7: empty-protocol
=== RUN   TestCtestMigrate/empty-protocol
  I0216 15:38:20.772596   87332 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0216 15:38:20.779431   87332 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0216 15:38:20.779451   87332 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server empty-protocol-0: [/usr/local/bin/etcd-3.0.17 --name empty-protocol-0 --initial-cluster empty-protocol-0=://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-empty-protocol-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls ://127.0.0.1:2380 --initial-advertise-peer-urls ://127.0.0.1:2380]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 8: malformed-start-version
=== RUN   TestCtestMigrate/malformed-start-version
--- FAIL: TestCtestMigrate (120.52s)
    --- FAIL: TestCtestMigrate/v3-v3-up (20.06s)
    --- FAIL: TestCtestMigrate/oldest-newest-up (20.05s)
    --- FAIL: TestCtestMigrate/v3-v3-up-with-additional-client-url (20.04s)
    --- FAIL: TestCtestMigrate/ha-v3-v3-up (20.22s)
    --- FAIL: TestCtestMigrate/v3-v3-down (20.10s)
    --- PASS: TestCtestMigrate/zero-member-count (0.00s)
    --- PASS: TestCtestMigrate/negative-member-count (0.00s)
    --- FAIL: TestCtestMigrate/empty-protocol (20.02s)
    --- FAIL: TestCtestMigrate/malformed-start-version (0.00s)
panic: malformed version file, expected <major>.<minor>.<patch>/<storage> but got invalid-version [recovered]
	panic: malformed version file, expected <major>.<minor>.<patch>/<storage> but got invalid-version

goroutine 220 [running]:
testing.tRunner.func1.2({0x10643d740, 0x140003ef290})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10643d740?, 0x140003ef290?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cluster/images/etcd/migrate.mustParseEtcdVersionPair(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:384
k8s.io/kubernetes/cluster/images/etcd/migrate.TestCtestMigrate.func1.1(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:87
k8s.io/kubernetes/cluster/images/etcd/migrate.TestCtestMigrate.func1(0x140006981c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:88 +0x7cc
testing.tRunner(0x140006981c0, 0x14000516600)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 69
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cluster/images/etcd/migrate	122.049s
	k8s.io/kubernetes/cluster/images/etcd-version-monitor		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/clicheck		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/cloud-controller-manager		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/dependencycheck		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/dependencyverifier		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/fieldnamedocscheck		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/gendocs		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 1.4% of statements in ./...
ok  	k8s.io/kubernetes/cmd/genkubedocs	2.718s	coverage: 1.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/genman		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/genswaggertypedocs		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/genutils	0.917s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/genyaml		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/gotemplate	2.144s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/import-boss	0.808s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/importverifier		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kube-apiserver		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kube-apiserver/app		coverage: 0.0% of statements
=== RUN   TestCtestGetServiceIPAndRanges
Starting TestCtestGetServiceIPAndRanges with 24 cases
Running case #0: ""
W0216 15:37:02.205981   87985 options.go:369] No CIDR for service cluster IPs specified. Default value which was 10.0.0.0/24 is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.
Running case #1: "192.0.2.1/24"
Running case #2: "192.0.2.1/24,192.168.128.0/17"
Running case #3: "192.0.2.1/24,2001:db2:1:3:4::1/112"
Running case #4: "2001:db2:1:3:4::1/112,192.0.2.1/24"
Running case #5: "192.0.2.1/30,192.168.128.0/17"
Running case #6: "192.0.2.1/33,192.168.128.0/17"
Running case #7: "192.0.2.1/24,192.168.128.0/33"
Running case #8: "2001:db2:1:3:4::1/129,192.0.2.1/24"
Running case #9: "192.0.2.1/24,2001:db2:1:3:4::1/129"
Running case #10: "192.0.2.1,192.168.128.0/17"
Running case #11: "192.0.2.1/24,192.168.128.1"
Running case #12: "2001:db2:1:3:4::1,192.0.2.1/24"
Running case #13: "192.0.2.1/24,2001:db2:1:3:4::1"
Running case #14: "bad.ip.range,192.168.0.2/24"
Running case #15: "192.168.0.2/24,bad.ip.range"
Running case #16: " 192.0.2.1/24"
    ctest_completion_test.go:51: case 16: expected apiServerServiceIP: 192.0.2.1, got: <nil>
    ctest_completion_test.go:55: case 16: expected primaryServiceIPRange: 192.0.2.0/24, got: <nil>
    ctest_completion_test.go:63: case 16: expected err to be: false, but it was true
Running case #17: "192.0.2.1/24 "
    ctest_completion_test.go:51: case 17: expected apiServerServiceIP: 192.0.2.1, got: <nil>
    ctest_completion_test.go:55: case 17: expected primaryServiceIPRange: 192.0.2.0/24, got: <nil>
    ctest_completion_test.go:63: case 17: expected err to be: false, but it was true
Running case #18: "\t192.0.2.1/24\n"
    ctest_completion_test.go:51: case 18: expected apiServerServiceIP: 192.0.2.1, got: <nil>
    ctest_completion_test.go:55: case 18: expected primaryServiceIPRange: 192.0.2.0/24, got: <nil>
    ctest_completion_test.go:63: case 18: expected err to be: false, but it was true
Running case #19: ","
Running case #20: "192.0.2.1/24,,192.168.128.0/17"
Running case #21: "192.0.2.1/24, "
Running case #22: "invalid"
Running case #23: "192.0.2.1/24,2001:db2:1:3:4::1/112,10.0.0.0/8"
Completed TestCtestGetServiceIPAndRanges
--- FAIL: TestCtestGetServiceIPAndRanges (0.00s)
=== RUN   TestCtestClusterServiceIPRange
=== RUN   TestCtestClusterServiceIPRange/no_service_cidr
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
W0216 15:37:02.206566   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/only_secondary_service_cidr
    ctest_validation_test.go:164: debug secondary CIDR: 10.0.0.0/16
W0216 15:37:02.206655   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v4-v4
    ctest_validation_test.go:164: debug secondary CIDR: 11.0.0.0/16
W0216 15:37:02.206733   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v6-v6
    ctest_validation_test.go:164: debug secondary CIDR: 3000::/108
W0216 15:37:02.206806   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/service_cidr_is_too_big
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
W0216 15:37:02.206880   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enbled
W0216 15:37:02.206918   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.206934   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled
W0216 15:37:02.206965   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.206981   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled#01
W0216 15:37:02.207014   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207028   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enabled
W0216 15:37:02.207060   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207076   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_gate_enabled
W0216 15:37:02.207116   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207131   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_and_gate_enabled
W0216 15:37:02.207163   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207177   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
=== RUN   TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big
W0216 15:37:02.207210   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from true to false
    ctest_validation_test.go:164: debug secondary CIDR: 3000::/64
W0216 15:37:02.207260   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_but_only_ipallocator_gate_enabled
W0216 15:37:02.207286   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207300   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: 3000::/48
=== RUN   TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_gate_enabled
W0216 15:37:02.207332   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207347   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: 3000::/48
=== RUN   TestCtestClusterServiceIPRange/more_than_two_entries
W0216 15:37:02.207380   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from true to false
    ctest_validation_test.go:164: debug secondary CIDR: 3000::/108
W0216 15:37:02.207441   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_primary
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
W0216 15:37:02.207522   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_primary,_class_E_range
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
W0216 15:37:02.207597   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_v4-v6_dual_stack
    ctest_validation_test.go:164: debug secondary CIDR: 3000::/108
W0216 15:37:02.207676   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_v6-v4_dual_stack
    ctest_validation_test.go:164: debug secondary CIDR: 10.0.0.0/16
W0216 15:37:02.207750   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/malformed_primary_CIDR
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
W0216 15:37:02.207829   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/malformed_secondary_CIDR
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
    ctest_validation_test.go:171: expected errors, no errors found
W0216 15:37:02.207906   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/empty_CIDRs_with_gates_enabled_(should_succeed)
W0216 15:37:02.207936   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0216 15:37:02.207952   87985 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:164: debug secondary CIDR: <nil>
    ctest_validation_test.go:167: expected no errors, errors found [--service-cluster-ip-range must contain at least one valid cidr]
W0216 15:37:02.207982   87985 feature_gate.go:568] SetEmulationVersion will change already queried feature:DisableAllocatorDualWrite from false to true
--- FAIL: TestCtestClusterServiceIPRange (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/no_service_cidr (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/only_secondary_service_cidr (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v4-v4 (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v6-v6 (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_is_too_big (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enbled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled#01 (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_and_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_but_only_ipallocator_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/more_than_two_entries (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_primary (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_primary,_class_E_range (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_v4-v6_dual_stack (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_v6-v4_dual_stack (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/malformed_primary_CIDR (0.00s)
    --- FAIL: TestCtestClusterServiceIPRange/malformed_secondary_CIDR (0.00s)
    --- FAIL: TestCtestClusterServiceIPRange/empty_CIDRs_with_gates_enabled_(should_succeed) (0.00s)
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv4_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv6_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families#01
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv4_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv6_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families#01
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/none_endpoint_reconciler_-_wrong_IP_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/empty_endpoint_reconciler_type_with_matching_families
--- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv4_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv6_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families#01 (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv4_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv6_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families#01 (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/none_endpoint_reconciler_-_wrong_IP_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/empty_endpoint_reconciler_type_with_matching_families (0.00s)
=== RUN   TestCtestValidateServiceNodePort
=== RUN   TestCtestValidateServiceNodePort/validate_port_less_than_0
=== RUN   TestCtestValidateServiceNodePort/validate_port_more_than_65535
=== RUN   TestCtestValidateServiceNodePort/validate_port_equal_0
=== RUN   TestCtestValidateServiceNodePort/validate_port_less_than_base
=== RUN   TestCtestValidateServiceNodePort/validate_port_minus_base_more_than_size
=== RUN   TestCtestValidateServiceNodePort/validate_success
=== RUN   TestCtestValidateServiceNodePort/zero_base_and_size
=== RUN   TestCtestValidateServiceNodePort/negative_base
--- PASS: TestCtestValidateServiceNodePort (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_less_than_0 (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_more_than_65535 (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_equal_0 (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_less_than_base (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_minus_base_more_than_size (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_success (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/zero_base_and_size (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/negative_base (0.00s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kube-apiserver/app/options	1.911s
	k8s.io/kubernetes/cmd/kube-apiserver/app/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kube-controller-manager		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/cmd/kube-controller-manager/app [build failed]
	k8s.io/kubernetes/cmd/kube-controller-manager/app/config		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kube-controller-manager/app/options	2.027s	coverage: 0.7% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kube-controller-manager/app/testing		coverage: 0.0% of statements
?   	k8s.io/kubernetes/cmd/kube-controller-manager/names	[no test files]
	k8s.io/kubernetes/cmd/kube-proxy		coverage: 0.0% of statements
=== RUN   TestCtest_detectNodeIPs
=== RUN   TestCtest_detectNodeIPs/Bind_address_IPv4_unicast_address_and_no_Node_object
=== RUN   TestCtest_detectNodeIPs/Bind_address_IPv6_unicast_address_and_no_Node_object
=== RUN   TestCtest_detectNodeIPs/No_Valid_IP_found_and_no_bind_address
    server.go:684: I0216 15:37:11.477563] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/No_Valid_IP_found_and_unspecified_bind_address
    server.go:684: I0216 15:37:11.477653] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv4_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv4_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv6_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv6_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_primary_IPv4
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_primary_IPv6
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_IPv4
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_IPv6
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv4
    server.go:684: I0216 15:37:11.477826] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv6
    server.go:684: I0216 15:37:11.477851] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Invalid_bind_address,_fallback_to_defaults
    server.go:684: I0216 15:37:11.477880] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Empty_bind_address_with_IPv4_node_IPs_present
--- PASS: TestCtest_detectNodeIPs (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_IPv4_unicast_address_and_no_Node_object (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_IPv6_unicast_address_and_no_Node_object (0.00s)
    --- PASS: TestCtest_detectNodeIPs/No_Valid_IP_found_and_no_bind_address (0.00s)
    --- PASS: TestCtest_detectNodeIPs/No_Valid_IP_found_and_unspecified_bind_address (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv4_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv4_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv6_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv6_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_primary_IPv4 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_primary_IPv6 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_IPv4 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_IPv6 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv4 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv6 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Invalid_bind_address,_fallback_to_defaults (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Empty_bind_address_with_IPv4_node_IPs_present (0.00s)
=== RUN   TestCtest_checkBadConfig
=== RUN   TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_single-stack_config
=== RUN   TestCtest_checkBadConfig/dual-stack_NodePortAddresses_with_dual-stack_config
=== RUN   TestCtest_checkBadConfig/empty_NodePortAddresses
=== RUN   TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_dual-stack_config
=== RUN   TestCtest_checkBadConfig/wrong-single-stack_NodePortAddresses
=== RUN   TestCtest_checkBadConfig/nil_Config_should_error
--- FAIL: TestCtest_checkBadConfig (0.00s)
    --- PASS: TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_single-stack_config (0.00s)
    --- PASS: TestCtest_checkBadConfig/dual-stack_NodePortAddresses_with_dual-stack_config (0.00s)
    --- PASS: TestCtest_checkBadConfig/empty_NodePortAddresses (0.00s)
    --- PASS: TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_dual-stack_config (0.00s)
    --- PASS: TestCtest_checkBadConfig/wrong-single-stack_NodePortAddresses (0.00s)
    --- FAIL: TestCtest_checkBadConfig/nil_Config_should_error (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x298 pc=0x102c6fc04]

goroutine 196 [running]:
testing.tRunner.func1.2({0x103750740, 0x105164a30})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x103750740?, 0x105164a30?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kube-proxy/app.checkBadConfig(0x14000573ce0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-proxy/app/server.go:295 +0xb4
k8s.io/kubernetes/cmd/kube-proxy/app.TestCtest_checkBadConfig.func1(0x14000103dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-proxy/app/ctest_server_test.go:281 +0x2c
testing.tRunner(0x14000103dc0, 0x140005b8a50)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 190
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kube-proxy/app	0.706s
	k8s.io/kubernetes/cmd/kube-scheduler		coverage: 0.0% of statements
=== RUN   TestCtestSetup
=== RUN   TestCtestSetup/component_configuration_v1_with_only_scheduler_name_configured
W0216 15:37:12.104115   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:12.257666   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:12.525315   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:12.525326   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:12.525333   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/default_config
W0216 15:37:12.531278   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:12.672955   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:12.817910   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:12.817925   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:12.817934   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/component_configuration_v1
W0216 15:37:12.820109   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:13.201883   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:13.720028   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:13.720039   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:13.720047   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/out-of-tree_component_configuration_v1
W0216 15:37:13.721103   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:13.995797   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:14.361086   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:14.361127   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:14.361147   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_CLI_args,_along_with_--config_arg
W0216 15:37:14.363297   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:14.511823   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:15.042033   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:15.042046   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:15.042058   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_CLI_args,_without_--config_arg
W0216 15:37:15.043219   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:15.379929   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:15.990495   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:15.990511   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:15.990525   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_settings_specified_by_ComponentConfig_only
W0216 15:37:15.991763   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:16.276344   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:16.450041   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:16.450068   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:16.450084   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_settings_specified_by_CLI_args_and_ComponentConfig
W0216 15:37:16.456977   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:16.699518   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:16.985573   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:16.985601   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:16.985616   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/emulated_version_out_of_range
W0216 15:37:16.992852   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
=== RUN   TestCtestSetup/default_feature_gates_at_binary_version
W0216 15:37:16.994459   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:17.131421   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:17.586162   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:17.586178   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:17.586187   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/default_feature_gates_at_emulated_version
W0216 15:37:17.587653   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:18.086130   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:18.634308   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:18.634326   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:18.634339   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/set_feature_gates_at_emulated_version
W0216 15:37:18.636543   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
W0216 15:37:18.637124   88065 registry.go:412] component has alpha features enabled in emulated version, this is unsupported: features=[kubeB]
I0216 15:37:19.157307   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:19.485771   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:19.485787   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:19.485798   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/cannot_set_locked_feature_gate
W0216 15:37:19.487316   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
=== RUN   TestCtestSetup/non‑existent_config_file_should_error
W0216 15:37:19.487792   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:20.026002   88065 serving.go:386] Generated self-signed cert in-memory
=== RUN   TestCtestSetup/malformed_yaml_config_should_error
W0216 15:37:20.026820   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:20.602399   88065 serving.go:386] Generated self-signed cert in-memory
=== RUN   TestCtestSetup/invalid_feature‑gate_name_should_error
W0216 15:37:20.604156   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
=== RUN   TestCtestSetup/negative_lease_duration_should_error
W0216 15:37:20.604716   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:20.787252   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:20.958326   88065 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0216 15:37:20.958342   88065 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0216 15:37:20.958355   88065 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
    ctest_server_test.go:504: expected Setup error, got nil
=== RUN   TestCtestSetup/missing_kubeconfig_and_no_config_should_error
W0216 15:37:20.961192   88065 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0216 15:37:21.472290   88065 serving.go:386] Generated self-signed cert in-memory
W0216 15:37:21.472920   88065 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
W0216 15:37:21.472957   88065 client_config.go:672] error creating inClusterConfig, falling back to default config: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined
--- FAIL: TestCtestSetup (9.37s)
    --- PASS: TestCtestSetup/component_configuration_v1_with_only_scheduler_name_configured (0.43s)
    --- PASS: TestCtestSetup/default_config (0.29s)
    --- PASS: TestCtestSetup/component_configuration_v1 (0.90s)
    --- PASS: TestCtestSetup/out-of-tree_component_configuration_v1 (0.64s)
    --- PASS: TestCtestSetup/leader_election_CLI_args,_along_with_--config_arg (0.68s)
    --- PASS: TestCtestSetup/leader_election_CLI_args,_without_--config_arg (0.95s)
    --- PASS: TestCtestSetup/leader_election_settings_specified_by_ComponentConfig_only (0.47s)
    --- PASS: TestCtestSetup/leader_election_settings_specified_by_CLI_args_and_ComponentConfig (0.53s)
    --- PASS: TestCtestSetup/emulated_version_out_of_range (0.00s)
    --- PASS: TestCtestSetup/default_feature_gates_at_binary_version (0.59s)
    --- PASS: TestCtestSetup/default_feature_gates_at_emulated_version (1.05s)
    --- PASS: TestCtestSetup/set_feature_gates_at_emulated_version (0.85s)
    --- PASS: TestCtestSetup/cannot_set_locked_feature_gate (0.00s)
    --- PASS: TestCtestSetup/non‑existent_config_file_should_error (0.54s)
    --- PASS: TestCtestSetup/malformed_yaml_config_should_error (0.58s)
    --- PASS: TestCtestSetup/invalid_feature‑gate_name_should_error (0.00s)
    --- FAIL: TestCtestSetup/negative_lease_duration_should_error (0.36s)
    --- PASS: TestCtestSetup/missing_kubeconfig_and_no_config_should_error (0.51s)
FAIL
coverage: 1.5% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kube-scheduler/app	12.262s
	k8s.io/kubernetes/cmd/kube-scheduler/app/config		coverage: 0.0% of statements
=== RUN   TestCtestLoadConfigFromFile
=== RUN   TestCtestLoadConfigFromFile/case_0:_Empty_scheduler_config_file_path
=== RUN   TestCtestLoadConfigFromFile/case_1:_Correct_scheduler_config
=== RUN   TestCtestLoadConfigFromFile/case_2:_Scheduler_config_with_decode_error
=== RUN   TestCtestLoadConfigFromFile/case_3:_Scheduler_config_version_too_old
=== RUN   TestCtestLoadConfigFromFile/case_4:_Empty_file_content
    ctest_configfile_test.go:115: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-scheduler/app/options/ctest_configfile_test.go:115
        	Error:      	Error "Object 'Kind' is missing in ''" does not contain "'apiVersion' is missing"
        	Test:       	TestCtestLoadConfigFromFile/case_4:_Empty_file_content
=== RUN   TestCtestLoadConfigFromFile/case_5:_Invalid_YAML_format
    ctest_configfile_test.go:115: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-scheduler/app/options/ctest_configfile_test.go:115
        	Error:      	Error "Object 'Kind' is missing in ':::'" does not contain "'apiVersion' is missing"
        	Test:       	TestCtestLoadConfigFromFile/case_5:_Invalid_YAML_format
--- FAIL: TestCtestLoadConfigFromFile (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_0:_Empty_scheduler_config_file_path (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_1:_Correct_scheduler_config (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_2:_Scheduler_config_with_decode_error (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_3:_Scheduler_config_version_too_old (0.00s)
    --- FAIL: TestCtestLoadConfigFromFile/case_4:_Empty_file_content (0.00s)
    --- FAIL: TestCtestLoadConfigFromFile/case_5:_Invalid_YAML_format (0.00s)
FAIL
coverage: 0.7% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kube-scheduler/app/options	3.441s
	k8s.io/kubernetes/cmd/kube-scheduler/app/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app		coverage: 0.0% of statements
?   	k8s.io/kubernetes/cmd/kubeadm/app/apis/bootstraptoken	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/bootstraptoken/v1	2.223s	coverage: 0.2% of statements in ./... [no tests to run]
=== RUN   TestCtestAPIEndpointFromString
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:1234
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.4:1234" into host and port: host="1.2.3.4", port="1234"
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:-1
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.4:-1" into host and port: host="1.2.3.4", port="-1"
=== RUN   TestCtestAPIEndpointFromString/1.2.3.::1234
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.::1234" into host and port: host="", port=""
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:65536
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.4:65536" into host and port: host="1.2.3.4", port="65536"
=== RUN   TestCtestAPIEndpointFromString/1.2.3.456:1234
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.456:1234" into host and port: host="1.2.3.456", port="1234"
=== RUN   TestCtestAPIEndpointFromString/[::1]:1234
2026/02/16 15:37:17 Parsing API endpoint "[::1]:1234" into host and port: host="::1", port="1234"
=== RUN   TestCtestAPIEndpointFromString/[::1]:-1
2026/02/16 15:37:17 Parsing API endpoint "[::1]:-1" into host and port: host="::1", port="-1"
=== RUN   TestCtestAPIEndpointFromString/[::1]:65536
2026/02/16 15:37:17 Parsing API endpoint "[::1]:65536" into host and port: host="::1", port="65536"
=== RUN   TestCtestAPIEndpointFromString/[::1:1234
2026/02/16 15:37:17 Parsing API endpoint "[::1:1234" into host and port: host="", port=""
=== RUN   TestCtestAPIEndpointFromString/[::g]:1234
2026/02/16 15:37:17 Parsing API endpoint "[::g]:1234" into host and port: host="::g", port="1234"
=== RUN   TestCtestAPIEndpointFromString/#00
2026/02/16 15:37:17 Parsing API endpoint "" into host and port: host="", port=""
=== RUN   TestCtestAPIEndpointFromString/localhost
2026/02/16 15:37:17 Parsing API endpoint "localhost" into host and port: host="", port=""
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.4" into host and port: host="", port=""
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.4:" into host and port: host="1.2.3.4", port=""
    ctest_apiendpoint_test.go:39: expected error true, got false, error: <nil>
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:0
2026/02/16 15:37:17 Parsing API endpoint "1.2.3.4:0" into host and port: host="1.2.3.4", port="0"
=== RUN   TestCtestAPIEndpointFromString/[::1]
2026/02/16 15:37:17 Parsing API endpoint "[::1]" into host and port: host="", port=""
=== RUN   TestCtestAPIEndpointFromString/[::1]:
2026/02/16 15:37:17 Parsing API endpoint "[::1]:" into host and port: host="::1", port=""
    ctest_apiendpoint_test.go:39: expected error true, got false, error: <nil>
=== RUN   TestCtestAPIEndpointFromString/[::1]:0
2026/02/16 15:37:17 Parsing API endpoint "[::1]:0" into host and port: host="::1", port="0"
--- FAIL: TestCtestAPIEndpointFromString (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:-1 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.::1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:65536 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.456:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:-1 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:65536 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::g]:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/#00 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/localhost (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4 (0.00s)
    --- FAIL: TestCtestAPIEndpointFromString/1.2.3.4: (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:0 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1] (0.00s)
    --- FAIL: TestCtestAPIEndpointFromString/[::1]: (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:0 (0.00s)
=== RUN   TestCtestString
=== RUN   TestCtestString/ipv4_and_port
=== RUN   TestCtestString/ipv6_and_port
=== RUN   TestCtestString/ipv4_zero_port
=== RUN   TestCtestString/ipv6_zero_port
=== RUN   TestCtestString/empty_address
--- PASS: TestCtestString (0.00s)
    --- PASS: TestCtestString/ipv4_and_port (0.00s)
    --- PASS: TestCtestString/ipv6_and_port (0.00s)
    --- PASS: TestCtestString/ipv4_zero_port (0.00s)
    --- PASS: TestCtestString/ipv6_zero_port (0.00s)
    --- PASS: TestCtestString/empty_address (0.00s)
=== RUN   TestCtestGetArgValue

==================== CTEST START ====================
Running 0 th test case: argument exists with non-empty value
=== RUN   TestCtestGetArgValue/argument_exists_with_non-empty_value
Running 1 th test case: argument exists with non-empty value (offset index)
=== RUN   TestCtestGetArgValue/argument_exists_with_non-empty_value_(offset_index)
Running 2 th test case: argument exists with empty value
=== RUN   TestCtestGetArgValue/argument_exists_with_empty_value
Running 3 th test case: argument does not exists
=== RUN   TestCtestGetArgValue/argument_does_not_exists
Running 4 th test case: empty args slice
=== RUN   TestCtestGetArgValue/empty_args_slice
Running 5 th test case: startIdx beyond slice length
=== RUN   TestCtestGetArgValue/startIdx_beyond_slice_length
    ctest_argument_test.go:94: expected index: -1, got: 0
    ctest_argument_test.go:97: expected value: , got: a1
Running 6 th test case: name empty string with match
=== RUN   TestCtestGetArgValue/name_empty_string_with_match
Running 7 th test case: multiple matching names, startIdx selects second
=== RUN   TestCtestGetArgValue/multiple_matching_names,_startIdx_selects_second

==================== CTEST END ======================
--- FAIL: TestCtestGetArgValue (0.00s)
    --- PASS: TestCtestGetArgValue/argument_exists_with_non-empty_value (0.00s)
    --- PASS: TestCtestGetArgValue/argument_exists_with_non-empty_value_(offset_index) (0.00s)
    --- PASS: TestCtestGetArgValue/argument_exists_with_empty_value (0.00s)
    --- PASS: TestCtestGetArgValue/argument_does_not_exists (0.00s)
    --- PASS: TestCtestGetArgValue/empty_args_slice (0.00s)
    --- FAIL: TestCtestGetArgValue/startIdx_beyond_slice_length (0.00s)
    --- PASS: TestCtestGetArgValue/name_empty_string_with_match (0.00s)
    --- PASS: TestCtestGetArgValue/multiple_matching_names,_startIdx_selects_second (0.00s)
=== RUN   TestCtestSetArgValues

==================== CTEST START ====================
Running 0 th test case: update 1 argument
=== RUN   TestCtestSetArgValues/update_1_argument
Running 1 th test case: update all arguments
=== RUN   TestCtestSetArgValues/update_all_arguments
Running 2 th test case: add new argument
=== RUN   TestCtestSetArgValues/add_new_argument
Running 3 th test case: no matching arguments, nArgs=0 (no change)
=== RUN   TestCtestSetArgValues/no_matching_arguments,_nArgs=0_(no_change)
    ctest_argument_test.go:179: expected args: []kubeadm.Arg{kubeadm.Arg{Name:"a", Value:"1"}, kubeadm.Arg{Name:"b", Value:"2"}}, got: []kubeadm.Arg{kubeadm.Arg{Name:"a", Value:"1"}, kubeadm.Arg{Name:"b", Value:"2"}, kubeadm.Arg{Name:"c", Value:"new"}}
Running 4 th test case: empty args slice, add new argument
=== RUN   TestCtestSetArgValues/empty_args_slice,_add_new_argument
Running 5 th test case: negative nArgs other than -1 (treated as all)
=== RUN   TestCtestSetArgValues/negative_nArgs_other_than_-1_(treated_as_all)
Running 6 th test case: multiple matching names, nArgs=2 updates first two
=== RUN   TestCtestSetArgValues/multiple_matching_names,_nArgs=2_updates_first_two
    ctest_argument_test.go:179: expected args: []kubeadm.Arg{kubeadm.Arg{Name:"dup", Value:"updated"}, kubeadm.Arg{Name:"dup", Value:"updated"}, kubeadm.Arg{Name:"dup", Value:"third"}}, got: []kubeadm.Arg{kubeadm.Arg{Name:"dup", Value:"first"}, kubeadm.Arg{Name:"dup", Value:"updated"}, kubeadm.Arg{Name:"dup", Value:"updated"}}

==================== CTEST END ======================
--- FAIL: TestCtestSetArgValues (0.00s)
    --- PASS: TestCtestSetArgValues/update_1_argument (0.00s)
    --- PASS: TestCtestSetArgValues/update_all_arguments (0.00s)
    --- PASS: TestCtestSetArgValues/add_new_argument (0.00s)
    --- FAIL: TestCtestSetArgValues/no_matching_arguments,_nArgs=0_(no_change) (0.00s)
    --- PASS: TestCtestSetArgValues/empty_args_slice,_add_new_argument (0.00s)
    --- PASS: TestCtestSetArgValues/negative_nArgs_other_than_-1_(treated_as_all) (0.00s)
    --- FAIL: TestCtestSetArgValues/multiple_matching_names,_nArgs=2_updates_first_two (0.00s)
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType

==================== CTEST START ====================
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_true,_return_ECDSA-P256
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_false,_return_the_default_RSA-2048
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_not_set,_return_the_field_value
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_and_field_are_not_set,_return_empty_string
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_empty,_expect_ECDSA-P256
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_invalid,_expect_ECDSA-P256
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_empty,_expect_default_RSA-2048
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_invalid,_expect_invalid_value
    ctest_types_test.go:108: expected result: unsupported-algo, got: RSA-2048
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_set,_expect_field_value
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_empty,_expect_empty_string

==================== CTEST END ======================
--- FAIL: TestCtestClusterConfigurationEncryptionAlgorithmType (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_true,_return_ECDSA-P256 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_false,_return_the_default_RSA-2048 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_not_set,_return_the_field_value (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_and_field_are_not_set,_return_empty_string (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_empty,_expect_ECDSA-P256 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_invalid,_expect_ECDSA-P256 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_empty,_expect_default_RSA-2048 (0.00s)
    --- FAIL: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_invalid,_expect_invalid_value (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_set,_expect_field_value (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_empty,_expect_empty_string (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm	2.971s
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/fuzzer	1.957s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme		coverage: 0.0% of statements
=== RUN   TestCtestConvertToArgs
[DEBUG-CTEST 2026-02-16 15:37:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:14]: Start TestCtestConvertToArgs
[DEBUG-CTEST 2026-02-16 15:37:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:40]: Number of test cases: 3
Running 0 th test case.
{nil map returns nil args map[] []}
=== RUN   TestCtestConvertToArgs/nil_map_returns_nil_args
Running 1 th test case.
{empty map returns nil args map[] []}
=== RUN   TestCtestConvertToArgs/empty_map_returns_nil_args
    ctest_conversion_test.go:47: expected args: []
        	 got: []
        	
Running 2 th test case.
{valid args are parsed (sorted) map[a:b c:d] [{a b} {c d}]}
=== RUN   TestCtestConvertToArgs/valid_args_are_parsed_(sorted)
--- FAIL: TestCtestConvertToArgs (0.00s)
    --- PASS: TestCtestConvertToArgs/nil_map_returns_nil_args (0.00s)
    --- FAIL: TestCtestConvertToArgs/empty_map_returns_nil_args (0.00s)
    --- PASS: TestCtestConvertToArgs/valid_args_are_parsed_(sorted) (0.00s)
=== RUN   TestCtestConvertFromArgs
[DEBUG-CTEST 2026-02-16 15:37:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:54]: Start TestCtestConvertFromArgs
[DEBUG-CTEST 2026-02-16 15:37:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:88]: Number of test cases: 4
Running 0 th test case.
{nil args return nil map [] map[]}
=== RUN   TestCtestConvertFromArgs/nil_args_return_nil_map
Running 1 th test case.
{empty args return nil map [] map[]}
=== RUN   TestCtestConvertFromArgs/empty_args_return_nil_map
    ctest_conversion_test.go:95: expected args: map[]
        	 got: map[]
        	
Running 2 th test case.
{valid args are parsed [{a b} {c d}] map[a:b c:d]}
=== RUN   TestCtestConvertFromArgs/valid_args_are_parsed
Running 3 th test case.
{duplicates are dropped [{a b} {c d1} {c d2}] map[a:b c:d2]}
=== RUN   TestCtestConvertFromArgs/duplicates_are_dropped
--- FAIL: TestCtestConvertFromArgs (0.00s)
    --- PASS: TestCtestConvertFromArgs/nil_args_return_nil_map (0.00s)
    --- FAIL: TestCtestConvertFromArgs/empty_args_return_nil_map (0.00s)
    --- PASS: TestCtestConvertFromArgs/valid_args_are_parsed (0.00s)
    --- PASS: TestCtestConvertFromArgs/duplicates_are_dropped (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta3	3.517s
	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/validation	3.071s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/apis/output		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/output/fuzzer	2.542s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/apis/output/scheme		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app/apis/output/v1alpha3		coverage: 0.0% of statements
=== RUN   TestCtestNewCmdCompletion
--- PASS: TestCtestNewCmdCompletion (0.00s)
=== RUN   TestCtestRunCompletion
=== RUN   TestCtestRunCompletion/invalid:_missing_argument
=== RUN   TestCtestRunCompletion/invalid:_too_many_arguments
=== RUN   TestCtestRunCompletion/invalid:_unsupported_shell_name
=== RUN   TestCtestRunCompletion/invalid:_empty_string_argument
=== RUN   TestCtestRunCompletion/invalid:_excessively_long_shell_name
=== RUN   TestCtestRunCompletion/invalid:_special_characters_in_shell_name
=== RUN   TestCtestRunCompletion/valid:_test_shell_bash
=== RUN   TestCtestRunCompletion/valid:_test_shell_zsh
--- PASS: TestCtestRunCompletion (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_missing_argument (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_too_many_arguments (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_unsupported_shell_name (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_empty_string_argument (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_excessively_long_shell_name (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_special_characters_in_shell_name (0.00s)
    --- PASS: TestCtestRunCompletion/valid:_test_shell_bash (0.00s)
    --- PASS: TestCtestRunCompletion/valid:_test_shell_zsh (0.00s)
=== RUN   TestCtestNewJoinData

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "cfg" in requested fixtures
2026/02/16 15:37:28 [DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:37:28 Mode: 1
2026/02/16 15:37:28 Base JSON size: 783 bytes
2026/02/16 15:37:28 Number of external values: 0
2026/02/16 15:37:28 [DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:37:28 [DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"/etc/kubernetes/pki/ca.crt","ControlPlane":{"CertificateKey":"c39a18bae4a72e71b178661f437363da218a3efb83ddb03f1cd91d9ae1da41bd","LocalAPIEndpoint":{"AdvertiseAddress":"","BindPort":0}},"Discovery":{"BootstrapToken":{"APIServerEndpoint":"1.2.3.4:6443","CACertHashes":null,"Token":"abcdef.0123456789abcdef","UnsafeSkipCAVerification":true},"File":null,"TLSBootstrapToken":"abcdef.0123456789abcdef","Timeout":"5m0s"},"DryRun":false,"NodeRegistration":{"CRISocket":"unix://var/run/containerd/containerd.sock","IgnorePreflightErrors":["c","d"],"ImagePullSerial":true,"KubeletExtraArgs":null,"Name":"somename","Taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"}],"imagePullPolicy":"IfNotPresent"},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_join_test.go:79]: New Json Test Configs: 

==================== CTEST END ======================
=== RUN   TestCtestNewJoinData/fails_if_no_discovery_method_set
=== RUN   TestCtestNewJoinData/fails_if_both_file_and_bootstrap_discovery_methods_set
=== RUN   TestCtestNewJoinData/pass_if_file_discovery_is_set
=== RUN   TestCtestNewJoinData/pass_if_bootstrap_discovery_is_set
  W0216 15:37:28.058926   88243 join.go:382] [preflight] WARNING: More than one API server endpoint supplied on command line [1.2.3.4:6443 5.6.7.8:6443]. Using the first one.
=== RUN   TestCtestNewJoinData/--token_sets_TLSBootstrapToken_and_BootstrapToken.Token_if_unset
=== RUN   TestCtestNewJoinData/--token_doesn't_override_TLSBootstrapToken_and_BootstrapToken.Token_if_set
=== RUN   TestCtestNewJoinData/control_plane_setting_are_preserved_if_--control-plane_flag_is_set
=== RUN   TestCtestNewJoinData/control_plane_setting_are_cleaned_up_if_--control-plane_flag_is_not_set
  W0216 15:37:28.059663   88243 join.go:402] [preflight] WARNING: --control-plane is also required when passing control-plane related flags such as [certificate-key, apiserver-advertise-address, apiserver-bind-port]
    ctest_join_test.go:308: expected warning "WARNING: --control-plane is also required when passing control-plane", got ""
=== RUN   TestCtestNewJoinData/fails_if_invalid_preflight_checks_are_provided
=== RUN   TestCtestNewJoinData/Pass_with_config_from_file
=== RUN   TestCtestNewJoinData/--node-name_flags_override_config_from_file
=== RUN   TestCtestNewJoinData/fail_if_mixedArguments_are_passed
=== RUN   TestCtestNewJoinData/pre-flights_errors_from_CLI_args_only
=== RUN   TestCtestNewJoinData/pre-flights_errors_from_JoinConfiguration_only
=== RUN   TestCtestNewJoinData/pre-flights_errors_from_both_CLI_args_and_JoinConfiguration
=== RUN   TestCtestNewJoinData/warn_if_--control-plane_flag_is_not_set
  W0216 15:37:28.061746   88243 join.go:402] [preflight] WARNING: --control-plane is also required when passing control-plane related flags such as [certificate-key, apiserver-advertise-address, apiserver-bind-port]
    ctest_join_test.go:308: expected warning "WARNING: --control-plane is also required when passing control-plane", got ""
=== RUN   TestCtestNewJoinData/no_warn_if_--control-plane_flag_is_set
=== RUN   TestCtestNewJoinData/fails_if_config_file_is_invalid_yaml
--- FAIL: TestCtestNewJoinData (0.01s)
    --- PASS: TestCtestNewJoinData/fails_if_no_discovery_method_set (0.00s)
    --- PASS: TestCtestNewJoinData/fails_if_both_file_and_bootstrap_discovery_methods_set (0.00s)
    --- PASS: TestCtestNewJoinData/pass_if_file_discovery_is_set (0.00s)
    --- PASS: TestCtestNewJoinData/pass_if_bootstrap_discovery_is_set (0.00s)
    --- PASS: TestCtestNewJoinData/--token_sets_TLSBootstrapToken_and_BootstrapToken.Token_if_unset (0.00s)
    --- PASS: TestCtestNewJoinData/--token_doesn't_override_TLSBootstrapToken_and_BootstrapToken.Token_if_set (0.00s)
    --- PASS: TestCtestNewJoinData/control_plane_setting_are_preserved_if_--control-plane_flag_is_set (0.00s)
    --- FAIL: TestCtestNewJoinData/control_plane_setting_are_cleaned_up_if_--control-plane_flag_is_not_set (0.00s)
    --- PASS: TestCtestNewJoinData/fails_if_invalid_preflight_checks_are_provided (0.00s)
    --- PASS: TestCtestNewJoinData/Pass_with_config_from_file (0.00s)
    --- PASS: TestCtestNewJoinData/--node-name_flags_override_config_from_file (0.00s)
    --- PASS: TestCtestNewJoinData/fail_if_mixedArguments_are_passed (0.00s)
    --- PASS: TestCtestNewJoinData/pre-flights_errors_from_CLI_args_only (0.00s)
    --- PASS: TestCtestNewJoinData/pre-flights_errors_from_JoinConfiguration_only (0.00s)
    --- PASS: TestCtestNewJoinData/pre-flights_errors_from_both_CLI_args_and_JoinConfiguration (0.00s)
    --- FAIL: TestCtestNewJoinData/warn_if_--control-plane_flag_is_not_set (0.00s)
    --- PASS: TestCtestNewJoinData/no_warn_if_--control-plane_flag_is_set (0.00s)
    --- PASS: TestCtestNewJoinData/fails_if_config_file_is_invalid_yaml (0.00s)
=== RUN   TestCtestKubeConfigSubCommandsThatWritesToOut

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_kubeconfig_test.go:106]: get default configs: {test_fixture.json [default kubeadm config spec] kubeadmConfigSpec [pods] {1.2.3.4 1234}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "kubeadmConfigSpec" in requested fixtures
2026/02/16 15:37:28 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:28 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"AdvertiseAddress":"1.2.3.4","BindPort":1234})[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_kubeconfig_test.go:115]: Skipping test execution. No new configurations generated. 

==================== CTEST END ======================
--- PASS: TestCtestKubeConfigSubCommandsThatWritesToOut (0.00s)
=== RUN   TestCtestNewCmdVersion
--- PASS: TestCtestNewCmdVersion (0.00s)
=== RUN   TestCtestRunVersion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:99]: Running TestCtestRunVersion with 10 cases
Running 0 th test case: valid: run without flags
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run without flags  false false false}
=== RUN   TestCtestRunVersion/valid:_run_without_flags
Running 1 th test case: valid: run with flag 'short'
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run with flag 'short' short false false false}
=== RUN   TestCtestRunVersion/valid:_run_with_flag_'short'
Running 2 th test case: valid: run with flag 'yaml'
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run with flag 'yaml' yaml false true false}
=== RUN   TestCtestRunVersion/valid:_run_with_flag_'yaml'
Running 3 th test case: valid: run with flag 'json'
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run with flag 'json' json false false true}
=== RUN   TestCtestRunVersion/valid:_run_with_flag_'json'
Running 4 th test case: invalid: run with unsupported flag
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: run with unsupported flag unsupported-flag true false false}
=== RUN   TestCtestRunVersion/invalid:_run_with_unsupported_flag
Running 5 th test case: invalid: empty flag string (explicit)
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: empty flag string (explicit)  false false false}
=== RUN   TestCtestRunVersion/invalid:_empty_flag_string_(explicit)
    ctest_version_test.go:125: Test case "invalid: empty flag string (explicit)": RunVersion expected error: false, saw: true; invalid output format: unsupported-flag
Running 6 th test case: invalid: whitespace flag
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: whitespace flag   true false false}
=== RUN   TestCtestRunVersion/invalid:_whitespace_flag
Running 7 th test case: invalid: excessively long flag
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: excessively long flag                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  true false false}
=== RUN   TestCtestRunVersion/invalid:_excessively_long_flag
Running 8 th test case: invalid: numeric flag
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: numeric flag 12345 true false false}
=== RUN   TestCtestRunVersion/invalid:_numeric_flag
Running 9 th test case: invalid: flag with newline
[DEBUG-CTEST 2026-02-16 15:37:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: flag with newline json
 true false false}
=== RUN   TestCtestRunVersion/invalid:_flag_with_newline

==================== CTEST END ======================
--- FAIL: TestCtestRunVersion (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_without_flags (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_with_flag_'short' (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_with_flag_'yaml' (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_with_flag_'json' (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_run_with_unsupported_flag (0.00s)
    --- FAIL: TestCtestRunVersion/invalid:_empty_flag_string_(explicit) (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_whitespace_flag (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_excessively_long_flag (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_numeric_flag (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_flag_with_newline (0.00s)
FAIL
coverage: 1.1% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd	4.091s
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/alpha		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/options	3.140s	coverage: 0.3% of statements in ./... [no tests to run]
=== RUN   TestCtestSetKubernetesVersion

==================== CTEST START ====================
Running test case 0: empty version is processed
=== RUN   TestCtestSetKubernetesVersion/empty_version_is_processed
Running test case 1: default version is processed
=== RUN   TestCtestSetKubernetesVersion/default_version_is_processed
Running test case 2: any other version is skipped
=== RUN   TestCtestSetKubernetesVersion/any_other_version_is_skipped
Running test case 3: whitespace version is treated as non-empty
=== RUN   TestCtestSetKubernetesVersion/whitespace_version_is_treated_as_non-empty
Running test case 4: malformed version string is skipped
=== RUN   TestCtestSetKubernetesVersion/malformed_version_string_is_skipped
Running test case 5: very long version string is skipped
=== RUN   TestCtestSetKubernetesVersion/very_long_version_string_is_skipped
Running test case 6: pre-release version is skipped
=== RUN   TestCtestSetKubernetesVersion/pre-release_version_is_skipped
Running test case 7: zero version is processed as empty after trim
=== RUN   TestCtestSetKubernetesVersion/zero_version_is_processed_as_empty_after_trim

==================== CTEST END ======================
--- PASS: TestCtestSetKubernetesVersion (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/empty_version_is_processed (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/default_version_is_processed (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/any_other_version_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/whitespace_version_is_treated_as_non-empty (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/malformed_version_string_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/very_long_version_string_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/pre-release_version_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/zero_version_is_processed_as_empty_after_trim (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases	2.148s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestGetAddonPhaseFlags

==================== CTEST START ====================
Running 0 th test case: all
=== RUN   TestCtestGetAddonPhaseFlags/all
Running 1 th test case: kube-proxy
=== RUN   TestCtestGetAddonPhaseFlags/kube-proxy
Running 2 th test case: coredns
=== RUN   TestCtestGetAddonPhaseFlags/coredns
Running 3 th test case: invalid_name
=== RUN   TestCtestGetAddonPhaseFlags/invalid_name
Running 4 th test case: 
=== RUN   TestCtestGetAddonPhaseFlags/#00
Running 5 th test case:    
=== RUN   TestCtestGetAddonPhaseFlags/___
Running 6 th test case: unknown-addon
=== RUN   TestCtestGetAddonPhaseFlags/unknown-addon
Running 7 th test case: extremely-long-addon-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
=== RUN   TestCtestGetAddonPhaseFlags/extremely-long-addon-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

==================== CTEST END ======================
--- PASS: TestCtestGetAddonPhaseFlags (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/all (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/kube-proxy (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/coredns (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/invalid_name (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/#00 (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/___ (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/unknown-addon (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/extremely-long-addon-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init	5.518s	coverage: 0.8% of statements in ./...
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join	4.630s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestGetEtcdDataDir

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:37:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/phases/reset/ctest_removeetcdmember_test.go:119]: Prepared pod yaml configurations for test cases
[DEBUG-CTEST 2026-02-16 15:37:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/phases/reset/ctest_removeetcdmember_test.go:120]: Number of test cases: 7
=== RUN   TestCtestGetEtcdDataDir/empty_pod_yaml
  W0216 15:37:34.622991   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/no_manifest_written
  W0216 15:37:34.624523   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
    ctest_removeetcdmember_test.go:145: getEtcdDataDir failed
        no manifest written
        expected error: true
        	got: false
        error: <nil>
=== RUN   TestCtestGetEtcdDataDir/non-existent_file_returns_default_data_dir
  W0216 15:37:34.624793   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/return_etcd_data_dir
  W0216 15:37:34.625397   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
    ctest_removeetcdmember_test.go:145: getEtcdDataDir failed
        return etcd data dir
        expected error: false
        	got: true
        error: failed to unmarshal manifest for "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGetEtcdDataDirreturn_etcd_data_dir2698778966/001/etcd.yaml": failed to decode metadata: {}
        spec:
          containers: null
          volumes:
          - hostPath:
              path: /path/to/etcd
              type: DirectoryOrCreate
            name: etcd-data
          - hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
            name: etcd-certs
        status: {}
         into runtime.Object: Object 'Kind' is missing in 'metadata: {}
        spec:
          containers: null
          volumes:
          - hostPath:
              path: /path/to/etcd
              type: DirectoryOrCreate
            name: etcd-data
          - hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
            name: etcd-certs
        status: {}
        '
=== RUN   TestCtestGetEtcdDataDir/invalid_etcd_pod
  W0216 15:37:34.626604   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/etcd_pod_spec_without_data_volume
  W0216 15:37:34.626911   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/kubeconfig_file_doesn't_exist
  W0216 15:37:34.627210   88314 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
    ctest_removeetcdmember_test.go:145: getEtcdDataDir failed
        kubeconfig file doesn't exist
        expected error: false
        	got: true
        error: failed to unmarshal manifest for "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGetEtcdDataDirkubeconfig_file_doesnt_exist1617087373/001/etcd.yaml": failed to decode  into runtime.Object: Object 'Kind' is missing in ''

==================== CTEST END ======================
--- FAIL: TestCtestGetEtcdDataDir (0.01s)
    --- PASS: TestCtestGetEtcdDataDir/empty_pod_yaml (0.00s)
    --- FAIL: TestCtestGetEtcdDataDir/no_manifest_written (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/non-existent_file_returns_default_data_dir (0.00s)
    --- FAIL: TestCtestGetEtcdDataDir/return_etcd_data_dir (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/invalid_etcd_pod (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/etcd_pod_spec_without_data_volume (0.00s)
    --- FAIL: TestCtestGetEtcdDataDir/kubeconfig_file_doesn't_exist (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/reset	4.059s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/upgrade	3.967s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/upgrade/apply	3.769s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/upgrade/node	4.079s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestComputePhaseRunFlags
=== RUN   TestCtestComputePhaseRunFlags/no_options_>_all_phases
=== RUN   TestCtestComputePhaseRunFlags/options_can_filter_phases
=== RUN   TestCtestComputePhaseRunFlags/options_can_filter_phases_-_hierarchy_is_considered
=== RUN   TestCtestComputePhaseRunFlags/options_can_skip_phases
=== RUN   TestCtestComputePhaseRunFlags/options_can_skip_phases_-_hierarchy_is_considered
=== RUN   TestCtestComputePhaseRunFlags/skip_options_have_higher_precedence_than_filter_options
=== RUN   TestCtestComputePhaseRunFlags/invalid_filter_option
=== RUN   TestCtestComputePhaseRunFlags/invalid_skip_option
=== RUN   TestCtestComputePhaseRunFlags/empty_filter_and_skip_slices_should_behave_like_no_options
=== RUN   TestCtestComputePhaseRunFlags/duplicate_filter_entries_are_ignored
=== RUN   TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_filter_entries_should_error
=== RUN   TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_skip_entries_should_error
--- PASS: TestCtestComputePhaseRunFlags (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/no_options_>_all_phases (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_filter_phases (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_filter_phases_-_hierarchy_is_considered (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_skip_phases (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_skip_phases_-_hierarchy_is_considered (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/skip_options_have_higher_precedence_than_filter_options (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/invalid_filter_option (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/invalid_skip_option (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/empty_filter_and_skip_slices_should_behave_like_no_options (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/duplicate_filter_entries_are_ignored (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_filter_entries_should_error (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_skip_entries_should_error (0.00s)
=== RUN   TestCtestRunOrderAndConditions
=== RUN   TestCtestRunOrderAndConditions/Run_respect_runCondition
=== RUN   TestCtestRunOrderAndConditions/Run_takes_options_into_account
=== RUN   TestCtestRunOrderAndConditions/explicit_empty_filters_and_skips_behave_as_default
--- PASS: TestCtestRunOrderAndConditions (0.00s)
    --- PASS: TestCtestRunOrderAndConditions/Run_respect_runCondition (0.00s)
    --- PASS: TestCtestRunOrderAndConditions/Run_takes_options_into_account (0.00s)
    --- PASS: TestCtestRunOrderAndConditions/explicit_empty_filters_and_skips_behave_as_default (0.00s)
=== RUN   TestCtestRunHandleErrors
=== RUN   TestCtestRunHandleErrors/no_errors
=== RUN   TestCtestRunHandleErrors/run_fails
=== RUN   TestCtestRunHandleErrors/run_condition_fails
=== RUN   TestCtestRunHandleErrors/empty_filters_should_succeed_(no_errors)
    ctest_runner_test.go:199: Unexpected error: error execution phase bar: run fails
--- FAIL: TestCtestRunHandleErrors (0.00s)
    --- PASS: TestCtestRunHandleErrors/no_errors (0.00s)
    --- PASS: TestCtestRunHandleErrors/run_fails (0.00s)
    --- PASS: TestCtestRunHandleErrors/run_condition_fails (0.00s)
    --- FAIL: TestCtestRunHandleErrors/empty_filters_should_succeed_(no_errors) (0.00s)
=== RUN   TestCtestBindToCommandArgRequirements
=== RUN   TestCtestBindToCommandArgRequirements/leaf_command,_no_defined_args,_follow_parent
=== RUN   TestCtestBindToCommandArgRequirements/container_cmd_expect_none,_custom_arg_check_for_leaf
    ctest_runner_test.go:283: cmd didn't have phase foo baz empty subcommand
--- FAIL: TestCtestBindToCommandArgRequirements (0.00s)
    --- PASS: TestCtestBindToCommandArgRequirements/leaf_command,_no_defined_args,_follow_parent (0.00s)
    --- FAIL: TestCtestBindToCommandArgRequirements/container_cmd_expect_none,_custom_arg_check_for_leaf (0.00s)
FAIL
coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow	3.740s
=== RUN   TestCtestNewApplyData
=== RUN   TestCtestNewApplyData/fails_if_no_upgrade_version_set
=== RUN   TestCtestNewApplyData/fails_if_invalid_preflight_checks_are_provided
=== RUN   TestCtestNewApplyData/fails_if_kubeconfig_file_doesn't_exists
=== RUN   TestCtestNewApplyData/fails_if_unknown_flag_is_provided
    ctest_apply_test.go:138: newApplyData returned unexpected error, expected: unknown flag, got couldn't create a Kubernetes client from file "/etc/kubernetes/admin.conf": failed to load admin kubeconfig: open /etc/kubernetes/admin.conf: no such file or directory
=== RUN   TestCtestNewApplyData/fails_if_config_file_is_empty
    ctest_apply_test.go:138: newApplyData returned unexpected error, expected: failed to read config file, got no UpgradeConfiguration found in the supplied config
--- FAIL: TestCtestNewApplyData (0.00s)
    --- PASS: TestCtestNewApplyData/fails_if_no_upgrade_version_set (0.00s)
    --- PASS: TestCtestNewApplyData/fails_if_invalid_preflight_checks_are_provided (0.00s)
    --- PASS: TestCtestNewApplyData/fails_if_kubeconfig_file_doesn't_exists (0.00s)
    --- FAIL: TestCtestNewApplyData/fails_if_unknown_flag_is_provided (0.00s)
    --- FAIL: TestCtestNewApplyData/fails_if_config_file_is_empty (0.00s)
=== RUN   TestCtestRunDiff
=== RUN   TestCtestRunDiff/valid:_run_diff_with_empty_config_path_on_valid_manifest_path
=== RUN   TestCtestRunDiff/valid:_run_diff_on_valid_manifest_path
=== RUN   TestCtestRunDiff/invalid:_missing_config_file
=== RUN   TestCtestRunDiff/invalid:_valid_config_but_bad_manifest_path
=== RUN   TestCtestRunDiff/invalid:_badly_formatted_version_as_argument
=== RUN   TestCtestRunDiff/invalid:_empty_manifest_path_with_flag_set
=== RUN   TestCtestRunDiff/invalid:_extremely_long_manifest_path
=== RUN   TestCtestRunDiff/invalid:_nil_args_slice
--- PASS: TestCtestRunDiff (0.01s)
    --- PASS: TestCtestRunDiff/valid:_run_diff_with_empty_config_path_on_valid_manifest_path (0.01s)
    --- PASS: TestCtestRunDiff/valid:_run_diff_on_valid_manifest_path (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_missing_config_file (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_valid_config_but_bad_manifest_path (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_badly_formatted_version_as_argument (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_empty_manifest_path_with_flag_set (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_extremely_long_manifest_path (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_nil_args_slice (0.00s)
=== RUN   TestCtestValidateManifests
=== RUN   TestCtestValidateManifests/valid:_valid_manifest_path
=== RUN   TestCtestValidateManifests/invalid:_one_is_empty_path
=== RUN   TestCtestValidateManifests/invalid:_manifest_path_is_directory
=== RUN   TestCtestValidateManifests/invalid:_manifest_path_does_not_exist
=== RUN   TestCtestValidateManifests/invalid:_duplicate_manifest_paths
    ctest_diff_test.go:216: expected error: true, saw: false, error: <nil>
=== RUN   TestCtestValidateManifests/invalid:_path_with_spaces
=== RUN   TestCtestValidateManifests/invalid:_nil_args_slice
    ctest_diff_test.go:216: expected error: true, saw: false, error: <nil>
=== RUN   TestCtestValidateManifests/invalid:_relative_parent_path_traversal
=== RUN   TestCtestValidateManifests/invalid:_empty_args_slice
    ctest_diff_test.go:216: expected error: true, saw: false, error: <nil>
--- FAIL: TestCtestValidateManifests (0.00s)
    --- PASS: TestCtestValidateManifests/valid:_valid_manifest_path (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_one_is_empty_path (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_manifest_path_is_directory (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_manifest_path_does_not_exist (0.00s)
    --- FAIL: TestCtestValidateManifests/invalid:_duplicate_manifest_paths (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_path_with_spaces (0.00s)
    --- FAIL: TestCtestValidateManifests/invalid:_nil_args_slice (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_relative_parent_path_traversal (0.00s)
    --- FAIL: TestCtestValidateManifests/invalid:_empty_args_slice (0.00s)
=== RUN   TestCtestSortedSliceFromStringStringArrayMap
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/the_returned_slice_should_be_alphabetically_sorted_based_on_the_string_keys_in_the_map
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/the_int_value_should_not_affect_this_func
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/slice_with_4_keys_and_different_values
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/this_should_work_for_version_numbers_as_well;_and_the_lowest_version_should_come_first
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/empty_map_returns_empty_slice
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/nil_map_returns_empty_slice
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/map_with_empty_slice_values
--- PASS: TestCtestSortedSliceFromStringStringArrayMap (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/the_returned_slice_should_be_alphabetically_sorted_based_on_the_string_keys_in_the_map (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/the_int_value_should_not_affect_this_func (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/slice_with_4_keys_and_different_values (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/this_should_work_for_version_numbers_as_well;_and_the_lowest_version_should_come_first (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/empty_map_returns_empty_slice (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/nil_map_returns_empty_slice (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/map_with_empty_slice_values (0.00s)
FAIL
coverage: 1.0% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade	3.110s
=== RUN   TestCtestValidateExactArgNumber
=== RUN   TestCtestValidateExactArgNumber/one_arg_given_and_one_arg_expected
=== RUN   TestCtestValidateExactArgNumber/two_args_given_and_two_args_expected
=== RUN   TestCtestValidateExactArgNumber/too_few_supplied_args
=== RUN   TestCtestValidateExactArgNumber/too_few_non-empty_args
=== RUN   TestCtestValidateExactArgNumber/too_many_args
=== RUN   TestCtestValidateExactArgNumber/nil_args_slice
=== RUN   TestCtestValidateExactArgNumber/nil_supportedArgs_slice
=== RUN   TestCtestValidateExactArgNumber/args_with_whitespace
    ctest_cmdutil_test.go:78: failed ValidateExactArgNumber:
        	expected error: true
        	  actual error: false
=== RUN   TestCtestValidateExactArgNumber/supportedArgs_contains_empty_string
    ctest_cmdutil_test.go:78: failed ValidateExactArgNumber:
        	expected error: true
        	  actual error: false
--- FAIL: TestCtestValidateExactArgNumber (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/one_arg_given_and_one_arg_expected (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/two_args_given_and_two_args_expected (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/too_few_supplied_args (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/too_few_non-empty_args (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/too_many_args (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/nil_args_slice (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/nil_supportedArgs_slice (0.00s)
    --- FAIL: TestCtestValidateExactArgNumber/args_with_whitespace (0.00s)
    --- FAIL: TestCtestValidateExactArgNumber/supportedArgs_contains_empty_string (0.00s)
=== RUN   TestCtestGetKubeConfigPath
=== RUN   TestCtestGetKubeConfigPath/provide_an_empty_value
=== RUN   TestCtestGetKubeConfigPath/provide_a_non-empty_value
=== RUN   TestCtestGetKubeConfigPath/provide_whitespace_only
=== RUN   TestCtestGetKubeConfigPath/provide_path_with_tilde_(user_home_expansion_not_performed)
--- PASS: TestCtestGetKubeConfigPath (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_an_empty_value (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_a_non-empty_value (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_whitespace_only (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_path_with_tilde_(user_home_expansion_not_performed) (0.00s)
=== RUN   TestCtestValueFromFlagsOrConfig
=== RUN   TestCtestValueFromFlagsOrConfig/string:_config_is_overridden_by_the_flag
=== RUN   TestCtestValueFromFlagsOrConfig/bool:_config_is_overridden_by_the_flag
=== RUN   TestCtestValueFromFlagsOrConfig/nil_bool_is_converted_to_false
=== RUN   TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(string)
=== RUN   TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(bool_true)
=== RUN   TestCtestValueFromFlagsOrConfig/flag_provided_with_zero_value_for_int_(unsupported_type)
    ctest_cmdutil_test.go:202: failed ValueFromFlagsOrConfig:
        	expected: 42
        	  actual: 0
--- FAIL: TestCtestValueFromFlagsOrConfig (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/string:_config_is_overridden_by_the_flag (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/bool:_config_is_overridden_by_the_flag (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/nil_bool_is_converted_to_false (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(string) (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(bool_true) (0.00s)
    --- FAIL: TestCtestValueFromFlagsOrConfig/flag_provided_with_zero_value_for_int_(unsupported_type) (0.00s)
=== RUN   TestCtestGetJoinCommand
=== RUN   TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_token
=== RUN   TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_empty_token
=== RUN   TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_very_long_token
=== RUN   TestCtestGetJoinCommand/Error_to_load_kubeconfig
=== RUN   TestCtestGetJoinCommand/Error_to_get_default_cluster_config
=== RUN   TestCtestGetJoinCommand/Error_when_CA_certificate_is_invalid
=== RUN   TestCtestGetJoinCommand/Error_when_CA_certificate_file_path_is_invalid
=== RUN   TestCtestGetJoinCommand/Error_when_CA_certificate_is_missing
--- PASS: TestCtestGetJoinCommand (0.01s)
    --- PASS: TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_token (0.00s)
    --- PASS: TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_empty_token (0.00s)
    --- PASS: TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_very_long_token (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_to_load_kubeconfig (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_to_get_default_cluster_config (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_when_CA_certificate_is_invalid (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_when_CA_certificate_file_path_is_invalid (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_when_CA_certificate_is_missing (0.00s)
FAIL
coverage: 0.6% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/util	1.886s
=== RUN   TestCtestChecksumForConfigMap

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/componentconfigs/ctest_checksums_test.go:264]: matched config: {test_fixture.json [checksum case 1] data [configmaps] {{{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} <nil> map[foo:bar] map[bar:[98 97 122]]} sha256:c8f8b724728a6d6684106e5e64e94ce811c9965d19dd44dd073cf86cf43bc238}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:37:45 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs	1.451s
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/constants	2.416s	coverage: 0.2% of statements in ./... [no tests to run]
=== RUN   TestCtestFor

==================== CTEST START ====================
=== RUN   TestCtestFor/default_Discovery
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [default Discovery] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/file_Discovery_with_a_path
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [file Discovery with a path] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> 0x140004a91c0  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":{"KubeConfigPath":"notnil"},"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/file_Discovery_with_an_url
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [file Discovery with an url] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> 0x140004a9eb0  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":{"KubeConfigPath":"https://localhost"},"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/BootstrapTokenDiscovery
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [BootstrapTokenDiscovery] discovery [pods] {{ } false {  [] [] []  <nil>}  {0x140005e4280 <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":{"APIServerEndpoint":"","CACertHashes":null,"Token":"foo.bar@foobar","UnsafeSkipCAVerification":false},"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/file_Discovery_empty_path
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [file Discovery empty path] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> 0x14000337e30  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":{"KubeConfigPath":""},"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/BootstrapToken_empty_token
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [BootstrapToken empty token] discovery [pods] {{ } false {  [] [] []  <nil>}  {0x140005e4480 <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":{"APIServerEndpoint":"","CACertHashes":null,"Token":"","UnsafeSkipCAVerification":false},"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/BootstrapToken_nil_discovery
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [BootstrapToken nil discovery] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/16 15:37:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:37:45 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestFor (0.02s)
    --- PASS: TestCtestFor/default_Discovery (0.00s)
    --- PASS: TestCtestFor/file_Discovery_with_a_path (0.00s)
    --- PASS: TestCtestFor/file_Discovery_with_an_url (0.00s)
    --- PASS: TestCtestFor/BootstrapTokenDiscovery (0.00s)
    --- PASS: TestCtestFor/file_Discovery_empty_path (0.00s)
    --- PASS: TestCtestFor/BootstrapToken_empty_token (0.00s)
    --- PASS: TestCtestFor/BootstrapToken_nil_discovery (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/discovery	2.304s	coverage: 0.8% of statements in ./...
	k8s.io/kubernetes/cmd/kubeadm/app/discovery/file		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app/discovery/https		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/discovery/token	1.924s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestKnownFeatures
--- PASS: TestCtestKnownFeatures (0.00s)
=== RUN   TestCtestNewFeatureGate
=== RUN   TestCtestNewFeatureGate/invalidValue
=== RUN   TestCtestNewFeatureGate/feature1=true,invalidValue
=== RUN   TestCtestNewFeatureGate/feature1=notABoolean
=== RUN   TestCtestNewFeatureGate/feature1=true,feature2=notABoolean
=== RUN   TestCtestNewFeatureGate/unknownFeature=false
=== RUN   TestCtestNewFeatureGate/feature1=true,unknownFeature=false
=== RUN   TestCtestNewFeatureGate/deprecated=true
W0216 15:37:49.572431   88458 features.go:163] Setting deprecated feature gate deprecated=true. It will be removed in a future release.
=== RUN   TestCtestNewFeatureGate/feature1=true
=== RUN   TestCtestNewFeatureGate/feature1=true,feature2=false
=== RUN   TestCtestNewFeatureGate/#00
=== RUN   TestCtestNewFeatureGate/feature1=true,
    ctest_features_test.go:134: NewFeatureGate didn't fail when expected
=== RUN   TestCtestNewFeatureGate/feature1=true,feature1=false
=== RUN   TestCtestNewFeatureGate/feature1=
=== RUN   TestCtestNewFeatureGate/_feature1_=_true_
=== RUN   TestCtestNewFeatureGate/feature1=true,feature2=false,deprecated=true
W0216 15:37:49.572830   88458 features.go:163] Setting deprecated feature gate deprecated=true. It will be removed in a future release.
--- FAIL: TestCtestNewFeatureGate (0.00s)
    --- PASS: TestCtestNewFeatureGate/invalidValue (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,invalidValue (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=notABoolean (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature2=notABoolean (0.00s)
    --- PASS: TestCtestNewFeatureGate/unknownFeature=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,unknownFeature=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/deprecated=true (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature2=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/#00 (0.00s)
    --- FAIL: TestCtestNewFeatureGate/feature1=true, (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature1=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1= (0.00s)
    --- PASS: TestCtestNewFeatureGate/_feature1_=_true_ (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature2=false,deprecated=true (0.00s)
=== RUN   TestCtestValidateVersion
=== RUN   TestCtestValidateVersion/no_min_version
=== RUN   TestCtestValidateVersion/min_version_but_correct_value_given
=== RUN   TestCtestValidateVersion/min_version_and_incorrect_value_given
=== RUN   TestCtestValidateVersion/invalid_version_format
=== RUN   TestCtestValidateVersion/empty_version_string_(treated_as_no_version)
    ctest_features_test.go:209: ValidateVersion didn't fail when expected
=== RUN   TestCtestValidateVersion/feature_not_defined_in_FeatureList
    ctest_features_test.go:209: ValidateVersion didn't fail when expected
=== RUN   TestCtestValidateVersion/nil_feature_map
--- FAIL: TestCtestValidateVersion (0.00s)
    --- PASS: TestCtestValidateVersion/no_min_version (0.00s)
    --- PASS: TestCtestValidateVersion/min_version_but_correct_value_given (0.00s)
    --- PASS: TestCtestValidateVersion/min_version_and_incorrect_value_given (0.00s)
    --- PASS: TestCtestValidateVersion/invalid_version_format (0.00s)
    --- FAIL: TestCtestValidateVersion/empty_version_string_(treated_as_no_version) (0.00s)
    --- FAIL: TestCtestValidateVersion/feature_not_defined_in_FeatureList (0.00s)
    --- PASS: TestCtestValidateVersion/nil_feature_map (0.00s)
=== RUN   TestCtestEnabledDefaults
--- PASS: TestCtestEnabledDefaults (0.00s)
=== RUN   TestCtestCheckDeprecatedFlags
=== RUN   TestCtestCheckDeprecatedFlags/deprecated_feature
=== RUN   TestCtestCheckDeprecatedFlags/valid_feature
=== RUN   TestCtestCheckDeprecatedFlags/invalid_feature
=== RUN   TestCtestCheckDeprecatedFlags/nil_feature_map
=== RUN   TestCtestCheckDeprecatedFlags/empty_feature_map
--- PASS: TestCtestCheckDeprecatedFlags (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/deprecated_feature (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/valid_feature (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/invalid_feature (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/nil_feature_map (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/empty_feature_map (0.00s)
=== RUN   TestCtestSupports
=== RUN   TestCtestSupports/the_feature_is_not_supported
=== RUN   TestCtestSupports/the_feature_is_supported
=== RUN   TestCtestSupports/empty_feature_name
--- PASS: TestCtestSupports (0.00s)
    --- PASS: TestCtestSupports/the_feature_is_not_supported (0.00s)
    --- PASS: TestCtestSupports/the_feature_is_supported (0.00s)
    --- PASS: TestCtestSupports/empty_feature_name (0.00s)
FAIL
coverage: 0.1% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/features	1.484s
=== RUN   TestCtestGetKubernetesImage

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ClusterConfiguration" in requested fixtures
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:37:51 Mode: 1
2026/02/16 15:37:51 Base JSON size: 763 bytes
2026/02/16 15:37:51 Number of external values: 0
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServer":{"CertSANs":null,"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null,"TimeoutForControlPlane":null},"CACertificateValidityPeriod":null,"CIImageRepository":"","CIKubernetesVersion":"","CertificateValidityPeriod":null,"CertificatesDir":"","ClusterName":"","ComponentConfigs":null,"ControlPlaneEndpoint":"","ControllerManager":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null},"DNS":{"Disabled":false,"ImageRepository":"","ImageTag":""},"EncryptionAlgorithm":"","Etcd":{"External":null,"Local":null},"FeatureGates":null,"ImageRepository":"registry.k8s.io","KubernetesVersion":"v1.99.0","Networking":{"DNSDomain":"","PodSubnet":"","ServiceSubnet":""},"Proxy":{"Disabled":false},"Scheduler":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null}})[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:31]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:32]: Num of Test Cases: 0
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:35]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetKubernetesImage (0.00s)
=== RUN   TestCtestGetEtcdImage

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ClusterConfiguration" in requested fixtures
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:37:51 Mode: 1
2026/02/16 15:37:51 Base JSON size: 878 bytes
2026/02/16 15:37:51 Number of external values: 0
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServer":{"CertSANs":null,"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null,"TimeoutForControlPlane":null},"CACertificateValidityPeriod":null,"CIImageRepository":"","CIKubernetesVersion":"","CertificateValidityPeriod":null,"CertificatesDir":"","ClusterName":"","ComponentConfigs":null,"ControlPlaneEndpoint":"","ControllerManager":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null},"DNS":{"Disabled":false,"ImageRepository":"","ImageTag":""},"EncryptionAlgorithm":"","Etcd":{"External":null,"Local":{"DataDir":"","ExtraArgs":null,"ExtraEnvs":null,"ImageRepository":"","ImageTag":"","PeerCertSANs":null,"ServerCertSANs":null}},"FeatureGates":null,"ImageRepository":"real.repo","KubernetesVersion":"v1.99.0","Networking":{"DNSDomain":"","PodSubnet":"","ServiceSubnet":""},"Proxy":{"Disabled":false},"Scheduler":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null}})[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:72]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:73]: Num of Test Cases: 0
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:76]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetEtcdImage (0.00s)
=== RUN   TestCtestGetPauseImage

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ClusterConfiguration" in requested fixtures
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:37:51 Mode: 1
2026/02/16 15:37:51 Base JSON size: 750 bytes
2026/02/16 15:37:51 Number of external values: 0
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:37:51 [DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServer":{"CertSANs":null,"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null,"TimeoutForControlPlane":null},"CACertificateValidityPeriod":null,"CIImageRepository":"","CIKubernetesVersion":"","CertificateValidityPeriod":null,"CertificatesDir":"","ClusterName":"","ComponentConfigs":null,"ControlPlaneEndpoint":"","ControllerManager":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null},"DNS":{"Disabled":false,"ImageRepository":"","ImageTag":""},"EncryptionAlgorithm":"","Etcd":{"External":null,"Local":null},"FeatureGates":null,"ImageRepository":"test.repo","KubernetesVersion":"","Networking":{"DNSDomain":"","PodSubnet":"","ServiceSubnet":""},"Proxy":{"Disabled":false},"Scheduler":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null}})[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:115]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:116]: Num of Test Cases: 0
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:119]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetPauseImage (0.00s)
=== RUN   TestCtestGetAllImages

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[deployments statefulsets daemonsets replicasets pods]
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[deployments statefulsets daemonsets replicasets pods], int=5)[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:37:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:37:51 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/images	1.295s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/addons/dns	2.023s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/addons/proxy	2.564s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace

==================== CTEST END ======================
--- PASS: TestCtestCreateBootstrapConfigMapIfNotExists (0.01s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#02 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#02 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#02 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#02 (0.00s)
=== RUN   TestCtestCreateClusterInfoRBACRules

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/bootstraptoken/clusterinfo/ctest_clusterinfo_test.go:135]: Generating hardcoded RBAC role from fixture
[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/bootstraptoken/clusterinfo/ctest_clusterinfo_test.go:142]: Matched config item: {test_fixture.json [default rbac role] rules [roles rolebindings] {{ } {kubeadm:bootstrap-signer-clusterinfo  kube-public    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[get] [] [Secret] [cluster-info] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[roles rolebindings]
[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[roles rolebindings], int=2)[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [rolebindings]
[DEBUG-CTEST 2026-02-16 15:38:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:38:02 load all fixtures failed: requested fixture keys not found in test_fixtures.json: rolebindings
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/bootstraptoken/clusterinfo	1.614s
=== RUN   TestCtestAllowBootstrapTokensToPostCSRs

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/bootstraptoken/node/ctest_tlsbootstrap_test.go:27]: get default configs: {test_fixture.json [existing binding]  [clusterrolebindings] &ClusterRoleBinding{ObjectMeta:{kubeadm:kubelet-bootstrap      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Subjects:[]Subject{Subject{Kind:Group,APIGroup:,Name:system:bootstrappers:kubeadm:default-node-token,Namespace:,},},RoleRef:RoleRef{APIGroup:rbac.authorization.k8s.io,Kind:ClusterRole,Name:system:node-bootstrapper,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[clusterrolebindings]
[DEBUG-CTEST 2026-02-16 15:38:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[clusterrolebindings], int=1)[DEBUG-CTEST 2026-02-16 15:38:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [clusterrolebindings]
[DEBUG-CTEST 2026-02-16 15:38:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:38:03 load all fixtures failed: requested fixture keys not found in test_fixtures.json: clusterrolebindings
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/bootstraptoken/node	2.867s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/certs	3.640s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/certs/renewal	3.587s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/empty_paths_should_return_no_volumes
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/only_ca_path_present
    ctest_volumes_test.go:82: Unexpected volumes.
        Expected: []
        Actual:   [{etcd-certs-0 {&HostPathVolumeSource{Path:/var/lib/certs/etcd,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    ctest_volumes_test.go:85: Unexpected volume mounts.
        Expected: []
        Actual:   [{etcd-certs-0 true <nil> /var/lib/certs/etcd  <nil> }]
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/ca_and_cert_in_same_dir,_key_outside
    ctest_volumes_test.go:82: Unexpected volumes.
        Expected: [{etcd-certs-0 {&HostPathVolumeSource{Path:/var/lib/certs/etcd,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
        Actual:   [{etcd-certs-0 {&HostPathVolumeSource{Path:/outside,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {etcd-certs-1 {&HostPathVolumeSource{Path:/var/lib/certs/etcd,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    ctest_volumes_test.go:85: Unexpected volume mounts.
        Expected: [{etcd-certs-0 true <nil> /var/lib/certs/etcd  <nil> }]
        Actual:   [{etcd-certs-0 true <nil> /outside  <nil> } {etcd-certs-1 true <nil> /var/lib/certs/etcd  <nil> }]
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/certs_located_in_kubeadm_certificates_dir_should_be_ignored
--- FAIL: TestCtestGetEtcdCertVolumes_EdgeCases (0.00s)
    --- PASS: TestCtestGetEtcdCertVolumes_EdgeCases/empty_paths_should_return_no_volumes (0.00s)
    --- FAIL: TestCtestGetEtcdCertVolumes_EdgeCases/only_ca_path_present (0.00s)
    --- FAIL: TestCtestGetEtcdCertVolumes_EdgeCases/ca_and_cert_in_same_dir,_key_outside (0.00s)
    --- PASS: TestCtestGetEtcdCertVolumes_EdgeCases/certs_located_in_kubeadm_certificates_dir_should_be_ignored (0.00s)
=== RUN   TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases
=== RUN   TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases/nil_config_should_produce_empty_maps
--- FAIL: TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases (0.00s)
    --- FAIL: TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases/nil_config_should_produce_empty_maps (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x1c0 pc=0x10679e0e8]

goroutine 42 [running]:
testing.tRunner.func1.2({0x107018100, 0x10834dcb0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x107018100?, 0x10834dcb0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane.getHostPathVolumesForTheControlPlane(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/controlplane/volumes.go:54 +0x168
k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane.TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases.func1(0x14000603c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/controlplane/ctest_volumes_test.go:183 +0x28
testing.tRunner(0x14000603c00, 0x140001284e0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 41
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane	3.329s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/copycerts	4.515s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/etcd	3.256s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/kubeconfig	2.215s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestBuildKubeletArgs

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/ctest_flags_test.go:114]: Number of test cases: 6
Running test case 0: hostname override
Running test case 1: register with taints
Running test case 2: pause image is set
Running test case 3: nil nodeRegOpts and empty criSocket
--- FAIL: TestCtestBuildKubeletArgs (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x8 pc=0x103ae1d90]

goroutine 71 [running]:
testing.tRunner.func1.2({0x1046cf680, 0x106ca9560})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1046cf680?, 0x106ca9560?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.GetNodeNameAndHostname(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/flags.go:53 +0xa0
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.buildKubeletArgsCommon({0x0, {0x0, 0x0}, 0x0, {0x103f953e8, 0x0}})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/flags.go:110 +0x388
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.buildKubeletArgs(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/flags.go:140
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.TestCtestBuildKubeletArgs(0x14000408e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/ctest_flags_test.go:117 +0x634
testing.tRunner(0x14000408e00, 0x104c15498)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet	2.862s
=== RUN   TestCtestMarkControlPlane

==================== CTEST UNION MODE START ====================
[DEBUG-CTEST 2026-02-16 15:38:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/markcontrolplane/ctest_markcontrolplane_test.go:40]: get default configs: {test_fixture.json [markcontrolplane base configs] node [nodes] [{[] [] [] {"metadata":{"labels":{"node-role.kubernetes.io/control-plane":"","node.kubernetes.io/exclude-from-external-load-balancers":""}}}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] {"spec":{"taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"}]}}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] {}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [{node.cloudprovider.kubernetes.io/uninitialized  NoSchedule <nil>}] [] {}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [{node.cloudprovider.kubernetes.io/uninitialized  NoSchedule <nil>}] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] {"spec":{"taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"},{"effect":"NoSchedule","key":"node.cloudprovider.kubernetes.io/uninitialized"}]}}}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-16 15:38:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-16 15:38:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-16 15:38:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:38:22 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/markcontrolplane	2.082s
=== RUN   TestCtestAnnotateCRISocket

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/patchnode/ctest_patchnode_test.go:33]: get default configs: {test_fixture.json [default node] annotations [nodes] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} { []  false [] nil } {map[] map[]  [] [] {{0}} {          nil} [] [] [] nil [] nil}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:38:21 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/patchnode	0.684s
=== RUN   TestCtestEnforceVersionPolicies
[DEBUG-CTEST 2026-02-16 15:38:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/upgrade/ctest_policy_test.go:265]: Running EnforceVersionPolicies tests, total: 23
=== RUN   TestCtestEnforceVersionPolicies/minor_upgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.5
=== RUN   TestCtestEnforceVersionPolicies/major_upgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.1
=== RUN   TestCtestEnforceVersionPolicies/downgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/same_version_upgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/new_version_must_be_higher_than_v1.12.0
[upgrade/versions] Cluster version: v1.12.3
[upgrade/versions] kubeadm version: v1.12.3
=== RUN   TestCtestEnforceVersionPolicies/upgrading_two_minor_versions_in_one_go_is_not_supported
[upgrade/versions] Cluster version: v1.11.3
[upgrade/versions] kubeadm version: v1.13.0
=== RUN   TestCtestEnforceVersionPolicies/upgrading_with_n-3_kubelet_is_supported
[upgrade/versions] Cluster version: v1.14.3
[upgrade/versions] kubeadm version: v1.15.0
=== RUN   TestCtestEnforceVersionPolicies/upgrading_with_n-4_kubelet_is_not_supported
[upgrade/versions] Cluster version: v1.14.3
[upgrade/versions] kubeadm version: v1.15.0
=== RUN   TestCtestEnforceVersionPolicies/downgrading_two_minor_versions_in_one_go_is_not_supported
[upgrade/versions] Cluster version: 1.6.0
[upgrade/versions] kubeadm version: 1.4.0
=== RUN   TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._However,_patch_version_skews_may_be_forced
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._Trying_to_upgrade_k8s_to_a_higher_minor_version_than_kubeadm_itself_should_never_be_supported
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version.
[upgrade/versions] Cluster version: v1.13.0
[upgrade/versions] kubeadm version: v1.13.0
=== RUN   TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version._This_may_be_forced_through_though.
[upgrade/versions] Cluster version: v1.14.0
[upgrade/versions] kubeadm version: v1.14.0
=== RUN   TestCtestEnforceVersionPolicies/experimental_upgrades_supported_if_the_flag_is_set
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-beta.1
=== RUN   TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-rc.1
=== RUN   TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set#01
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-rc.1
=== RUN   TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_experimental_version_if_they_haven't_opted_into_that
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-beta.1
=== RUN   TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_release_candidate_version_if_they_haven't_opted_into_that
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-rc.1
=== RUN   TestCtestEnforceVersionPolicies/the_user_can't_use_a_newer_minor_version_of_kubeadm_to_upgrade_an_older_version_of_kubeadm
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0
=== RUN   TestCtestEnforceVersionPolicies/build_release_supported_at_MinimumControlPlaneVersion
[upgrade/versions] Cluster version: 1.3.0
[upgrade/versions] kubeadm version: 1.3.0+build
=== RUN   TestCtestEnforceVersionPolicies/empty_version_strings
=== RUN   TestCtestEnforceVersionPolicies/malformed_version_strings
=== RUN   TestCtestEnforceVersionPolicies/nil_version_getter
--- PASS: TestCtestEnforceVersionPolicies (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/minor_upgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/major_upgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/downgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/same_version_upgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/new_version_must_be_higher_than_v1.12.0 (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/upgrading_two_minor_versions_in_one_go_is_not_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/upgrading_with_n-3_kubelet_is_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/upgrading_with_n-4_kubelet_is_not_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/downgrading_two_minor_versions_in_one_go_is_not_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._However,_patch_version_skews_may_be_forced (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._Trying_to_upgrade_k8s_to_a_higher_minor_version_than_kubeadm_itself_should_never_be_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version. (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version._This_may_be_forced_through_though. (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/experimental_upgrades_supported_if_the_flag_is_set (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set#01 (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_experimental_version_if_they_haven't_opted_into_that (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_release_candidate_version_if_they_haven't_opted_into_that (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_user_can't_use_a_newer_minor_version_of_kubeadm_to_upgrade_an_older_version_of_kubeadm (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/build_release_supported_at_MinimumControlPlaneVersion (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/empty_version_strings (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/malformed_version_strings (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/nil_version_getter (0.00s)
=== RUN   TestCtestWriteKubeletConfigFiles
  W0216 15:38:21.888130   88746 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config1425693222 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config1425693222/config.yaml
[dryrun] would read the flag --container-runtime-endpoint value from "kubeadm-flags.env", which is missing. Using default socket "unix:///var/run/containerd/containerd.sock" instead[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-upgrade-dryrun1919217918/instance-config.yaml"
[dryrun] Would write file "/var/lib/kubelet/instance-config.yaml" with content:
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
[patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-upgrade-dryrun1919217918/config.yaml"
[dryrun] Would write file "/var/lib/kubelet/config.yaml" with content:
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
  W0216 15:38:21.888808   88746 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config3121410598 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config3121410598/config.yaml
[dryrun] would read the flag --container-runtime-endpoint value from "kubeadm-flags.env", which is missing. Using default socket "unix:///var/run/containerd/containerd.sock" instead[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-upgrade-dryrun1209006712/instance-config.yaml"
[dryrun] Would write file "/var/lib/kubelet/instance-config.yaml" with content:
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
  W0216 15:38:21.889398   88746 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config635192873 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config635192873/config.yaml
[dryrun] would read the flag --container-runtime-endpoint value from "kubeadm-flags.env", which is missing. Using default socket "unix:///var/run/containerd/containerd.sock" instead[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-upgrade-dryrun1263789117/instance-config.yaml"
[dryrun] Would write file "/var/lib/kubelet/instance-config.yaml" with content:
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
[patches] Reading patches from path "Bogus"
  W0216 15:38:21.889973   88746 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config320477819 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles1221195848/001/tmp/kubeadm-kubelet-config320477819/config.yaml
--- FAIL: TestCtestWriteKubeletConfigFiles (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x210 pc=0x108413d30]

goroutine 91 [running]:
testing.tRunner.func1.2({0x1090a7100, 0x10b841550})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1090a7100?, 0x10b841550?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.WriteKubeletConfigFiles(0x0, {0x1400005ef60?, 0x5f?}, {0x0, 0x0}, 0x1, {0x10964b3b8, 0x14000094048})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/upgrade/postupgrade.go:132 +0x5e0
k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.TestCtestWriteKubeletConfigFiles(0x140007ed340)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/upgrade/ctest_postupgrade_test.go:71 +0x2b8
testing.tRunner(0x140007ed340, 0x109635ac0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade	1.479s
=== RUN   TestCtestUploadConfiguration
[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/uploadconfig/ctest_uploadconfig_test.go:25]: Start TestCtestUploadConfiguration
[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/uploadconfig/ctest_uploadconfig_test.go:33]: Matched config: {test_fixture.json [default cluster config] kubernetesVersion [configmaps] &TypeMeta{Kind:,APIVersion:,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-16 15:38:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:38:24 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/uploadconfig	3.311s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/preflight	2.599s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestArgumentsToCommand
=== RUN   TestCtestArgumentsToCommand/override_an_argument_from_the_base
=== RUN   TestCtestArgumentsToCommand/override_an_argument_from_the_base_and_add_duplicate
=== RUN   TestCtestArgumentsToCommand/override_all_duplicate_arguments_from_base
=== RUN   TestCtestArgumentsToCommand/add_an_argument_that_is_not_in_base
=== RUN   TestCtestArgumentsToCommand/allow_empty_strings_in_base
=== RUN   TestCtestArgumentsToCommand/allow_empty_strings_in_overrides
=== RUN   TestCtestArgumentsToCommand/empty_base_and_overrides
    ctest_arguments_test.go:153: failed ArgumentsToCommand:
        expected:
        []
        saw:
        []
=== RUN   TestCtestArgumentsToCommand/override_with_empty_value
=== RUN   TestCtestArgumentsToCommand/large_number_of_arguments
--- FAIL: TestCtestArgumentsToCommand (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_an_argument_from_the_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_an_argument_from_the_base_and_add_duplicate (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_all_duplicate_arguments_from_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/add_an_argument_that_is_not_in_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/allow_empty_strings_in_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/allow_empty_strings_in_overrides (0.00s)
    --- FAIL: TestCtestArgumentsToCommand/empty_base_and_overrides (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_with_empty_value (0.00s)
    --- PASS: TestCtestArgumentsToCommand/large_number_of_arguments (0.00s)
=== RUN   TestCtestArgumentsFromCommand
=== RUN   TestCtestArgumentsFromCommand/normal_case
=== RUN   TestCtestArgumentsFromCommand/test_that_feature-gates_is_working
=== RUN   TestCtestArgumentsFromCommand/test_that_a_binary_can_be_the_first_arg
=== RUN   TestCtestArgumentsFromCommand/allow_duplicate_args
=== RUN   TestCtestArgumentsFromCommand/empty_args_slice
=== RUN   TestCtestArgumentsFromCommand/only_binary_name,_no_flags
=== RUN   TestCtestArgumentsFromCommand/invalid_flag_without_equals
--- PASS: TestCtestArgumentsFromCommand (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/normal_case (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/test_that_feature-gates_is_working (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/test_that_a_binary_can_be_the_first_arg (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/allow_duplicate_args (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/empty_args_slice (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/only_binary_name,_no_flags (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/invalid_flag_without_equals (0.00s)
=== RUN   TestCtestRoundtrip
=== RUN   TestCtestRoundtrip/normal_case
=== RUN   TestCtestRoundtrip/test_that_feature-gates_is_working
=== RUN   TestCtestRoundtrip/empty_args_slice
    ctest_arguments_test.go:277: failed TestRoundtrip:
        expected:
        []
        saw:
        []
--- FAIL: TestCtestRoundtrip (0.00s)
    --- PASS: TestCtestRoundtrip/normal_case (0.00s)
    --- PASS: TestCtestRoundtrip/test_that_feature-gates_is_working (0.00s)
    --- FAIL: TestCtestRoundtrip/empty_args_slice (0.00s)
=== RUN   TestCtestParseArgument
=== RUN   TestCtestParseArgument/arg_cannot_be_empty
=== RUN   TestCtestParseArgument/arg_must_contain_--_and_=
=== RUN   TestCtestParseArgument/arg_must_contain_--_and_=#01
=== RUN   TestCtestParseArgument/arg_must_contain_--
=== RUN   TestCtestParseArgument/arg_must_contain_a_key
=== RUN   TestCtestParseArgument/arg_can_contain_key_but_no_value
=== RUN   TestCtestParseArgument/simple_case
=== RUN   TestCtestParseArgument/keys/values_with_'-'_should_be_supported
=== RUN   TestCtestParseArgument/numbers_should_be_handled_correctly
=== RUN   TestCtestParseArgument/lists_should_be_handled_correctly
=== RUN   TestCtestParseArgument/more_than_one_'='_should_be_allowed
=== RUN   TestCtestParseArgument/just_double_dash_without_key
=== RUN   TestCtestParseArgument/key_with_spaces
--- PASS: TestCtestParseArgument (0.00s)
    --- PASS: TestCtestParseArgument/arg_cannot_be_empty (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_--_and_= (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_--_and_=#01 (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_-- (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_a_key (0.00s)
    --- PASS: TestCtestParseArgument/arg_can_contain_key_but_no_value (0.00s)
    --- PASS: TestCtestParseArgument/simple_case (0.00s)
    --- PASS: TestCtestParseArgument/keys/values_with_'-'_should_be_supported (0.00s)
    --- PASS: TestCtestParseArgument/numbers_should_be_handled_correctly (0.00s)
    --- PASS: TestCtestParseArgument/lists_should_be_handled_correctly (0.00s)
    --- PASS: TestCtestParseArgument/more_than_one_'='_should_be_allowed (0.00s)
    --- PASS: TestCtestParseArgument/just_double_dash_without_key (0.00s)
    --- PASS: TestCtestParseArgument/key_with_spaces (0.00s)
=== RUN   TestCtestGetControlPlaneEndpoint

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_endpoint_test.go:232]: Number of test cases: 19
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_if_fully_defined
  W0216 15:38:31.912552   88836 endpoint.go:56] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_if_fully_defined
  W0216 15:38:31.912614   88836 endpoint.go:56] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_if_fully_defined
  W0216 15:38:31.912672   88836 endpoint.go:56] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port
=== RUN   TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined
=== RUN   TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_BindPort
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(dns)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip4)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip6)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(port)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip4)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip6)
=== RUN   TestCtestGetControlPlaneEndpoint/empty_ControlPlaneEndpoint_and_AdvertiseAddress
=== RUN   TestCtestGetControlPlaneEndpoint/negative_BindPort
=== RUN   TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_with_whitespace
=== RUN   TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_without_host
--- PASS: TestCtestGetControlPlaneEndpoint (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_if_fully_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_if_fully_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_if_fully_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_BindPort (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(dns) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip4) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip6) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(port) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip4) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip6) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/empty_ControlPlaneEndpoint_and_AdvertiseAddress (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/negative_BindPort (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_with_whitespace (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_without_host (0.00s)
=== RUN   TestCtestParsePort

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_endpoint_test.go:468]: Number of test cases: 9
=== RUN   TestCtestParsePort/valid_port
=== RUN   TestCtestParsePort/invalid_port_(not_a_number)
=== RUN   TestCtestParsePort/invalid_port_(<1)
=== RUN   TestCtestParsePort/invalid_port_(>65535)
=== RUN   TestCtestParsePort/empty_string
=== RUN   TestCtestParsePort/zero_port
=== RUN   TestCtestParsePort/max_valid_port
=== RUN   TestCtestParsePort/negative_zero_string
=== RUN   TestCtestParsePort/huge_number_overflow
--- PASS: TestCtestParsePort (0.00s)
    --- PASS: TestCtestParsePort/valid_port (0.00s)
    --- PASS: TestCtestParsePort/invalid_port_(not_a_number) (0.00s)
    --- PASS: TestCtestParsePort/invalid_port_(<1) (0.00s)
    --- PASS: TestCtestParsePort/invalid_port_(>65535) (0.00s)
    --- PASS: TestCtestParsePort/empty_string (0.00s)
    --- PASS: TestCtestParsePort/zero_port (0.00s)
    --- PASS: TestCtestParsePort/max_valid_port (0.00s)
    --- PASS: TestCtestParsePort/negative_zero_string (0.00s)
    --- PASS: TestCtestParsePort/huge_number_overflow (0.00s)
=== RUN   TestCtestMergeKubeadmEnvVars

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_env_test.go:91]: Number of test cases: 4
Running 0 th test case: normal case without duplicated env
=== RUN   TestCtestMergeKubeadmEnvVars/normal_case_without_duplicated_env
Running 1 th test case: extraEnv env take precedence over the proxyEnv
=== RUN   TestCtestMergeKubeadmEnvVars/extraEnv_env_take_precedence_over_the_proxyEnv
Running 2 th test case: proxyEnv has duplicate keys, extraEnv empty
=== RUN   TestCtestMergeKubeadmEnvVars/proxyEnv_has_duplicate_keys,_extraEnv_empty
Running 3 th test case: extraEnv overrides multiple proxyEnv entries
=== RUN   TestCtestMergeKubeadmEnvVars/extraEnv_overrides_multiple_proxyEnv_entries

==================== CTEST END ======================
--- PASS: TestCtestMergeKubeadmEnvVars (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/normal_case_without_duplicated_env (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/extraEnv_env_take_precedence_over_the_proxyEnv (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/proxyEnv_has_duplicate_keys,_extraEnv_empty (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/extraEnv_overrides_multiple_proxyEnv_entries (0.00s)
=== RUN   TestCtestGetProxyEnvVars

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_env_test.go:164]: Number of test cases: 4
Running 0 th test case: environment with lowercase proxy vars
=== RUN   TestCtestGetProxyEnvVars/environment_with_lowercase_proxy_vars
Running 1 th test case: environment with uppercase proxy vars
=== RUN   TestCtestGetProxyEnvVars/environment_with_uppercase_proxy_vars
Running 2 th test case: empty environment slice
=== RUN   TestCtestGetProxyEnvVars/empty_environment_slice
Running 3 th test case: environment with malformed entries and duplicates
=== RUN   TestCtestGetProxyEnvVars/environment_with_malformed_entries_and_duplicates
    ctest_env_test.go:170: expected env: [{{http_proxy  nil}} {{https_proxy  nil}} {{no_proxy p3 nil}} {{NO_PROXY override nil}}], got: [{{http_proxy p1 nil}} {{http_proxy p2 nil}} {{no_proxy p3 nil}} {{NO_PROXY override nil}}]

==================== CTEST END ======================
--- FAIL: TestCtestGetProxyEnvVars (0.00s)
    --- PASS: TestCtestGetProxyEnvVars/environment_with_lowercase_proxy_vars (0.00s)
    --- PASS: TestCtestGetProxyEnvVars/environment_with_uppercase_proxy_vars (0.00s)
    --- PASS: TestCtestGetProxyEnvVars/empty_environment_slice (0.00s)
    --- FAIL: TestCtestGetProxyEnvVars/environment_with_malformed_entries_and_duplicates (0.00s)
=== RUN   TestCtestMarshalUnmarshalYaml

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:26]: matched config: {test_fixture.json [marshal-unmarshal yaml pod] spec [pods] {[] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:38:31 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:31 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"restartPolicy":"Always"})[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/16 15:38:31 [DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:36]: New Json Test Configs: [{"volumes":[{"name":"config","hostPath":{"path":"/etc/kubernetes"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"restartPolicy":"Always"}]
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:37]: Num of test cases: 1
Running 0 th test case.
{[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    ctest_marshal_test.go:62: 
        apiVersion: v1
        kind: Pod
        metadata:
          labels:
            test: "yes"
          name: someName-e50ff4e5-ba70-4a35-be35-69f4bb96c606
          namespace: testNamespace-55c3ee9b-fe21-4d59-980d-68491309bb82
        spec:
          containers: null
          restartPolicy: Always
          volumes:
          - hostPath:
              path: /etc/kubernetes
            name: config
          - hostPath:
              path: /var/lib/kms/
              type: DirectoryOrCreate
            name: socket-dir
        status: {}

==================== CTEST END ======================
--- PASS: TestCtestMarshalUnmarshalYaml (0.01s)
=== RUN   TestCtestUnmarshalJson

==================== CTEST OVERRIDE ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:95]: matched config: {test_fixture.json [unmarshal json pod] spec [pods] {[] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:38:31 [DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:38:31 Mode: 1
2026/02/16 15:38:31 Base JSON size: 44 bytes
2026/02/16 15:38:31 Number of external values: 1
2026/02/16 15:38:31   [OVERRIDE] containers: <nil> → [map[args:[--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config] image:registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 name:barbican-kms resources:map[] volumeMounts:[map[mountPath:/etc/kubernetes/ name:config] map[mountPath:/kms/ name:socket-dir]]]]
2026/02/16 15:38:31   [KEEP] restartPolicy: Always (missing in external)
2026/02/16 15:38:31 [DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:38:31 [DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"restartPolicy":"Always"})[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/16 15:38:31 [DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[] [] [{barbican-kms registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[] [] [{barbican-kms registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:105]: New Json Test Configs: [{"containers":[{"name":"barbican-kms","image":"registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0","args":["--socketpath=/kms/kms.sock","--cloud-config=/etc/kubernetes/cloud-config"],"resources":{},"volumeMounts":[{"name":"config","mountPath":"/etc/kubernetes/"},{"name":"socket-dir","mountPath":"/kms/"}]}],"restartPolicy":"Always"}]
[DEBUG-CTEST 2026-02-16 15:38:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:106]: Num of test cases: 1
Running 0 th test case.
{[] [] [{barbican-kms registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    ctest_marshal_test.go:128: 
        {
          "apiVersion": "v1",
          "kind": "Pod",
          "metadata": {
            "name": "someName",
            "namespace": "testNamespace",
            "labels": {
              "test": "yes"
            }
          },
          "spec": {
            "restartPolicy": "Always"
          }
        }

==================== CTEST END ======================
--- PASS: TestCtestUnmarshalJson (0.00s)
=== RUN   TestCtestKubernetesReleaseVersion
=== RUN   TestCtestKubernetesReleaseVersion/empty_input
=== RUN   TestCtestKubernetesReleaseVersion/label_as_input
  I0216 15:38:31.921365   88836 version.go:260] remote version is much newer: 1.0.0-placeholder-version; falling back to: stable-0.0
    ctest_version_test.go:57: expected error: false, got: true, error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:60: expected output: v1.0.0-placeholder-version, got: 
=== RUN   TestCtestKubernetesReleaseVersion/whitespace_only
=== RUN   TestCtestKubernetesReleaseVersion/unknown_label_without_fetcher_entry
--- FAIL: TestCtestKubernetesReleaseVersion (0.00s)
    --- PASS: TestCtestKubernetesReleaseVersion/empty_input (0.00s)
    --- FAIL: TestCtestKubernetesReleaseVersion/label_as_input (0.00s)
    --- PASS: TestCtestKubernetesReleaseVersion/whitespace_only (0.00s)
    --- PASS: TestCtestKubernetesReleaseVersion/unknown_label_without_fetcher_entry (0.00s)
=== RUN   TestCtestValidVersion
=== RUN   TestCtestValidVersion/v1.3.0
    ctest_version_test.go:83: Valid:  v1.3.0 v1.3.0 <nil>
=== RUN   TestCtestValidVersion/v1.4.0-alpha.0
    ctest_version_test.go:83: Valid:  v1.4.0-alpha.0 v1.4.0-alpha.0 <nil>
=== RUN   TestCtestValidVersion/v1.4.5
    ctest_version_test.go:83: Valid:  v1.4.5 v1.4.5 <nil>
=== RUN   TestCtestValidVersion/v1.4.0-beta.0
    ctest_version_test.go:83: Valid:  v1.4.0-beta.0 v1.4.0-beta.0 <nil>
=== RUN   TestCtestValidVersion/v2.0.0
    ctest_version_test.go:83: Valid:  v2.0.0 v2.0.0 <nil>
=== RUN   TestCtestValidVersion/v1.6.0-alpha.0.536+d60d9f3269288f
    ctest_version_test.go:83: Valid:  v1.6.0-alpha.0.536+d60d9f3269288f v1.6.0-alpha.0.536+d60d9f3269288f <nil>
=== RUN   TestCtestValidVersion/v1.5.0-alpha.0.1078+1044b6822497da-pull
    ctest_version_test.go:83: Valid:  v1.5.0-alpha.0.1078+1044b6822497da-pull v1.5.0-alpha.0.1078+1044b6822497da-pull <nil>
=== RUN   TestCtestValidVersion/v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci
    ctest_version_test.go:83: Valid:  v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci <nil>
=== RUN   TestCtestValidVersion/v1.6.1+coreos.0
    ctest_version_test.go:83: Valid:  v1.6.1+coreos.0 v1.6.1+coreos.0 <nil>
=== RUN   TestCtestValidVersion/1.7.1
    ctest_version_test.go:83: Valid:  1.7.1 v1.7.1 <nil>
=== RUN   TestCtestValidVersion/v9999.9999.9999-alpha.0+build.1234567890
    ctest_version_test.go:83: Valid:  v9999.9999.9999-alpha.0+build.1234567890 v9999.9999.9999-alpha.0+build.1234567890 <nil>
--- PASS: TestCtestValidVersion (0.00s)
    --- PASS: TestCtestValidVersion/v1.3.0 (0.00s)
    --- PASS: TestCtestValidVersion/v1.4.0-alpha.0 (0.00s)
    --- PASS: TestCtestValidVersion/v1.4.5 (0.00s)
    --- PASS: TestCtestValidVersion/v1.4.0-beta.0 (0.00s)
    --- PASS: TestCtestValidVersion/v2.0.0 (0.00s)
    --- PASS: TestCtestValidVersion/v1.6.0-alpha.0.536+d60d9f3269288f (0.00s)
    --- PASS: TestCtestValidVersion/v1.5.0-alpha.0.1078+1044b6822497da-pull (0.00s)
    --- PASS: TestCtestValidVersion/v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci (0.00s)
    --- PASS: TestCtestValidVersion/v1.6.1+coreos.0 (0.00s)
    --- PASS: TestCtestValidVersion/1.7.1 (0.00s)
    --- PASS: TestCtestValidVersion/v9999.9999.9999-alpha.0+build.1234567890 (0.00s)
=== RUN   TestCtestInvalidVersion
=== RUN   TestCtestInvalidVersion/v1.3
    ctest_version_test.go:109: Invalid:  v1.3  version "v1.3" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/1.4
    ctest_version_test.go:109: Invalid:  1.4  version "1.4" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/b1.4.0
    ctest_version_test.go:109: Invalid:  b1.4.0  version "b1.4.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/c1.4.5+git
    ctest_version_test.go:109: Invalid:  c1.4.5+git  version "c1.4.5+git" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/something1.2
    ctest_version_test.go:109: Invalid:  something1.2  version "something1.2" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/#00
    ctest_version_test.go:109: Invalid:    invalid version ""
=== RUN   TestCtestInvalidVersion/v
    ctest_version_test.go:109: Invalid:  v  version "v" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/v1
    ctest_version_test.go:109: Invalid:  v1  version "v1" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/v1.2.3.4
    ctest_version_test.go:109: Invalid:  v1.2.3.4 v1.2.3.4 <nil>
    ctest_version_test.go:111: kubernetesReleaseVersion error expected for version "v1.2.3.4", but returned successfully
    ctest_version_test.go:114: kubernetesReleaseVersion should return empty string in case of error. Returned "v1.2.3.4" for version "v1.2.3.4"
--- FAIL: TestCtestInvalidVersion (0.00s)
    --- PASS: TestCtestInvalidVersion/v1.3 (0.00s)
    --- PASS: TestCtestInvalidVersion/1.4 (0.00s)
    --- PASS: TestCtestInvalidVersion/b1.4.0 (0.00s)
    --- PASS: TestCtestInvalidVersion/c1.4.5+git (0.00s)
    --- PASS: TestCtestInvalidVersion/something1.2 (0.00s)
    --- PASS: TestCtestInvalidVersion/#00 (0.00s)
    --- PASS: TestCtestInvalidVersion/v (0.00s)
    --- PASS: TestCtestInvalidVersion/v1 (0.00s)
    --- FAIL: TestCtestInvalidVersion/v1.2.3.4 (0.00s)
=== RUN   TestCtestValidConvenientForUserVersion
=== RUN   TestCtestValidConvenientForUserVersion/1.4.0
    ctest_version_test.go:130: Valid:  1.4.0 v1.4.0 <nil>
=== RUN   TestCtestValidConvenientForUserVersion/1.4.5+git
    ctest_version_test.go:130: Valid:  1.4.5+git v1.4.5+git <nil>
=== RUN   TestCtestValidConvenientForUserVersion/1.6.1_coreos.0
    ctest_version_test.go:130: Valid:  1.6.1_coreos.0 v1.6.1_coreos.0 <nil>
=== RUN   TestCtestValidConvenientForUserVersion/#00
    ctest_version_test.go:130: Valid:    invalid version ""
--- PASS: TestCtestValidConvenientForUserVersion (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/1.4.0 (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/1.4.5+git (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/1.6.1_coreos.0 (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/#00 (0.00s)
=== RUN   TestCtestVersionFromNetwork
=== RUN   TestCtestVersionFromNetwork/nonexistent
    ctest_version_test.go:179: Key: "nonexistent". Result: "", Error: version "nonexistent" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/latest-1.5
  W0216 15:38:31.921767   88836 version.go:108] could not fetch a Kubernetes version from the internet: expected error
  W0216 15:38:31.921771   88836 version.go:109] falling back to the local client version: v0.0.0-master.0
    ctest_version_test.go:179: Key: "latest-1.5". Result: "v0.0.0-master.0", Error: <nil>
    ctest_version_test.go:186: unexpected result for "latest-1.5". Expected: "v1.0.0-placeholder-version" Actual: "v0.0.0-master.0"
=== RUN   TestCtestVersionFromNetwork/stable
    ctest_version_test.go:179: Key: "stable". Result: "", Error: remote version error: could not parse "stable-1" as version
    ctest_version_test.go:182: unexpected error for "stable": remote version error: could not parse "stable-1" as version
=== RUN   TestCtestVersionFromNetwork/stable-1
  I0216 15:38:31.921835   88836 version.go:260] remote version is much newer: v1.4.6; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "stable-1". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "stable-1": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/stable-1.3
  I0216 15:38:31.921858   88836 version.go:260] remote version is much newer: v1.3.10; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "stable-1.3". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "stable-1.3": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/latest
  I0216 15:38:31.921886   88836 version.go:260] remote version is much newer: v1.6.0-alpha.0; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "latest". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "latest": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/latest-1.3
  I0216 15:38:31.921926   88836 version.go:260] remote version is much newer: v1.3.11-beta.0; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "latest-1.3". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "latest-1.3": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/invalid-version
    ctest_version_test.go:179: Key: "invalid-version". Result: "", Error: version "invalid-version" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
--- FAIL: TestCtestVersionFromNetwork (0.00s)
    --- PASS: TestCtestVersionFromNetwork/nonexistent (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/latest-1.5 (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/stable (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/stable-1 (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/stable-1.3 (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/latest (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/latest-1.3 (0.00s)
    --- PASS: TestCtestVersionFromNetwork/invalid-version (0.00s)
=== RUN   TestCtestVersionToTag
=== RUN   TestCtestVersionToTag/input:/expected:
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "". Result: "". Expected: ""
=== RUN   TestCtestVersionToTag/input:v1.0.0/expected:v1.0.0
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v1.0.0". Result: "v1.0.0". Expected: "v1.0.0"
=== RUN   TestCtestVersionToTag/input:v10.1.2-alpha.1.100+0123456789abcdef+SOMETHING/expected:v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v10.1.2-alpha.1.100+0123456789abcdef+SOMETHING". Result: "v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING". Expected: "v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING"
=== RUN   TestCtestVersionToTag/input:v1,0!0+üñµ/expected:v1_0_0____
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v1,0!0+üñµ". Result: "v1_0_0____". Expected: "v1_0_0____"
=== RUN   TestCtestVersionToTag/input:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0/expected:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0". Result: "v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0". Expected: "v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0"
--- PASS: TestCtestVersionToTag (0.00s)
    --- PASS: TestCtestVersionToTag/input:/expected: (0.00s)
    --- PASS: TestCtestVersionToTag/input:v1.0.0/expected:v1.0.0 (0.00s)
    --- PASS: TestCtestVersionToTag/input:v10.1.2-alpha.1.100+0123456789abcdef+SOMETHING/expected:v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING (0.00s)
    --- PASS: TestCtestVersionToTag/input:v1,0!0+üñµ/expected:v1_0_0____ (0.00s)
    --- PASS: TestCtestVersionToTag/input:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0/expected:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0 (0.00s)
=== RUN   TestCtestSplitVersion
=== RUN   TestCtestSplitVersion/input:v1.7.0/label:v1.7.0
=== RUN   TestCtestSplitVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/label:v1.8.0-alpha.2.1231+afabd012389d53a
=== RUN   TestCtestSplitVersion/input:release/v1.7.0/label:v1.7.0
=== RUN   TestCtestSplitVersion/input:release/latest-1.7/label:latest-1.7
=== RUN   TestCtestSplitVersion/input:ci/latest/label:latest
=== RUN   TestCtestSplitVersion/input:ci/latest-1.7/label:latest-1.7
=== RUN   TestCtestSplitVersion/input:unknown-1/label:unknown-1
=== RUN   TestCtestSplitVersion/input:unknown/latest-1/label:
=== RUN   TestCtestSplitVersion/input:/label:
=== RUN   TestCtestSplitVersion/input:ci//label:
=== RUN   TestCtestSplitVersion/input:ci/label:
    ctest_version_test.go:242: splitVersion: error expected for "ci", but result is "https://dl.k8s.io/release", "ci"
--- FAIL: TestCtestSplitVersion (0.00s)
    --- PASS: TestCtestSplitVersion/input:v1.7.0/label:v1.7.0 (0.00s)
    --- PASS: TestCtestSplitVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/label:v1.8.0-alpha.2.1231+afabd012389d53a (0.00s)
    --- PASS: TestCtestSplitVersion/input:release/v1.7.0/label:v1.7.0 (0.00s)
    --- PASS: TestCtestSplitVersion/input:release/latest-1.7/label:latest-1.7 (0.00s)
    --- PASS: TestCtestSplitVersion/input:ci/latest/label:latest (0.00s)
    --- PASS: TestCtestSplitVersion/input:ci/latest-1.7/label:latest-1.7 (0.00s)
    --- PASS: TestCtestSplitVersion/input:unknown-1/label:unknown-1 (0.00s)
    --- PASS: TestCtestSplitVersion/input:unknown/latest-1/label: (0.00s)
    --- PASS: TestCtestSplitVersion/input:/label: (0.00s)
    --- PASS: TestCtestSplitVersion/input:ci//label: (0.00s)
    --- FAIL: TestCtestSplitVersion/input:ci/label: (0.00s)
=== RUN   TestCtestKubernetesIsCIVersion
=== RUN   TestCtestKubernetesIsCIVersion/input:/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:v1.0.0/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "v1.0.0". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:release/v1.0.0/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "release/v1.0.0". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:ci/latest-1/expected:true
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci/latest-1". Result: true. Expected: true
=== RUN   TestCtestKubernetesIsCIVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:true
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a". Result: true. Expected: true
=== RUN   TestCtestKubernetesIsCIVersion/input:ci//expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci/". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:ci/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci". Result: false. Expected: false
--- PASS: TestCtestKubernetesIsCIVersion (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:/expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:v1.0.0/expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:release/v1.0.0/expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci/latest-1/expected:true (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:true (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci//expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci/expected:false (0.00s)
=== RUN   TestCtestCIBuildVersion
=== RUN   TestCtestCIBuildVersion/input:v1.7.0/expected:v1.7.0
    ctest_version_test.go:303: Input: "v1.7.0". Result: "v1.7.0", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:release/v1.8.0/expected:v1.8.0
    ctest_version_test.go:303: Input: "release/v1.8.0". Result: "v1.8.0", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:1.4.0-beta.0/expected:v1.4.0-beta.0
    ctest_version_test.go:303: Input: "1.4.0-beta.0". Result: "v1.4.0-beta.0", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:release/0invalid/expected:
    ctest_version_test.go:303: Input: "release/0invalid". Result: "", Error: version "release/0invalid" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestCIBuildVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a
    ctest_version_test.go:303: Input: "ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a". Result: "v1.9.0-alpha.1.123+acbcbfd53bfa0a", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:ci/1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a
    ctest_version_test.go:303: Input: "ci/1.9.0-alpha.1.123+acbcbfd53bfa0a". Result: "v1.9.0-alpha.1.123+acbcbfd53bfa0a", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:ci/0invalid/expected:
    ctest_version_test.go:303: Input: "ci/0invalid". Result: "", Error: version "ci/0invalid" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestCIBuildVersion/input:0invalid/expected:
    ctest_version_test.go:303: Input: "0invalid". Result: "", Error: version "0invalid" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestCIBuildVersion/input:ci/expected:
    ctest_version_test.go:303: Input: "ci". Result: "", Error: version "ci" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
--- PASS: TestCtestCIBuildVersion (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:v1.7.0/expected:v1.7.0 (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:release/v1.8.0/expected:v1.8.0 (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:1.4.0-beta.0/expected:v1.4.0-beta.0 (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:release/0invalid/expected: (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/0invalid/expected: (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:0invalid/expected: (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/expected: (0.00s)
=== RUN   TestCtestNormalizedBuildVersionVersion
=== RUN   TestCtestNormalizedBuildVersionVersion/input:v1.7.0/expected:v1.7.0
=== RUN   TestCtestNormalizedBuildVersionVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/expected:v1.8.0-alpha.2.1231+afabd012389d53a
=== RUN   TestCtestNormalizedBuildVersionVersion/input:1.7.0/expected:v1.7.0
=== RUN   TestCtestNormalizedBuildVersionVersion/input:unknown-1/expected:
=== RUN   TestCtestNormalizedBuildVersionVersion/input:/expected:
=== RUN   TestCtestNormalizedBuildVersionVersion/input:v/expected:
--- PASS: TestCtestNormalizedBuildVersionVersion (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:v1.7.0/expected:v1.7.0 (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/expected:v1.8.0-alpha.2.1231+afabd012389d53a (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:1.7.0/expected:v1.7.0 (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:unknown-1/expected: (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:/expected: (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:v/expected: (0.00s)
=== RUN   TestCtestKubeadmVersion
=== RUN   TestCtestKubeadmVersion/valid_version_with_label_and_metadata
=== RUN   TestCtestKubeadmVersion/valid_version_with_label_and_extra_metadata
=== RUN   TestCtestKubeadmVersion/valid_patch_version_with_label_and_extra_metadata
=== RUN   TestCtestKubeadmVersion/valid_version_with_label_extra
=== RUN   TestCtestKubeadmVersion/valid_patch_version_with_label
=== RUN   TestCtestKubeadmVersion/handle_version_with_partial_label
=== RUN   TestCtestKubeadmVersion/handle_version_missing_'v'
=== RUN   TestCtestKubeadmVersion/valid_version_without_label_and_metadata
=== RUN   TestCtestKubeadmVersion/valid_patch_version_without_label_and_metadata
=== RUN   TestCtestKubeadmVersion/invalid_version
=== RUN   TestCtestKubeadmVersion/invalid_version_with_stray_dash
=== RUN   TestCtestKubeadmVersion/invalid_version_without_patch_release
=== RUN   TestCtestKubeadmVersion/invalid_version_with_label_and_stray_dot
=== RUN   TestCtestKubeadmVersion/invalid_version_empty
=== RUN   TestCtestKubeadmVersion/invalid_version_malformed
--- PASS: TestCtestKubeadmVersion (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_with_label_and_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_with_label_and_extra_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_patch_version_with_label_and_extra_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_with_label_extra (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_patch_version_with_label (0.00s)
    --- PASS: TestCtestKubeadmVersion/handle_version_with_partial_label (0.00s)
    --- PASS: TestCtestKubeadmVersion/handle_version_missing_'v' (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_without_label_and_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_patch_version_without_label_and_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_with_stray_dash (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_without_patch_release (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_with_label_and_stray_dot (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_empty (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_malformed (0.00s)
=== RUN   TestCtestValidateStableVersion
=== RUN   TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[1]
  I0216 15:38:31.922560   88836 version.go:260] remote version is much newer: v1.12.0; falling back to: stable-1.11
=== RUN   TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[2]
  I0216 15:38:31.922584   88836 version.go:260] remote version is much newer: v2.0.0; falling back to: stable-1.11
=== RUN   TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[3]
  I0216 15:38:31.922593   88836 version.go:260] remote version is much newer: v2.1.5; falling back to: stable-1.11
=== RUN   TestCtestValidateStableVersion/valid:_return_the_remote_version_as_it_is_part_of_the_same_release
=== RUN   TestCtestValidateStableVersion/valid:_return_the_same_version
=== RUN   TestCtestValidateStableVersion/invalid:_client_version_is_empty
=== RUN   TestCtestValidateStableVersion/invalid:_error_parsing_the_remote_version
=== RUN   TestCtestValidateStableVersion/invalid:_error_parsing_the_client_version
=== RUN   TestCtestValidateStableVersion/invalid:_both_versions_empty
--- PASS: TestCtestValidateStableVersion (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[1] (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[2] (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[3] (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_return_the_remote_version_as_it_is_part_of_the_same_release (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_return_the_same_version (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_client_version_is_empty (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_error_parsing_the_remote_version (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_error_parsing_the_client_version (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_both_versions_empty (0.00s)
=== RUN   TestCtestFetchFromURL
=== RUN   TestCtestFetchFromURL/normal_success
=== RUN   TestCtestFetchFromURL/HTTP_error_status
=== RUN   TestCtestFetchFromURL/Request_timeout
=== RUN   TestCtestFetchFromURL/Server_never_responds_within_timeout
--- PASS: TestCtestFetchFromURL (0.40s)
    --- PASS: TestCtestFetchFromURL/normal_success (0.00s)
    --- PASS: TestCtestFetchFromURL/HTTP_error_status (0.00s)
    --- PASS: TestCtestFetchFromURL/Request_timeout (0.20s)
    --- PASS: TestCtestFetchFromURL/Server_never_responds_within_timeout (0.20s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util	4.350s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient	2.532s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/certs		coverage: 0.0% of statements
=== RUN   TestCtestDefaultTaintsMarshaling

==================== CTEST EXTEND ONLY START ====================
Running 0 th test case.
{test_fixture.json [uninitialized nodeRegistration] nodeRegistration.taints [] {{ } [] false {  [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "nodeRegistration.taints" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"localAPIEndpoint":{},"nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:38]: Skipping test execution. No new configurations generated.
Running 1 th test case.
{test_fixture.json [empty taints slice] nodeRegistration.taints [] {{ } [] false {  [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "nodeRegistration.taints" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"localAPIEndpoint":{},"nodeRegistration":{"taints":[]}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:38]: Skipping test execution. No new configurations generated.
Running 2 th test case.
{test_fixture.json [custom taints] nodeRegistration.taints [] {{ } [] false {  [{taint1   <nil>} {taint2   <nil>}] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "nodeRegistration.taints" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"localAPIEndpoint":{},"nodeRegistration":{"taints":[{"effect":"","key":"taint1"},{"effect":"","key":"taint2"}]}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:38]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestDefaultTaintsMarshaling (0.01s)
=== RUN   TestCtestBytesToInitConfiguration

==================== CTEST EXTEND ONLY START ====================
Running 0 th test case.
{test_fixture.json [default config is set correctly] initConfiguration [] {{InitConfiguration kubeadm.k8s.io/v1beta4} [] false {  [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","kind":"InitConfiguration","localAPIEndpoint":{},"nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:85]: Skipping test execution. No new configurations generated.
Running 1 th test case.
{test_fixture.json [partial config with custom values] initConfiguration [] {{InitConfiguration kubeadm.k8s.io/v1beta4} [] false {test-node unix:///var/run/containerd/containerd.sock [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","kind":"InitConfiguration","localAPIEndpoint":{},"nodeRegistration":{"criSocket":"unix:///var/run/containerd/containerd.sock","name":"test-node","taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:85]: Skipping test execution. No new configurations generated.
Running 2 th test case.
{test_fixture.json [invalid configuration type] upgradeConfiguration [] {{UpgradeConfiguration kubeadm.k8s.io/v1beta4} { <nil> <nil> <nil> <nil> <nil> <nil> [] <nil> <nil> []  <nil>} { 0} {<nil> <nil> <nil> [] [] <nil>  <nil>} { <nil> <nil> <nil> <nil> [] <nil>} <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "upgradeConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","apply":{},"diff":{},"kind":"UpgradeConfiguration","node":{},"plan":{}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:85]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestBytesToInitConfiguration (0.01s)
=== RUN   TestCtestBytesToJoinConfiguration
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:29]: Start TestCtestBytesToJoinConfiguration
Running 0 th test case: Normal configuration
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [normal join config] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","caCertPath":"/some/cert.crt","discovery":{"bootstrapToken":{"apiServerEndpoint":"1.2.3.4:6443","caCertHashes":["aaaa"],"token":"abcdef.1234567890123456"},"tlsBootstrapToken":"abcdef.1234567890123456"},"kind":"JoinConfiguration","nodeRegistration":{"criSocket":"unix:///var/run/crio/crio.sock","name":"node-1","taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.
Running 1 th test case: Only contains Discovery configuration
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [discovery only join config] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","discovery":{"bootstrapToken":{"apiServerEndpoint":"1.2.3.4:6443","caCertHashes":["aaaa"],"token":"abcdef.1234567890123456"},"tlsBootstrapToken":"abcdef.1234567890123456"},"kind":"JoinConfiguration","nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.
Running 2 th test case: Edge – empty NodeRegistration
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [edge empty nodereg] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","discovery":{"bootstrapToken":{"apiServerEndpoint":"1.2.3.4:6443","caCertHashes":["aaaa"],"token":"abcdef.1234567890123456"},"tlsBootstrapToken":"abcdef.1234567890123456"},"kind":"JoinConfiguration","nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.
Running 3 th test case: Edge – nil Discovery
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [edge nil discovery] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/16 15:38:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:38:33 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","caCertPath":"/some/cert.crt","discovery":{},"kind":"JoinConfiguration","nodeRegistration":{"criSocket":"unix:///var/run/crio/crio.sock","name":"node-1","taints":null}})[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestBytesToJoinConfiguration (0.01s)
=== RUN   TestCtestLoadJoinConfigurationFromFile
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:174]: Start TestCtestLoadJoinConfigurationFromFile
Running 0 th file test case: Config file does not exist
Running 1 th file test case: Valid kubeadm config
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "fileContent" in requested fixtures
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:38:33 Mode: 1
2026/02/16 15:38:33 Base JSON size: 244 bytes
2026/02/16 15:38:33 Number of external values: 0
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="\napiVersion: kubeadm.k8s.io/v1beta4\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: 1.2.3.4:6443\n    caCertHashes:\n    - aaaa\n    token: abcdef.1234567890123456\n  tlsBootstrapToken: abcdef.1234567890123456\nkind: JoinConfiguration")[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:248]: No content generated
Running 2 th file test case: Edge – empty file
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "fileContent" in requested fixtures
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:38:33 Mode: 1
2026/02/16 15:38:33 Base JSON size: 2 bytes
2026/02/16 15:38:33 Number of external values: 0
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="")[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:248]: No content generated
Running 3 th file test case: Edge – malformed yaml
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "fileContent" in requested fixtures
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:38:33 Mode: 1
2026/02/16 15:38:33 Base JSON size: 30 bytes
2026/02/16 15:38:33 Number of external values: 0
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:38:33 [DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="apiVersion: ???\n: malformed")[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:248]: No content generated

==================== CTEST END ======================
--- PASS: TestCtestLoadJoinConfigurationFromFile (0.00s)
=== RUN   TestCtestBytesToUpgradeConfiguration
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:26]: Starting TestCtestBytesToUpgradeConfiguration
=== RUN   TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_JSON
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: default config is set correctly format: JSON
=== RUN   TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_YAML
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: default config is set correctly format: YAML
=== RUN   TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_JSON
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: cfg has part of fields configured format: JSON
=== RUN   TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_YAML
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: cfg has part of fields configured format: YAML
=== RUN   TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_JSON
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: no UpgradeConfiguration found format: JSON
  W0216 15:38:33.616515   88843 upgradeconfiguration.go:42] [config] WARNING: Ignored configuration document with GroupVersionKind kubeadm.k8s.io/v1beta4, Kind=InitConfiguration
=== RUN   TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_YAML
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: no UpgradeConfiguration found format: YAML
  W0216 15:38:33.616580   88843 upgradeconfiguration.go:42] [config] WARNING: Ignored configuration document with GroupVersionKind kubeadm.k8s.io/v1beta4, Kind=InitConfiguration
=== RUN   TestCtestBytesToUpgradeConfiguration/nil_cfg_value_JSON
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: nil cfg value format: JSON
=== RUN   TestCtestBytesToUpgradeConfiguration/nil_cfg_value_YAML
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: nil cfg value format: YAML
=== RUN   TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_JSON
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: empty UpgradeConfiguration struct format: JSON
    ctest_upgradeconfiguration_test.go:147: failed BytesToUpgradeConfiguration:
        	expected error: false
        	actual error: invalid configuration for GroupVersionKind /, Kind=: kind and apiVersion is mandatory information that must be specified
=== RUN   TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_YAML
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: empty UpgradeConfiguration struct format: YAML
    ctest_upgradeconfiguration_test.go:147: failed BytesToUpgradeConfiguration:
        	expected error: false
        	actual error: invalid configuration for GroupVersionKind /, Kind=: kind and apiVersion is mandatory information that must be specified
--- FAIL: TestCtestBytesToUpgradeConfiguration (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_YAML (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_YAML (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_YAML (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/nil_cfg_value_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/nil_cfg_value_YAML (0.00s)
    --- FAIL: TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_JSON (0.00s)
    --- FAIL: TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_YAML (0.00s)
=== RUN   TestCtestLoadUpgradeConfigurationFromFile
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:160]: Starting TestCtestLoadUpgradeConfigurationFromFile
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Config_file_does_not_exists
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Config file does not exists
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Valid_kubeadm_config
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Valid kubeadm config
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Empty_file
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Empty file
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Malformed_yaml
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Malformed yaml
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Wrong_kind
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Wrong kind
  W0216 15:38:33.617839   88843 upgradeconfiguration.go:42] [config] WARNING: Ignored configuration document with GroupVersionKind kubeadm.k8s.io/v1beta4, Kind=InitConfiguration
--- PASS: TestCtestLoadUpgradeConfigurationFromFile (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Config_file_does_not_exists (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Valid_kubeadm_config (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Empty_file (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Malformed_yaml (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Wrong_kind (0.00s)
=== RUN   TestCtestDefaultedUpgradeConfiguration
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:240]: Starting TestCtestDefaultedUpgradeConfiguration
=== RUN   TestCtestDefaultedUpgradeConfiguration/config_is_empty
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:369]: Running subtest: config is empty
=== RUN   TestCtestDefaultedUpgradeConfiguration/config_has_some_fields_configured
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:369]: Running subtest: config has some fields configured
=== RUN   TestCtestDefaultedUpgradeConfiguration/nil_config_pointer
[DEBUG-CTEST 2026-02-16 15:38:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:369]: Running subtest: nil config pointer
--- FAIL: TestCtestDefaultedUpgradeConfiguration (0.00s)
    --- PASS: TestCtestDefaultedUpgradeConfiguration/config_is_empty (0.00s)
    --- PASS: TestCtestDefaultedUpgradeConfiguration/config_has_some_fields_configured (0.00s)
    --- FAIL: TestCtestDefaultedUpgradeConfiguration/nil_config_pointer (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0xe0 pc=0x106bf8710]

goroutine 94 [running]:
testing.tRunner.func1.2({0x108971f00, 0x10afda2e0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x108971f00?, 0x10afda2e0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4.SetDefaults_UpgradeConfiguration(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta4/defaults.go:276 +0x90
k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4.SetObjectDefaults_UpgradeConfiguration(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta4/zz_generated.defaults.go:93 +0x94
k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4.RegisterDefaults.func5({0x108d72b40?, 0x0?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta4/zz_generated.defaults.go:36 +0x78
k8s.io/apimachinery/pkg/runtime.(*Scheme).Default(0x140000a5cf8?, {0x108eecb20, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apimachinery/pkg/runtime/scheme.go:356 +0x154
k8s.io/kubernetes/cmd/kubeadm/app/util/config.DefaultedUpgradeConfiguration(0x0, {0x48?, 0x40?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/upgradeconfiguration.go:141 +0xb8
k8s.io/kubernetes/cmd/kubeadm/app/util/config.TestCtestDefaultedUpgradeConfiguration.func1(0x140004b2e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:370 +0xe4
testing.tRunner(0x140004b2e00, 0x1400073a240)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 91
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/config	2.464s
=== RUN   TestCtestVerifyUnmarshalStrict
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:76]: Start strict unmarshal verification test
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:78]: total test cases: 22
=== RUN   TestCtestVerifyUnmarshalStrict/0-invalid_casesensitive_field.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_casesensitive_field.yaml ClusterConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/1-invalid_duplicate_field_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_clustercfg.yaml InitConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/2-invalid_duplicate_field_joincfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_joincfg.yaml JoinConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/3-invalid_duplicate_field_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/4-invalid_duplicate_field_kubeproxycfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_kubeproxycfg.yaml KubeProxyConfiguration {kubeproxy.config.k8s.io v1alpha1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/5-invalid_unknown_field_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_clustercfg.yaml ClusterConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/6-invalid_unknown_field_initcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_initcfg.yaml InitConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/7-invalid_unknown_field_joincfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_joincfg.yaml JoinConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/8-invalid_unknown_field_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/9-invalid_unknown_field_kubeproxycfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_kubeproxycfg.yaml KubeProxyConfiguration {kubeproxy.config.k8s.io v1alpha1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/10-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml ClusterConfiguration {someGroup v1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/11-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml SomeUnknownKind {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/12-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml ClusterConfiguration {kubeadm.k8s.io v1beta4} false}
=== RUN   TestCtestVerifyUnmarshalStrict/13-valid_initcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_initcfg.yaml InitConfiguration {kubeadm.k8s.io v1beta4} false}
=== RUN   TestCtestVerifyUnmarshalStrict/14-valid_joincfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_joincfg.yaml JoinConfiguration {kubeadm.k8s.io v1beta4} false}
=== RUN   TestCtestVerifyUnmarshalStrict/15-valid_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} false}
=== RUN   TestCtestVerifyUnmarshalStrict/16-valid_kubeproxycfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_kubeproxycfg.yaml KubeProxyConfiguration {kubeproxy.config.k8s.io v1alpha1} false}
=== RUN   TestCtestVerifyUnmarshalStrict/17-
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: { ClusterConfiguration {kubeadm.k8s.io v1beta4} true}
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:90]: expected read error: read testdata: is a directory
=== RUN   TestCtestVerifyUnmarshalStrict/18-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml  {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/19-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml ClusterConfiguration { } true}
=== RUN   TestCtestVerifyUnmarshalStrict/20-valid_initcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_initcfg.yaml NonExistentKind {unknownGroup v1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/21-valid_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-16 15:38:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} true}
    ctest_strict_test.go:100: expected error true, got false, error: <nil>

==================== CTEST END ======================
--- FAIL: TestCtestVerifyUnmarshalStrict (0.01s)
    --- PASS: TestCtestVerifyUnmarshalStrict/0-invalid_casesensitive_field.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/1-invalid_duplicate_field_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/2-invalid_duplicate_field_joincfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/3-invalid_duplicate_field_kubeletcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/4-invalid_duplicate_field_kubeproxycfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/5-invalid_unknown_field_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/6-invalid_unknown_field_initcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/7-invalid_unknown_field_joincfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/8-invalid_unknown_field_kubeletcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/9-invalid_unknown_field_kubeproxycfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/10-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/11-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/12-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/13-valid_initcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/14-valid_joincfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/15-valid_kubeletcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/16-valid_kubeproxycfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/17- (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/18-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/19-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/20-valid_initcfg.yaml (0.00s)
    --- FAIL: TestCtestVerifyUnmarshalStrict/21-valid_kubeletcfg.yaml (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/config/strict	4.050s
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/crypto	2.299s	coverage: 0.2% of statements in ./... [no tests to run]
=== RUN   TestCtestPrintDryRunFiles
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_empty
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_a_file_that_does_not_exist
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_a_readable_file
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_empty_and_PrintPath_empty_(both_empty)
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_a_readable_file_with_non‑empty_PrintPath
--- PASS: TestCtestPrintDryRunFiles (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_empty (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_a_file_that_does_not_exist (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_a_readable_file (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_empty_and_PrintPath_empty_(both_empty) (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_a_readable_file_with_non‑empty_PrintPath (0.00s)
=== RUN   TestCtestNewFileToPrint
=== RUN   TestCtestNewFileToPrint/TestNewFileToPrint
=== RUN   TestCtestNewFileToPrint/TestNewFileToPrint#01
=== RUN   TestCtestNewFileToPrint/TestNewFileToPrint#02
--- PASS: TestCtestNewFileToPrint (0.00s)
    --- PASS: TestCtestNewFileToPrint/TestNewFileToPrint (0.00s)
    --- PASS: TestCtestNewFileToPrint/TestNewFileToPrint#01 (0.00s)
    --- PASS: TestCtestNewFileToPrint/TestNewFileToPrint#02 (0.00s)
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/dryrun	2.200s	coverage: 0.5% of statements in ./...
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/errors	1.134s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/etcd	1.756s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestTagFromImage

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:38:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/image/ctest_image_test.go:39]: Total test cases: 22
Running test case input: ":latest" expected: "latest"
Running test case input: "repo@:digest" expected: ""
    ctest_image_test.go:44: TagFromImage("repo@:digest") = "digest", expected ""
Running test case input: "repo:tag@sha256:" expected: "tag"
    ctest_image_test.go:44: TagFromImage("repo:tag@sha256:") = "", expected "tag"
Running test case input: "kindest/node:v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: "v1.17.0"
Running test case input: "example.com/kindest/node:v1.17.0" expected: "v1.17.0"
Running test case input: "example.com/kindest/node:v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: "v1.17.0"
Running test case input: "example.com:3000/kindest/node" expected: ""
Running test case input: "" expected: ""
Running test case input: "repo@" expected: ""
Running test case input: "kindest/node:v1.17.0" expected: "v1.17.0"
Running test case input: "example.com/kindest/node" expected: ""
Running test case input: "example.com:3000/kindest/node:latest" expected: "latest"
Running test case input: "example.com:3000/kindest/node:v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: "v1.17.0"
Running test case input: "repo:" expected: ""
Running test case input: "repo:tag@invaliddigest" expected: "tag"
    ctest_image_test.go:44: TagFromImage("repo:tag@invaliddigest") = "", expected "tag"
Running test case input: "kindest/node:latest" expected: "latest"
Running test case input: "kindest/node@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: ""
Running test case input: "example.com/kindest/node:latest" expected: "latest"
Running test case input: "example.com:3000/kindest/node:v1.17.0" expected: "v1.17.0"
Running test case input: "example.com:3000/kindest/node@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: ""
Running test case input: "kindest/node" expected: ""
Running test case input: "example.com/kindest/node@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: ""

==================== CTEST END ======================
--- FAIL: TestCtestTagFromImage (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/image	3.568s
	k8s.io/kubernetes/cmd/kubeadm/app/util/initsystem		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/kubeconfig	2.246s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/output		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/patches	3.247s	coverage: 0.2% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/pkiutil	3.831s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/pkiutil/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/pubkeypin	2.964s	coverage: 0.0% of statements in ./... [no tests to run]
=== RUN   TestCtestNewContainerRuntime
=== RUN   TestCtestNewContainerRuntime/valid
=== RUN   TestCtestNewContainerRuntime/invalid:_new_runtime_service_fails
=== RUN   TestCtestNewContainerRuntime/invalid:_new_image_service_fails
=== RUN   TestCtestNewContainerRuntime/invalid:_both_services_fail
--- PASS: TestCtestNewContainerRuntime (0.00s)
    --- PASS: TestCtestNewContainerRuntime/valid (0.00s)
    --- PASS: TestCtestNewContainerRuntime/invalid:_new_runtime_service_fails (0.00s)
    --- PASS: TestCtestNewContainerRuntime/invalid:_new_image_service_fails (0.00s)
    --- PASS: TestCtestNewContainerRuntime/invalid:_both_services_fail (0.00s)
=== RUN   TestCtestIsRunning
=== RUN   TestCtestIsRunning/valid
=== RUN   TestCtestIsRunning/invalid:_runtime_status_fails
=== RUN   TestCtestIsRunning/invalid:_runtime_condition_status_not_'true'
=== RUN   TestCtestIsRunning/valid:_runtime_condition_type_does_not_match
=== RUN   TestCtestIsRunning/invalid:_nil_RuntimeStatus
    ctest_runtime_test.go:131: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/runtime/ctest_runtime_test.go:131
        	Error:      	Not equal: 
        	            	expected: true
        	            	actual  : false
        	Test:       	TestCtestIsRunning/invalid:_nil_RuntimeStatus
--- FAIL: TestCtestIsRunning (0.00s)
    --- PASS: TestCtestIsRunning/valid (0.00s)
    --- PASS: TestCtestIsRunning/invalid:_runtime_status_fails (0.00s)
    --- PASS: TestCtestIsRunning/invalid:_runtime_condition_status_not_'true' (0.00s)
    --- PASS: TestCtestIsRunning/valid:_runtime_condition_type_does_not_match (0.00s)
    --- FAIL: TestCtestIsRunning/invalid:_nil_RuntimeStatus (0.00s)
=== RUN   TestCtestListKubeContainers
=== RUN   TestCtestListKubeContainers/valid
=== RUN   TestCtestListKubeContainers/invalid:_list_pod_sandbox_fails
=== RUN   TestCtestListKubeContainers/valid:_empty_list_returns_empty_slice
--- PASS: TestCtestListKubeContainers (0.00s)
    --- PASS: TestCtestListKubeContainers/valid (0.00s)
    --- PASS: TestCtestListKubeContainers/invalid:_list_pod_sandbox_fails (0.00s)
    --- PASS: TestCtestListKubeContainers/valid:_empty_list_returns_empty_slice (0.00s)
=== RUN   TestCtestSandboxImage
=== RUN   TestCtestSandboxImage/valid
=== RUN   TestCtestSandboxImage/invalid:_runtime_status_fails
=== RUN   TestCtestSandboxImage/invalid:_no_config_JSON
=== RUN   TestCtestSandboxImage/invalid:_no_config
=== RUN   TestCtestSandboxImage/valid:_empty_sandboxImage_value
    ctest_runtime_test.go:246: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/runtime/ctest_runtime_test.go:246
        	Error:      	Not equal: 
        	            	expected: false
        	            	actual  : true
        	Test:       	TestCtestSandboxImage/valid:_empty_sandboxImage_value
--- FAIL: TestCtestSandboxImage (0.00s)
    --- PASS: TestCtestSandboxImage/valid (0.00s)
    --- PASS: TestCtestSandboxImage/invalid:_runtime_status_fails (0.00s)
    --- PASS: TestCtestSandboxImage/invalid:_no_config_JSON (0.00s)
    --- PASS: TestCtestSandboxImage/invalid:_no_config (0.00s)
    --- FAIL: TestCtestSandboxImage/valid:_empty_sandboxImage_value (0.00s)
=== RUN   TestCtestRemoveContainers
=== RUN   TestCtestRemoveContainers/valid
=== RUN   TestCtestRemoveContainers/valid:_two_containers
=== RUN   TestCtestRemoveContainers/invalid:_remove_pod_sandbox_fails
=== RUN   TestCtestRemoveContainers/invalid:_stop_pod_sandbox_fails
=== RUN   TestCtestRemoveContainers/valid:_nil_slice_(no_containers)
--- PASS: TestCtestRemoveContainers (0.00s)
    --- PASS: TestCtestRemoveContainers/valid (0.00s)
    --- PASS: TestCtestRemoveContainers/valid:_two_containers (0.00s)
    --- PASS: TestCtestRemoveContainers/invalid:_remove_pod_sandbox_fails (0.00s)
    --- PASS: TestCtestRemoveContainers/invalid:_stop_pod_sandbox_fails (0.00s)
    --- PASS: TestCtestRemoveContainers/valid:_nil_slice_(no_containers) (0.00s)
=== RUN   TestCtestPullImage
=== RUN   TestCtestPullImage/valid
=== RUN   TestCtestPullImage/invalid:_pull_image_fails
=== RUN   TestCtestPullImage/valid:_pull_image_succeeds_with_non‑empty_reference
--- PASS: TestCtestPullImage (0.00s)
    --- PASS: TestCtestPullImage/valid (0.00s)
    --- PASS: TestCtestPullImage/invalid:_pull_image_fails (0.00s)
    --- PASS: TestCtestPullImage/valid:_pull_image_succeeds_with_non‑empty_reference (0.00s)
=== RUN   TestCtestImageExists
=== RUN   TestCtestImageExists/valid
=== RUN   TestCtestImageExists/invalid:_image_status_fails
W0216 15:38:39.939704   88940 runtime.go:234] Failed to get image status, image: "", error: test
=== RUN   TestCtestImageExists/invalid:_nil_image_in_response
--- PASS: TestCtestImageExists (0.00s)
    --- PASS: TestCtestImageExists/valid (0.00s)
    --- PASS: TestCtestImageExists/invalid:_image_status_fails (0.00s)
    --- PASS: TestCtestImageExists/invalid:_nil_image_in_response (0.00s)
=== RUN   TestCtestIsExistingSocket
=== RUN   TestCtestIsExistingSocket/Valid_domain_socket_is_detected_as_such
=== RUN   TestCtestIsExistingSocket/Regular_file_is_not_a_domain_socket
=== RUN   TestCtestIsExistingSocket/Non_existent_socket_is_not_a_domain_socket
=== RUN   TestCtestIsExistingSocket/Empty_string_is_not_a_socket
--- PASS: TestCtestIsExistingSocket (0.00s)
    --- PASS: TestCtestIsExistingSocket/Valid_domain_socket_is_detected_as_such (0.00s)
    --- PASS: TestCtestIsExistingSocket/Regular_file_is_not_a_domain_socket (0.00s)
    --- PASS: TestCtestIsExistingSocket/Non_existent_socket_is_not_a_domain_socket (0.00s)
    --- PASS: TestCtestIsExistingSocket/Empty_string_is_not_a_socket (0.00s)
=== RUN   TestCtestDetectCRISocketImpl
=== RUN   TestCtestDetectCRISocketImpl/No_existing_sockets,_use_default
=== RUN   TestCtestDetectCRISocketImpl/One_valid_CRI_socket_leads_to_success
=== RUN   TestCtestDetectCRISocketImpl/Multiple_CRI_sockets_lead_to_an_error
=== RUN   TestCtestDetectCRISocketImpl/Invalid_socket_format_should_error
    ctest_runtime_test.go:504: detectCRISocketImpl returned unexpected result
        	Expected error: true
        	Got error: false
--- FAIL: TestCtestDetectCRISocketImpl (0.00s)
    --- PASS: TestCtestDetectCRISocketImpl/No_existing_sockets,_use_default (0.00s)
    --- PASS: TestCtestDetectCRISocketImpl/One_valid_CRI_socket_leads_to_success (0.00s)
    --- PASS: TestCtestDetectCRISocketImpl/Multiple_CRI_sockets_lead_to_an_error (0.00s)
    --- FAIL: TestCtestDetectCRISocketImpl/Invalid_socket_format_should_error (0.00s)
=== RUN   TestCtestPullImagesInParallel
=== RUN   TestCtestPullImagesInParallel/valid
=== RUN   TestCtestPullImagesInParallel/valid:_ifNotPresent_is_true
=== RUN   TestCtestPullImagesInParallel/invalid:_pull_fails
=== RUN   TestCtestPullImagesInParallel/invalid:_ifNotPresent_true_and_pull_fails
--- PASS: TestCtestPullImagesInParallel (0.00s)
    --- PASS: TestCtestPullImagesInParallel/valid (0.00s)
    --- PASS: TestCtestPullImagesInParallel/valid:_ifNotPresent_is_true (0.00s)
    --- PASS: TestCtestPullImagesInParallel/invalid:_pull_fails (0.00s)
    --- PASS: TestCtestPullImagesInParallel/invalid:_ifNotPresent_true_and_pull_fails (0.00s)
FAIL
coverage: 0.4% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/runtime	2.526s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/staticpod	1.377s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/users		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/test		coverage: 0.0% of statements
=== RUN   TestCtestCmdCompletion

==================== CTEST START ====================
--- FAIL: TestCtestCmdCompletion (0.00s)
panic: the environment variable KUBEADM_PATH must point to the kubeadm binary path [recovered]
	panic: the environment variable KUBEADM_PATH must point to the kubeadm binary path

goroutine 84 [running]:
testing.tRunner.func1.2({0x103625600, 0x103ba39f0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x103625600?, 0x103ba39f0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/test/cmd.getKubeadmPath(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/test/cmd/util.go:80
k8s.io/kubernetes/cmd/kubeadm/test/cmd.TestCtestCmdCompletion(0x14000504a80)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/test/cmd/ctest_completion_test.go:12 +0x5b0
testing.tRunner(0x14000504a80, 0x103b9c488)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/test/cmd	0.680s
	k8s.io/kubernetes/cmd/kubeadm/test/kubeconfig		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/test/resources		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubectl		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubectl-convert		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubelet		coverage: 0.0% of statements
=== RUN   TestCtestValueOfAllocatableResources
    ctest_server_test.go:87: negative zero quantities: error expected
--- FAIL: TestCtestValueOfAllocatableResources (0.00s)
=== RUN   TestCtestMergeKubeletConfigurations
=== RUN   TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf
=== RUN   TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_subfield_override
=== RUN   TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_slices/lists
=== RUN   TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf.d
=== RUN   TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf
=== RUN   TestCtestMergeKubeletConfigurations/json_conversion_is_correct
=== RUN   TestCtestMergeKubeletConfigurations/invalid_drop-in_apiVersion
=== RUN   TestCtestMergeKubeletConfigurations/invalid_drop-in_kind
=== RUN   TestCtestMergeKubeletConfigurations/empty_drop-in_apiVersion/kind
=== RUN   TestCtestMergeKubeletConfigurations/identical_kubelet_config_and_drop-in_file
--- PASS: TestCtestMergeKubeletConfigurations (0.02s)
    --- PASS: TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_subfield_override (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_slices/lists (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf.d (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/json_conversion_is_correct (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/invalid_drop-in_apiVersion (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/invalid_drop-in_kind (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/empty_drop-in_apiVersion/kind (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/identical_kubelet_config_and_drop-in_file (0.00s)
FAIL
coverage: 1.0% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubelet/app	2.007s
=== RUN   TestCtestRoundTrip
    ctest_options_test.go:101: default flags are eliminated: args: []
    ctest_options_test.go:101: default flag values round trip: args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address=127.0.0.1 --healthz-port=10248 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-high-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-low-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-service-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kernel-memcg-notification has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-content-type has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --local-storage-capacity-isolation has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-info-buffer-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-split-stream has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --logging-format has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --make-iptables-util-chains has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --manifest-url has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-open-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --maximum-dead-containers has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --maximum-dead-containers-per-container has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --memory-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --minimum-image-ttl-duration has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-max-images has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --oom-score-adj has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-cidr has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-max-pids has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pods-per-core has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --protect-kernel-defaults has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-node has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-memory has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-server-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runonce has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --streaming-connection-idle-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --sync-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-scope has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-stats-agg-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
    ctest_options_test.go:101: nil address does not fail for optional argument: args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address= --healthz-port=10248 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-high-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-low-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-service-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kernel-memcg-notification has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-content-type has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --local-storage-capacity-isolation has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-info-buffer-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-split-stream has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --logging-format has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --make-iptables-util-chains has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --manifest-url has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-open-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --maximum-dead-containers has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --maximum-dead-containers-per-container has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --memory-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --minimum-image-ttl-duration has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-max-images has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --oom-score-adj has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-cidr has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-max-pids has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pods-per-core has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --protect-kernel-defaults has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-node has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-memory has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-server-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runonce has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --streaming-connection-idle-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --sync-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-scope has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-stats-agg-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
    ctest_options_test.go:101: excessively long healthz bind address: args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address=                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 --healthz-port=10248 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
    ctest_options_test.go:107: excessively long healthz bind address: unexpected flag error: invalid argument "\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" for "--healthz-bind-address" flag: "\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" is not a valid IP address
    ctest_options_test.go:101: invalid numeric flag value (negative port): args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address=127.0.0.1 --healthz-port=-1 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-high-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-low-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-service-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kernel-memcg-notification has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-content-type has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --local-storage-capacity-isolation has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-info-buffer-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-split-stream has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --logging-format has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --make-iptables-util-chains has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --manifest-url has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-open-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --maximum-dead-containers has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --maximum-dead-containers-per-container has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --memory-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --minimum-image-ttl-duration has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-max-images has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --oom-score-adj has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-cidr has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-max-pids has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pods-per-core has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --protect-kernel-defaults has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-node has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-memory has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-server-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runonce has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --streaming-connection-idle-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --sync-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-scope has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-stats-agg-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
--- FAIL: TestCtestRoundTrip (0.00s)
=== RUN   TestCtestValidateKubeletFlags
=== RUN   TestCtestValidateKubeletFlags/Invalid_kubernetes.io_label
=== RUN   TestCtestValidateKubeletFlags/Valid_label_outside_of_kubernetes.io_and_k8s.io
=== RUN   TestCtestValidateKubeletFlags/Empty_label_list
=== RUN   TestCtestValidateKubeletFlags/Invalid_label
=== RUN   TestCtestValidateKubeletFlags/Empty_label_key
=== RUN   TestCtestValidateKubeletFlags/Label_key_with_invalid_characters
    ctest_options_test.go:186: ValidateKubeletFlags should have failed with labels: map[invalid/key:value]
=== RUN   TestCtestValidateKubeletFlags/Excessively_long_label_key_and_value
--- FAIL: TestCtestValidateKubeletFlags (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Invalid_kubernetes.io_label (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Valid_label_outside_of_kubernetes.io_and_k8s.io (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Empty_label_list (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Invalid_label (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Empty_label_key (0.00s)
    --- FAIL: TestCtestValidateKubeletFlags/Label_key_with_invalid_characters (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Excessively_long_label_key_and_value (0.00s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/cmd/kubelet/app/options	2.558s
	k8s.io/kubernetes/cmd/kubemark		coverage: 0.0% of statements
=== RUN   TestCtestHollowNode
=== RUN   TestCtestHollowNode/proxy
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0216 15:38:50.165766   89056 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
I0216 15:38:50.166590   89056 server.go:527] "Version info" version="v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79"
I0216 15:38:50.166600   89056 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0216 15:38:50.166656   89056 server.go:538] "Failed to apply OOMScore" err="setting OOM scores is unsupported in this build"
I0216 15:38:50.166911   89056 config.go:200] "Starting service config controller"
I0216 15:38:50.166951   89056 config.go:106] "Starting endpoint slice config controller"
I0216 15:38:50.166962   89056 config.go:403] "Starting serviceCIDR config controller"
I0216 15:38:50.167003   89056 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0216 15:38:50.167024   89056 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0216 15:38:50.167035   89056 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0216 15:38:50.167453   89056 reflector.go:358] "Starting reflector" type="*v1.EndpointSlice" resyncPeriod="30s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:50.167481   89056 reflector.go:404] "Listing and watching" type="*v1.EndpointSlice" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:50.167569   89056 reflector.go:358] "Starting reflector" type="*v1.Service" resyncPeriod="30s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:50.167578   89056 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:50.167665   89056 reflector.go:358] "Starting reflector" type="*v1.ServiceCIDR" resyncPeriod="30s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:50.167692   89056 reflector.go:404] "Listing and watching" type="*v1.ServiceCIDR" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:50.171335   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0216 15:38:50.171442   89056 event_broadcaster.go:279] "Unable to write event (may retry after sleeping)" err="serializer for text/plain; charset=utf-8 doesn't exist"
E0216 15:38:50.171516   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.ServiceCIDR: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0216 15:38:50.171704   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.EndpointSlice: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
I0216 15:38:51.240051   89056 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:51.240466   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
I0216 15:38:51.415064   89056 reflector.go:404] "Listing and watching" type="*v1.EndpointSlice" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:51.415350   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.EndpointSlice: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
I0216 15:38:51.605275   89056 reflector.go:404] "Listing and watching" type="*v1.ServiceCIDR" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:51.605574   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.ServiceCIDR: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
I0216 15:38:53.142044   89056 reflector.go:404] "Listing and watching" type="*v1.EndpointSlice" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:53.142423   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.EndpointSlice: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
    ctest_hollow_node_test.go:74: Morph "proxy" hasn't crashed for 3s. Calling success.
=== RUN   TestCtestHollowNode/kubelet
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0216 15:38:53.166800   89056 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
I0216 15:38:53.167934   89056 hollow_kubelet.go:152] Using /tmp/hollow-kubelet.4108436115 as root dir for hollow-kubelet
I0216 15:38:53.168142   89056 feature_gate.go:385] feature gates: {map[]}
I0216 15:38:53.168342   89056 logging.go:31] "[core] [Server #1]Server created\n"
I0216 15:38:53.168624   89056 log.go:25] "Connecting to runtime service" endpoint="unix:///tmp/kubelet_remote_2168930015020025273.sock"
I0216 15:38:53.168696   89056 clientconn.go:1678] "[core] original dial target is: \"/tmp/kubelet_remote_2168930015020025273.sock\"\n"
I0216 15:38:53.168735   89056 clientconn.go:333] "[core] [Channel #3]Channel created\n"
I0216 15:38:53.168749   89056 logging.go:31] "[core] [Server #1 ListenSocket #2]ListenSocket created\n"
I0216 15:38:53.168750   89056 logging.go:39] "[core] [Channel #3]parsed dial target is: resolver.Target{URL:url.URL{Scheme:\"passthrough\", Opaque:\"\", User:(*url.Userinfo)(nil), Host:\"\", Path:\"//tmp/kubelet_remote_2168930015020025273.sock\", RawPath:\"\", OmitHost:false, ForceQuery:false, RawQuery:\"\", Fragment:\"\", RawFragment:\"\"}}\n"
I0216 15:38:53.168792   89056 logging.go:39] "[core] [Channel #3]Channel authority set to \"localhost\"\n"
I0216 15:38:53.168922   89056 logging.go:39] "[core] [Channel #3]Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/tmp/kubelet_remote_2168930015020025273.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/tmp/kubelet_remote_2168930015020025273.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
I0216 15:38:53.168935   89056 logging.go:39] "[core] [Channel #3]Channel switches to new LB policy \"pick_first\"\n"
I0216 15:38:53.168986   89056 pickfirstleaf.go:279] "[pick-first-leaf-lb] [pick-first-leaf-lb 0x140006d0000] Received new config {\n  \"shuffleAddressList\": false\n}, resolver state {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/tmp/kubelet_remote_2168930015020025273.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/tmp/kubelet_remote_2168930015020025273.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n}\n"
I0216 15:38:53.168997   89056 logging.go:39] "[core] [Channel #3]Channel Connectivity change to CONNECTING\n"
I0216 15:38:53.169006   89056 clientconn.go:857] "[core] [Channel #3 SubChannel #4]Subchannel created\n"
I0216 15:38:53.169016   89056 clientconn.go:333] "[core] [Channel #3]Channel exiting idle mode\n"
I0216 15:38:53.169020   89056 log.go:25] "Validating the CRI v1 API runtime version"
I0216 15:38:53.169098   89056 logging.go:39] "[core] [Channel #3 SubChannel #4]Subchannel Connectivity change to CONNECTING\n"
I0216 15:38:53.169126   89056 logging.go:39] "[core] [Channel #3 SubChannel #4]Subchannel picks a new address \"/tmp/kubelet_remote_2168930015020025273.sock\" to connect\n"
I0216 15:38:53.169420   89056 syscall_nonlinux.go:39] "[core] CPU time info is unavailable on non-linux environments.\n"
I0216 15:38:53.169515   89056 logging.go:39] "[core] [Channel #3 SubChannel #4]Subchannel Connectivity change to READY\n"
I0216 15:38:53.169559   89056 pickfirstleaf.go:633] "[pick-first-leaf-lb] [pick-first-leaf-lb 0x140006d0000] SubConn 0x1400035e9b0 reported connectivity state READY and the health listener is disabled. Transitioning SubConn to READY.\n"
I0216 15:38:53.169579   89056 logging.go:39] "[core] [Channel #3]Channel Connectivity change to READY\n"
I0216 15:38:53.170135   89056 log.go:25] "Validated CRI v1 runtime API"
    server.go:1234: I0216 15:38:53.170236] Using root directory path="/tmp/hollow-kubelet.4108436115"
I0216 15:38:53.170418   89056 kubelet.go:475] "Attempting to sync node with API server"
I0216 15:38:53.170440   89056 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:53.170444   89056 kubelet.go:376] "Adding static pod path" path="/tmp/hollow-kubelet.4108436115/static-pods1705250017"
I0216 15:38:53.170450   89056 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:53.170613   89056 file.go:68] "Watching path" path="/tmp/hollow-kubelet.4108436115/static-pods1705250017"
E0216 15:38:53.170635   89056 file_unsupported.go:29] "Watching source file is unsupported in this build"
I0216 15:38:53.170639   89056 kubelet.go:387] "Adding apiserver pod source"
I0216 15:38:53.170646   89056 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0216 15:38:53.170807   89056 state_mem.go:40] "Initialized new in-memory state store for pod resource information tracking"
I0216 15:38:53.170827   89056 state_mem.go:40] "Initialized new in-memory state store for pod resource information tracking"
I0216 15:38:53.170851   89056 reflector.go:358] "Starting reflector" type="*v1.Service" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:53.170844   89056 config.go:292] "Setting pods for source" source="file"
I0216 15:38:53.170862   89056 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:53.171049   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0216 15:38:53.171064   89056 state_mem.go:36] "Initialized new in-memory state store"
I0216 15:38:53.171105   89056 fake_topology_manager.go:33] "NewFakeManager"
E0216 15:38:53.171141   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
    kuberuntime_manager.go:291: I0216 15:38:53.171313] Container runtime initialized containerRuntime="fakeRuntime" version="0.1.0" apiVersion="0.1.0"
I0216 15:38:53.171490   89056 kubelet.go:940] "Not starting ClusterTrustBundle informer because we are in static kubelet mode or the ClusterTrustBundleProjection featuregate is disabled"
I0216 15:38:53.171505   89056 kubelet.go:964] "Not starting PodCertificateRequest manager because we are in static kubelet mode or the PodCertificateProjection feature gate is disabled"
I0216 15:38:53.171578   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
I0216 15:38:53.171584   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
I0216 15:38:53.171588   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
I0216 15:38:53.171591   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
I0216 15:38:53.171594   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/secret"
I0216 15:38:53.171609   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
I0216 15:38:53.171613   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
I0216 15:38:53.171625   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/fc"
I0216 15:38:53.171632   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
I0216 15:38:53.171638   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/projected"
I0216 15:38:53.171656   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
I0216 15:38:53.171660   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
I0216 15:38:53.171697   89056 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:38:53.171787   89056 fake_topology_manager.go:33] "NewFakeManager"
I0216 15:38:53.172112   89056 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0216 15:38:53.176139   89056 kubelet_pods.go:148] "user namespaces: user not found, using default mappings" user="kubelet"
I0216 15:38:53.176260   89056 userns_manager.go:157] "User namespace manager mapping" offset=1 length=65535 idsPerPod=65536
    server.go:1258: E0216 15:38:53.176384] Failed to set rlimit on max file handles err="SetRLimit unsupported in this platform"
    server.go:1262: I0216 15:38:53.176436] Started kubelet
I0216 15:38:53.176683   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={}
I0216 15:38:53.176967   89056 server.go:180] "Starting to listen" address="0.0.0.0" port=0
I0216 15:38:53.177053   89056 healthz.go:208] Installing health checkers for (/healthz): "ping","log","syncloop"
E0216 15:38:53.177197   89056 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
    ratelimit.go:56: I0216 15:38:53.177273] Setting rate limiting for endpoint service="podresources" qps=<rate.Limit>: 100 burstTokens=10
I0216 15:38:53.177309   89056 logging.go:31] "[core] [Server #7]Server created\n"
    server_v1.go:49: I0216 15:38:53.177329] podresources method="list" useActivePods=true
I0216 15:38:53.177442   89056 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0216 15:38:53.177514   89056 status_manager.go:244] "Starting to sync pod status with apiserver"
E0216 15:38:53.177574   89056 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0216 15:38:53.177593   89056 kubelet.go:2428] "Starting kubelet main sync loop"
E0216 15:38:53.177817   89056 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"kellys-mbp-2\" not found"
I0216 15:38:53.177852   89056 server.go:249] "Starting to serve the podresources API" endpoint="unix:/tmp/hollow-kubelet.4108436115/pod-resources/kubelet.sock"
I0216 15:38:53.177864   89056 logging.go:31] "[core] [Server #7 ListenSocket #8]ListenSocket created\n"
I0216 15:38:53.177866   89056 volume_manager.go:311] "The desired_state_of_world populator starts"
I0216 15:38:53.177874   89056 volume_manager.go:313] "Starting Kubelet Volume Manager"
I0216 15:38:53.178045   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0216 15:38:53.178096   89056 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
E0216 15:38:53.178185   89056 kubelet.go:1615] "Image garbage collection failed once. Stats initialization may not have completed yet" err="imageFs information is unavailable"
I0216 15:38:53.178296   89056 reflector.go:358] "Starting reflector" type="*v1.RuntimeClass" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:53.178305   89056 reflector.go:404] "Listing and watching" type="*v1.RuntimeClass" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:53.178355   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.178369   89056 reconstruct.go:97] "Volume reconstruction finished"
E0216 15:38:53.178407   89056 kubelet.go:2452] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
I0216 15:38:53.177593   89056 server.go:310] "Adding debug handlers to kubelet server"
I0216 15:38:53.178377   89056 reconciler.go:29] "Reconciler: start to sync state"
E0216 15:38:53.178497   89056 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="200ms"
I0216 15:38:53.178503   89056 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:38:53.178511   89056 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:53.178510   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
    generic.go:240: I0216 15:38:53.178695] GenericPLEG: Relisting
I0216 15:38:53.178805   89056 kubelet.go:3005] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:; Handlers: , Features: nil"
I0216 15:38:53.178843   89056 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0216 15:38:53.178860   89056 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
E0216 15:38:53.178848   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0216 15:38:53.179053   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0216 15:38:53.179133   89056 kubelet.go:1593] "Container garbage collection succeeded"
I0216 15:38:53.179135   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0216 15:38:53.221292   89056 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0216 15:38:53.221333   89056 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0216 15:38:53.221686   89056 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0216 15:38:53.221693   89056 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0216 15:38:53.221696   89056 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0216 15:38:53.221698   89056 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0216 15:38:53.221702   89056 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0216 15:38:53.221740   89056 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0216 15:38:53.221755   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0216 15:38:53.221759   89056 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0216 15:38:53.221762   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0216 15:38:53.221770   89056 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0216 15:38:53.221775   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0216 15:38:53.221778   89056 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0216 15:38:53.221788   89056 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0216 15:38:53.221805   89056 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0216 15:38:53.221812   89056 container_manager_stub.go:52] "Starting stub container manager"
I0216 15:38:53.221830   89056 state_mem.go:36] "Initializing new in-memory state store" logger="memory-mgr.fake"
I0216 15:38:53.221851   89056 eviction_manager.go:189] "Eviction manager: starting control loop"
I0216 15:38:53.221878   89056 container_log_manager.go:146] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
I0216 15:38:53.221887   89056 kubelet.go:1722] "Starting plugin manager"
I0216 15:38:53.221912   89056 eviction_manager.go:251] "Eviction manager: synchronize housekeeping"
I0216 15:38:53.221938   89056 container_log_manager.go:186] "Starting container log rotation worker" workerID=1
I0216 15:38:53.221943   89056 container_log_manager.go:195] "Starting container log rotation sequence"
E0216 15:38:53.221963   89056 eviction_manager.go:259] "Eviction manager: failed to get HasDedicatedImageFs" err="imagefs device is not found"
E0216 15:38:53.221976   89056 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: imagefs device is not found"
I0216 15:38:53.221983   89056 plugin_watcher.go:51] "Plugin Watcher Start" path="/tmp/hollow-kubelet.4108436115/plugins_registry"
I0216 15:38:53.221989   89056 plugin_watcher.go:100] "Ensuring Plugin directory" path="/tmp/hollow-kubelet.4108436115/plugins_registry"
I0216 15:38:53.222070   89056 kubelet.go:3005] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:; Handlers: , Features: nil"
I0216 15:38:53.222096   89056 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
I0216 15:38:53.222099   89056 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0216 15:38:53.278914   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={}
E0216 15:38:53.278980   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.279001   89056 kubelet.go:2515] "SyncLoop ADD" source="file" pods=[]
I0216 15:38:53.322959   89056 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0216 15:38:53.322986   89056 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0216 15:38:53.324881   89056 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0216 15:38:53.324895   89056 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0216 15:38:53.325160   89056 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0216 15:38:53.325167   89056 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0216 15:38:53.325170   89056 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0216 15:38:53.325172   89056 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0216 15:38:53.325175   89056 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0216 15:38:53.325179   89056 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0216 15:38:53.325184   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0216 15:38:53.325189   89056 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0216 15:38:53.325192   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0216 15:38:53.325197   89056 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0216 15:38:53.325202   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0216 15:38:53.325204   89056 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0216 15:38:53.325211   89056 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0216 15:38:53.325217   89056 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0216 15:38:53.325221   89056 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0216 15:38:53.326126   89056 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
E0216 15:38:53.379106   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.379121   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:53.379517   89056 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="400ms"
I0216 15:38:53.479003   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:53.479036   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.526238   89056 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0216 15:38:53.526279   89056 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0216 15:38:53.528207   89056 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0216 15:38:53.528233   89056 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0216 15:38:53.528615   89056 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0216 15:38:53.528623   89056 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0216 15:38:53.528630   89056 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0216 15:38:53.528635   89056 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0216 15:38:53.528642   89056 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0216 15:38:53.528649   89056 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0216 15:38:53.528659   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0216 15:38:53.528667   89056 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0216 15:38:53.528674   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0216 15:38:53.528681   89056 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0216 15:38:53.528687   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0216 15:38:53.528694   89056 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0216 15:38:53.528707   89056 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0216 15:38:53.528715   89056 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0216 15:38:53.528723   89056 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0216 15:38:53.529203   89056 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
E0216 15:38:53.578750   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.578751   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:53.681273   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:53.681345   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.778319   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:53.778370   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:53.780168   89056 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="800ms"
E0216 15:38:53.879058   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.879075   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:53.934813   89056 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0216 15:38:53.934864   89056 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0216 15:38:53.937469   89056 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0216 15:38:53.937497   89056 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0216 15:38:53.937959   89056 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0216 15:38:53.937969   89056 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0216 15:38:53.937975   89056 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0216 15:38:53.937979   89056 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0216 15:38:53.937986   89056 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0216 15:38:53.937991   89056 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0216 15:38:53.938000   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0216 15:38:53.938006   89056 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0216 15:38:53.938011   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0216 15:38:53.938017   89056 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0216 15:38:53.938023   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0216 15:38:53.938029   89056 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0216 15:38:53.938041   89056 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0216 15:38:53.938049   89056 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0216 15:38:53.938056   89056 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0216 15:38:53.938809   89056 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0216 15:38:53.978171   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:53.978438   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:53.989694   89056 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:53.990465   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0216 15:38:53.999686   89056 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:54.000234   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
I0216 15:38:54.079019   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.079069   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.130740   89056 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:54.131789   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
I0216 15:38:54.171123   89056 apiserver.go:50] "node sync has not completed yet"
I0216 15:38:54.176843   89056 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0216 15:38:54.178848   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.179686   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
    generic.go:240: I0216 15:38:54.179729] GenericPLEG: Relisting
I0216 15:38:54.180133   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0216 15:38:54.211044   89056 reflector.go:404] "Listing and watching" type="*v1.ServiceCIDR" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:54.211546   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.ServiceCIDR: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
I0216 15:38:54.278207   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.278438   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.382156   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.384616   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.443958   89056 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:54.444423   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0216 15:38:54.478528   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.478528   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:54.579050   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.579098   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.580712   89056 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="1.6s"
I0216 15:38:54.631589   89056 reflector.go:404] "Listing and watching" type="*v1.RuntimeClass" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:54.634594   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
I0216 15:38:54.678692   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.678750   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.739907   89056 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0216 15:38:54.739946   89056 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0216 15:38:54.742071   89056 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0216 15:38:54.742101   89056 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0216 15:38:54.742406   89056 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0216 15:38:54.742416   89056 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0216 15:38:54.742422   89056 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0216 15:38:54.742434   89056 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0216 15:38:54.742441   89056 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0216 15:38:54.742447   89056 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0216 15:38:54.742457   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0216 15:38:54.742469   89056 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0216 15:38:54.742477   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0216 15:38:54.742490   89056 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0216 15:38:54.742495   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0216 15:38:54.742501   89056 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0216 15:38:54.742514   89056 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0216 15:38:54.742523   89056 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0216 15:38:54.742531   89056 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0216 15:38:54.742931   89056 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0216 15:38:54.779972   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.779982   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.879631   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.879702   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:54.978280   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:54.978445   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:55.079992   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:55.081010   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:55.171368   89056 apiserver.go:50] "node sync has not completed yet"
I0216 15:38:55.172667   89056 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0216 15:38:55.178143   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:55.178161   89056 kubelet.go:2609] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
I0216 15:38:55.178167   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:55.178454   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
    generic.go:240: I0216 15:38:55.180414] GenericPLEG: Relisting
I0216 15:38:55.180826   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
E0216 15:38:55.279022   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:55.279089   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:55.378214   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:55.378445   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:55.479077   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:55.479168   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:55.579054   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:55.579079   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:55.678436   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:55.678491   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:55.778380   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:55.778778   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:55.879974   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:55.881124   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:55.979045   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:55.979072   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:56.079068   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.079123   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
    ctest_hollow_node_test.go:74: Morph "kubelet" hasn't crashed for 3s. Calling success.
=== RUN   TestCtestHollowNode/#00
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0216 15:38:56.168660   89056 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
    ctest_hollow_node_test.go:71: Run finished unexpectedly with error for morph "": Unknown morph: . allowed values: [kubelet proxy]
=== RUN   TestCtestHollowNode/invalid-morph
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0216 15:38:56.168793   89056 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
    ctest_hollow_node_test.go:71: Run finished unexpectedly with error for morph "invalid-morph": Unknown morph: invalid-morph. allowed values: [kubelet proxy]
=== RUN   TestCtestHollowNode/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0216 15:38:56.168872   89056 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
    ctest_hollow_node_test.go:71: Run finished unexpectedly with error for morph "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa": Unknown morph: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa. allowed values: [kubelet proxy]
--- FAIL: TestCtestHollowNode (6.01s)
    --- PASS: TestCtestHollowNode/proxy (3.00s)
    --- PASS: TestCtestHollowNode/kubelet (3.00s)
    --- FAIL: TestCtestHollowNode/#00 (0.00s)
    --- FAIL: TestCtestHollowNode/invalid-morph (0.00s)
    --- FAIL: TestCtestHollowNode/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (0.00s)
FAIL
I0216 15:38:56.171614   89056 apiserver.go:50] "node sync has not completed yet"
I0216 15:38:56.172940   89056 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: Get "https://127.0.0.1:65273/apis/storage.k8s.io/v1/csinodes/kellys-mbp-2?resourceVersion=0": dial tcp 127.0.0.1:65273: connect: connection refused
I0216 15:38:56.178262   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.178512   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.181169   89056 generic.go:240] "GenericPLEG: Relisting" logger="TestCtestHollowNode/kubelet leaked goroutine"
E0216 15:38:56.181539   89056 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://127.0.0.1:65273/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/kellys-mbp-2?timeout=10s\": dial tcp 127.0.0.1:65273: connect: connection refused" interval="3.2s"
I0216 15:38:56.181904   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0216 15:38:56.279178   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.279262   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
coverage: 2.2% of statements in ./...
I0216 15:38:56.343586   89056 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0216 15:38:56.343621   89056 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0216 15:38:56.346123   89056 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0216 15:38:56.346143   89056 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0216 15:38:56.346787   89056 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0216 15:38:56.346794   89056 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0216 15:38:56.346797   89056 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0216 15:38:56.346799   89056 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0216 15:38:56.346802   89056 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0216 15:38:56.346805   89056 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0216 15:38:56.346810   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0216 15:38:56.346814   89056 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0216 15:38:56.346817   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0216 15:38:56.346820   89056 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0216 15:38:56.346824   89056 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0216 15:38:56.346827   89056 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0216 15:38:56.346834   89056 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0216 15:38:56.346839   89056 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0216 15:38:56.346843   89056 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0216 15:38:56.347451   89056 kubelet_node_status.go:107] "Unable to register node with API server" err="Post \"https://127.0.0.1:65273/api/v1/nodes\": dial tcp 127.0.0.1:65273: connect: connection refused" node="kellys-mbp-2"
I0216 15:38:56.378177   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.378509   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.478295   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.478569   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.495528   89056 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:56.496287   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://127.0.0.1:65273/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 127.0.0.1:65273: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0216 15:38:56.578198   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.578449   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:56.679799   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.679929   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:56.779461   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.780037   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.879043   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:56.879102   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0216 15:38:56.979043   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:56.979100   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:57.079046   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:57.079066   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:57.128272   89056 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:57.128734   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: Get \"https://127.0.0.1:65273/api/v1/nodes?fieldSelector=metadata.name%3Dkellys-mbp-2&limit=500&resourceVersion=0\": dial tcp 127.0.0.1:65273: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0216 15:38:57.163298   89056 reflector.go:404] "Listing and watching" type="*v1.RuntimeClass" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:57.163817   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: Get \"https://127.0.0.1:65273/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 127.0.0.1:65273: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
I0216 15:38:57.172952   89056 apiserver.go:50] "node sync has not completed yet"
I0216 15:38:57.182411   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:57.182445   89056 kubelet.go:2609] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
I0216 15:38:57.182555   89056 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: Get "https://127.0.0.1:65273/apis/storage.k8s.io/v1/csinodes/kellys-mbp-2?resourceVersion=0": dial tcp 127.0.0.1:65273: connect: connection refused
I0216 15:38:57.182676   89056 generic.go:240] "GenericPLEG: Relisting" logger="TestCtestHollowNode/kubelet leaked goroutine"
I0216 15:38:57.183153   89056 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0216 15:38:57.183195   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:57.184104   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:57.197670   89056 reflector.go:404] "Listing and watching" type="*v1.EndpointSlice" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:57.198271   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://127.0.0.1:65273/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&limit=500&resourceVersion=0\": dial tcp 127.0.0.1:65273: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
I0216 15:38:57.278366   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:57.278428   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:57.309286   89056 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0216 15:38:57.309789   89056 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: Get \"https://127.0.0.1:65273/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 127.0.0.1:65273: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0216 15:38:57.379003   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:57.379004   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:57.478990   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:57.479021   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:57.578475   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0216 15:38:57.578532   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0216 15:38:57.678830   89056 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0216 15:38:57.678868   89056 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
FAIL	k8s.io/kubernetes/cmd/kubemark/app	8.765s
	k8s.io/kubernetes/cmd/preferredimports		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/prune-junit-xml	0.870s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/cmd/prune-junit-xml/logparse	1.061s	coverage: 0.0% of statements in ./... [no tests to run]
?   	k8s.io/kubernetes/hack/boilerplate/test	[no test files]
	k8s.io/kubernetes/hack/conformance		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/api/endpoints/testing		coverage: 0.0% of statements
=== RUN   TestCtestWarningsForJobSpec

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:38:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/job/ctest_warnings_test.go:43]: get default configs: {test_fixture.json [base job spec for warnings] spec [jobs] {<nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> nil <nil> {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[a:b] map[] [] [] []} {[] [] [{abc image [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] <nil> <nil> <nil> <nil>  File IfNotPresent <nil> false false false}] [] OnFailure <nil> <nil> ClusterFirst map[]  <nil>  <nil> []   <nil> <nil>  [] []  <nil> <nil> <nil> [] <nil> map[] <nil> [] <nil> [] [] <nil> <nil>}} <nil> 0x14000550380 <nil> <nil> <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[jobs]
[DEBUG-CTEST 2026-02-16 15:38:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[jobs], int=1)[DEBUG-CTEST 2026-02-16 15:38:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec" in requested fixtures
[DEBUG-CTEST 2026-02-16 15:38:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/job/ctest_warnings_test.go:48]: Failed to generate effective config: %v mode-combination failed: no values found for field "spec" in requested fixtures
    ctest_warnings_test.go:49: GenerateEffectiveConfigReturnType failed: mode-combination failed: no values found for field "spec" in requested fixtures
--- FAIL: TestCtestWarningsForJobSpec (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/api/job	3.088s
?   	k8s.io/kubernetes/pkg/api/legacyscheme	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/node	1.885s	coverage: 0.1% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/persistentvolume	2.413s	coverage: 0.3% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/persistentvolumeclaim	2.795s	coverage: 0.3% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/pod	0.825s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/api/pod/testing		coverage: 0.0% of statements
=== RUN   TestCtestGetWarningsForService
=== RUN   TestCtestGetWarningsForService/new_topology_mode_set
Running edge test case #0: new topology mode set
=== RUN   TestCtestGetWarningsForService/deprecated_hints_annotation_set
Running edge test case #1: deprecated hints annotation set
=== RUN   TestCtestGetWarningsForService/externalIPs_set_when_type_is_ExternalName
Running edge test case #2: externalIPs set when type is ExternalName
=== RUN   TestCtestGetWarningsForService/externalName_set_when_type_is_not_ExternalName
Running edge test case #3: externalName set when type is not ExternalName
=== RUN   TestCtestGetWarningsForService/LoadBalancerIP_set_when_headless_service
Running edge test case #4: LoadBalancerIP set when headless service
=== RUN   TestCtestGetWarningsForService/ExternalIPs_set_when_headless_service
Running edge test case #5: ExternalIPs set when headless service
=== RUN   TestCtestGetWarningsForService/SessionAffinity_Client_IP_set_when_headless_service
Running edge test case #6: SessionAffinity Client IP set when headless service
=== RUN   TestCtestGetWarningsForService/SessionAffinity_None_set_when_headless_service
Running edge test case #7: SessionAffinity None set when headless service
=== RUN   TestCtestGetWarningsForService/ExternalIPs,_LoadBalancerIP_and_SessionAffinity_set_when_headless_service
Running edge test case #8: ExternalIPs, LoadBalancerIP and SessionAffinity set when headless service
=== RUN   TestCtestGetWarningsForService/empty_annotations
Running edge test case #9: empty annotations
=== RUN   TestCtestGetWarningsForService/both_new_and_deprecated_topology_annotations_set
Running edge test case #10: both new and deprecated topology annotations set
=== RUN   TestCtestGetWarningsForService/externalIPs_empty_slice_with_ExternalName_type_(no_warning)
Running edge test case #11: externalIPs empty slice with ExternalName type (no warning)
=== RUN   TestCtestGetWarningsForService/invalid_Service_type_(empty_string)
Running edge test case #12: invalid Service type (empty string)
=== RUN   TestCtestGetWarningsForService/nil_Service_pointer_(should_not_panic,_no_warnings)
Running edge test case #13: nil Service pointer (should not panic, no warnings)
--- PASS: TestCtestGetWarningsForService (0.00s)
    --- PASS: TestCtestGetWarningsForService/new_topology_mode_set (0.00s)
    --- PASS: TestCtestGetWarningsForService/deprecated_hints_annotation_set (0.00s)
    --- PASS: TestCtestGetWarningsForService/externalIPs_set_when_type_is_ExternalName (0.00s)
    --- PASS: TestCtestGetWarningsForService/externalName_set_when_type_is_not_ExternalName (0.00s)
    --- PASS: TestCtestGetWarningsForService/LoadBalancerIP_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/ExternalIPs_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/SessionAffinity_Client_IP_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/SessionAffinity_None_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/ExternalIPs,_LoadBalancerIP_and_SessionAffinity_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/empty_annotations (0.00s)
    --- PASS: TestCtestGetWarningsForService/both_new_and_deprecated_topology_annotations_set (0.00s)
    --- PASS: TestCtestGetWarningsForService/externalIPs_empty_slice_with_ExternalName_type_(no_warning) (0.00s)
    --- PASS: TestCtestGetWarningsForService/invalid_Service_type_(empty_string) (0.00s)
    --- PASS: TestCtestGetWarningsForService/nil_Service_pointer_(should_not_panic,_no_warnings) (0.00s)
=== RUN   TestCtestGetWarningsForServiceClusterIPs
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv4_No_failures
Running test case: IPv4 No failures
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv6_No_failures
Running test case: IPv6 No failures
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv4_with_leading_zeros
Running test case: IPv4 with leading zeros
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros
Running test case: Dual Stack IPv4-IPv6 and IPv4 with leading zeros
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros
Running test case: Dual Stack IPv6-IPv4 and IPv4 with leading zeros
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv6_non_canonical_format
Running test case: IPv6 non canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv6_non-canonical_format
Running test case: Dual Stack IPv4-IPv6 and IPv6 non-canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv6_non-canonical_formats
Running test case: Dual Stack IPv6-IPv4 and IPv6 non-canonical formats
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format
Running test case: Dual Stack IPv4-IPv6 and IPv4 with leading zeros and IPv6 non-canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format
Running test case: Dual Stack IPv6-IPv4 and IPv4 with leading zeros and IPv6 non-canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Service_with_all_IPs_fields_with_errors
Running test case: Service with all IPs fields with errors
=== RUN   TestCtestGetWarningsForServiceClusterIPs/empty_clusterIPs_slice_(no_warnings)
Running test case: empty clusterIPs slice (no warnings)
=== RUN   TestCtestGetWarningsForServiceClusterIPs/nil_service_pointer_(should_not_panic,_no_warnings)
Running test case: nil service pointer (should not panic, no warnings)
=== RUN   TestCtestGetWarningsForServiceClusterIPs/service_with_duplicate_IPs_(should_produce_duplicate_warnings)
Running test case: service with duplicate IPs (should produce duplicate warnings)
    ctest_warnings_test.go:313: GetWarningsForService() =   []string(
        - 	nil,
        + 	{`spec.clusterIPs[1]: duplicate IP address "192.12.2.2"`},
          )
--- FAIL: TestCtestGetWarningsForServiceClusterIPs (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv4_No_failures (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv6_No_failures (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv4_with_leading_zeros (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv6_non_canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv6_non-canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv6_non-canonical_formats (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Service_with_all_IPs_fields_with_errors (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/empty_clusterIPs_slice_(no_warnings) (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/nil_service_pointer_(should_not_panic,_no_warnings) (0.00s)
    --- FAIL: TestCtestGetWarningsForServiceClusterIPs/service_with_duplicate_IPs_(should_produce_duplicate_warnings) (0.00s)
FAIL
coverage: 0.2% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/api/service	2.743s
	k8s.io/kubernetes/pkg/api/service/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/servicecidr	2.837s	coverage: 0.3% of statements in ./... [no tests to run]
=== RUN   TestCtestStorageClassWarnings

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/storage/ctest_util_test.go:23]: hardcoded config: [{test_fixture.json [null] allowedTopologies [storageclasses] <nil>} {test_fixture.json [no warning] allowedTopologies [storageclasses] 0x140003a1680} {test_fixture.json [warning] allowedTopologies [storageclasses] 0x140003a1800} {test_fixture.json [empty allowedTopologies] allowedTopologies [storageclasses] 0x140003a1980} {test_fixture.json [nil matchLabelExpressions] allowedTopologies [storageclasses] 0x140003a1b00}]
[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/storage/ctest_util_test.go:27]: processing testInfo: [null]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[storageclasses]
[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[storageclasses], int=1)[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [storageclasses]
[DEBUG-CTEST 2026-02-16 15:38:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:38:58 load all fixtures failed: requested fixture keys not found in test_fixtures.json: storageclasses
FAIL	k8s.io/kubernetes/pkg/api/storage	0.746s
=== RUN   TestCtestCompatibility_v1_PodSecurityContext
[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/testing/ctest_backward_compatibility_test.go:95]: Running case: hostNetwork true
[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/testing/ctest_backward_compatibility_test.go:102]: Matched config: {test_fixture.json [hostNetwork true] hostNetwork [pods deployments statefulsets daemonsets replicasets] {[] [] [{a my-container-image [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] [] Never <nil> <nil>  map[]   <nil>  true false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:39:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:00 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/api/testing	0.741s
	k8s.io/kubernetes/pkg/api/testing/compat		coverage: 0.0% of statements
=== RUN   TestCtestPackSubsets

==================== CTEST OVERRIDE ONLY START ====================
    ctest_util_test.go:520: case "negative port number (invalid, should be ignored)": expected ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=1) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.2",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) (len=1) {
              (v1.EndpointPort) {
                Name: (string) "",
                Port: (int32) -1,
                Protocol: (v1.Protocol) "",
                AppProtocol: (*string)(<nil>)
              }
            }
          }
        }
        , got ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=1) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.2",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) <nil>
          }
        }
    ctest_util_test.go:520: case "duplicate IPs with mixed nil and non-nil TargetRef": expected ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=1) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.4",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) (len=1) {
              (v1.EndpointPort) {
                Name: (string) "",
                Port: (int32) 8080,
                Protocol: (v1.Protocol) "",
                AppProtocol: (*string)(<nil>)
              }
            }
          }
        }
        , got ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=2) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.4",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              },
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.4",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)({
                  Kind: (string) "",
                  Namespace: (string) "",
                  Name: (string) "",
                  UID: (types.UID) (len=7) "uid-dup",
                  APIVersion: (string) "",
                  ResourceVersion: (string) "",
                  FieldPath: (string) ""
                })
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) (len=1) {
              (v1.EndpointPort) {
                Name: (string) "",
                Port: (int32) 8080,
                Protocol: (v1.Protocol) "",
                AppProtocol: (*string)(<nil>)
              }
            }
          }
        }
--- FAIL: TestCtestPackSubsets (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/api/v1/endpoints	2.104s
=== RUN   TestCtestPVSecrets

==================== CTEST UNION MODE START ====================
[DEBUG-CTEST 2026-02-16 15:39:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/persistentvolume/ctest_util_test.go:30]: get default configs: {test_fixture.json [pv secret extraction] spec [persistentvolumes] [{{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil &AzureFilePersistentVolumeSource{SecretName:Spec.PersistentVolumeSource.AzureFile.SecretName,ShareName:,ReadOnly:false,SecretNamespace:nil,} nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil &AzureFilePersistentVolumeSource{SecretName:Spec.PersistentVolumeSource.AzureFile.SecretName,ShareName:,ReadOnly:false,SecretNamespace:*Spec.PersistentVolumeSource.AzureFile.SecretNamespace,} nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil &CephFSPersistentVolumeSource{Monitors:[],Path:,User:,SecretFile:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CephFS.SecretRef,Namespace:cephfs,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil &CephFSPersistentVolumeSource{Monitors:[],Path:,User:,SecretFile:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CephFS.SecretRef,Namespace:,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil &CinderPersistentVolumeSource{VolumeID:,FSType:,ReadOnly:false,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.Cinder.SecretRef,Namespace:cinder,},} nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] nil   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil &FlexPersistentVolumeSource{Driver:,FSType:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.FlexVolume.SecretRef,Namespace:flexns,},ReadOnly:false,Options:map[string]string{},} nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil &FlexPersistentVolumeSource{Driver:,FSType:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.FlexVolume.SecretRef,Namespace:,},ReadOnly:false,Options:map[string]string{},} nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil &RBDPersistentVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.RBD.SecretRef,Namespace:,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil &RBDPersistentVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.RBD.SecretRef,Namespace:rbdns,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ScaleIOPersistentVolumeSource{Gateway:,System:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ScaleIO.SecretRef,Namespace:,},SSLEnabled:false,ProtectionDomain:,StoragePool:,StorageMode:,VolumeName:,FSType:,ReadOnly:false,} nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ScaleIOPersistentVolumeSource{Gateway:,System:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ScaleIO.SecretRef,Namespace:scaleions,},SSLEnabled:false,ProtectionDomain:,StoragePool:,StorageMode:,VolumeName:,FSType:,ReadOnly:false,} nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil &ISCSIPersistentVolumeSource{TargetPortal:,IQN:,Lun:0,ISCSIInterface:,FSType:,ReadOnly:false,Portals:[],DiscoveryCHAPAuth:false,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ISCSI.SecretRef,Namespace:iscsi,},SessionCHAPAuth:false,InitiatorName:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil &ISCSIPersistentVolumeSource{TargetPortal:,IQN:,Lun:0,ISCSIInterface:,FSType:,ReadOnly:false,Portals:[],DiscoveryCHAPAuth:false,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ISCSI.SecretRef,Namespace:,},SessionCHAPAuth:false,InitiatorName:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &StorageOSPersistentVolumeSource{VolumeName:,VolumeNamespace:,FSType:,ReadOnly:false,SecretRef:&ObjectReference{Kind:,Namespace:storageosns,Name:Spec.PersistentVolumeSource.StorageOS.SecretRef,UID:,APIVersion:,ResourceVersion:,FieldPath:,},} nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.ControllerPublishSecretRef,Namespace:csi,},NodeStageSecretRef:nil,NodePublishSecretRef:nil,ControllerExpandSecretRef:nil,NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.NodePublishSecretRef,Namespace:csi,},ControllerExpandSecretRef:nil,NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.NodeStageSecretRef,Namespace:csi,},NodePublishSecretRef:nil,ControllerExpandSecretRef:nil,NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:nil,ControllerExpandSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.ControllerExpandSecretRef,Namespace:csi,},NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:nil,ControllerExpandSecretRef:nil,NodeExpandSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.NodeExpandSecretRef,Namespace:csi,},}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-16 15:39:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-16 15:39:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-16 15:39:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:01 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/api/v1/persistentvolume	1.560s
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/v1/pod	1.715s	coverage: 0.3% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/apis/admissionregistration/v1beta1_test
# [k8s.io/kubernetes/pkg/apis/admissionregistration/v1beta1_test]
pkg/apis/admissionregistration/v1beta1/ctest_defaults_test.go:345:11: non-constant format string in call to (*testing.common).Fatalf
=== RUN   TestCtestGetResourceRequest

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/resource/ctest_helpers_test.go:138]: Number of test cases: 17

==================== CTEST END ======================
--- PASS: TestCtestGetResourceRequest (0.00s)
=== RUN   TestCtestExtractResourceValue

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/resource/ctest_helpers_test.go:310]: Number of test cases: 18
    ctest_helpers_test.go:316: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/resource/ctest_helpers_test.go:316
        	Error:      	Error message not equal:
        	            	expected: "invalid resource name"
        	            	actual  : "unsupported container resource : invalid.resource"
        	Test:       	TestCtestExtractResourceValue
        	Messages:   	expected test case [16] to fail with error invalid resource name; got unsupported container resource : invalid.resource
--- FAIL: TestCtestExtractResourceValue (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/api/v1/resource	3.427s
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/api/v1/service	3.018s	coverage: 0.2% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/abac		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/abac/fuzzer		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/apis/abac/latest	[no test files]
=== RUN   TestCtestV0Conversion
    ctest_conversion_test.go:158: empty namespace with resource: expected
        	&abac.Policy{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, Spec:abac.PolicySpec{User:"", Group:"system:authenticated", Readonly:false, APIGroup:"*", Resource:"myresource", Namespace:"", NonResourcePath:""}},
        got
        	&abac.Policy{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, Spec:abac.PolicySpec{User:"", Group:"system:authenticated", Readonly:false, APIGroup:"*", Resource:"myresource", Namespace:"*", NonResourcePath:""}}
--- FAIL: TestCtestV0Conversion (0.00s)
FAIL
coverage: 0.2% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/abac/v0	2.817s
=== RUN   TestCtestV1Beta1Conversion

==================== CTEST START ====================
=== RUN   TestCtestV1Beta1Conversion/*_user
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case * user passed
=== RUN   TestCtestV1Beta1Conversion/empty_user
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case empty user passed
=== RUN   TestCtestV1Beta1Conversion/empty_group
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case empty group passed
=== RUN   TestCtestV1Beta1Conversion/user_and_group_both_set
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case user and group both set passed
=== RUN   TestCtestV1Beta1Conversion/both_star
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case both star passed
=== RUN   TestCtestV1Beta1Conversion/user
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case user passed
=== RUN   TestCtestV1Beta1Conversion/*_group
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case * group passed
=== RUN   TestCtestV1Beta1Conversion/star_user_and_explicit_group
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case star user and explicit group passed
=== RUN   TestCtestV1Beta1Conversion/star_group_and_explicit_user
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case star group and explicit user passed
=== RUN   TestCtestV1Beta1Conversion/group
[DEBUG-CTEST 2026-02-16 15:39:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case group passed

==================== CTEST END ======================
--- PASS: TestCtestV1Beta1Conversion (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/*_user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/empty_user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/empty_group (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/user_and_group_both_set (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/both_star (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/*_group (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/star_user_and_explicit_group (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/star_group_and_explicit_user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/group (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/abac/v1beta1	1.909s	coverage: 0.8% of statements in ./...
	k8s.io/kubernetes/pkg/apis/admission		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admissionregistration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admissionregistration/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admissionregistration/install		coverage: 0.0% of statements
=== RUN   TestCtestDefaultAdmissionWebhook

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:39]: Number of hardcoded config entries: 4
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [ValidatingWebhookConfiguration]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{},"name":"","sideEffects":null}]})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:51]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:53]: Skipping nil config objects
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [MutatingWebhookConfiguration]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{},"name":"","sideEffects":null}]})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:81]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:83]: Skipping nil config objects
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [scope=*]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{},"name":"","rules":[{}],"sideEffects":null}]})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:81]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:83]: Skipping nil config objects
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [port=443]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{"service":{"name":"","namespace":""}},"name":"","sideEffects":null}]})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:81]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:83]: Skipping nil config objects

==================== CTEST END ======================
--- PASS: TestCtestDefaultAdmissionWebhook (0.01s)
=== RUN   TestCtestDefaultAdmissionPolicy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:146]: Number of hardcoded config entries: 3
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:149]: Processing config for: [ValidatingAdmissionPolicy]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec.matchConstraints" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"matchConstraints":{}},"status":{}})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:158]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:160]: Skipping nil config objects
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:149]: Processing config for: [ValidatingAdmissionPolicyBinding]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec.matchResources" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"matchResources":{}}})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:187]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:189]: Skipping nil config objects
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:149]: Processing config for: [scope=*]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec.matchConstraints" in requested fixtures
2026/02/16 15:39:08 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:08 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"matchConstraints":{"resourceRules":[{}]}},"status":{}})[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:215]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:217]: Skipping nil config objects

==================== CTEST END ======================
--- PASS: TestCtestDefaultAdmissionPolicy (0.01s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/admissionregistration/v1	2.162s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestDefaultAdmissionPolicy
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:26]: Start TestCtestDefaultAdmissionPolicy
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicy
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicy] spec [] &ValidatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Validations:[]Validation{},FailurePolicy:nil,AuditAnnotations:[]AuditAnnotation{},MatchConditions:[]MatchCondition{},Variables:[]Variable{},},Status:ValidatingAdmissionPolicyStatus{ObservedGeneration:0,TypeChecking:nil,Conditions:[]Condition{},},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 58 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicyBinding
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicyBinding] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:nil,MatchResources:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 44 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/scope=*
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: scope=*
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [scope=*] spec [] &ValidatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{NamedRuleWithOperations{ResourceNames:[],RuleWithOperations:{[] {[] [] [] <nil>}},},},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Validations:[]Validation{},FailurePolicy:nil,AuditAnnotations:[]AuditAnnotation{},MatchConditions:[]MatchCondition{},Variables:[]Variable{},},Status:ValidatingAdmissionPolicyStatus{ObservedGeneration:0,TypeChecking:nil,Conditions:[]Condition{},},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 78 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: MutatingAdmissionPolicy
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [MutatingAdmissionPolicy] spec [] &MutatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:MutatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Variables:[]Variable{},Mutations:[]Mutation{Mutation{PatchType:ApplyConfiguration,ApplyConfiguration:&ApplyConfiguration{Expression:fake string,},JSONPatch:nil,},},FailurePolicy:nil,MatchConditions:[]MatchCondition{},ReinvocationPolicy:Never,},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 174 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy.emptySpec
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicy.emptySpec
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicy.emptySpec] spec [] &ValidatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:nil,Validations:[]Validation{},FailurePolicy:nil,AuditAnnotations:[]AuditAnnotation{},MatchConditions:[]MatchCondition{},Variables:[]Variable{},},Status:ValidatingAdmissionPolicyStatus{ObservedGeneration:0,TypeChecking:nil,Conditions:[]Condition{},},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 37 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding.emptySpec
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicyBinding.emptySpec
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicyBinding.emptySpec] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:nil,MatchResources:nil,ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 25 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy.emptyMutations
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: MutatingAdmissionPolicy.emptyMutations
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [MutatingAdmissionPolicy.emptyMutations] spec [] &MutatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:MutatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Variables:[]Variable{},Mutations:[]Mutation{},FailurePolicy:nil,MatchConditions:[]MatchCondition{},ReinvocationPolicy:Never,},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 75 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object

==================== CTEST END ======================
--- FAIL: TestCtestDefaultAdmissionPolicy (0.05s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/scope=* (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy.emptySpec (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding.emptySpec (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy.emptyMutations (0.01s)
=== RUN   TestCtestDefaultAdmissionPolicyBinding
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:316]: Start TestCtestDefaultAdmissionPolicyBinding
=== RUN   TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:361]: Running test case: ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:368]: Matched config: {test_fixture.json [ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:&ParamRef{Name:,Namespace:,Selector:nil,ParameterNotFoundAction:nil,},MatchResources:nil,ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 38 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.MatchResources
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:361]: Running test case: ValidatingAdmissionPolicyBinding.MatchResources
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:368]: Matched config: {test_fixture.json [ValidatingAdmissionPolicyBinding.MatchResources] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:nil,MatchResources:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 44 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.empty
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:361]: Running test case: ValidatingAdmissionPolicyBinding.ParamRef.empty
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:368]: Matched config: {test_fixture.json [ValidatingAdmissionPolicyBinding.ParamRef.empty] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:&ParamRef{Name:,Namespace:,Selector:nil,ParameterNotFoundAction:nil,},MatchResources:nil,ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:08 Mode: 1
2026/02/16 15:39:08 Base JSON size: 38 bytes
2026/02/16 15:39:08 Number of external values: 89
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:08   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:08 [DEBUG-CTEST 2026-02-16 15:39:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object

==================== CTEST END ======================
--- FAIL: TestCtestDefaultAdmissionPolicyBinding (0.02s)
    --- FAIL: TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.MatchResources (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.empty (0.00s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/admissionregistration/v1alpha1	4.224s
FAIL	k8s.io/kubernetes/pkg/apis/admissionregistration/v1beta1 [build failed]
# k8s.io/kubernetes/pkg/apis/autoscaling/v2_test
# [k8s.io/kubernetes/pkg/apis/autoscaling/v2_test]
pkg/apis/autoscaling/v2/ctest_defaults_test.go:590:4: (*testing.common).Errorf format %#v reads arg #2, but call has 1 arg
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/admissionregistration/validation	4.880s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/apidiscovery		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apidiscovery/v2		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apidiscovery/v2beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apiserverinternal		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apiserverinternal/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/apiserverinternal/install	2.932s	coverage: 0.2% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/apiserverinternal/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/apiserverinternal/validation	3.275s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/apps		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apps/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apps/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/apps/v1	2.874s	coverage: 0.7% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/apps/v1beta1	2.568s	coverage: 0.7% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/apps/v1beta2	2.910s	coverage: 0.7% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/apps/validation	1.680s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/authentication		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/validation		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateSARSpec
=== RUN   TestCtestValidateSARSpec/neither_request
=== RUN   TestCtestValidateSARSpec/both_requests
=== RUN   TestCtestValidateSARSpec/no_subject
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_both
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_neither
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_no_key
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_not_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_exists
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_not_exists
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_both
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_neither
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_no_key
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_invalid_label_name
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_not_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_exists
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_not_exists
=== RUN   TestCtestValidateSARSpec/empty_user_string
=== RUN   TestCtestValidateSARSpec/group_with_empty_string
    ctest_validation_test.go:358: group with empty string: expected failure for "spec.groups[0]: Invalid value: \"\": at least one of user or group must be specified"
    ctest_validation_test.go:365: group with empty string: expected failure for "spec.groups[0]: Invalid value: \"\": at least one of user or group must be specified"
    ctest_validation_test.go:371: group with empty string: expected failure for "spec.groups[0]: Invalid value: \"\": at least one of user or group must be specified"
--- FAIL: TestCtestValidateSARSpec (0.00s)
    --- PASS: TestCtestValidateSARSpec/neither_request (0.00s)
    --- PASS: TestCtestValidateSARSpec/both_requests (0.00s)
    --- PASS: TestCtestValidateSARSpec/no_subject (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_both (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_neither (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_no_key (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_not_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_not_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_both (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_neither (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_no_key (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_invalid_label_name (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_not_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_not_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/empty_user_string (0.00s)
    --- FAIL: TestCtestValidateSARSpec/group_with_empty_string (0.00s)
=== RUN   TestCtestValidateSelfSAR
    ctest_validation_test.go:434: empty resource attributes: expected failure for "spec.resourceAttributes: Required value: at least one selector must be specified"
    ctest_validation_test.go:441: empty resource attributes: expected failure for "spec.resourceAttributes: Required value: at least one selector must be specified"
--- FAIL: TestCtestValidateSelfSAR (0.00s)
=== RUN   TestCtestValidateLocalSAR
    ctest_validation_test.go:535: empty object name with namespace set: unexpected error: "spec.resourceAttributes.namespace: Invalid value: \"\": must match metadata.namespace", expected: "must be empty except for namespace"
--- FAIL: TestCtestValidateLocalSAR (0.00s)
FAIL
coverage: 0.2% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/authorization/validation	1.153s
	k8s.io/kubernetes/pkg/apis/autoscaling		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/autoscaling/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/autoscaling/install		coverage: 0.0% of statements
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth1
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth2
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithoutMetrics
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUUtilizationOnly
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_NilMinReplicas_ZeroMaxReplicas_NilMetrics
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_UnknownMetricType
--- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth1 (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth2 (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithoutMetrics (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUUtilizationOnly (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_NilMinReplicas_ZeroMaxReplicas_NilMetrics (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_UnknownMetricType (0.00s)
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/autoscaling/v1	0.962s	coverage: 0.7% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/autoscaling/v2 [build failed]
=== RUN   TestCtestNilOrEmptyConversion
--- FAIL: TestCtestNilOrEmptyConversion (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x8 pc=0x100fb93c0]

goroutine 48 [running]:
testing.tRunner.func1.2({0x103a346a0, 0x1055ac3c0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x103a346a0?, 0x1055ac3c0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1.Convert_autoscaling_ExternalMetricSource_To_v2beta1_ExternalMetricSource(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta1/conversion.go:107
k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1.RegisterConversions.func16({0x103a5d780?, 0x0?}, {0x103d2a180?, 0x0?}, {0x103d2a168?, 0x1037e1303?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta1/zz_generated.conversion.go:119 +0xf0
k8s.io/apimachinery/pkg/conversion.(*Converter).Convert(0x14000880108, {0x103a5d780, 0x0}, {0x103d2a180, 0x0}, 0x14000062820)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apimachinery/pkg/conversion/converter.go:210 +0x2b8
k8s.io/apimachinery/pkg/runtime.(*Scheme).Convert(0x140002e19d0, {0x103a5d780, 0x0}, {0x103d2a180, 0x0}, {0x0, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apimachinery/pkg/runtime/scheme.go:458 +0x454
k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1.TestCtestNilOrEmptyConversion(0x1400058a700)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta1/ctest_conversion_test.go:81 +0x3b8
testing.tRunner(0x1400058a700, 0x103e48d28)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1	1.286s
=== RUN   TestCtestGenerateScaleDownRules
[DEBUG-CTEST 2026-02-16 15:39:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta2/ctest_defaults_test.go:27]: Running TestCtestGenerateScaleDownRules
Running test case 0: Default values
=== RUN   TestCtestGenerateScaleDownRules/Default_values
Running test case 1: All parameters are specified
=== RUN   TestCtestGenerateScaleDownRules/All_parameters_are_specified
Running test case 2: Percent policy is specified
=== RUN   TestCtestGenerateScaleDownRules/Percent_policy_is_specified
Running test case 3: Pods policy is specified
=== RUN   TestCtestGenerateScaleDownRules/Pods_policy_is_specified
Running test case 4: Nil selectPolicy and nil stabilization (should use defaults)
=== RUN   TestCtestGenerateScaleDownRules/Nil_selectPolicy_and_nil_stabilization_(should_use_defaults)
Running test case 5: Negative Pods rate
=== RUN   TestCtestGenerateScaleDownRules/Negative_Pods_rate
Running test case 6: Large Percent value
=== RUN   TestCtestGenerateScaleDownRules/Large_Percent_value
--- PASS: TestCtestGenerateScaleDownRules (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Default_values (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/All_parameters_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Percent_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Pods_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Nil_selectPolicy_and_nil_stabilization_(should_use_defaults) (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Negative_Pods_rate (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Large_Percent_value (0.00s)
=== RUN   TestCtestGenerateScaleUpRules
[DEBUG-CTEST 2026-02-16 15:39:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta2/ctest_defaults_test.go:151]: Running TestCtestGenerateScaleUpRules
Running test case 0: Default values
=== RUN   TestCtestGenerateScaleUpRules/Default_values
Running test case 1: All parameters are specified
=== RUN   TestCtestGenerateScaleUpRules/All_parameters_are_specified
Running test case 2: Pod policy is specified
=== RUN   TestCtestGenerateScaleUpRules/Pod_policy_is_specified
Running test case 3: Percent policy is specified
=== RUN   TestCtestGenerateScaleUpRules/Percent_policy_is_specified
Running test case 4: Pod policy and stabilization window are specified
=== RUN   TestCtestGenerateScaleUpRules/Pod_policy_and_stabilization_window_are_specified
Running test case 5: Percent policy and stabilization window are specified
=== RUN   TestCtestGenerateScaleUpRules/Percent_policy_and_stabilization_window_are_specified
Running test case 6: Nil selectPolicy and zero stabilization (defaults)
=== RUN   TestCtestGenerateScaleUpRules/Nil_selectPolicy_and_zero_stabilization_(defaults)
Running test case 7: Negative Percent value
=== RUN   TestCtestGenerateScaleUpRules/Negative_Percent_value
Running test case 8: Huge Pods period
=== RUN   TestCtestGenerateScaleUpRules/Huge_Pods_period
--- PASS: TestCtestGenerateScaleUpRules (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Default_values (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/All_parameters_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Pod_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Percent_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Pod_policy_and_stabilization_window_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Percent_policy_and_stabilization_window_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Nil_selectPolicy_and_zero_stabilization_(defaults) (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Negative_Percent_value (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Huge_Pods_period (0.00s)
=== RUN   TestCtestHorizontalPodAutoscalerAnnotations
[DEBUG-CTEST 2026-02-16 15:39:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta2/ctest_defaults_test.go:298]: Running TestCtestHorizontalPodAutoscalerAnnotations
--- PASS: TestCtestHorizontalPodAutoscalerAnnotations (0.00s)
PASS
coverage: 0.9% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/autoscaling/v2beta2	1.936s	coverage: 0.9% of statements in ./...
=== RUN   TestCtestValidateScaleForDeclarative

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/validation/ctest_declarative_validation_test.go:32]: get default configs: {test_fixture.json [default scale spec] replicas [] {{ } {abc  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {1} {0 }}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:19 [DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:19 Mode: 1
2026/02/16 15:39:19 Base JSON size: 96 bytes
2026/02/16 15:39:19 Number of external values: 22
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19   [REPLACE ALL] : entire structure replaced
2026/02/16 15:39:19 [DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:19 [DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=22)
[DEBUG-CTEST 2026-02-16 15:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"Replicas":1},"Status":{"Replicas":0,"Selector":""},"name":"abc","namespace":"default"})    ctest_declarative_validation_test.go:37: failed to generate effective config: k8s json unmarshal into autoscaling.Scale failed: json: cannot unmarshal number into Go value of type autoscaling.Scale
--- FAIL: TestCtestValidateScaleForDeclarative (0.00s)
FAIL
coverage: 1.3% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/autoscaling/validation	2.829s
	k8s.io/kubernetes/pkg/apis/batch		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/batch/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/batch/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/batch/v1	1.887s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestSetDefaultCronJob

==================== CTEST OVERRIDE ONLY START ====================
=== RUN   TestCtestSetDefaultCronJob/empty_batchv1beta1.CronJob_should_default_batchv1beta1.ConcurrencyPolicy_and_Suspend
[DEBUG-CTEST 2026-02-16 15:39:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/batch/v1beta1/ctest_defaults_test.go:70]: get default configs: {test_fixture.json [empty batchv1beta1.CronJob should default batchv1beta1.ConcurrencyPolicy and Suspend] spec [cronjobs] { <nil> <nil> Allow 0x140004aef58 {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {<nil> <nil> <nil> nil nil <nil> <nil> <nil> nil <nil> {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}} <nil> <nil> <nil> <nil> <nil>}} 0x140004aef5c 0x140004aef80}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[cronjobs]
[DEBUG-CTEST 2026-02-16 15:39:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[cronjobs], int=1)[DEBUG-CTEST 2026-02-16 15:39:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [cronjobs]
[DEBUG-CTEST 2026-02-16 15:39:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:22 load all fixtures failed: requested fixture keys not found in test_fixtures.json: cronjobs
FAIL	k8s.io/kubernetes/pkg/apis/batch/v1beta1	0.958s
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/batch/validation	0.873s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/certificates		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestIsKubeletServingCSR

==================== CTEST START ====================
=== RUN   TestCtestIsKubeletServingCSR/defaults_for_kubelet-serving
=== RUN   TestCtestIsKubeletServingCSR/defaults_without_key_encipherment_for_kubelet-serving
=== RUN   TestCtestIsKubeletServingCSR/does_not_default_to_kube-apiserver-client-kubelet_if_org_is_not_'system:nodes'
=== RUN   TestCtestIsKubeletServingCSR/nil_request
--- FAIL: TestCtestIsKubeletServingCSR (0.00s)
    --- PASS: TestCtestIsKubeletServingCSR/defaults_for_kubelet-serving (0.00s)
    --- PASS: TestCtestIsKubeletServingCSR/defaults_without_key_encipherment_for_kubelet-serving (0.00s)
    --- PASS: TestCtestIsKubeletServingCSR/does_not_default_to_kube-apiserver-client-kubelet_if_org_is_not_'system:nodes' (0.00s)
    --- FAIL: TestCtestIsKubeletServingCSR/nil_request (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0xb8 pc=0x10325f888]

goroutine 88 [running]:
testing.tRunner.func1.2({0x105bd4820, 0x1076cb5d0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x105bd4820?, 0x1076cb5d0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/certificates.ValidateKubeletServingCSR(0x0, 0x1400064ec90)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/helpers.go:69 +0xc8
k8s.io/kubernetes/pkg/apis/certificates.IsKubeletServingCSR(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/helpers.go:66
k8s.io/kubernetes/pkg/apis/certificates/v1beta1.IsKubeletServingCSR(0x0, {0x1076e9060, 0x3, 0x102717dfc?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/v1beta1/defaults.go:69 +0x438
k8s.io/kubernetes/pkg/apis/certificates/v1beta1.TestCtestIsKubeletServingCSR.func2(0x140003cfdc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/v1beta1/ctest_defaults_test.go:76 +0x34
testing.tRunner(0x140003cfdc0, 0x1400064ec30)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 84
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/apis/certificates/v1beta1	0.689s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/certificates/validation	1.537s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/coordination		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/v1alpha2		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateLeaseSpec

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:114]: Number of test cases: 9
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 4
Running 5 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 5
Running 6 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 6
Running 7 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 7
Running 8 th test case.

==================== CTEST END ======================
--- FAIL: TestCtestValidateLeaseSpec (0.00s)
=== RUN   TestCtestValidateLeaseCandidateSpec

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:249]: Number of test cases: 12
Running 0 th test case: valid
Running 1 th test case: valid custom strategy should not require emulationVersion
Running 2 th test case: binaryVersion is required
Running 3 th test case: no lease name
Running 4 th test case: bad binaryVersion
Running 5 th test case: emulation should be greater than or equal to binary version
Running 6 th test case: strategy bad
Running 7 th test case: strategy missing
Running 8 th test case: strategy good but emulationVersion missing
Running 9 th test case: empty binaryVersion string
Running 10 th test case: extremely long binaryVersion
Running 11 th test case: missing strategy with only binary version

==================== CTEST END ======================
--- PASS: TestCtestValidateLeaseCandidateSpec (0.00s)
=== RUN   TestCtestValidateLeaseCandidateUpdate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:337]: Number of test cases: 4
Running 0 th test case: valid update
Running 1 th test case: update LeaseName should fail
Running 2 th test case: changing Strategy should fail
    ctest_validation_test.go:347: Expected err, got no err for tc: changing Strategy should fail
Running 3 th test case: missing ResourceVersion on update (should be ignored)

==================== CTEST END ======================
--- FAIL: TestCtestValidateLeaseCandidateUpdate (0.00s)
=== RUN   TestCtestValidateCoordinatedLeaseStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:397]: Number of test cases: 8
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
Running 5 th test case.
Running 6 th test case.
Running 7 th test case.

==================== CTEST END ======================
--- PASS: TestCtestValidateCoordinatedLeaseStrategy (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/coordination/validation	3.889s
=== RUN   TestCtestTaintToString
    ctest_taint_test.go:67: [4] expected taint &{empty   <nil>} converted to empty=:, got empty
--- FAIL: TestCtestTaintToString (0.00s)
=== RUN   TestCtestMatchTaint
--- PASS: TestCtestMatchTaint (0.00s)
=== RUN   TestCtestMatchToleration
--- PASS: TestCtestMatchToleration (0.00s)
FAIL
coverage: 0.1% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/core	1.186s
	k8s.io/kubernetes/pkg/apis/core/fuzzer		coverage: 0.0% of statements
=== RUN   TestCtestSemantic
--- PASS: TestCtestSemantic (0.00s)
=== RUN   TestCtestIsStandardResource
--- PASS: TestCtestIsStandardResource (0.00s)
=== RUN   TestCtestIsStandardContainerResource
--- PASS: TestCtestIsStandardContainerResource (0.00s)
=== RUN   TestCtestGetAccessModesFromString
--- PASS: TestCtestGetAccessModesFromString (0.00s)
=== RUN   TestCtestRemoveDuplicateAccessModes
--- PASS: TestCtestRemoveDuplicateAccessModes (0.00s)
=== RUN   TestCtestIsHugePageResourceName
    ctest_helpers_test.go:177: resource: hugepages- expected result: false
--- FAIL: TestCtestIsHugePageResourceName (0.00s)
=== RUN   TestCtestIsHugePageResourceValueDivisible
--- PASS: TestCtestIsHugePageResourceValueDivisible (0.00s)
=== RUN   TestCtestHugePageResourceName
    ctest_helpers_test.go:267: pageSize: 0, expected: , but got: hugepages-0
--- FAIL: TestCtestHugePageResourceName (0.00s)
=== RUN   TestCtestHugePageSizeFromResourceName
--- PASS: TestCtestHugePageSizeFromResourceName (0.00s)
=== RUN   TestCtestIsOvercommitAllowed
    ctest_helpers_test.go:335: Unexpected result for unknown
--- FAIL: TestCtestIsOvercommitAllowed (0.00s)
=== RUN   TestCtestIsServiceIPSet
=== RUN   TestCtestIsServiceIPSet/nil_cluster_ip
=== RUN   TestCtestIsServiceIPSet/headless_service
=== RUN   TestCtestIsServiceIPSet/one_ipv4
=== RUN   TestCtestIsServiceIPSet/one_ipv6
=== RUN   TestCtestIsServiceIPSet/v4,_v6
=== RUN   TestCtestIsServiceIPSet/v6,_v4
=== RUN   TestCtestIsServiceIPSet/empty_spec
--- PASS: TestCtestIsServiceIPSet (0.00s)
    --- PASS: TestCtestIsServiceIPSet/nil_cluster_ip (0.00s)
    --- PASS: TestCtestIsServiceIPSet/headless_service (0.00s)
    --- PASS: TestCtestIsServiceIPSet/one_ipv4 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/one_ipv6 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/v4,_v6 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/v6,_v4 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/empty_spec (0.00s)
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/valid_values
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/empty_terms
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/invalid_label_value
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/nil_slice
--- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/valid_values (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/empty_terms (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/invalid_label_value (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/nil_slice (0.00s)
FAIL
coverage: 0.3% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/core/helper	0.816s
	k8s.io/kubernetes/pkg/apis/core/helper/qos		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/core/install	2.837s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/core/pods	1.324s	coverage: 0.1% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/core/v1	2.556s	coverage: 0.7% of statements in ./... [no tests to run]
=== RUN   TestCtestIsNativeResource
=== RUN   TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false
=== CONT  TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false
=== CONT  TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false
=== CONT  TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false
--- PASS: TestCtestIsNativeResource (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false (0.00s)
=== RUN   TestCtestHugePageSizeFromResourceName
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI}
=== NAME  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
    ctest_helpers_test.go:125: [4]expected error but got none.
    ctest_helpers_test.go:131: Got {{0 -3} {<nil>}  DecimalSI} but expected {{0 0} {<nil>}  }
--- FAIL: TestCtestHugePageSizeFromResourceName (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI} (0.00s)
    --- FAIL: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__} (0.00s)
=== RUN   TestCtestHugePageSizeFromMedium
=== RUN   TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium
=== RUN   TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate)
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate)
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate)
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate)
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional)
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional)
=== CONT  TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional)
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize
=== CONT  TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate)
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate)
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi
=== NAME  TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
    ctest_helpers_test.go:213: [7]expected error but got none.
    ctest_helpers_test.go:219: Got {{0 0} {<nil>}  BinarySI} but expected {{0 0} {<nil>}  }
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi
--- FAIL: TestCtestHugePageSizeFromMedium (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional) (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate) (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate) (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi (0.00s)
    --- FAIL: TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi (0.00s)
=== RUN   TestCtestIsOvercommitAllowed
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true
    ctest_helpers_test.go:266: Got false but expected true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false
--- FAIL: TestCtestIsOvercommitAllowed (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false (0.00s)
    --- FAIL: TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false (0.00s)
=== RUN   TestCtestGetAccessModesFromString
--- PASS: TestCtestGetAccessModesFromString (0.00s)
=== RUN   TestCtestRemoveDuplicateAccessModes
--- PASS: TestCtestRemoveDuplicateAccessModes (0.00s)
=== RUN   TestCtestTopologySelectorRequirementsAsSelector
    ctest_helpers_test.go:418: [6]expected error but got none.
    ctest_helpers_test.go:424: [6]expected:
        	<nil>
        but got:
        	dup in (a),dup in (b)
--- FAIL: TestCtestTopologySelectorRequirementsAsSelector (0.00s)
=== RUN   TestCtestMatchTopologySelectorTerms
=== RUN   TestCtestMatchTopologySelectorTerms/nil_term_list
=== RUN   TestCtestMatchTopologySelectorTerms/nil_term
=== RUN   TestCtestMatchTopologySelectorTerms/label_matches_MatchLabelExpressions_terms
=== RUN   TestCtestMatchTopologySelectorTerms/label_does_not_match_MatchLabelExpressions_terms
=== RUN   TestCtestMatchTopologySelectorTerms/multi-values_in_one_requirement,_one_matched
=== RUN   TestCtestMatchTopologySelectorTerms/multi-terms_was_set,_one_matched
=== RUN   TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_fully_matched
=== RUN   TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_partial_matched
=== RUN   TestCtestMatchTopologySelectorTerms/empty_labels_map_with_terms
--- PASS: TestCtestMatchTopologySelectorTerms (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/nil_term_list (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/nil_term (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/label_matches_MatchLabelExpressions_terms (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/label_does_not_match_MatchLabelExpressions_terms (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-values_in_one_requirement,_one_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-terms_was_set,_one_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_fully_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_partial_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/empty_labels_map_with_terms (0.00s)
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/empty_set_of_keys_in_empty_set_of_terms
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_all_keys_specified
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_one_of_the_keys_specified
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_without_any_of_the_keys_specified
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_empty_set_of_terms
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/requirement_with_empty_key
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/terms_with_empty_MatchExpressions
--- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/empty_set_of_keys_in_empty_set_of_terms (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_all_keys_specified (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_one_of_the_keys_specified (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_without_any_of_the_keys_specified (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_empty_set_of_terms (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/requirement_with_empty_key (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/terms_with_empty_MatchExpressions (0.00s)
=== RUN   TestCtestHugePageUnitSizeFromByteSize
    ctest_helpers_test.go:881: HugePageUnitSizeFromByteSize() expected error = size: 3069MB must be guaranteed to divisible into the largest units
    ctest_helpers_test.go:888: HugePageUnitSizeFromByteSize() expected 0B but got 0PB
    ctest_helpers_test.go:888: HugePageUnitSizeFromByteSize() expected  but got -1KB
    ctest_helpers_test.go:888: HugePageUnitSizeFromByteSize() expected  but got 1PB
--- FAIL: TestCtestHugePageUnitSizeFromByteSize (0.00s)
FAIL
coverage: 0.4% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/core/v1/helper	2.690s
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/core/v1/helper/qos	2.915s	coverage: 0.3% of statements in ./... [no tests to run]
=== RUN   TestCtestValidateResourceRequirements
=== RUN   TestCtestValidateResourceRequirements/Resources_with_Requests_equal_to_Limits
=== RUN   TestCtestValidateResourceRequirements/Resources_with_only_Limits
=== RUN   TestCtestValidateResourceRequirements/Resources_with_only_Requests
=== RUN   TestCtestValidateResourceRequirements/Resources_with_Requests_Less_Than_Limits
=== RUN   TestCtestValidateResourceRequirements/Empty_ResourceRequirements_(no_requests,_no_limits)
=== RUN   TestCtestValidateResourceRequirements/Resources_with_Requests_Larger_Than_Limits
=== RUN   TestCtestValidateResourceRequirements/Invalid_Resources_with_Requests
=== RUN   TestCtestValidateResourceRequirements/Invalid_Resources_with_Limits
=== RUN   TestCtestValidateResourceRequirements/Empty_Requests_map_with_non‑empty_Limits_(should_error_on_missing_request)
    ctest_validation_test.go:133: expected error
    validation_test.go:152: error must contain limit name
    validation_test.go:155: error must contain limit value
=== RUN   TestCtestValidateResourceRequirements/Zero_quantity_in_Limits_(invalid)
    ctest_validation_test.go:133: expected error
    validation_test.go:152: error must contain limit name
--- FAIL: TestCtestValidateResourceRequirements (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_Requests_equal_to_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_only_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_only_Requests (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_Requests_Less_Than_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Empty_ResourceRequirements_(no_requests,_no_limits) (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_Requests_Larger_Than_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Invalid_Resources_with_Requests (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Invalid_Resources_with_Limits (0.00s)
    --- FAIL: TestCtestValidateResourceRequirements/Empty_Requests_map_with_non‑empty_Limits_(should_error_on_missing_request) (0.00s)
    --- FAIL: TestCtestValidateResourceRequirements/Zero_quantity_in_Limits_(invalid) (0.00s)
=== RUN   TestCtestValidateContainerResourceName
=== RUN   TestCtestValidateContainerResourceName/CPU_resource
=== RUN   TestCtestValidateContainerResourceName/Memory_resource
=== RUN   TestCtestValidateContainerResourceName/Hugepages_resource
=== RUN   TestCtestValidateContainerResourceName/Namespaced_resource
=== RUN   TestCtestValidateContainerResourceName/Extended_Resource
=== RUN   TestCtestValidateContainerResourceName/Empty_string_(treated_as_valid_in_this_context)
    ctest_validation_test.go:167: unexpected error: [[]: Invalid value: "": name part must be non-empty []: Invalid value: "": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]') []: Invalid value: "": must be a standard resource for containers]
=== RUN   TestCtestValidateContainerResourceName/Invalid_standard_resource
=== RUN   TestCtestValidateContainerResourceName/Invalid_namespaced_resource
=== RUN   TestCtestValidateContainerResourceName/Invalid_extended_resource
=== RUN   TestCtestValidateContainerResourceName/Resource_name_with_spaces
=== RUN   TestCtestValidateContainerResourceName/Resource_name_with_leading_dash
--- FAIL: TestCtestValidateContainerResourceName (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/CPU_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Memory_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Hugepages_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Namespaced_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Extended_Resource (0.00s)
    --- FAIL: TestCtestValidateContainerResourceName/Empty_string_(treated_as_valid_in_this_context) (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Invalid_standard_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Invalid_namespaced_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Invalid_extended_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Resource_name_with_spaces (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Resource_name_with_leading_dash (0.00s)
=== RUN   TestCtestValidatePodLogOptions
=== RUN   TestCtestValidatePodLogOptions/Empty_PodLogOptions
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_TailLines
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_only_sinceSeconds
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines_with_SinceSeconds
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_stdout_Stream
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_stderr_Stream_and_Follow
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_All_Stream,_TailLines_and_LimitBytes
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_SinceTime_only_(valid)
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_Negative_TailLines
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_LimitBytes
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_SinceSeconds
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_both_SinceSeconds_and_SinceTime_set
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_invalid_Stream
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stdout_Stream_and_TailLines_set
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stderr_Stream_and_TailLines_set
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_nil_Stream_but_TailLines_set
    ctest_validation_test.go:345: expected error
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_empty_string_Stream_(treated_as_invalid)
--- FAIL: TestCtestValidatePodLogOptions (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Empty_PodLogOptions (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_TailLines (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_only_sinceSeconds (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines_with_SinceSeconds (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_stdout_Stream (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_stderr_Stream_and_Follow (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_All_Stream,_TailLines_and_LimitBytes (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_SinceTime_only_(valid) (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_Negative_TailLines (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_LimitBytes (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_SinceSeconds (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_both_SinceSeconds_and_SinceTime_set (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_invalid_Stream (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stdout_Stream_and_TailLines_set (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stderr_Stream_and_TailLines_set (0.00s)
    --- FAIL: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_nil_Stream_but_TailLines_set (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_empty_string_Stream_(treated_as_invalid) (0.00s)
=== RUN   TestCtestAccumulateUniqueHostPorts
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_the_same_port_with_different_protocol
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_different_ports
=== RUN   TestCtestAccumulateUniqueHostPorts/No_containers_(empty_slice)_should_succeed
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated_while_containers_use_the_same_port_with_UDP
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_zero_(invalid)_with_UDP
    ctest_validation_test.go:449: expected error, but got nil
--- FAIL: TestCtestAccumulateUniqueHostPorts (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_the_same_port_with_different_protocol (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_different_ports (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/No_containers_(empty_slice)_should_succeed (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated_while_containers_use_the_same_port_with_UDP (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated (0.00s)
    --- FAIL: TestCtestAccumulateUniqueHostPorts/HostPort_zero_(invalid)_with_UDP (0.00s)
FAIL
coverage: 0.4% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/core/v1/validation	2.275s
=== RUN   TestCtestIsKubernetesSignerName
Starting TestCtestIsKubernetesSignerName
Number of test cases: 11
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io
Running test case #0: kubernetes.io
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/a
Running test case #1: kubernetes.io/a
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/a/b.c/d.e
Running test case #2: kubernetes.io/a/b.c/d.e
=== RUN   TestCtestIsKubernetesSignerName/foo.kubernetes.io
Running test case #3: foo.kubernetes.io
=== RUN   TestCtestIsKubernetesSignerName/fookubernetes.io
Running test case #4: fookubernetes.io
=== RUN   TestCtestIsKubernetesSignerName/foo.com/a
Running test case #5: foo.com/a
=== RUN   TestCtestIsKubernetesSignerName/#00
Running test case #6: 
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/
Running test case #7: kubernetes.io/
    ctest_names_test.go:66: IsKubernetesSignerName("kubernetes.io/"); got true, want false
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io//a
Running test case #8: kubernetes.io//a
    ctest_names_test.go:66: IsKubernetesSignerName("kubernetes.io//a"); got true, want false
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/@@@@
Running test case #9: kubernetes.io/@@@@
    ctest_names_test.go:66: IsKubernetesSignerName("kubernetes.io/@@@@"); got true, want false
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y/z
Running test case #10: kubernetes.io/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y/z
--- FAIL: TestCtestIsKubernetesSignerName (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io/a (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io/a/b.c/d.e (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/foo.kubernetes.io (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/fookubernetes.io (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/foo.com/a (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/#00 (0.00s)
    --- FAIL: TestCtestIsKubernetesSignerName/kubernetes.io/ (0.00s)
    --- FAIL: TestCtestIsKubernetesSignerName/kubernetes.io//a (0.00s)
    --- FAIL: TestCtestIsKubernetesSignerName/kubernetes.io/@@@@ (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y/z (0.00s)
FAIL
coverage: 0.3% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/core/validation	1.617s
	k8s.io/kubernetes/pkg/apis/discovery		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/discovery/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/discovery/install		coverage: 0.0% of statements
=== RUN   TestCtestSetDefaultEndpointPort

==================== CTEST START ====================
=== RUN   TestCtestSetDefaultEndpointPort/should_set_appropriate_defaults
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints]
[DEBUG-CTEST 2026-02-16 15:39:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints], int=1)[DEBUG-CTEST 2026-02-16 15:39:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints]
[DEBUG-CTEST 2026-02-16 15:39:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:32 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints
FAIL	k8s.io/kubernetes/pkg/apis/discovery/v1	1.173s
=== RUN   TestCtestEndpointZoneConversion
=== RUN   TestCtestEndpointZoneConversion/no_topology_field
=== RUN   TestCtestEndpointZoneConversion/non_empty_topology_map,_but_no_zone
=== RUN   TestCtestEndpointZoneConversion/non_empty_topology_map,_with_zone
=== RUN   TestCtestEndpointZoneConversion/only_zone_in_topology_map
=== RUN   TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_different_values
=== RUN   TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_same_values
=== RUN   TestCtestEndpointZoneConversion/only_topology[hostname]_is_populated
=== RUN   TestCtestEndpointZoneConversion/only_nodeName_is_populated_(with_matching_hostname_in_topology)
=== RUN   TestCtestEndpointZoneConversion/empty_topology_map
    ctest_conversion_test.go:180: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:180
        	Error:      	Not equal: 
        	            	expected: v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string{}, NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	actual  : v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string(nil), NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -9,4 +9,3 @@
        	            	  TargetRef: (*v1.ObjectReference)(<nil>),
        	            	- Topology: (map[string]string) {
        	            	- },
        	            	+ Topology: (map[string]string) <nil>,
        	            	  NodeName: (*string)(<nil>),
        	Test:       	TestCtestEndpointZoneConversion/empty_topology_map
        	Messages:   	discovery.Endpoint -> v1beta1.Endpoint
=== RUN   TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value
    ctest_conversion_test.go:176: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:176
        	Error:      	Not equal: 
        	            	expected: discovery.Endpoint{Addresses:[]string(nil), Conditions:discovery.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*core.ObjectReference)(nil), DeprecatedTopology:map[string]string(nil), NodeName:(*string)(nil), Zone:(*string)(nil), Hints:(*discovery.EndpointHints)(nil)}
        	            	actual  : discovery.Endpoint{Addresses:[]string(nil), Conditions:discovery.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*core.ObjectReference)(nil), DeprecatedTopology:map[string]string(nil), NodeName:(*string)(nil), Zone:(*string)(0x14000045990), Hints:(*discovery.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -11,3 +11,3 @@
        	            	  NodeName: (*string)(<nil>),
        	            	- Zone: (*string)(<nil>),
        	            	+ Zone: (*string)(""),
        	            	  Hints: (*discovery.EndpointHints)(<nil>)
        	Test:       	TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value
        	Messages:   	v1beta1.Endpoint -> discovery.Endpoint
    ctest_conversion_test.go:180: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:180
        	Error:      	Not equal: 
        	            	expected: v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string{"topology.kubernetes.io/zone":""}, NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	actual  : v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string(nil), NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -9,5 +9,3 @@
        	            	  TargetRef: (*v1.ObjectReference)(<nil>),
        	            	- Topology: (map[string]string) (len=1) {
        	            	-  (string) (len=27) "topology.kubernetes.io/zone": (string) ""
        	            	- },
        	            	+ Topology: (map[string]string) <nil>,
        	            	  NodeName: (*string)(<nil>),
        	Test:       	TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value
        	Messages:   	discovery.Endpoint -> v1beta1.Endpoint
=== RUN   TestCtestEndpointZoneConversion/nodeName_empty_string
    ctest_conversion_test.go:180: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:180
        	Error:      	Not equal: 
        	            	expected: v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string(nil), NodeName:(*string)(0x14000045210), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	actual  : v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string{"kubernetes.io/hostname":""}, NodeName:(*string)(0x14000045220), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -9,3 +9,5 @@
        	            	  TargetRef: (*v1.ObjectReference)(<nil>),
        	            	- Topology: (map[string]string) <nil>,
        	            	+ Topology: (map[string]string) (len=1) {
        	            	+  (string) (len=22) "kubernetes.io/hostname": (string) ""
        	            	+ },
        	            	  NodeName: (*string)(""),
        	Test:       	TestCtestEndpointZoneConversion/nodeName_empty_string
        	Messages:   	discovery.Endpoint -> v1beta1.Endpoint
=== RUN   TestCtestEndpointZoneConversion/nil_nodeName_pointer_with_non-empty_topology
=== RUN   TestCtestEndpointZoneConversion/topology_with_multiple_unrelated_keys
--- FAIL: TestCtestEndpointZoneConversion (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/no_topology_field (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/non_empty_topology_map,_but_no_zone (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/non_empty_topology_map,_with_zone (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/only_zone_in_topology_map (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_different_values (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_same_values (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/only_topology[hostname]_is_populated (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/only_nodeName_is_populated_(with_matching_hostname_in_topology) (0.00s)
    --- FAIL: TestCtestEndpointZoneConversion/empty_topology_map (0.00s)
    --- FAIL: TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value (0.00s)
    --- FAIL: TestCtestEndpointZoneConversion/nodeName_empty_string (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/nil_nodeName_pointer_with_non-empty_topology (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/topology_with_multiple_unrelated_keys (0.00s)
FAIL
coverage: 0.3% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/discovery/v1beta1	1.216s
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/discovery/validation	0.906s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/events		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/events/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/events/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/events/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/extensions		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/extensions/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/extensions/install		coverage: 0.0% of statements
=== RUN   TestCtestIngressBackendConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/extensions/v1beta1/ctest_conversion_test.go:134]: get default configs: {test_fixture.json [service-port-number] backend [ingressws] {{<nil> &IngressBackend{ServiceName:test-backend,ServicePort:{0 8080 },Resource:nil,} [] []} {<nil> 0x14000012480 [] []}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[ingressws]
[DEBUG-CTEST 2026-02-16 15:39:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[ingressws], int=1)[DEBUG-CTEST 2026-02-16 15:39:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [ingressws]
[DEBUG-CTEST 2026-02-16 15:39:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:34 load all fixtures failed: requested fixture keys not found in test_fixtures.json: ingressws
FAIL	k8s.io/kubernetes/pkg/apis/extensions/v1beta1	0.706s
	k8s.io/kubernetes/pkg/apis/flowcontrol		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/flowcontrol/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/flowcontrol/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/internalbootstrap	2.065s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/flowcontrol/util		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/v1	1.651s	coverage: 0.2% of statements in ./... [no tests to run]
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value)
--- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response (0.00s)
    --- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value) (0.00s)
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value)
--- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response (0.00s)
    --- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value) (0.00s)
=== RUN   TestCtestDefaultWithPriorityLevelConfiguration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:58]: get default configs: {test_fixture.json [Defaulting for Exempt] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {Exempt nil &ExemptPriorityLevelConfiguration{NominalConcurrencyShares:nil,LendablePercent:nil,}} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:36 Mode: 1
2026/02/16 15:39:36 Base JSON size: 64 bytes
2026/02/16 15:39:36 Number of external values: 1
2026/02/16 15:39:36   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:36   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:36   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"exempt":{},"type":"Exempt"},"status":{}})[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:68]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:58]: get default configs: {test_fixture.json [LendablePercent is not specified, should default to zero] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {Limited &LimitedPriorityLevelConfiguration{AssuredConcurrencyShares:5,LimitResponse:LimitResponse{Type:Reject,Queuing:nil,},LendablePercent:nil,BorrowingLimitPercent:nil,} nil} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:36 Mode: 1
2026/02/16 15:39:36 Base JSON size: 128 bytes
2026/02/16 15:39:36 Number of external values: 1
2026/02/16 15:39:36   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:36   [KEEP] spec: map[limited:map[assuredConcurrencyShares:5 limitResponse:map[type:Reject]] type:Limited] (missing in external)
2026/02/16 15:39:36   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"limited":{"assuredConcurrencyShares":5,"limitResponse":{"type":"Reject"}},"type":"Limited"},"status":{}})[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:68]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:58]: get default configs: {test_fixture.json [Edge: No Spec fields] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} { nil nil} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:36 Mode: 1
2026/02/16 15:39:36 Base JSON size: 46 bytes
2026/02/16 15:39:36 Number of external values: 1
2026/02/16 15:39:36   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:36   [KEEP] spec: map[type:] (missing in external)
2026/02/16 15:39:36   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:36 [DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"type":""},"status":{}})[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:68]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestDefaultWithPriorityLevelConfiguration (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/v1beta1	1.724s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_treated_as_zero_value
--- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_treated_as_zero_value (0.00s)
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_preserved_through_conversion
--- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_preserved_through_conversion (0.00s)
=== RUN   TestCtestDefaultWithPriorityLevelConfiguration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta2/ctest_defaults_test.go:155]: get default configs: {test_fixture.json [prioritylevelconfiguration exempt] spec [] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {Exempt nil &ExemptPriorityLevelConfiguration{NominalConcurrencyShares:nil,LendablePercent:nil,}} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:38 [DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:38 Mode: 1
2026/02/16 15:39:38 Base JSON size: 64 bytes
2026/02/16 15:39:38 Number of external values: 89
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] metadata: map[] (missing in external)
2026/02/16 15:39:38   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/16 15:39:38   [KEEP] status: map[] (missing in external)
2026/02/16 15:39:38 [DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:38 [DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"exempt":{},"type":"Exempt"},"status":{}})[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta2/ctest_defaults_test.go:31]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestDefaultWithPriorityLevelConfiguration (0.01s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/v1beta2	2.636s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestConvert_v1beta3_PriorityLevelConfiguration_To_flowcontrol_PriorityLevelConfiguration

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:39:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:38 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/apis/flowcontrol/v1beta3	0.715s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/validation	2.344s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/imagepolicy		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/imagepolicy/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/imagepolicy/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/imagepolicy/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/networking		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/networking/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/networking/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/networking/v1	1.030s	coverage: 0.7% of statements in ./... [no tests to run]
=== RUN   TestCtestIngressBackendConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:28]: Base config: {test_fixture.json [base external spec] backend [ingresses] {<nil> &IngressBackend{ServiceName:test-backend,ServicePort:{0 8080 },Resource:nil,} [] []}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[ingresses]
[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[ingresses], int=1)[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:39:43 [DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:43 Mode: 1
2026/02/16 15:39:43 Base JSON size: 61 bytes
2026/02/16 15:39:43 Number of external values: 1
2026/02/16 15:39:43   [KEEP] backend: map[serviceName:test-backend servicePort:8080] (missing in external)
2026/02/16 15:39:43 [DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:43 [DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"backend":{"serviceName":"test-backend","servicePort":8080}})[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:37]: Base JSON: 
[DEBUG-CTEST 2026-02-16 15:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:61]: Total external test cases: 6
=== RUN   TestCtestIngressBackendConversion/external-0
=== RUN   TestCtestIngressBackendConversion/external-1
=== RUN   TestCtestIngressBackendConversion/external-2
=== RUN   TestCtestIngressBackendConversion/external-3
    ctest_conversion_test.go:78: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:78
        	Error:      	Not equal: 
        	            	expected: v1beta1.IngressSpec{IngressClassName:(*string)(nil), Backend:(*v1beta1.IngressBackend)(0x1400006f740), TLS:[]v1beta1.IngressTLS(nil), Rules:[]v1beta1.IngressRule(nil)}
        	            	actual  : v1beta1.IngressSpec{IngressClassName:(*string)(nil), Backend:(*v1beta1.IngressBackend)(0x1400006fd40), TLS:[]v1beta1.IngressTLS(nil), Rules:[]v1beta1.IngressRule(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -5,3 +5,3 @@
        	            	   ServicePort: (intstr.IntOrString) {
        	            	-   Type: (intstr.Type) 1,
        	            	+   Type: (intstr.Type) 0,
        	            	    IntVal: (int32) 0,
        	Test:       	TestCtestIngressBackendConversion/external-3
        	Messages:   	networking.IngressSpec -> v1beta1.IngressSpec
=== RUN   TestCtestIngressBackendConversion/external-4
=== RUN   TestCtestIngressBackendConversion/external-5

==================== CTEST END ======================
--- FAIL: TestCtestIngressBackendConversion (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-0 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-1 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-2 (0.00s)
    --- FAIL: TestCtestIngressBackendConversion/external-3 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-4 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-5 (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/networking/v1beta1	2.790s
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/networking/validation	2.246s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/node		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/node/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/node/v1		coverage: 0.0% of statements
=== RUN   TestCtestRuntimeClassConversion
=== RUN   TestCtestRuntimeClassConversion/empty
=== RUN   TestCtestRuntimeClassConversion/nil-overhead
=== RUN   TestCtestRuntimeClassConversion/nil-scheduling
=== RUN   TestCtestRuntimeClassConversion/empty-name
=== RUN   TestCtestRuntimeClassConversion/missing-handler
=== RUN   TestCtestRuntimeClassConversion/fully-specified
=== RUN   TestCtestRuntimeClassConversion/empty-overhead
=== RUN   TestCtestRuntimeClassConversion/empty-scheduling
--- PASS: TestCtestRuntimeClassConversion (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/nil-overhead (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/nil-scheduling (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty-name (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/missing-handler (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/fully-specified (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty-overhead (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty-scheduling (0.00s)
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/node/v1alpha1	0.891s	coverage: 0.3% of statements in ./...
	k8s.io/kubernetes/pkg/apis/node/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateRuntimeClass

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:29]: get default configs: {test_fixture.json [baseline runtimeclass] runtimeClass [] {{ } {foo      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} bar <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "runtimeClass" in requested fixtures
2026/02/16 15:39:44 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:44 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Handler":"bar","Overhead":null,"Scheduling":null,"name":"foo"})[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:37]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateRuntimeClass (0.01s)
=== RUN   TestCtestValidateRuntimeUpdate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "runtimeClass" in requested fixtures
2026/02/16 15:39:44 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:44 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Handler":"bar","Overhead":null,"Scheduling":null,"name":"foo"})[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:139]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateRuntimeUpdate (0.00s)
=== RUN   TestCtestValidateOverhead

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "overhead" in requested fixtures
2026/02/16 15:39:44 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:39:44 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"PodFixed":{"cpu":"10","memory":"10G"}})[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:232]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateOverhead (0.00s)
=== RUN   TestCtestValidateScheduling

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:39:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:44 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/apis/node/validation	1.296s
	k8s.io/kubernetes/pkg/apis/policy		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/policy/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/policy/install		coverage: 0.0% of statements
=== RUN   TestCtestConversion
=== RUN   TestCtestConversion/v1_to_internal_with_match_none_selector
=== RUN   TestCtestConversion/v1_to_internal_with_nil_selector
=== RUN   TestCtestConversion/v1_to_internal_with_match_all_selector
=== RUN   TestCtestConversion/v1_to_internal_with_selector_having_empty_key
=== RUN   TestCtestConversion/v1_to_internal_with_selector_having_both_MatchLabels_and_MatchExpressions
=== RUN   TestCtestConversion/v1_to_internal_with_selector_having_empty_MatchExpressions_slice
--- PASS: TestCtestConversion (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_match_none_selector (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_nil_selector (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_match_all_selector (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_selector_having_empty_key (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_selector_having_both_MatchLabels_and_MatchExpressions (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_selector_having_empty_MatchExpressions_slice (0.00s)
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/policy/v1	1.542s	coverage: 0.2% of statements in ./...
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/policy/v1beta1	1.856s	coverage: 0.1% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/policy/validation	1.471s	coverage: 0.3% of statements in ./... [no tests to run]
=== RUN   TestCtestResourceMatches
[DEBUG-CTEST 2026-02-16 15:39:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:21]: Start TestCtestResourceMatches
[DEBUG-CTEST 2026-02-16 15:39:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:147]: Number of test cases: 16
Running 0 th test case.
{all matches 01 [*] foo  true}
=== RUN   TestCtestResourceMatches/all_matches_01
Running 1 th test case.
{checks all rules [doesn't match *] foo  true}
=== RUN   TestCtestResourceMatches/checks_all_rules
Running 2 th test case.
{matches exact rule [foo/bar] foo/bar bar true}
=== RUN   TestCtestResourceMatches/matches_exact_rule
Running 3 th test case.
{matches exact rule 02 [foo/bar] foo  false}
=== RUN   TestCtestResourceMatches/matches_exact_rule_02
Running 4 th test case.
{matches subresource [*/scale] foo/scale scale true}
=== RUN   TestCtestResourceMatches/matches_subresource
Running 5 th test case.
{doesn't match partial subresource hit [foo/bar */other] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_match_partial_subresource_hit
Running 6 th test case.
{matches subresource with multiple slashes [*/other/segment] foo/other/segment other/segment true}
=== RUN   TestCtestResourceMatches/matches_subresource_with_multiple_slashes
Running 7 th test case.
{doesn't fail on empty [] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_fail_on_empty
Running 8 th test case.
{doesn't fail on slash [/] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_fail_on_slash
Running 9 th test case.
{doesn't fail on missing subresource [*/] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_fail_on_missing_subresource
Running 10 th test case.
{doesn't match on not star [*something/other/segment] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_match_on_not_star
Running 11 th test case.
{doesn't match on something else [something/other/segment] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_match_on_something_else
Running 12 th test case.
{nil resources slice [] foo  false}
=== RUN   TestCtestResourceMatches/nil_resources_slice
Running 13 th test case.
{empty combined resource with wildcard [*]   false}
=== RUN   TestCtestResourceMatches/empty_combined_resource_with_wildcard
    ctest_helpers_test.go:158: expected false, got true
Running 14 th test case.
{wildcard with subpath not allowed [foo/*] foo/bar  false}
=== RUN   TestCtestResourceMatches/wildcard_with_subpath_not_allowed
Running 15 th test case.
{resource with trailing slash only [foo/] foo  false}
=== RUN   TestCtestResourceMatches/resource_with_trailing_slash_only
--- FAIL: TestCtestResourceMatches (0.00s)
    --- PASS: TestCtestResourceMatches/all_matches_01 (0.00s)
    --- PASS: TestCtestResourceMatches/checks_all_rules (0.00s)
    --- PASS: TestCtestResourceMatches/matches_exact_rule (0.00s)
    --- PASS: TestCtestResourceMatches/matches_exact_rule_02 (0.00s)
    --- PASS: TestCtestResourceMatches/matches_subresource (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_match_partial_subresource_hit (0.00s)
    --- PASS: TestCtestResourceMatches/matches_subresource_with_multiple_slashes (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_fail_on_empty (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_fail_on_slash (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_fail_on_missing_subresource (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_match_on_not_star (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_match_on_something_else (0.00s)
    --- PASS: TestCtestResourceMatches/nil_resources_slice (0.00s)
    --- FAIL: TestCtestResourceMatches/empty_combined_resource_with_wildcard (0.00s)
    --- PASS: TestCtestResourceMatches/wildcard_with_subpath_not_allowed (0.00s)
    --- PASS: TestCtestResourceMatches/resource_with_trailing_slash_only (0.00s)
=== RUN   TestCtestPolicyRuleBuilder
[DEBUG-CTEST 2026-02-16 15:39:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:165]: Start TestCtestPolicyRuleBuilder
[DEBUG-CTEST 2026-02-16 15:39:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:330]: Number of test cases: 12
Running 0 th test case.
{all empty [] [] [] [] [] false {[] [] [] [] []}}
Running 1 th test case.
{normal resource case [get] [] [pod] [gakki] [] true {[get] [] [pod] [gakki] []}}
Running 2 th test case.
{normal noResourceURLs case [get] [] [] [] [/api/registry/healthz] true {[get] [] [] [] [/api/registry/healthz]}}
Running 3 th test case.
{nonResourceURLs with no-empty groups [get] [] [] [] [/api/registry/healthz] false {[] [] [] [] []}}
Running 4 th test case.
{nonResourceURLs with no-empty resources [get] [] [deployments secrets] [] [/api/registry/healthz] false {[] [] [] [] []}}
Running 5 th test case.
{nonResourceURLs with no-empty resourceNames [get] [] [] [gakki] [/api/registry/healthz] false {[] [] [] [] []}}
Running 6 th test case.
{resource without apiGroups [get] [] [pod] [] [] false {[] [] [] [] []}}
Running 7 th test case.
{resourceNames with illegal verb [list watch create deletecollection] [] [pod] [gakki] [] false {[] [] [] [] []}}
Running 8 th test case.
{no nonResourceURLs nor resources [get] [rbac.authorization.k8s.io] [] [gakki] [] false {[] [] [] [] []}}
Running 9 th test case.
{verbs nil but urls present [] [] [] [] [/healthz] false {[] [] [] [] []}}
Running 10 th test case.
{empty verb string [] [] [pod] [] [] false {[] [] [] [] []}}
    ctest_helpers_test.go:343: Expected PolicyRule{} got PolicyRule{APIGroups:[""], Resources:["pod"], Verbs:[""]}.
Running 11 th test case.
{duplicate verbs with valid resources [get get] [] [pod] [] [] true {[get get] [] [pod] [] []}}
    ctest_helpers_test.go:343: Expected PolicyRule{APIGroups:[""], Resources:["pod"], Verbs:["get" "get"]} got PolicyRule{APIGroups:[""], Resources:["pod"], Verbs:["get"]}.
--- FAIL: TestCtestPolicyRuleBuilder (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/rbac	1.719s
	k8s.io/kubernetes/pkg/apis/rbac/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/rbac/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/rbac/v1		coverage: 0.0% of statements
=== RUN   TestCtestConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/v1alpha1/ctest_conversion_test.go:32]: get default configs: {test_fixture.json [specific user] rolebinding [rolebindings] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{User rbac.authorization.k8s.io/v1alpha1 bob }] {rbac.authorization.k8s.io  foo}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[rolebindings]
[DEBUG-CTEST 2026-02-16 15:39:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[rolebindings], int=1)[DEBUG-CTEST 2026-02-16 15:39:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [rolebindings]
[DEBUG-CTEST 2026-02-16 15:39:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:48 load all fixtures failed: requested fixture keys not found in test_fixtures.json: rolebindings
FAIL	k8s.io/kubernetes/pkg/apis/rbac/v1alpha1	0.688s
	k8s.io/kubernetes/pkg/apis/rbac/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateClusterRoleBinding

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/validation/ctest_validation_test.go:26]: get default configs: {test_fixture.json [valid clusterrolebinding] roleRef [clusterrolebindings] {{ } {master      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{ServiceAccount  validsaname foo} {User rbac.authorization.k8s.io valid@username } {Group rbac.authorization.k8s.io valid@groupname }] {rbac.authorization.k8s.io ClusterRole valid}}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[clusterrolebindings]
[DEBUG-CTEST 2026-02-16 15:39:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[clusterrolebindings], int=1)[DEBUG-CTEST 2026-02-16 15:39:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [clusterrolebindings]
[DEBUG-CTEST 2026-02-16 15:39:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:50 load all fixtures failed: requested fixture keys not found in test_fixtures.json: clusterrolebindings
FAIL	k8s.io/kubernetes/pkg/apis/rbac/validation	1.294s
	k8s.io/kubernetes/pkg/apis/resource		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/resource/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/resource/install	1.357s	coverage: 0.4% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/resource/v1	1.651s	coverage: 0.4% of statements in ./... [no tests to run]
=== RUN   TestCtestSetDefaultDeviceTaint

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/v1alpha3/ctest_defaults_test.go:34]: get default configs: {test_fixture.json [device taint default and preset] spec [customresourcedefinitions] [&DeviceTaintRule{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:DeviceTaintRuleSpec{DeviceSelector:nil,Taint:,},} &DeviceTaintRule{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:DeviceTaintRuleSpec{DeviceSelector:nil,Taint:,},}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:53 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/apis/resource/v1alpha3	1.017s
=== RUN   TestCtestConversion
=== RUN   TestCtestConversion/v1beta1_to_internal_without_alternatives
=== RUN   TestCtestConversion/internal_to_v1beta1_without_alternatives
=== RUN   TestCtestConversion/v1beta1_to_internal_with_alternatives
=== RUN   TestCtestConversion/internal_to_v1beta1_with_alternatives
=== RUN   TestCtestConversion/v1beta1_to_internal_missing_selectors_and_capacity
    ctest_conversion_test.go:511: unexpected result:
           &resource.ResourceClaim{
          	TypeMeta:   {},
          	ObjectMeta: {},
          	Spec: resource.ResourceClaimSpec{
          		Devices: resource.DeviceClaim{
          			Requests: []resource.DeviceRequest{
          				{
          					Name: "foo",
          					Exactly: &resource.ExactDeviceRequest{
          						DeviceClassName: "class-a",
        - 						Selectors:       []resource.DeviceSelector{},
        + 						Selectors:       nil,
          						AllocationMode:  "ExactCount",
          						Count:           1,
          						... // 3 identical fields
          					},
          					FirstAvailable: nil,
          				},
          			},
          			Constraints: nil,
          			Config:      nil,
          		},
          	},
          	Status: {},
          }
=== RUN   TestCtestConversion/internal_to_v1beta1_missing_selectors_and_capacity
=== RUN   TestCtestConversion/v1beta1_to_internal_zero_count
=== RUN   TestCtestConversion/v1beta1_to_internal_large_count
--- FAIL: TestCtestConversion (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_without_alternatives (0.00s)
    --- PASS: TestCtestConversion/internal_to_v1beta1_without_alternatives (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_with_alternatives (0.00s)
    --- PASS: TestCtestConversion/internal_to_v1beta1_with_alternatives (0.00s)
    --- FAIL: TestCtestConversion/v1beta1_to_internal_missing_selectors_and_capacity (0.00s)
    --- PASS: TestCtestConversion/internal_to_v1beta1_missing_selectors_and_capacity (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_zero_count (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_large_count (0.00s)
FAIL
coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/resource/v1beta1	1.234s
=== RUN   TestCtestSetDefaultAllocationMode

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/v1beta2/ctest_defaults_test.go:35]: get default configs: {test_fixture.json [empty exactly] exactly [resourceclaims] { []  0 <nil> [] nil}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceclaims]
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceclaims], int=1)[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceclaims]
[DEBUG-CTEST 2026-02-16 15:39:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:39:53 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceclaims
FAIL	k8s.io/kubernetes/pkg/apis/resource/v1beta2	0.948s
=== RUN   TestCtestTruncateIfTooLong

==================== CTEST START ====================
=== RUN   TestCtestTruncateIfTooLong/nop
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: nop
Input: "hello", maxLen: 10, expected: "hello"
=== RUN   TestCtestTruncateIfTooLong/truncate-to-limit
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-to-limit
Input: "hello world how are you", maxLen: 18, expected: "hello wo...are you"
=== RUN   TestCtestTruncateIfTooLong/truncate-odd-string-even-limit
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-odd-string-even-limit
Input: "abcdefghijklmnopqrs", maxLen: 16, expected: "abcdefg...nopqrs"
=== RUN   TestCtestTruncateIfTooLong/truncate-even-string-even-limit
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-even-string-even-limit
Input: "abcdefghijklmnopqrst", maxLen: 16, expected: "abcdefg...opqrst"
=== RUN   TestCtestTruncateIfTooLong/truncate-even-string-odd-limit
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-even-string-odd-limit
Input: "abcdefghijklmnopqrst", maxLen: 17, expected: "abcdefg...nopqrst"
=== RUN   TestCtestTruncateIfTooLong/very-large-maxlen
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: very-large-maxlen
Input: "short", maxLen: 1000, expected: "short"
=== RUN   TestCtestTruncateIfTooLong/truncate-to-builtin-limit
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-to-builtin-limit
Input: "hello world how are you", maxLen: 1, expected: "hello w...re you"
=== RUN   TestCtestTruncateIfTooLong/truncate-odd-string-odd-limit
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-odd-string-odd-limit
Input: "abcdefghijklmnopqrs", maxLen: 17, expected: "abcdefg...mnopqrs"
=== RUN   TestCtestTruncateIfTooLong/empty-string
[DEBUG-CTEST 2026-02-16 15:39:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: empty-string
Input: "", maxLen: 5, expected: ""

==================== CTEST END ======================
--- PASS: TestCtestTruncateIfTooLong (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/nop (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-to-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-odd-string-even-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-even-string-even-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-even-string-odd-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/very-large-maxlen (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-to-builtin-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-odd-string-odd-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/empty-string (0.00s)
=== RUN   TestCtestValidateDeviceCapacity
=== RUN   TestCtestValidateDeviceCapacity/no-policy
=== RUN   TestCtestValidateDeviceCapacity/invalid-options-duplicate-normalized
=== RUN   TestCtestValidateDeviceCapacity/policy-with-nil-valid-values-pointer
=== RUN   TestCtestValidateDeviceCapacity/policy-with-valid-range-step-zero
--- FAIL: TestCtestValidateDeviceCapacity (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/no-policy (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/invalid-options-duplicate-normalized (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/policy-with-nil-valid-values-pointer (0.00s)
    --- FAIL: TestCtestValidateDeviceCapacity/policy-with-valid-range-step-zero (0.00s)
panic: runtime error: integer divide by zero [recovered]
	panic: runtime error: integer divide by zero

goroutine 98 [running]:
testing.tRunner.func1.2({0x10686bf40, 0x108494550})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10686bf40?, 0x108494550?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/resource/validation.validateRequestPolicyRangeStep({{0x40000000, 0x0}, {0x0}, {0x105d8e1d5, 0x3}, {0x105d94519, 0x8}}, {{0x40000000, 0x0}, {0x0}, ...}, ...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:1042 +0x4f8
k8s.io/kubernetes/pkg/apis/resource/validation.validateRequestPolicyRange({{0x40000000, 0x0}, {0x0}, {0x105d8e1d5, 0x3}, {0x105d94519, 0x8}}, {{0x280000000, 0x0}, {0x0}, ...}, ...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:1028 +0x14e8
k8s.io/kubernetes/pkg/apis/resource/validation.validateValidRequestValues({{0x280000000, 0x0}, {0x0}, {0x105d8e8a9, 0x4}, {0x105d94519, 0x8}}, 0x14000679680, 0x14000679e30)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:962 +0xc84
k8s.io/kubernetes/pkg/apis/resource/validation.validateRequestPolicy({{0x280000000, 0x0}, {0x0}, {0x105d8e8a9, 0x4}, {0x105d94519, 0x8}}, 0x140000a5ef8?, 0x16545899b?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:944 +0x298
k8s.io/kubernetes/pkg/apis/resource/validation.validateMultiAllocatableDeviceCapacity({{{0x280000000, 0x0}, {0x0}, {0x105d8e8a9, 0x4}, {0x105d94519, 0x8}}, 0x14000679680}, 0x14000678ab0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:922 +0x3d0
k8s.io/kubernetes/pkg/apis/resource/validation.TestCtestValidateDeviceCapacity.func1(0x14000583a40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_device_capacity_test.go:177 +0x48
testing.tRunner(0x14000583a40, 0x14000702f50)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 78
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/apis/resource/validation	1.434s
	k8s.io/kubernetes/pkg/apis/scheduling		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/scheduling/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/scheduling/install		coverage: 0.0% of statements
=== RUN   TestCtestIsKnownSystemPriorityClass

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/scheduling/v1/ctest_helpers_test.go:27]: get default configs: {test_fixture.json [priorityclass basic config] priorityClass [] {{ } {example-priorityclass      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} 1000 false example description <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "priorityClass" in requested fixtures
2026/02/16 15:39:55 [DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:39:55 Mode: 1
2026/02/16 15:39:55 Base JSON size: 94 bytes
2026/02/16 15:39:55 Number of external values: 0
2026/02/16 15:39:55 [DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:39:55 [DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"description":"example description","metadata":{"name":"example-priorityclass"},"value":1000})[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:39:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/scheduling/v1/ctest_helpers_test.go:37]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestIsKnownSystemPriorityClass (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/scheduling/v1	3.699s	coverage: 0.8% of statements in ./...
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/scheduling/v1alpha1	1.278s	coverage: 0.3% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/scheduling/v1beta1	1.664s	coverage: 0.3% of statements in ./... [no tests to run]
=== RUN   TestCtestValidatePriorityClass
    ctest_validation_test.go:84: Expected error for negative value, but it succeeded
--- FAIL: TestCtestValidatePriorityClass (0.00s)
=== RUN   TestCtestValidatePriorityClassUpdate
--- PASS: TestCtestValidatePriorityClassUpdate (0.00s)
FAIL
coverage: 0.4% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/apis/scheduling/validation	2.873s
	k8s.io/kubernetes/pkg/apis/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storage/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storage/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storage/util		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/storage/v1	2.091s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/storage/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/storage/v1beta1	2.328s	coverage: 0.4% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/apis/storage/validation	2.400s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/apis/storagemigration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storagemigration/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storagemigration/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestValidateStorageVersionMigration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/storagemigration/validation/ctest_validation_test.go:24]: get default configs: {test_fixture.json [default svm resource] resource [customresourcedefinitions] {non-empty non-empty non-empty}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:00 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/apis/storagemigration/validation	0.906s
=== RUN   TestCtestAuthorizeV0EdgeCases
W0216 15:39:59.591995   90138 abac.go:112] Policy file /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/abac_test3058052586 contained unversioned rules. See docs/admin/authorization.md#abac-mode for ABAC file format details.
--- PASS: TestCtestAuthorizeV0EdgeCases (0.00s)
=== RUN   TestCtestAuthorizeV1beta1EdgeCases
    ctest_abac_test.go:118: expected Allow for /api path, got 2
--- FAIL: TestCtestAuthorizeV1beta1EdgeCases (0.00s)
=== RUN   TestCtestSubjectMatchesEdgeCases
--- FAIL: TestCtestSubjectMatchesEdgeCases (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x100ddd578]

goroutine 39 [running]:
testing.tRunner.func1.2({0x100fb46c0, 0x10144e960})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x100fb46c0?, 0x10144e960?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/auth/authorizer/abac.TestCtestSubjectMatchesEdgeCases(0x14000102e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/auth/authorizer/abac/ctest_abac_test.go:140 +0x38
testing.tRunner(0x14000102e00, 0x101070f08)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/auth/authorizer/abac	0.307s
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/auth/nodeidentifier/ctest_default_test.go:12]: Starting TestCtestDefaultNodeIdentifier_NodeIdentity
Running test case #0: nil user
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/nil_user
Running test case #1: node username without group
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/node_username_without_group
Running test case #2: node group without username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_without_username
Running test case #3: node group and username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_and_username
Running test case #4: empty username with node group
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/empty_username_with_node_group
Running test case #5: malformed node username (missing prefix)
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/malformed_node_username_(missing_prefix)
Running test case #6: multiple node groups, valid username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/multiple_node_groups,_valid_username
Running test case #7: username with extra colon segments
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/username_with_extra_colon_segments
    ctest_default_test.go:80: DefaultNodeIdentifier.NodeIdentity() got nodeName = foo:extra, want 
    ctest_default_test.go:83: DefaultNodeIdentifier.NodeIdentity() got isNode = true, want false
Running test case #8: nil groups slice with node username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/nil_groups_slice_with_node_username
[DEBUG-CTEST 2026-02-16 15:40:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/auth/nodeidentifier/ctest_default_test.go:87]: Completed TestCtestDefaultNodeIdentifier_NodeIdentity
--- FAIL: TestCtestDefaultNodeIdentifier_NodeIdentity (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/nil_user (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/node_username_without_group (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_without_username (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_and_username (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/empty_username_with_node_group (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/malformed_node_username_(missing_prefix) (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/multiple_node_groups,_valid_username (0.00s)
    --- FAIL: TestCtestDefaultNodeIdentifier_NodeIdentity/username_with_extra_colon_segments (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/nil_groups_slice_with_node_username (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/auth/nodeidentifier	2.469s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/capabilities	1.787s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/certauthorization		coverage: 0.0% of statements
=== RUN   TestCtestPodRunning
[DEBUG-CTEST 2026-02-16 15:40:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:14]: Start TestCtestPodRunning
[DEBUG-CTEST 2026-02-16 15:40:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:121]: Total test cases: 8
Running 0 th test case.
=== RUN   TestCtestPodRunning/Watch_type_is_deleted
Running 1 th test case.
=== RUN   TestCtestPodRunning/Pod_Status_type_is_PodRunning
Running 2 th test case.
=== RUN   TestCtestPodRunning/Pod_Status_is_PodFailed
Running 3 th test case.
=== RUN   TestCtestPodRunning/Object_type_is_not_pod
Running 4 th test case.
=== RUN   TestCtestPodRunning/Watch_type_is_Modified_with_PodRunning
    ctest_conditions_test.go:131: PodRunning() = true, want false
Running 5 th test case.
=== RUN   TestCtestPodRunning/Object_is_nil
    ctest_conditions_test.go:127: PodRunning() error = <nil>, wantErr true
Running 6 th test case.
=== RUN   TestCtestPodRunning/Pod_Phase_empty_string
Running 7 th test case.
=== RUN   TestCtestPodRunning/Pod_Phase_unknown_value
--- FAIL: TestCtestPodRunning (0.00s)
    --- PASS: TestCtestPodRunning/Watch_type_is_deleted (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Status_type_is_PodRunning (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Status_is_PodFailed (0.00s)
    --- PASS: TestCtestPodRunning/Object_type_is_not_pod (0.00s)
    --- FAIL: TestCtestPodRunning/Watch_type_is_Modified_with_PodRunning (0.00s)
    --- FAIL: TestCtestPodRunning/Object_is_nil (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Phase_empty_string (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Phase_unknown_value (0.00s)
=== RUN   TestCtestPodCompleted
[DEBUG-CTEST 2026-02-16 15:40:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:138]: Start TestCtestPodCompleted
[DEBUG-CTEST 2026-02-16 15:40:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:245]: Total test cases: 8
Running 0 th test case.
=== RUN   TestCtestPodCompleted/Watch_type_is_deleted
Running 1 th test case.
=== RUN   TestCtestPodCompleted/Pod_Status_is_PodSucceeded
Running 2 th test case.
=== RUN   TestCtestPodCompleted/Pod_Status_is_PodRunning
Running 3 th test case.
=== RUN   TestCtestPodCompleted/Object_type_is_not_pod
Running 4 th test case.
=== RUN   TestCtestPodCompleted/Watch_type_is_Modified_with_PodSucceeded
    ctest_conditions_test.go:255: PodCompleted() = true, want false
Running 5 th test case.
=== RUN   TestCtestPodCompleted/Object_is_nil
    ctest_conditions_test.go:251: PodCompleted() error = <nil>, wantErr true
Running 6 th test case.
=== RUN   TestCtestPodCompleted/Pod_Phase_empty_string
Running 7 th test case.
=== RUN   TestCtestPodCompleted/Pod_Phase_unknown_value
--- FAIL: TestCtestPodCompleted (0.00s)
    --- PASS: TestCtestPodCompleted/Watch_type_is_deleted (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Status_is_PodSucceeded (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Status_is_PodRunning (0.00s)
    --- PASS: TestCtestPodCompleted/Object_type_is_not_pod (0.00s)
    --- FAIL: TestCtestPodCompleted/Watch_type_is_Modified_with_PodSucceeded (0.00s)
    --- FAIL: TestCtestPodCompleted/Object_is_nil (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Phase_empty_string (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Phase_unknown_value (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/client/conditions	1.826s
=== RUN   TestCtestForwardPorts
=== RUN   TestCtestForwardPorts/forward_1_port_with_no_data_either_direction
Forwarding from 127.0.0.1:65293 -> 5000
Forwarding from [::1]:65293 -> 5000
=== RUN   TestCtestForwardPorts/forward_2_ports_with_bidirectional_data
Forwarding from 127.0.0.1:65298 -> 5001
Forwarding from [::1]:65298 -> 5001
Forwarding from 127.0.0.1:65299 -> 6000
Forwarding from [::1]:65299 -> 6000
Handling connection for 65298
Handling connection for 65299
=== RUN   TestCtestForwardPorts/empty_ports_list_should_error
=== RUN   TestCtestForwardPorts/malformed_port_specification_should_error
--- PASS: TestCtestForwardPorts (0.01s)
    --- PASS: TestCtestForwardPorts/forward_1_port_with_no_data_either_direction (0.00s)
    --- PASS: TestCtestForwardPorts/forward_2_ports_with_bidirectional_data (0.01s)
    --- PASS: TestCtestForwardPorts/empty_ports_list_should_error (0.00s)
    --- PASS: TestCtestForwardPorts/malformed_port_specification_should_error (0.00s)
=== RUN   TestCtestForwardPortsReturnsErrorWhenAllBindsFailed
Forwarding from 127.0.0.1:65306 -> 5555
Forwarding from [::1]:65306 -> 5555
Unable to listen on port 65306: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:65306: bind: address already in use unable to create listener: Error listen tcp6 [::1]:65306: bind: address already in use]
--- PASS: TestCtestForwardPortsReturnsErrorWhenAllBindsFailed (0.00s)
=== RUN   TestCtestStream

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/tests/ctest_remotecommand_test.go:130]: Number of test cases: 8
=== RUN   TestCtestStream/error_(exec)
  E0216 15:40:02.807185   90162 exec.go:72] "Unhandled Error" err="error executing command in container: bail" logger="UnhandledError"
=== RUN   TestCtestStream/error_(attach)
  E0216 15:40:02.807874   90162 attach.go:53] "Unhandled Error" err="error attaching to container: bail" logger="UnhandledError"
=== RUN   TestCtestStream/in/out/err_(exec)
=== RUN   TestCtestStream/in/out/err_(attach)
=== RUN   TestCtestStream/oversized_stdin_(exec)
=== RUN   TestCtestStream/oversized_stdin_(attach)
=== RUN   TestCtestStream/in/out/tty_(exec)
=== RUN   TestCtestStream/in/out/tty_(attach)
=== RUN   TestCtestStream/empty_all_fields_(exec)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: empty all fields (exec): unexpected error: 
=== RUN   TestCtestStream/empty_all_fields_(attach)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: empty all fields (attach): unexpected error: 
=== RUN   TestCtestStream/zero_message_count_with_stdout_(exec)
=== RUN   TestCtestStream/zero_message_count_with_stdout_(attach)
=== RUN   TestCtestStream/invalid_protocol_names_(exec)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: invalid protocol names (exec): unexpected error: 
=== RUN   TestCtestStream/invalid_protocol_names_(attach)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: invalid protocol names (attach): unexpected error: 
=== RUN   TestCtestStream/negative_message_count_(treated_as_zero)_(exec)
--- FAIL: TestCtestStream (0.05s)
    --- PASS: TestCtestStream/error_(exec) (0.00s)
    --- PASS: TestCtestStream/error_(attach) (0.00s)
    --- PASS: TestCtestStream/in/out/err_(exec) (0.00s)
    --- PASS: TestCtestStream/in/out/err_(attach) (0.00s)
    --- PASS: TestCtestStream/oversized_stdin_(exec) (0.02s)
    --- PASS: TestCtestStream/oversized_stdin_(attach) (0.01s)
    --- PASS: TestCtestStream/in/out/tty_(exec) (0.00s)
    --- PASS: TestCtestStream/in/out/tty_(attach) (0.00s)
    --- FAIL: TestCtestStream/empty_all_fields_(exec) (0.00s)
    --- FAIL: TestCtestStream/empty_all_fields_(attach) (0.00s)
    --- PASS: TestCtestStream/zero_message_count_with_stdout_(exec) (0.00s)
    --- PASS: TestCtestStream/zero_message_count_with_stdout_(attach) (0.00s)
    --- FAIL: TestCtestStream/invalid_protocol_names_(exec) (0.00s)
    --- FAIL: TestCtestStream/invalid_protocol_names_(attach) (0.00s)
    --- FAIL: TestCtestStream/negative_message_count_(treated_as_zero)_(exec) (0.00s)
panic: strings: negative Repeat count [recovered]
	panic: strings: negative Repeat count

goroutine 738 [running]:
testing.tRunner.func1.2({0x103c59780, 0x104217290})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x103c59780?, 0x104217290?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
strings.Repeat({0x1033714f1?, 0x1042492b8?}, 0x105acb980?)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/strings/strings.go:624 +0x588
k8s.io/kubernetes/pkg/client/tests.TestCtestStream.func1(0x140023fc000)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/tests/ctest_remotecommand_test.go:219 +0x1c5c
testing.tRunner(0x140023fc000, 0x1400074aab0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 145
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/client/tests	0.729s
?   	k8s.io/kubernetes/pkg/cluster/ports	[no test files]
=== RUN   TestCtestClaimPods
=== RUN   TestCtestClaimPods/Claim_pods_with_correct_label
=== RUN   TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_pods
=== RUN   TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_new_pods
=== RUN   TestCtestClaimPods/Controller_can_not_claim_pods_owned_by_another_controller
=== RUN   TestCtestClaimPods/Controller_releases_claimed_pods_when_selector_doesn't_match
I0216 15:40:07.414276   90210 controller_ref_manager.go:240] "Patching pod to remove its controllerRef" pod="default/pod2" gvk="/, Kind=" controller=""
=== RUN   TestCtestClaimPods/Controller_does_not_claim_orphaned_pods_marked_for_deletion
=== RUN   TestCtestClaimPods/Controller_claims_or_release_pods_according_to_selector_with_finalizers
I0216 15:40:07.414792   90210 controller_ref_manager.go:240] "Patching pod to remove its controllerRef" pod="default/pod2" gvk="/, Kind=" controller=""
=== RUN   TestCtestClaimPods/Controller_does_not_claim_pods_of_different_namespace
=== RUN   TestCtestClaimPods/Cluster_scoped_controller_claims_pods_of_specified_namespace
=== RUN   TestCtestClaimPods/Pod_with_nil_labels_map_should_not_be_claimed
=== RUN   TestCtestClaimPods/Pod_with_empty_labels_map_should_not_be_claimed
=== RUN   TestCtestClaimPods/Pod_with_matching_label_but_controller_UID_empty_should_not_be_claimed_as_controller
--- PASS: TestCtestClaimPods (0.00s)
    --- PASS: TestCtestClaimPods/Claim_pods_with_correct_label (0.00s)
    --- PASS: TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_pods (0.00s)
    --- PASS: TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_new_pods (0.00s)
    --- PASS: TestCtestClaimPods/Controller_can_not_claim_pods_owned_by_another_controller (0.00s)
    --- PASS: TestCtestClaimPods/Controller_releases_claimed_pods_when_selector_doesn't_match (0.00s)
    --- PASS: TestCtestClaimPods/Controller_does_not_claim_orphaned_pods_marked_for_deletion (0.00s)
    --- PASS: TestCtestClaimPods/Controller_claims_or_release_pods_according_to_selector_with_finalizers (0.00s)
    --- PASS: TestCtestClaimPods/Controller_does_not_claim_pods_of_different_namespace (0.00s)
    --- PASS: TestCtestClaimPods/Cluster_scoped_controller_claims_pods_of_specified_namespace (0.00s)
    --- PASS: TestCtestClaimPods/Pod_with_nil_labels_map_should_not_be_claimed (0.00s)
    --- PASS: TestCtestClaimPods/Pod_with_empty_labels_map_should_not_be_claimed (0.00s)
    --- PASS: TestCtestClaimPods/Pod_with_matching_label_but_controller_UID_empty_should_not_be_claimed_as_controller (0.00s)
=== RUN   TestCtestGeneratePatchBytesForDelete
=== RUN   TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes
=== RUN   TestCtestGeneratePatchBytesForDelete/check_if_parent_uid_is_escaped
=== RUN   TestCtestGeneratePatchBytesForDelete/check_if_revision_uid_uid_is_escaped
=== RUN   TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_multiple_owners
=== RUN   TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_a_finalizer_and_multiple_owners
=== RUN   TestCtestGeneratePatchBytesForDelete/empty_ownerUID_slice_should_produce_uid_only
    ctest_controller_ref_manager_test.go:323: generatePatchBytesForDelete() got = {"metadata":{"uid":"dep","ownerReferences":null}}, want {"metadata":{"uid":"dep","ownerReferences":[]}}
--- FAIL: TestCtestGeneratePatchBytesForDelete (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_if_parent_uid_is_escaped (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_if_revision_uid_uid_is_escaped (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_multiple_owners (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_a_finalizer_and_multiple_owners (0.00s)
    --- FAIL: TestCtestGeneratePatchBytesForDelete/empty_ownerUID_slice_should_produce_uid_only (0.00s)
FAIL
coverage: 0.6% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller	3.250s
	k8s.io/kubernetes/pkg/controller/apis/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/apis/config/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/apis/config/scheme	2.356s	coverage: 0.2% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/apis/config/v1alpha1	2.490s	coverage: 0.2% of statements in ./... [no tests to run]
=== RUN   TestCtestSimpleSign

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/bootstrap/ctest_bootstrapsigner_test.go:210]: get default configs: {test_fixture.json [default configmap data] data [configmaps] map[kubeconfig:payload]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-16 15:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-16 15:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-16 15:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:09 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/controller/bootstrap	0.779s
# k8s.io/kubernetes/pkg/controller/certificates/signer
# [k8s.io/kubernetes/pkg/controller/certificates/signer]
pkg/controller/certificates/signer/ctest_signer_test.go:294:32: conversion from untyped int to KeyUsage (string) yields a string of one rune, not a string of digits
=== RUN   TestCtestCertificateController

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/ctest_certificate_controller_test.go:189]: get default configs: {test_fixture.json [default csr] metadata.name [certificatesigningrequests] {{ } {test-csr      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[]  <nil> []   [] map[]} {[] []}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[certificatesigningrequests]
[DEBUG-CTEST 2026-02-16 15:40:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[certificatesigningrequests], int=1)[DEBUG-CTEST 2026-02-16 15:40:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [certificatesigningrequests]
[DEBUG-CTEST 2026-02-16 15:40:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:10 load all fixtures failed: requested fixture keys not found in test_fixtures.json: certificatesigningrequests
FAIL	k8s.io/kubernetes/pkg/controller/certificates	1.847s
=== RUN   TestCtestHandle
=== RUN   TestCtestHandle/recognized:false,allowed:false,err:false,clientError:false
=== RUN   TestCtestHandle/recognized:false,allowed:true,err:false,clientError:false
=== RUN   TestCtestHandle/recognized:true,allowed:false,err:true,clientError:false
=== RUN   TestCtestHandle/recognized:true,allowed:true,err:false,clientError:false
=== RUN   TestCtestHandle/recognized:true,allowed:true,err:true,clientError:true
    ctest_sarapprove_test.go:101: expected no client actions due to client error but got: []testing.Action{testing.CreateActionImpl{ActionImpl:testing.ActionImpl{Namespace:"", Verb:"create", Resource:schema.GroupVersionResource{Group:"authorization.k8s.io", Version:"v1", Resource:"subjectaccessreviews"}, Subresource:""}, Name:"", Object:(*v1.SubjectAccessReview)(0x14000028780), CreateOptions:v1.CreateOptions{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, DryRun:[]string(nil), FieldManager:"", FieldValidation:""}}}
--- FAIL: TestCtestHandle (0.00s)
    --- PASS: TestCtestHandle/recognized:false,allowed:false,err:false,clientError:false (0.00s)
    --- PASS: TestCtestHandle/recognized:false,allowed:true,err:false,clientError:false (0.00s)
    --- PASS: TestCtestHandle/recognized:true,allowed:false,err:true,clientError:false (0.00s)
    --- PASS: TestCtestHandle/recognized:true,allowed:true,err:false,clientError:false (0.00s)
    --- FAIL: TestCtestHandle/recognized:true,allowed:true,err:true,clientError:true (0.00s)
FAIL
coverage: 0.6% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/certificates/approver	3.024s
=== RUN   TestCtestCertificateAuthority
=== RUN   TestCtestCertificateAuthority/ca_info
=== RUN   TestCtestCertificateAuthority/key_usage
=== RUN   TestCtestCertificateAuthority/ext_key_usage
=== RUN   TestCtestCertificateAuthority/backdate_without_short
=== RUN   TestCtestCertificateAuthority/backdate_without_short_and_super_small_ttl
=== RUN   TestCtestCertificateAuthority/backdate_with_short
=== RUN   TestCtestCertificateAuthority/backdate_with_short_and_super_small_ttl
=== RUN   TestCtestCertificateAuthority/backdate_with_short_but_longer_ttl
=== RUN   TestCtestCertificateAuthority/truncate_expiration
=== RUN   TestCtestCertificateAuthority/uri_sans
=== RUN   TestCtestCertificateAuthority/expired_ca
=== RUN   TestCtestCertificateAuthority/expired_ca_with_backdate
=== RUN   TestCtestCertificateAuthority/zero_ttl
=== RUN   TestCtestCertificateAuthority/very_large_ttl_(expect_truncation_to_24h)
=== RUN   TestCtestCertificateAuthority/negative_ttl_(treated_as_zero)
    ctest_authority_test.go:275: unexpected diff:   x509.Certificate{
          	... // 13 ignored fields
          	NotBefore: Inverse(RoundTime, s"2026-02-16 15:40:10 -0600 CST"),
        - 	NotAfter:  time.Time(Inverse(RoundTime, s"2026-02-16 20:40:10 +0000 UTC")),
        + 	NotAfter:  time.Time(Inverse(RoundTime, s"2026-02-16 15:40:10 -0600 CST")),
          	KeyUsage:  0,
          	... // 26 ignored and 10 identical fields
          }
--- FAIL: TestCtestCertificateAuthority (0.01s)
    --- PASS: TestCtestCertificateAuthority/ca_info (0.00s)
    --- PASS: TestCtestCertificateAuthority/key_usage (0.00s)
    --- PASS: TestCtestCertificateAuthority/ext_key_usage (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_without_short (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_without_short_and_super_small_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_with_short (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_with_short_and_super_small_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_with_short_but_longer_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/truncate_expiration (0.00s)
    --- PASS: TestCtestCertificateAuthority/uri_sans (0.00s)
    --- PASS: TestCtestCertificateAuthority/expired_ca (0.00s)
    --- PASS: TestCtestCertificateAuthority/expired_ca_with_backdate (0.00s)
    --- PASS: TestCtestCertificateAuthority/zero_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/very_large_ttl_(expect_truncation_to_24h) (0.00s)
    --- FAIL: TestCtestCertificateAuthority/negative_ttl_(treated_as_zero) (0.00s)
FAIL
coverage: 0.2% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/certificates/authority	3.112s
=== RUN   TestCtestCleanerWithApprovedExpiredCSR

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/cleaner/ctest_cleaner_test.go:249]: Total test cases: 18
Running 0 th test case: no delete approved not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline
Running 1 th test case: no delete approved passed deadline not issued
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_passed_deadline_not_issued
Running 2 th test case: delete approved passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline
  I0216 15:40:12.853507   90270 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="fake-csr" approvedExpiration="1h0m0s"
Running 3 th test case: no delete denied not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_denied_not_passed_deadline
Running 4 th test case: delete denied passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_denied_passed_deadline
  I0216 15:40:12.853746   90270 cleaner.go:149] "Cleaning CSR as it is more than deniedExpiration duration old and denied." csr="fake-csr" deniedExpiration="1h0m0s"
Running 5 th test case: no delete failed not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_failed_not_passed_deadline
Running 6 th test case: delete failed passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_failed_passed_deadline
  I0216 15:40:12.853785   90270 cleaner.go:162] "Cleaning CSR as it is more than deniedExpiration duration old and failed." csr="fake-csr" deniedExpiration="1h0m0s"
Running 7 th test case: no delete pending not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_pending_not_passed_deadline
Running 8 th test case: delete pending passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_pending_passed_deadline
  I0216 15:40:12.853820   90270 cleaner.go:137] "Cleaning CSR as it is more than pendingExpiration duration old and unhandled." csr="fake-csr" pendingExpiration="24h0m0s"
Running 9 th test case: no delete approved not passed deadline unexpired
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline_unexpired
Running 10 th test case: delete approved not passed deadline expired
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_approved_not_passed_deadline_expired
  I0216 15:40:12.853949   90270 cleaner.go:123] "Cleaning CSR as the associated certificate is expired." csr="fake-csr"
Running 11 th test case: delete approved passed deadline unparseable
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline_unparseable
  I0216 15:40:12.853979   90270 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="fake-csr" approvedExpiration="1h0m0s"
Running 12 th test case: zero creation timestamp
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/zero_creation_timestamp
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions
Running 13 th test case: future creation timestamp
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/future_creation_timestamp
Running 14 th test case: unknown condition type
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/unknown_condition_type
Running 15 th test case: nil conditions slice
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/nil_conditions_slice
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions
Running 16 th test case: empty certificate data
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/empty_certificate_data
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions
Running 17 th test case: certificate nil with approved condition past deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/certificate_nil_with_approved_condition_past_deadline
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions

==================== CTEST END ======================
--- FAIL: TestCtestCleanerWithApprovedExpiredCSR (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_passed_deadline_not_issued (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_denied_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_denied_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_failed_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_failed_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_pending_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_pending_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline_unexpired (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_approved_not_passed_deadline_expired (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline_unparseable (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/zero_creation_timestamp (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/future_creation_timestamp (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/unknown_condition_type (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/nil_conditions_slice (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/empty_certificate_data (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/certificate_nil_with_approved_condition_past_deadline (0.00s)
=== RUN   TestCtestPCRCleaner
=== RUN   TestCtestPCRCleaner/Pending_request_within_the_threshold_should_be_left_alone
=== RUN   TestCtestPCRCleaner/Pending_request_outside_the_threshold_should_be_deleted
=== RUN   TestCtestPCRCleaner/Terminal_request_within_the_threshold_should_be_left_alone
=== RUN   TestCtestPCRCleaner/Terminal_request_outside_the_threshold_should_be_deleted
=== RUN   TestCtestPCRCleaner/Pending_request_with_zero_MaxExpirationSeconds_within_threshold
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_nil_PKIXPublicKey
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_nil_ProofOfPossession
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_negative_MaxExpirationSeconds
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_huge_MaxExpirationSeconds_within_threshold
=== RUN   TestCtestPCRCleaner/Pending_request_without_ServiceAccountUID_within_threshold
--- FAIL: TestCtestPCRCleaner (0.05s)
    --- PASS: TestCtestPCRCleaner/Pending_request_within_the_threshold_should_be_left_alone (0.05s)
    --- PASS: TestCtestPCRCleaner/Pending_request_outside_the_threshold_should_be_deleted (0.00s)
    --- PASS: TestCtestPCRCleaner/Terminal_request_within_the_threshold_should_be_left_alone (0.00s)
    --- PASS: TestCtestPCRCleaner/Terminal_request_outside_the_threshold_should_be_deleted (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_zero_MaxExpirationSeconds_within_threshold (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_nil_PKIXPublicKey (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_nil_ProofOfPossession (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_negative_MaxExpirationSeconds (0.00s)
    --- PASS: TestCtestPCRCleaner/Pending_request_with_huge_MaxExpirationSeconds_within_threshold (0.00s)
    --- PASS: TestCtestPCRCleaner/Pending_request_without_ServiceAccountUID_within_threshold (0.00s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/certificates/cleaner	2.323s
=== RUN   TestCtestSyncCounter

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/clustertrustbundlepublisher/ctest_metrics_test.go:112]: Number of test cases: 7
Running #0 test case: nil error
=== RUN   TestCtestSyncCounter/nil_error
Running #1 test case: kube api error
=== RUN   TestCtestSyncCounter/kube_api_error
Running #2 test case: nested kube api error
=== RUN   TestCtestSyncCounter/nested_kube_api_error
Running #3 test case: kube api error without code
=== RUN   TestCtestSyncCounter/kube_api_error_without_code
Running #4 test case: general error
=== RUN   TestCtestSyncCounter/general_error
Running #5 test case: custom wrapped error without kube code
=== RUN   TestCtestSyncCounter/custom_wrapped_error_without_kube_code
Running #6 test case: nil error (duplicate edge)
=== RUN   TestCtestSyncCounter/nil_error_(duplicate_edge)

==================== CTEST END ======================
--- PASS: TestCtestSyncCounter (0.00s)
    --- PASS: TestCtestSyncCounter/nil_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_error (0.00s)
    --- PASS: TestCtestSyncCounter/nested_kube_api_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_error_without_code (0.00s)
    --- PASS: TestCtestSyncCounter/general_error (0.00s)
    --- PASS: TestCtestSyncCounter/custom_wrapped_error_without_kube_code (0.00s)
    --- PASS: TestCtestSyncCounter/nil_error_(duplicate_edge) (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/certificates/clustertrustbundlepublisher	2.162s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestSyncCounter

==================== CTEST START ====================
Running 0 th test case: nil error
=== RUN   TestCtestSyncCounter/nil_error
Running 1 th test case: kube api notfound error
=== RUN   TestCtestSyncCounter/kube_api_notfound_error
Running 2 th test case: kube api generic status error without code
=== RUN   TestCtestSyncCounter/kube_api_generic_status_error_without_code
Running 3 th test case: general error
=== RUN   TestCtestSyncCounter/general_error
Running 4 th test case: empty error string
=== RUN   TestCtestSyncCounter/empty_error_string
Running 5 th test case: api conflict error (code 409)
=== RUN   TestCtestSyncCounter/api_conflict_error_(code_409)
Running 6 th test case: api error with empty resource
=== RUN   TestCtestSyncCounter/api_error_with_empty_resource
Running 7 th test case: nil pointer error (treated as nil)
=== RUN   TestCtestSyncCounter/nil_pointer_error_(treated_as_nil)

==================== CTEST END ======================
--- PASS: TestCtestSyncCounter (0.01s)
    --- PASS: TestCtestSyncCounter/nil_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_notfound_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_generic_status_error_without_code (0.00s)
    --- PASS: TestCtestSyncCounter/general_error (0.00s)
    --- PASS: TestCtestSyncCounter/empty_error_string (0.00s)
    --- PASS: TestCtestSyncCounter/api_conflict_error_(code_409) (0.00s)
    --- PASS: TestCtestSyncCounter/api_error_with_empty_resource (0.00s)
    --- PASS: TestCtestSyncCounter/nil_pointer_error_(treated_as_nil) (0.00s)
=== RUN   TestCtestConfigMapCreation

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/rootcacertpublisher/ctest_publisher_test.go:44]: get default configs: {test_fixture.json [default-ca-configmap] data [configmaps] map[ca.crt:placeholder-root-ca]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-16 15:40:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-16 15:40:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-16 15:40:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:17 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/controller/certificates/rootcacertpublisher	1.551s
FAIL	k8s.io/kubernetes/pkg/controller/certificates/signer [build failed]
	k8s.io/kubernetes/pkg/controller/certificates/signer/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/certificates/signer/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/clusterroleaggregation	3.117s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/cronjob	3.486s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/cronjob/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/cronjob/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/cronjob/metrics		coverage: 0.0% of statements
=== RUN   TestCtestStoreDaemonSetStatus
=== RUN   TestCtestStoreDaemonSetStatus/succeed_immediately
=== RUN   TestCtestStoreDaemonSetStatus/succeed_after_one_update_failure
=== RUN   TestCtestStoreDaemonSetStatus/fail_after_two_update_failures
=== RUN   TestCtestStoreDaemonSetStatus/fail_after_one_update_failure_and_one_get_failure
=== RUN   TestCtestStoreDaemonSetStatus/fail_when_get_always_errors_(edge_case)
    ctest_daemon_controller_test.go:129: storeDaemonSetStatus() got <nil>, expected fake get error
    ctest_daemon_controller_test.go:132: Get() was called 0 times, expected 3 times
    ctest_daemon_controller_test.go:135: UpdateStatus() was called 1 times, expected 0 times
=== RUN   TestCtestStoreDaemonSetStatus/fail_when_update_errors_exceed_max_retries_(edge_case)
    ctest_daemon_controller_test.go:132: Get() was called 1 times, expected 2 times
    ctest_daemon_controller_test.go:135: UpdateStatus() was called 2 times, expected 3 times
--- FAIL: TestCtestStoreDaemonSetStatus (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/succeed_immediately (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/succeed_after_one_update_failure (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/fail_after_two_update_failures (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/fail_after_one_update_failure_and_one_get_failure (0.00s)
    --- FAIL: TestCtestStoreDaemonSetStatus/fail_when_get_always_errors_(edge_case) (0.00s)
    --- FAIL: TestCtestStoreDaemonSetStatus/fail_when_update_errors_exceed_max_retries_(edge_case) (0.00s)
FAIL
coverage: 0.6% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/daemon	1.879s
	k8s.io/kubernetes/pkg/controller/daemon/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/daemon/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/daemon/util	1.698s	coverage: 0.3% of statements in ./... [no tests to run]
=== RUN   TestCtestRequeueStuckDeployment

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestRequeueStuckDeployment/nil_progressDeadlineSeconds_specified
Running test case 0: nil progressDeadlineSeconds specified
=== RUN   TestCtestRequeueStuckDeployment/infinite_progressDeadlineSeconds_specified
Running test case 1: infinite progressDeadlineSeconds specified
=== RUN   TestCtestRequeueStuckDeployment/no_progressing_condition_found
Running test case 2: no progressing condition found
=== RUN   TestCtestRequeueStuckDeployment/complete_deployment_does_not_need_to_be_requeued
Running test case 3: complete deployment does not need to be requeued
=== RUN   TestCtestRequeueStuckDeployment/already_failed_deployment_does_not_need_to_be_requeued
Running test case 4: already failed deployment does not need to be requeued
=== RUN   TestCtestRequeueStuckDeployment/stuck_deployment_-_30s
Running test case 5: stuck deployment - 30s
    progress.go:195: I0216 15:40:25.930693] Queueing up deployment for a progress check deployment="progress-test" queueAfter=30
=== RUN   TestCtestRequeueStuckDeployment/stuck_deployment_-_1s
Running test case 6: stuck deployment - 1s
    progress.go:195: I0216 15:40:25.931372] Queueing up deployment for a progress check deployment="progress-test" queueAfter=1
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_less_than_a_second_=>_now
Running test case 7: failed deployment - less than a second => now
    progress.go:191: I0216 15:40:25.931414] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_now
Running test case 8: failed deployment - now
    progress.go:191: I0216 15:40:25.931444] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_1s_after_deadline
Running test case 9: failed deployment - 1s after deadline
    progress.go:191: I0216 15:40:25.931506] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_60s_after_deadline
Running test case 10: failed deployment - 60s after deadline
    progress.go:191: I0216 15:40:25.931534] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/edge:_zero_progressDeadlineSeconds_with_stuck_condition
Running test case 11: edge: zero progressDeadlineSeconds with stuck condition
    progress.go:191: I0216 15:40:25.931565] Queueing up deployment for a progress check now deployment="progress-test"

==================== CTEST END ======================
--- PASS: TestCtestRequeueStuckDeployment (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/nil_progressDeadlineSeconds_specified (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/infinite_progressDeadlineSeconds_specified (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/no_progressing_condition_found (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/complete_deployment_does_not_need_to_be_requeued (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/already_failed_deployment_does_not_need_to_be_requeued (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/stuck_deployment_-_30s (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/stuck_deployment_-_1s (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_less_than_a_second_=>_now (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_now (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_1s_after_deadline (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_60s_after_deadline (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/edge:_zero_progressDeadlineSeconds_with_stuck_condition (0.00s)
=== RUN   TestCtestSyncRolloutStatus

==================== CTEST OVERRIDE ONLY START ====================
=== RUN   TestCtestSyncRolloutStatus/General:_remove_Progressing_condition_and_do_not_estimate_progress_if_deployment_has_no_Progress_Deadline
Running test case 0: General: remove Progressing condition and do not estimate progress if deployment has no Progress Deadline
=== RUN   TestCtestSyncRolloutStatus/General:_do_not_estimate_progress_of_deployment_with_only_one_active_ReplicaSet
Running test case 1: General: do not estimate progress of deployment with only one active ReplicaSet
=== RUN   TestCtestSyncRolloutStatus/DeploymentProgressing:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True
Running test case 2: DeploymentProgressing: dont update lastTransitionTime if deployment already has Progressing=True
=== RUN   TestCtestSyncRolloutStatus/DeploymentProgressing:_update_everything_if_deployment_has_Progressing=False
Running test case 3: DeploymentProgressing: update everything if deployment has Progressing=False
=== RUN   TestCtestSyncRolloutStatus/DeploymentProgressing:_create_Progressing_condition_if_it_does_not_exist
Running test case 4: DeploymentProgressing: create Progressing condition if it does not exist
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True
Running test case 5: DeploymentComplete: dont update lastTransitionTime if deployment already has Progressing=True
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_update_everything_if_deployment_has_Progressing=False
Running test case 6: DeploymentComplete: update everything if deployment has Progressing=False
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_create_Progressing_condition_if_it_does_not_exist
Running test case 7: DeploymentComplete: create Progressing condition if it does not exist
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_defend_against_NPE_when_newRS=nil
Running test case 8: DeploymentComplete: defend against NPE when newRS=nil
=== RUN   TestCtestSyncRolloutStatus/DeploymentTimedOut:_update_status_if_rollout_exceeds_Progress_Deadline
Running test case 9: DeploymentTimedOut: update status if rollout exceeds Progress Deadline
    deployment_util.go:812: I0216 15:40:25.932211] Deployment timed out from last progress check deployment="progress-test" timeout=true from="2017-02-15 18:49:00 +0000 UTC" now="2026-02-16 15:40:25.932151 -0600 CST m=+0.051383001"
=== RUN   TestCtestSyncRolloutStatus/DeploymentTimedOut:_do_not_update_status_if_deployment_has_existing_timedOut_condition
Running test case 10: DeploymentTimedOut: do not update status if deployment has existing timedOut condition
=== RUN   TestCtestSyncRolloutStatus/Edge:_nil_ProgressDeadlineSeconds_and_newRS=nil
Running test case 11: Edge: nil ProgressDeadlineSeconds and newRS=nil

==================== CTEST END ======================
--- PASS: TestCtestSyncRolloutStatus (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/General:_remove_Progressing_condition_and_do_not_estimate_progress_if_deployment_has_no_Progress_Deadline (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/General:_do_not_estimate_progress_of_deployment_with_only_one_active_ReplicaSet (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentProgressing:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentProgressing:_update_everything_if_deployment_has_Progressing=False (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentProgressing:_create_Progressing_condition_if_it_does_not_exist (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_update_everything_if_deployment_has_Progressing=False (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_create_Progressing_condition_if_it_does_not_exist (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_defend_against_NPE_when_newRS=nil (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentTimedOut:_update_status_if_rollout_exceeds_Progress_Deadline (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentTimedOut:_do_not_update_status_if_deployment_has_existing_timedOut_condition (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/Edge:_nil_ProgressDeadlineSeconds_and_newRS=nil (0.00s)
=== RUN   TestCtestScaleDownOldReplicaSets

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/deployment/ctest_recreate_test.go:31]: get default configs: {test_fixture.json [scale down old replica sets deployment] spec [deployments replicasets] &Deployment{ObjectMeta:{foo      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:DeploymentSpec{Replicas:*3,Selector:&v1.LabelSelector{MatchLabels:map[string]string{foo: bar,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[foo:bar] map[] [] [] []} {[] [] [{default-container busybox [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}},Strategy:DeploymentStrategy{Type:,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:nil,Paused:false,ProgressDeadlineSeconds:nil,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,TerminatingReplicas:nil,},}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[deployments replicasets]
[DEBUG-CTEST 2026-02-16 15:40:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[deployments replicasets], int=2)[DEBUG-CTEST 2026-02-16 15:40:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [replicasets]
[DEBUG-CTEST 2026-02-16 15:40:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:25 load all fixtures failed: requested fixture keys not found in test_fixtures.json: replicasets
FAIL	k8s.io/kubernetes/pkg/controller/deployment	1.280s
	k8s.io/kubernetes/pkg/controller/deployment/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/deployment/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/deployment/util	1.978s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/devicetainteviction	3.325s	coverage: 0.7% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/devicetainteviction/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/disruption	3.018s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestStaleEndpointsTracker

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/endpoint/ctest_endpoints_tracker_test.go:27]: get default configs: {test_fixture.json [default endpoints subset] subsets [endpoints] [{[{6.7.8.9  0x1069278d0 nil}] [] [{ 1000  <nil>}]}]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints]
[DEBUG-CTEST 2026-02-16 15:40:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints], int=1)[DEBUG-CTEST 2026-02-16 15:40:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints]
[DEBUG-CTEST 2026-02-16 15:40:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:27 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints
FAIL	k8s.io/kubernetes/pkg/controller/endpoint	1.145s
	k8s.io/kubernetes/pkg/controller/endpoint/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/endpoint/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/endpointslice	2.245s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/endpointslice/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/endpointslice/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestShouldMirror

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints services endpointSlices pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:40:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints services endpointSlices pods deployments statefulsets daemonsets replicasets], int=8)[DEBUG-CTEST 2026-02-16 15:40:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints endpointSlices statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:40:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:31 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints, endpointSlices, statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/controller/endpointslicemirroring	1.037s
	k8s.io/kubernetes/pkg/controller/endpointslicemirroring/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/endpointslicemirroring/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/endpointslicemirroring/metrics	0.967s	coverage: 0.3% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/garbagecollector	1.812s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/garbagecollector/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/garbagecollector/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/garbagecollector/metaonly		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/garbagecollector/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/history	1.369s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestNewBackoffRecord
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_and_two_new_failures
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures_followed_by_success
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures,_one_success_and_two_more_failures
=== RUN   TestCtestNewBackoffRecord/Backoff_store_having_failure_count_2_and_one_new_failure
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_with_no_success/failure
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_with_one_success
=== RUN   TestCtestNewBackoffRecord/Nil_store_initializer_(should_behave_like_empty)
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_and_one_new_failure
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_with_success_and_failure_at_same_timestamp
=== RUN   TestCtestNewBackoffRecord/Empty_store_with_zero_(epoch)_failure_time
--- PASS: TestCtestNewBackoffRecord (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_and_two_new_failures (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures_followed_by_success (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures,_one_success_and_two_more_failures (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Backoff_store_having_failure_count_2_and_one_new_failure (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_with_no_success/failure (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_with_one_success (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Nil_store_initializer_(should_behave_like_empty) (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_and_one_new_failure (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_with_success_and_failure_at_same_timestamp (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_store_with_zero_(epoch)_failure_time (0.00s)
=== RUN   TestCtestGetFinishedTime
=== RUN   TestCtestGetFinishedTime/Pod_with_terminated_container_but_nil_FinishedAt
=== RUN   TestCtestGetFinishedTime/Pod_with_multiple_containers_and_all_containers_terminated
=== RUN   TestCtestGetFinishedTime/fallback_to_deletionTimestamp,_decremented_by_grace_period
=== RUN   TestCtestGetFinishedTime/fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_False
=== RUN   TestCtestGetFinishedTime/skip_fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_True
=== RUN   TestCtestGetFinishedTime/fallback_to_creationTimestamp
=== RUN   TestCtestGetFinishedTime/Pod_with_no_container_statuses_and_no_timestamps
=== RUN   TestCtestGetFinishedTime/Pod_with_nil_DeletionTimestamp_and_zero_CreationTimestamp
=== RUN   TestCtestGetFinishedTime/Pod_with_multiple_containers;_two_containers_in_terminated_state_and_one_in_running_state;_fallback_to_deletionTimestamp
=== RUN   TestCtestGetFinishedTime/fallback_to_deletionTimestamp
=== RUN   TestCtestGetFinishedTime/Pod_with_sidecar_container_and_all_containers_terminated
--- PASS: TestCtestGetFinishedTime (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_terminated_container_but_nil_FinishedAt (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_multiple_containers_and_all_containers_terminated (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_deletionTimestamp,_decremented_by_grace_period (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_False (0.00s)
    --- PASS: TestCtestGetFinishedTime/skip_fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_True (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_creationTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_no_container_statuses_and_no_timestamps (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_nil_DeletionTimestamp_and_zero_CreationTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_multiple_containers;_two_containers_in_terminated_state_and_one_in_running_state;_fallback_to_deletionTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_deletionTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_sidecar_container_and_all_containers_terminated (0.00s)
=== RUN   TestCtestGetRemainingBackoffTime
=== RUN   TestCtestGetRemainingBackoffTime/no_failures
=== RUN   TestCtestGetRemainingBackoffTime/eight_failures;_current_time_and_failure_time_are_same;_backoff_not_exceeding_maxBackoff
=== RUN   TestCtestGetRemainingBackoffTime/nine_failures;_current_time_and_failure_time_are_same;_backoff_exceeding_maxBackoff
=== RUN   TestCtestGetRemainingBackoffTime/zero_default_backoff_should_yield_zero_duration_regardless
=== RUN   TestCtestGetRemainingBackoffTime/maxBackoff_less_than_calculated_backoff_clamps_to_maxBackoff
=== RUN   TestCtestGetRemainingBackoffTime/nil_lastFailureTime_should_behave_like_no_failures
--- FAIL: TestCtestGetRemainingBackoffTime (0.00s)
    --- PASS: TestCtestGetRemainingBackoffTime/no_failures (0.00s)
    --- PASS: TestCtestGetRemainingBackoffTime/eight_failures;_current_time_and_failure_time_are_same;_backoff_not_exceeding_maxBackoff (0.00s)
    --- PASS: TestCtestGetRemainingBackoffTime/nine_failures;_current_time_and_failure_time_are_same;_backoff_exceeding_maxBackoff (0.00s)
    --- PASS: TestCtestGetRemainingBackoffTime/zero_default_backoff_should_yield_zero_duration_regardless (0.00s)
    --- PASS: TestCtestGetRemainingBackoffTime/maxBackoff_less_than_calculated_backoff_clamps_to_maxBackoff (0.00s)
    --- FAIL: TestCtestGetRemainingBackoffTime/nil_lastFailureTime_should_behave_like_no_failures (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x103530b04]

goroutine 116 [running]:
testing.tRunner.func1.2({0x104259f80, 0x106740c40})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x104259f80?, 0x106740c40?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/controller/job.getRemainingTimeForFailuresCount({0x104800300?, 0x140002f1950?}, 0x0?, 0x1067a7c20?, 0xa5f08?, 0x10012a040?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/job/backoff_utils.go:272 +0x1f4
k8s.io/kubernetes/pkg/controller/job.backoffRecord.getRemainingTime(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/job/backoff_utils.go:240
k8s.io/kubernetes/pkg/controller/job.TestCtestGetRemainingBackoffTime.func1(0x14000582a80)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/job/ctest_backoff_utils_test.go:585 +0x130
testing.tRunner(0x14000582a80, 0x1400062ef60)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 110
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controller/job	0.779s
	k8s.io/kubernetes/pkg/controller/job/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/job/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/job/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/job/util	1.525s	coverage: 0.2% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/namespace		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/namespace/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/namespace/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestUpdateConditions
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:21]: Starting TestCtestUpdateConditions
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:221]: Total test cases: 10
Running 0 th test case: leave unknown
=== RUN   TestCtestUpdateConditions/leave_unknown
Running 1 th test case: replace with success
=== RUN   TestCtestUpdateConditions/replace_with_success
Running 2 th test case: leave different order
=== RUN   TestCtestUpdateConditions/leave_different_order
Running 3 th test case: overwrite with failure
=== RUN   TestCtestUpdateConditions/overwrite_with_failure
Running 4 th test case: write new failure
=== RUN   TestCtestUpdateConditions/write_new_failure
Running 5 th test case: empty starting status and no new conditions
=== RUN   TestCtestUpdateConditions/empty_starting_status_and_no_new_conditions
Running 6 th test case: starting status nil (should not panic)
=== RUN   TestCtestUpdateConditions/starting_status_nil_(should_not_panic)
    ctest_status_condition_utils_test.go:230: panic occurred with nil startingStatus: runtime error: invalid memory address or nil pointer dereference
Running 7 th test case: new condition with unknown type and true status
=== RUN   TestCtestUpdateConditions/new_condition_with_unknown_type_and_true_status
    ctest_status_condition_utils_test.go:242: expected 6 conditions, got 5: [{NamespaceDeletionDiscoveryFailure False 2026-02-16 15:40:40.047649 -0600 CST m=+0.041027251 ResourcesDiscovered All resources successfully discovered} {NamespaceDeletionGroupVersionParsingFailure False 2026-02-16 15:40:40.047649 -0600 CST m=+0.041027376 ParsedGroupVersions All legacy kube types successfully parsed} {NamespaceDeletionContentFailure False 2026-02-16 15:40:40.047649 -0600 CST m=+0.041027542 ContentDeleted All content successfully deleted, may be waiting on finalization} {NamespaceContentRemaining False 2026-02-16 15:40:40.047649 -0600 CST m=+0.041027709 ContentRemoved All content successfully removed} {NamespaceFinalizersRemaining False 2026-02-16 15:40:40.047649 -0600 CST m=+0.041027834 ContentHasNoFinalizers All content-preserving finalizers finished}]
Running 8 th test case: duplicate condition types with differing statuses
=== RUN   TestCtestUpdateConditions/duplicate_condition_types_with_differing_statuses
    ctest_status_condition_utils_test.go:249: condition mismatch at index 0: expected v1.NamespaceCondition{Type:"NamespaceDeletionDiscoveryFailure", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}, got v1.NamespaceCondition{Type:"NamespaceDeletionDiscoveryFailure", Status:"False", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}
Running 9 th test case: very large number of existing unknown conditions
=== RUN   TestCtestUpdateConditions/very_large_number_of_existing_unknown_conditions
--- FAIL: TestCtestUpdateConditions (0.00s)
    --- PASS: TestCtestUpdateConditions/leave_unknown (0.00s)
    --- PASS: TestCtestUpdateConditions/replace_with_success (0.00s)
    --- PASS: TestCtestUpdateConditions/leave_different_order (0.00s)
    --- PASS: TestCtestUpdateConditions/overwrite_with_failure (0.00s)
    --- PASS: TestCtestUpdateConditions/write_new_failure (0.00s)
    --- PASS: TestCtestUpdateConditions/empty_starting_status_and_no_new_conditions (0.00s)
    --- FAIL: TestCtestUpdateConditions/starting_status_nil_(should_not_panic) (0.00s)
    --- FAIL: TestCtestUpdateConditions/new_condition_with_unknown_type_and_true_status (0.00s)
    --- FAIL: TestCtestUpdateConditions/duplicate_condition_types_with_differing_statuses (0.00s)
    --- PASS: TestCtestUpdateConditions/very_large_number_of_existing_unknown_conditions (0.00s)
=== RUN   TestCtestProcessContentTotals
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:260]: Starting TestCtestProcessContentTotals
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:381]: Total ProcessContentTotals test cases: 8
Running 0 th ProcessContentTotals test case: nothing
=== RUN   TestCtestProcessContentTotals/nothing
Running 1 th ProcessContentTotals test case: just remaining
=== RUN   TestCtestProcessContentTotals/just_remaining
Running 2 th ProcessContentTotals test case: just finalizers
=== RUN   TestCtestProcessContentTotals/just_finalizers
Running 3 th ProcessContentTotals test case: both
=== RUN   TestCtestProcessContentTotals/both
Running 4 th ProcessContentTotals test case: nil maps (should behave like empty)
=== RUN   TestCtestProcessContentTotals/nil_maps_(should_behave_like_empty)
Running 5 th ProcessContentTotals test case: very large remaining counts
=== RUN   TestCtestProcessContentTotals/very_large_remaining_counts
Running 6 th ProcessContentTotals test case: empty GroupVersionResource key
=== RUN   TestCtestProcessContentTotals/empty_GroupVersionResource_key
Running 7 th ProcessContentTotals test case: negative counts (invalid, should be ignored)
=== RUN   TestCtestProcessContentTotals/negative_counts_(invalid,_should_be_ignored)
    ctest_status_condition_utils_test.go:396: expected 0 conditions, got 2
--- FAIL: TestCtestProcessContentTotals (0.00s)
    --- PASS: TestCtestProcessContentTotals/nothing (0.00s)
    --- PASS: TestCtestProcessContentTotals/just_remaining (0.00s)
    --- PASS: TestCtestProcessContentTotals/just_finalizers (0.00s)
    --- PASS: TestCtestProcessContentTotals/both (0.00s)
    --- PASS: TestCtestProcessContentTotals/nil_maps_(should_behave_like_empty) (0.00s)
    --- PASS: TestCtestProcessContentTotals/very_large_remaining_counts (0.00s)
    --- PASS: TestCtestProcessContentTotals/empty_GroupVersionResource_key (0.00s)
    --- FAIL: TestCtestProcessContentTotals/negative_counts_(invalid,_should_be_ignored) (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/namespace/deletion	2.047s
	k8s.io/kubernetes/pkg/controller/nodeipam		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/nodeipam/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/nodeipam/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestOccupyServiceCIDR
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/nodeipam/ipam/ctest_controller_test.go:17]: Running TestCtestOccupyServiceCIDR with extended test cases
Running test case #0: serviceCIDR=10.0.255.0/24
Running test case #1: serviceCIDR=10.1.0.0/24
Running test case #2: serviceCIDR=10.1.255.0/24
Running test case #3: serviceCIDR=10.2.0.0/24
Running test case #4: serviceCIDR=10.1.0.0/32
Running test case #5: serviceCIDR=10.1.0.0/16
Running test case #6: serviceCIDR=10.0.0.0/24
Running test case #7: serviceCIDR=10.1.255.255/32
--- PASS: TestCtestOccupyServiceCIDR (0.00s)
=== RUN   TestCtestTimeout

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/nodeipam/ipam/ctest_timeout_test.go:23]: get default configs: {test_fixture.json [default timeout config] Resync [] {10s 5s 1s 0 0}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "Resync" in requested fixtures
2026/02/16 15:40:40 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:40:40 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"InitialRetry":1000000000,"MaxBackoff":5000000000,"Resync":10000000000})[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/nodeipam/ipam/ctest_timeout_test.go:33]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestTimeout (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/nodeipam/ipam	2.500s	coverage: 0.8% of statements in ./...
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/nodeipam/ipam/cidrset	0.863s	coverage: 0.1% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/nodeipam/ipam/sync	2.667s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/nodeipam/ipam/test		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/nodelifecycle	2.258s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/nodelifecycle/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/nodelifecycle/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler	0.822s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/podautoscaler	2.155s	coverage: 0.7% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/podautoscaler/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/podautoscaler/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/podautoscaler/metrics	1.639s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/podautoscaler/monitor		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/podgc	1.919s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/podgc/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/podgc/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/podgc/metrics		coverage: 0.0% of statements
=== RUN   TestCtestCalculateStatus

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replicaset/ctest_replica_set_utils_test.go:39]: get default configs: [{test_fixture.json [default minReadySeconds] minReadySeconds [replicasets deployments statefulsets daemonsets] 3600}]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[replicasets deployments statefulsets daemonsets]
[DEBUG-CTEST 2026-02-16 15:40:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[replicasets deployments statefulsets daemonsets], int=4)[DEBUG-CTEST 2026-02-16 15:40:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [replicasets statefulsets daemonsets]
[DEBUG-CTEST 2026-02-16 15:40:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:48 load all fixtures failed: requested fixture keys not found in test_fixtures.json: replicasets, statefulsets, daemonsets
FAIL	k8s.io/kubernetes/pkg/controller/replicaset	0.773s
	k8s.io/kubernetes/pkg/controller/replicaset/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replicaset/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replicaset/metrics		coverage: 0.0% of statements
=== RUN   TestCtestGetCondition

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:25]: get default configs: {test_fixture.json [replicationcontroller get condition status] conditions [] {0 0 0 0 0 [{ReplicaFailure True 0001-01-01 00:00:00 +0000 UTC OtherFailure }]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "conditions" in requested fixtures
2026/02/16 15:40:50 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:40:50 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"conditions":[{"lastTransitionTime":null,"reason":"OtherFailure","status":"True","type":"ReplicaFailure"}],"replicas":0})[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:33]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetCondition (0.00s)
=== RUN   TestCtestSetCondition

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:99]: get default configs: {test_fixture.json [replicationcontroller set condition status] conditions [] {0 0 0 0 0 []}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "conditions" in requested fixtures
2026/02/16 15:40:50 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:40:50 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"replicas":0})[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:107]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestSetCondition (0.00s)
=== RUN   TestCtestRemoveCondition

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:191]: get default configs: {test_fixture.json [replicationcontroller remove condition status] conditions [] {0 0 0 0 0 [{ReplicaFailure True 0001-01-01 00:00:00 +0000 UTC OtherFailure }]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "conditions" in requested fixtures
2026/02/16 15:40:50 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:40:50 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"conditions":[{"lastTransitionTime":null,"reason":"OtherFailure","status":"True","type":"ReplicaFailure"}],"replicas":0})[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:40:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:199]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestRemoveCondition (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/replication	2.492s	coverage: 0.8% of statements in ./...
	k8s.io/kubernetes/pkg/controller/replication/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replication/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/resourceclaim	3.004s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/resourceclaim/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/resourcequota	2.770s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/resourcequota/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/resourcequota/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/serviceaccount	1.714s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/serviceaccount/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/serviceaccount/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/servicecidrs	1.951s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestStatefulSetCompatibility

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:40:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[statefulsets]
[DEBUG-CTEST 2026-02-16 15:40:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[statefulsets], int=1)[DEBUG-CTEST 2026-02-16 15:40:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:40:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets]
[DEBUG-CTEST 2026-02-16 15:40:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:40:56 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets
FAIL	k8s.io/kubernetes/pkg/controller/statefulset	0.796s
	k8s.io/kubernetes/pkg/controller/statefulset/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/statefulset/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 1.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/storageversiongc	4.619s	coverage: 1.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/storageversionmigrator		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/tainteviction	4.019s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/tainteviction/metrics		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/testutil		coverage: 0.0% of statements
=== RUN   TestCtestPatchNode

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttl/ctest_ttl_controller_test.go:33]: matched config: {test_fixture.json [default node config patch] annotations [nodes] &Node{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{},Allocatable:ResourceList{},Phase:,Conditions:[]NodeCondition{},Addresses:[]NodeAddress{},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:0,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:,KubeProxyVersion:,OperatingSystem:,Architecture:,Swap:nil,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,RuntimeHandlers:[]NodeRuntimeHandler{},Features:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:41:05 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/pkg/controller/ttl	0.762s
=== RUN   TestCtestTimeLeft

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttlafterfinished/ctest_ttlafterfinished_controller_test.go:32]: get default configs: {test_fixture.json [default job spec] ttlSecondsAfterFinished [jobs] {<nil> <nil> <nil> nil nil <nil> <nil> <nil> nil <nil> {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}} 0x14000394638 <nil> <nil> <nil> <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[jobs]
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[jobs], int=1)[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ttlSecondsAfterFinished" in requested fixtures
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttlafterfinished/ctest_ttlafterfinished_controller_test.go:37]: Failed to get matched fixtures: mode-combination failed: no values found for field "ttlSecondsAfterFinished" in requested fixtures
[DEBUG-CTEST 2026-02-16 15:41:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttlafterfinished/ctest_ttlafterfinished_controller_test.go:38]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestTimeLeft (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/ttlafterfinished	3.296s	coverage: 0.8% of statements in ./...
	k8s.io/kubernetes/pkg/controller/ttlafterfinished/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/ttlafterfinished/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/ttlafterfinished/metrics		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/util/endpointslice		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/util/node		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/util/selectors	3.453s	coverage: 0.0% of statements in ./... [no tests to run]
=== RUN   TestCtestTypeChecking

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestTypeChecking/deployment_with_correct_expression
  I0216 15:41:11.115741   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:11.115925   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_with_type_confusion
  I0216 15:41:12.237025   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:12.237091   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/two_expressions_different_type_checking_errors
  I0216 15:41:13.348007   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:13.348027   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/one_expression,_two_warnings
  I0216 15:41:14.458478   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:14.458499   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_with_empty_validations
  I0216 15:41:15.565752   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:15.565769   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_with_nil_validations
  I0216 15:41:16.679946   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:16.680009   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_without_match_constraints
  I0216 15:41:17.790789   90871 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0216 15:41:17.790824   90871 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"

==================== CTEST END ======================
--- PASS: TestCtestTypeChecking (7.79s)
    --- PASS: TestCtestTypeChecking/deployment_with_correct_expression (1.12s)
    --- PASS: TestCtestTypeChecking/deployment_with_type_confusion (1.12s)
    --- PASS: TestCtestTypeChecking/two_expressions_different_type_checking_errors (1.11s)
    --- PASS: TestCtestTypeChecking/one_expression,_two_warnings (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_with_empty_validations (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_with_nil_validations (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_without_match_constraints (1.11s)
PASS
coverage: 1.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/validatingadmissionpolicystatus	10.577s	coverage: 1.6% of statements in ./...
	k8s.io/kubernetes/pkg/controller/validatingadmissionpolicystatus/config		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/validatingadmissionpolicystatus/config/v1alpha1	0.985s	coverage: 0.1% of statements in ./... [no tests to run]
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_attached
I0216 15:41:11.578765   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:11.578855   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:11.578860   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:11.580800   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:11.581021   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581021   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581028   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581030   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581067   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581080   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581156   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581161   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581208   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581224   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581261   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581266   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581270   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581331   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581354   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581378   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581379   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581392   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581396   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581471   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:11.581498   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0216 15:41:11.581507] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-0"
    attach_detach_controller.go:669: I0216 15:41:11.581652] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:11.581741] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:11.581807] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:11.581824] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:11.581841] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:11.581862] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:11.581882] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:11.581896] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:11.581912] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:11.581927] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:11.681005   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:11.681033] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:11.681092] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:11.681114] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:11.681126] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:11.681143] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:11.681154] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:11.681163] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:11.681170] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:11.681177] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:11.681185] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:11.681192] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:11.681198] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:11.681206] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:11.681213] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:11.681219] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:11.681224] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:11.681229] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:11.681239] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:11.681244] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:11.681252] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:11.681257] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0216 15:41:11.681289] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 attachState="Detached"
    attach_detach_controller.go:424: I0216 15:41:11.681303] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0216 15:41:11.681677] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:11.682273] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:11.682299] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:11.682362] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:11.682374] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:11.682458   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0216 15:41:11.682470   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0216 15:41:11.682582] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:11.682602] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:11.682617] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:11.682631   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0216 15:41:11.682635   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    actual_state_of_world.go:434: I0216 15:41:11.682697] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:11.682843] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0216 15:41:11.682880] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:11.682892] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:11.682900] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:11.682997   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:11.683005   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0216 15:41:11.683036] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:11.683047] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:11.683063] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:11.683072] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:11.683122   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0216 15:41:11.683127   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0216 15:41:11.683179] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:11.683191] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:11.683221] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:11.683230] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:11.683236] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0216 15:41:11.683246] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:11.683251] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0216 15:41:11.683313   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0216 15:41:11.683320   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    reconciler.go:361: I0216 15:41:11.685090] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0216 15:41:11.685401] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-3","mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2"]
I0216 15:41:11.685977   90872 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0216 15:41:11.686029   90872 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0216 15:41:11.785915] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:11.785947] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:11.887057] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:11.887091] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:11.987345] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:11.987372] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.088512] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.088593] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.188805] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.188836] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.289234] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.289268] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.390354] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.390385] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.491523] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.491584] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0216 15:41:12.582492   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.582513   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.582520   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:12.582695] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:12.582770] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:12.582794] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:12.582817] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:12.582839] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:12.582853] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:12.582886] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:12.582922] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:12.582938] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:12.582945] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:12.592672] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:251: I0216 15:41:12.592736] RemoveVolumeFromReportAsAttached failed while removing volume from node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 err="volume \"kubernetes.io/testPlugin/vol1\" does not exist in volumesToReportAsAttached list or node \"mynode-1\" does not exist in nodesToUpdateStatusFor list"
    reconciler.go:271: I0216 15:41:12.592761] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:279: I0216 15:41:12.592801] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:127: I0216 15:41:12.592828] Starting reconciling attached volumes still attached
I0216 15:41:12.592840   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
E0216 15:41:12.592847   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
I0216 15:41:12.592852   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0216 15:41:12.681934   90872 watch.go:142] "Stopping fake watcher"
I0216 15:41:12.681990   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.682042   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.682037   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.682066   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.682082   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.682090   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.682040   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_unattached
I0216 15:41:12.683243   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:12.683252   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:12.683255   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:12.683346   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:12.683377   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683381   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683393   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683397   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683413   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683418   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683377   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683441   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683451   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683456   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683473   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683477   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683483   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683488   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683530   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683537   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:12.683521] processVolumesInUse for node node="mynode"
I0216 15:41:12.683545   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.683559   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:12.683552] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:12.683564] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:12.683570] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:12.683580] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:12.683586] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:12.683592   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:12.683591] processVolumesInUse for node node="mynode-3"
I0216 15:41:12.683597   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:12.683597] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:12.683603] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:12.683608] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:12.683616   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:12.783529   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:12.783566] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:12.783619] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:12.783660] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:12.783693] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:12.783704] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:12.783718] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:12.783724] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:12.783730] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:12.783736] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:12.783783] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:12.783804] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:12.783822] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:12.783842] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:12.783887] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:12.783894] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:12.783902] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:12.783910] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:12.783925] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:12.783937] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:12.783948] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:12.783959] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0216 15:41:12.783976] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 attachState="Detached"
    attach_detach_controller.go:424: I0216 15:41:12.783986] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0216 15:41:12.784084] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:12.785335] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:12.785349] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:12.785406] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:12.785428] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:12.785483   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0216 15:41:12.785491   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0216 15:41:12.785548] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:12.785577] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:12.785633] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:12.785643] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:12.785683   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0216 15:41:12.785702   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    node_status_updater.go:129: I0216 15:41:12.785740] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:12.785748] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:12.785757] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:12.785790] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:12.785843   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0216 15:41:12.785851   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0216 15:41:12.785895] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:12.785905] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:12.785916] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:12.785925] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:12.785999   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0216 15:41:12.786006   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    node_status_updater.go:129: I0216 15:41:12.786003] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0216 15:41:12.786014] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:12.786027] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:12.786034] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.786041] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0216 15:41:12.786048] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.786060] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0216 15:41:12.786069   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:12.786078   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    reconciler.go:361: I0216 15:41:12.786862] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0216 15:41:12.786888] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-3","mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2"]
I0216 15:41:12.787409   90872 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0216 15:41:12.787419   90872 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0216 15:41:12.887257] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.887292] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:12.987384] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:12.987408] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:13.087833] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:13.087877] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:13.188967] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:13.188996] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:13.290047] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:13.290073] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:13.390260] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:13.390291] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:13.491363] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:13.491396] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0216 15:41:13.592464] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:13.592493] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0216 15:41:13.684476   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.684535   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.684577   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:13.684643] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:13.684669] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:13.684699] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:13.684715] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.684729] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:13.684735] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.684741] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:13.684746] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.684752] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:13.684757] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:13.693541] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:251: I0216 15:41:13.693577] RemoveVolumeFromReportAsAttached failed while removing volume from node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 err="volume \"kubernetes.io/testPlugin/vol1\" does not exist in volumesToReportAsAttached list or node \"mynode-1\" does not exist in nodesToUpdateStatusFor list"
    reconciler.go:271: I0216 15:41:13.693587] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:279: I0216 15:41:13.693607] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:127: I0216 15:41:13.693618] Starting reconciling attached volumes still attached
E0216 15:41:13.693627   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
I0216 15:41:13.693644   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0216 15:41:13.693652   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0216 15:41:13.785127   90872 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Scheduled_Pod_with_migrated_PV
I0216 15:41:13.785164   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785171   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785181   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785182   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785198   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785206   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785219   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785291   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:13.785302   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:13.785305   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:13.785612   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:13.785642   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785647   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785649   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785652   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.785690   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.787971   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.787996   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788141   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788154   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788154   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788194   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.787982   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788212   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788238   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788265   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788479   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788482   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788491   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.788493   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:13.788389] processVolumesInUse for node node="mynode"
I0216 15:41:13.788524   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:13.788556] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:13.788599] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:13.788627] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:13.788646   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:13.788649] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:13.788681] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.788697] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:13.788717] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.788734] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:13.788755] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:13.886714   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:13.886743] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:13.886797] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:13.886822] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:13.886834] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:13.886854] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:506: I0216 15:41:13.886863] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:13.886870] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/vol1
    actual_state_of_world.go:400: I0216 15:41:13.886878] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:13.886884] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.886893] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:13.886899] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.886905] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:13.886928] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.886934] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:13.886940] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.886946] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:13.886953] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.886959] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:13.886965] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.886971] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:424: I0216 15:41:13.886999] Populating DesiredStateOfworld
I0216 15:41:13.887071   90872 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Deleted_Pod_with_migrated_PV
I0216 15:41:13.887089   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887089   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887089   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887095   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887102   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887108   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887113   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887159   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:13.887166   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:13.887169   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:13.887259   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:13.887286   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887288   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887289   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887293   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887294   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887289   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887303   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887286   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887311   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887307   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887325   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887329   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887326   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887337   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887357   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887362   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887368   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.887373   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:13.887374] processVolumesInUse for node node="mynode"
I0216 15:41:13.887368   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:13.887393] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0216 15:41:13.887418   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:13.887436] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:13.887443] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.887451] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:13.887458] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.887466] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:13.887472] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.887479] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:13.887485] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:13.918734   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988301   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:13.988318] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:13.988344] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:13.988359] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:13.988373] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:13.988385] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:13.988395] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.988403] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:13.988409] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.988416] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:13.988423] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.988429] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:13.988436] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.988443] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:13.988453] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.988459] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:13.988466] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.988472] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:13.988478] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:13.988484] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:13.988490] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:13.988497] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0216 15:41:13.988518] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/vol1 attachState="Detached"
    attach_detach_controller.go:424: I0216 15:41:13.988526] Populating DesiredStateOfworld
I0216 15:41:13.988601   90872 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Empty_volume_name
I0216 15:41:13.988620   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988633   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988657   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988654   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988654   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988675   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988675   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988719   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:13.988726   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:13.988729   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:13.988827   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:13.988854   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988860   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988854   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988875   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988880   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988883   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988899   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988855   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988904   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988910   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988895   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0216 15:41:13.988922] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-0"
I0216 15:41:13.988935   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0216 15:41:13.988934] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-1"
I0216 15:41:13.988938   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0216 15:41:13.988941] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-2"
I0216 15:41:13.988902   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988952   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0216 15:41:13.988950] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-3"
I0216 15:41:13.988910   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988946   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:13.988948   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:13.988961] processVolumesInUse for node node="mynode"
I0216 15:41:13.988982   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:13.988977] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:13.989016] processVolumesInUse for node node="mynode-1"
I0216 15:41:13.989005   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:13.989028] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.989036] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:13.989042] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.989050] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:13.989058] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:13.989064] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:13.989072] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:13.989084   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:14.088945   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:14.088967] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:14.088993] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:14.089010] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:14.089022] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:14.089032] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:14.089042] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:14.089050] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:14.089068] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:14.089076] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:14.089083] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:14.089092] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:14.089099] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:14.089106] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:14.089113] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:14.089121] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:14.089130] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:14.089137] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:14.089144] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:14.089150] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:14.089158] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:14.089170] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:731: E0216 15:41:14.089188] Failed to find unique name for volume err="failed to GetVolumeName from volumePlugin for volumeSpec \"pv-empty\" err=<nil>" node="mynode-1" vaName="va-empty" PV="pv-empty"
    attach_detach_controller.go:424: I0216 15:41:14.089204] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0216 15:41:14.089357] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:14.089709] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0216 15:41:14.089723] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:14.089739] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:14.089750] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:14.089790   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0216 15:41:14.089800] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
I0216 15:41:14.089807   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    reconciler.go:271: I0216 15:41:14.089808] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:14.089823] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:14.089831] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:14.089857   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0216 15:41:14.089865   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0216 15:41:14.089918] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:14.089928] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:14.089940] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:14.089951] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:14.089990] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:14.089998] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:14.090008] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:14.090016] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:14.090021   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0216 15:41:14.090033   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0216 15:41:14.090052   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0216 15:41:14.090055] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
I0216 15:41:14.090062   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    reconciler.go:271: I0216 15:41:14.090063] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:14.090082] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:14.090092] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.090099] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:14.090124   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0216 15:41:14.090137   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    reconciler.go:361: I0216 15:41:14.090506] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0216 15:41:14.090536] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3"]
I0216 15:41:14.090578   90872 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0216 15:41:14.090584   90872 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0216 15:41:14.190679] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.291758] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.392904] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.494074] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.595203] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.696292] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.797363] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:14.898437] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:14.989967   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:14.990006   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:14.990012   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:14.990063] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:14.990108] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:14.990123] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:14.990154] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:14.990184] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:14.990198] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:14.990206] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:14.990217] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:14.990229] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:14.990236] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:14.999508] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:14.999535] Starting reconciling attached volumes still attached
E0216 15:41:14.999546   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.100388] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.201322] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.302418] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.403087] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.504230] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.605308] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.706386] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.807477] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:15.908548] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:15.991066   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:15.991095   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:15.991138] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:15.991175] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:15.991190] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:15.991204] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:15.991217] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:15.991231] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:15.991247] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:15.991259] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:15.991273] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:15.991299] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0216 15:41:15.991317   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:16.009665] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:16.009711] Starting reconciling attached volumes still attached
E0216 15:41:16.009731   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.110841] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.211984] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.313122] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.414629] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.515887] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.617037] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.718143] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.819331] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:16.919869] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:16.991429   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:16.991474] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:16.991496] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:16.991504] processVolumesInUse for node node="mynode-3"
I0216 15:41:16.991502   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:16.991510] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:16.991518] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:16.991524] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:16.991529] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:16.991540] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0216 15:41:16.991543   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:16.991547] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:16.991554] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:17.020406] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:17.020435] Starting reconciling attached volumes still attached
E0216 15:41:17.020455   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.121529] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.222621] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.323769] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.424914] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.525808] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.626985] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.728110] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.829208] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:17.930360] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:17.992457   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:17.992493   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:17.992495] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:17.992520] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:17.992528] processVolumesInUse for node node="mynode-1"
I0216 15:41:17.992542   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:17.992543] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:17.992550] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:17.992556] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:17.992563] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:17.992568] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:17.992586] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:17.992600] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:18.031449] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:18.031501] Starting reconciling attached volumes still attached
E0216 15:41:18.031562   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.131881] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.233083] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.334236] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.435388] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.535972] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.637027] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.738086] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.839158] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:18.940222] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:18.992835   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:18.992936   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:18.992956] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:18.992990] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:18.993001] processVolumesInUse for node node="mynode-1"
I0216 15:41:18.992998   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:18.993007] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:18.993069] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:18.993080] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:18.993117] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:18.993123] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:18.993131] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:18.993139] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:19.040664] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:19.040689] Starting reconciling attached volumes still attached
E0216 15:41:19.040704   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.141759] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.242834] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.343124] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.444200] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.544594] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.644754] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.745864] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.846440] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:19.947495] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:19.993907   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:19.993932   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:19.993950] processVolumesInUse for node node="mynode-3"
I0216 15:41:19.993948   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:19.993980] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:19.993989] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:19.993995] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:19.994018] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:19.994035] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:19.994044] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:19.994051] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:19.994060] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:19.994067] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:20.048591] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:20.048622] Starting reconciling attached volumes still attached
E0216 15:41:20.048646   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.149766] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.250945] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.352109] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.453218] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.554054] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.655142] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.756290] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.857444] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:20.958536] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:20.994971   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:20.994971   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:20.995060   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:20.995100] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:20.995133] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:20.995148] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:20.995155] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:20.995162] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:20.995168] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:20.995173] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:20.995179] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:20.995188] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:20.995203] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:21.059626] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:21.059667] Starting reconciling attached volumes still attached
E0216 15:41:21.059679   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.160747] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.261831] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.362918] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.464008] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.565087] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.665182] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.766257] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.867187] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:21.968271] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:21.995886   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:21.995894   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:21.995884   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:21.995921] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:21.995950] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:21.995976] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:21.995984] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:21.995990] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:21.995997] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:21.996003] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:21.996013] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:21.996029] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:21.996039] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:22.069348] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:22.069381] Starting reconciling attached volumes still attached
E0216 15:41:22.069398   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.170471] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.271554] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.372717] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.473861] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.574960] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.676047] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.776765] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.876886] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:22.977972] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:22.996893   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:22.996893   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:22.996902   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:22.996942] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:22.996958] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:22.996968] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:22.996974] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:22.996982] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:22.996989] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:22.996995] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:22.997001] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:22.997007] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:22.997016] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0216 15:41:23.079085] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:23.079121] Starting reconciling attached volumes still attached
E0216 15:41:23.079134   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.179435] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.280575] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.381729] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.482712] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.592750] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.693895] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.795007] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.896085] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:23.996369] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:23.997020   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:23.997096   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:23.997136] processVolumesInUse for node node="mynode-1"
I0216 15:41:23.997124   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:23.997164] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:23.997180] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:23.997194] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:23.997208] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:23.997221] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:23.997234] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:23.997246] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:23.997260] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:23.997280] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0216 15:41:24.097042] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:24.097218] Starting reconciling attached volumes still attached
E0216 15:41:24.097416   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    attach_detach_controller_test.go:704: Expected detach operation not found, node:mynode-1, volume: , tries: 10
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Nil_pod_name_(should_be_ignored)
I0216 15:41:24.100153   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:24.100187   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:24.100193   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:24.100219   90872 watch.go:142] "Stopping fake watcher"
I0216 15:41:24.100273   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100306   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100325   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100342   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100362   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100372   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:24.100385   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100402   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100478   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100483   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100853   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.100859   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101261   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101343   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101347   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101441   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101474   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101477   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101501   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101523   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101525   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101569   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101606   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101609   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101629   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0216 15:41:24.101796] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-0"
    util.go:211: I0216 15:41:24.101818] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-1"
    util.go:211: I0216 15:41:24.101824] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-2"
    util.go:211: I0216 15:41:24.101830] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-3"
    util.go:211: I0216 15:41:24.101836] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-4"
I0216 15:41:24.101857   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101860   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.101881   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:24.102482   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:24.102509] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:24.102532] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:24.102541] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:24.102546] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:24.102803] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:24.102813] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:24.102820] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:24.102829] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:24.102836] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:24.102842] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:24.201733   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:24.201754] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:24.201864] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:24.201883] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:24.201895] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:24.201907] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:24.201919] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:24.201926] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:24.201936] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:24.202035] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:24.202058] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:24.202076] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:24.202091] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:24.202106] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:24.202119] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:24.202132] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:24.202146] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:24.202163] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:24.202181] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:24.202195] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:24.202209] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:24.202222] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0216 15:41:24.202261] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod attachState="Detached"
    attach_detach_controller.go:424: I0216 15:41:24.202282] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0216 15:41:24.202553] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:24.202819] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:24.202845] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:24.202874] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:24.202893] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:24.202988] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:24.203004] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:24.203022] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:24.203039] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:24.203173] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0216 15:41:24.203190] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:24.203208] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:24.203224] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:24.203305] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:24.203321] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:24.203338] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:24.203354] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:24.203432] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:24.203447] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:24.203465] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:24.203482] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.203495] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0216 15:41:24.203510] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.203523] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
I0216 15:41:24.203917   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0216 15:41:24.203926   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0216 15:41:24.203939   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0216 15:41:24.203942   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0216 15:41:24.203948   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:24.203950   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:24.203956   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0216 15:41:24.203958   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    reconciler.go:361: I0216 15:41:24.204334] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0216 15:41:24.204394] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3"]
I0216 15:41:24.204475   90872 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0216 15:41:24.204482   90872 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
I0216 15:41:24.204505   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0216 15:41:24.204511   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    reconciler.go:241: I0216 15:41:24.309383] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.309438] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.411953] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.411989] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.513262] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.513315] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.615496] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.615531] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.716647] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.716742] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.817874] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:24.817924] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.919011] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0216 15:41:24.919046] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:25.020626] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:25.020721] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
I0216 15:41:25.101857   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.101900   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.102591   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:25.102653] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:25.102709] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:25.102751] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:25.102766] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:25.102781] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:25.102794] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:25.102807] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:25.102821] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:25.102843] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:25.102855] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:25.121118] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:251: I0216 15:41:25.121174] RemoveVolumeFromReportAsAttached failed while removing volume from node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod err="volume \"kubernetes.io/testPlugin/vol-nil-pod\" does not exist in volumesToReportAsAttached list or node \"mynode-1\" does not exist in nodesToUpdateStatusFor list"
    reconciler.go:271: I0216 15:41:25.121204] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:279: I0216 15:41:25.121240] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:127: I0216 15:41:25.121253] Starting reconciling attached volumes still attached
E0216 15:41:25.121265   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
I0216 15:41:25.121307   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "pv-nil-pod" (UniqueName: "kubernetes.io/testPlugin/vol-nil-pod") on node "mynode-1" 
I0216 15:41:25.121312   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "pv-nil-pod" (UniqueName: "kubernetes.io/testPlugin/vol-nil-pod") on node "mynode-1" 
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Invalid_node_name_characters
I0216 15:41:25.203838   90872 watch.go:142] "Stopping fake watcher"
I0216 15:41:25.203914   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.203966   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.203970   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.203988   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.203995   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204006   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.203988   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204336   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:25.204345   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:25.204348   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:25.204593   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:25.204641   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204645   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204655   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204673   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204681   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204721   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204748   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204751   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204765   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204785   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204789   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204873   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204879   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204886   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204889   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204910   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.204913   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:25.204933] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:25.204966] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:25.204982] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:25.204988] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:25.205009] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:25.205014] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:25.205020] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:25.205026] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:25.205031] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:25.205036] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:25.205055   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.205181   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.205200   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.205264   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:25.305664   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:25.305725] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:25.305756] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:25.305770] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:25.305778] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:25.305786] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:25.305800] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:25.305806] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:25.305812] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:25.305818] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:25.305823] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:25.305830] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:25.305841] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:25.305856] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:25.305864] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:25.305881] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:25.305900] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:25.305909] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:25.305922] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:25.305932] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:25.305954] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:25.305969] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0216 15:41:25.305989] Marking volume attachment as uncertain as volume is not attached node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node attachState="Detached"
    attach_detach_controller.go:424: I0216 15:41:25.306003] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0216 15:41:25.306089] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:25.307714] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:25.307727] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:25.307749] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:25.307760] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:25.307812] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:25.307822] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:25.307822   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    reconciler.go:279: I0216 15:41:25.307832] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:25.307836   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    actual_state_of_world.go:434: I0216 15:41:25.307840] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:25.307871   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0216 15:41:25.307876   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    node_status_updater.go:129: I0216 15:41:25.307926] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0216 15:41:25.307948] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:25.307961] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:25.307971] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:25.308000   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:25.308005   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0216 15:41:25.308025] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:25.308035] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:25.308045] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:25.308056] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:25.308059   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0216 15:41:25.308068   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0216 15:41:25.308094] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:25.308102] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:25.308113] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:25.308125   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    actual_state_of_world.go:434: I0216 15:41:25.308122] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:25.308130   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    reconciler.go:241: I0216 15:41:25.308132] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0216 15:41:25.308139] Set detach request time to current time for volume on node node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:25.308148] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.308155] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.308162] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:361: I0216 15:41:25.309929] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0216 15:41:25.309974] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3"]
I0216 15:41:25.310016   90872 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0216 15:41:25.310023   90872 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0216 15:41:25.410694] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:25.410736] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.410744] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.410761] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:25.511007] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:25.511090] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.511103] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.511113] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:25.612215] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:25.612269] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.612279] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.612289] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:25.715449] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:25.715483] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.715491] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.715499] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:25.816676] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:25.816725] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.816738] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.816746] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:25.917186] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:25.917227] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:25.917246] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:25.917255] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:26.017433] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.017600] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.017648] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.017715] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.118676] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.118797] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.118883] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.118934] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:26.206024   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:26.206348   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:26.206805] processVolumesInUse for node node="mynode"
I0216 15:41:26.206941   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:26.206924] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:26.206991] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:26.207018] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:26.207036] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:26.207049] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:26.207062] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:26.207075] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:26.207089] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:26.207103] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:26.220012] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.220060] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.220077] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.220086] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:26.220107] Starting reconciling attached volumes still attached
E0216 15:41:26.220130   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:26.321224] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.321315] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.321325] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.321337] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.422401] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.422434] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.422443] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.422450] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.523394] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.524401] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.524539] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.524600] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.625735] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.625802] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.625821] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.625838] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.727225] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.727273] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.727282] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.727290] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.828466] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.828537] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.828554] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.828575] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:26.929829] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:26.929868] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:26.929877] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:26.929885] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.030655] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.030681] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.030692] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.030700] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.130909] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.130956] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.130995] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.131004] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:27.206578   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:27.206774   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:27.206901] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:27.206940] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:27.206960] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:27.206977] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:27.207099   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:27.207060] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:27.207238] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:27.207286] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:27.207307] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:27.207316] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:27.207321] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:27.232320] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.232415] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.232433] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.232466] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:27.232507] Starting reconciling attached volumes still attached
E0216 15:41:27.232541   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:27.333306] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.333335] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.333343] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.333350] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.434025] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.434056] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.434063] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.434070] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.534641] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.534702] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.534711] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.534719] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.635331] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.635371] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.635389] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.635405] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.736471] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.736513] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.736524] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.736552] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:27.836638] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.836673] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.836687] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:27.836696] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:27.937841] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:27.937891] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:27.937902] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:27.937912] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.039134] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.039194] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.039269] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.039301] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.140045] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.140095] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.140107] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.140114] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:28.207109   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:28.207165   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:28.207201   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:28.207255] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:28.207285] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:28.207296] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:28.207303] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:28.207309] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:28.207343] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:28.207361] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:28.207368] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:28.207375] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:28.207383] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:218: I0216 15:41:28.241175] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.241328] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.241342] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.241353] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:28.241364] Starting reconciling attached volumes still attached
E0216 15:41:28.241387   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:28.341521] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.341591] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.341671] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.341711] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.442820] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.442861] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.442873] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.442881] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:28.543944] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.543976] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.543991] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.544001] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:28.644941] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.644977] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.644996] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.645004] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.745865] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.746129] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.746163] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.746186] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.847362] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.847412] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.847422] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.847431] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:28.948498] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:28.948523] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:28.948530] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:28.948537] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.049592] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.049617] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.049624] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.049632] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:29.150680] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.150731] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.150773] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.150797] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:29.207846   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:29.207869   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:29.207874   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:29.207889] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:29.207913] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:29.207948] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:29.207957] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:29.207970] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:29.207977] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:29.207982] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:29.207993] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:29.208003] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:29.208009] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:29.251863] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.251905] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.251914] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.251928] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:29.251942] Starting reconciling attached volumes still attached
E0216 15:41:29.251952   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:29.353013] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.353036] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.353053] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.353076] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.454179] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.454241] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.454254] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.454273] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.555437] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.555514] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.555533] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.555550] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.656653] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.656687] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.656696] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.656723] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.757828] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.757876] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.757895] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.757919] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:29.859078] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.859140] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.859190] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:29.859211] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:29.960401] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:29.960581] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:29.960614] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:29.960638] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.061774] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.061838] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.061849] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.061874] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.161952] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.162022] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.162035] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.162045] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:30.208069   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:30.208058   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:30.208167   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:30.208276] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:30.208312] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:30.208334] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:30.208341] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:30.208348] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:30.208353] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:30.208365] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:30.208372] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:30.208378] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:30.208385] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:30.262687] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.262724] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.262741] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.262761] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:30.262836] Starting reconciling attached volumes still attached
E0216 15:41:30.262867   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:30.363937] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.363965] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.363985] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.364001] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.465056] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.465089] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.465097] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.465104] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.565863] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.565891] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.565899] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.565907] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.666164] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.666190] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.666212] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.666223] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.767341] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.767382] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.767391] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.767408] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.868467] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:30.868540] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.868553] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.868562] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:30.969463] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:30.969513] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:30.969537] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:30.969555] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:31.070755] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.070783] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.070791] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.070798] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:31.171225] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.171279] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.171304] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.171367] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:31.208844   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:31.208883   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:31.208865   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:31.208900] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:31.209000] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:31.209053] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:31.209088] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:31.209105] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:31.209121] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:31.209153] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:31.209168] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:31.209230] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:31.209245] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:218: I0216 15:41:31.272486] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.272541] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.272568] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.272589] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:31.272610] Starting reconciling attached volumes still attached
E0216 15:41:31.272631   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.373778] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.373951] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.374051] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.374071] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:31.475245] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.475300] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.475318] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.475351] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:31.576499] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.576580] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.576619] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.576671] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:31.676920] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.676980] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.677028] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.677083] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.778197] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.778246] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.778267] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.778393] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.879493] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.879552] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.879609] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.879629] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:31.980057] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:31.980083] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:31.980091] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:31.980098] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:32.080408] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.080453] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.080514] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.080560] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:32.181722] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.181783] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.181801] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.183492] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:32.209902   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:32.209914   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:32.209919   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:32.209971] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:32.210008] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:32.210037] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:32.210050] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:32.210062] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:32.210070] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:32.210091] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:32.210105] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:32.210115] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:32.210123] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:32.284286] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.284316] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.284418] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.284456] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:32.284495] Starting reconciling attached volumes still attached
E0216 15:41:32.284514   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:32.385663] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.385686] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.385693] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.385700] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.487740] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.487767] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.487774] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.487782] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.588822] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.588849] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.588863] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.588876] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.689630] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.689666] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.689674] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.689697] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.789816] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.789881] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.789929] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.789950] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.890584] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.890621] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.890630] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.890637] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:32.991139] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:32.991213] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:32.991223] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:32.991231] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.093265] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.093303] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.093312] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.093319] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.194408] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.194434] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.194442] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.194452] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:33.210964   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:33.211015   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:33.211094   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:33.211113] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:33.211133] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:33.211140] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:33.211147] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:33.211154] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:33.211162] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:33.211169] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:33.211197] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:33.211208] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:33.211214] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:33.295478] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.295516] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.295525] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.295534] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:33.295544] Starting reconciling attached volumes still attached
E0216 15:41:33.295555   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:33.397477] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.397509] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.397517] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.397526] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.498354] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.498404] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.498435] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.498451] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.598802] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.598880] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.598919] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.598938] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.699768] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.699834] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.699884] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.699898] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.801020] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.801241] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.801263] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.801300] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:33.902434] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:33.902506] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:33.902523] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:33.902544] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.003742] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.003798] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.003807] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.003826] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.103970] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.104057] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.104072] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.104081] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.204432] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.204467] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.204476] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.204483] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:34.211822   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:34.211846   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:34.211823   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:34.211925] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:34.211944] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:34.211951] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:34.211957] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:34.211977] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:34.212030] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:34.212041] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:34.212064] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:34.212086] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:34.212094] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0216 15:41:34.305635] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.305710] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.305727] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.305761] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:34.305792] Starting reconciling attached volumes still attached
E0216 15:41:34.305820   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:34.406930] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.406974] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.406984] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.406993] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.507246] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.507284] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.507321] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.507331] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.608490] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.608546] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.608564] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.608648] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.709843] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.709889] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.709897] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.709912] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.810540] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:34.810633] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.810675] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.810711] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0216 15:41:34.910993] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:34.911020] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:34.911032] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:34.911049] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:35.011751] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:35.011836] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:35.011914] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:35.011974] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0216 15:41:35.112688] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:35.112764] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:35.112775] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:35.112783] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0216 15:41:35.212307   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.212277   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.212463   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:35.212755] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:35.212806] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:35.212871] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:35.212891] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:35.212899] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:35.212908] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    attach_detach_controller.go:669: I0216 15:41:35.212925] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:35.212934] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:35.212942] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:35.212948] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:35.212954] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:35.212959] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:35.212965] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:35.212977] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0216 15:41:35.314004] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0216 15:41:35.314062] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0216 15:41:35.314129] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0216 15:41:35.314163] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0216 15:41:35.314187] Starting reconciling attached volumes still attached
E0216 15:41:35.314227   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    attach_detach_controller_test.go:692: Expected node not found, node:node!@#, op: detach, tries: 10
I0216 15:41:35.324572   90872 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Zero-length_VA_name
I0216 15:41:35.327830   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.327841   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.327847   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.327861   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.327839   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.327830   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.327881   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.344688   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:35.348632   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:35.348647   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:35.363000   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:35.365576   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365597   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365611   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365610   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365627   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365590   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365642   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365589   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365672   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365589   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365612   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365580   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365733   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.365709   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.369230   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.369296   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.369365   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.369815   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.369821   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:35.369854] processVolumesInUse for node node="mynode"
I0216 15:41:35.369955   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:35.369990   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:35.369880] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:35.370035] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:35.370124] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:35.370133] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:35.370139] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:35.370146] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:35.370153] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:35.370161] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:35.370170] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:35.463951   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:35.463989] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:35.464070] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:35.464158] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:35.464181] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:35.464202] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:35.464241] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:35.464290] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:35.464303] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:35.464312] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:35.464320] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:35.464327] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:35.464334] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:35.464477] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:35.464518] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:35.464526] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:35.464534] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:35.464541] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:35.464547] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:35.464553] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:35.464559] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:35.464565] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:424: I0216 15:41:35.464572] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0216 15:41:35.465760] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:35.480197] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0216 15:41:35.480412] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:35.480629] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:35.480643] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:35.480744] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:35.480758] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:35.480773] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:35.480781] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0216 15:41:35.480913] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:35.480922] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:35.480948] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:35.481006] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:35.481160   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0216 15:41:35.481160] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
I0216 15:41:35.481168   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    reconciler.go:271: I0216 15:41:35.481173] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:35.481185] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:35.481188   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    actual_state_of_world.go:434: I0216 15:41:35.481195] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0216 15:41:35.481202   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0216 15:41:35.481206   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:35.481210   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0216 15:41:35.481233   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0216 15:41:35.481236   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0216 15:41:35.481256] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0216 15:41:35.481282] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0216 15:41:35.481292] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0216 15:41:35.481301] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:35.481311   90872 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    reconciler.go:241: I0216 15:41:35.481308] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:35.481313   90872 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    reconciler.go:361: I0216 15:41:35.488676] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0216 15:41:35.489729] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-3","mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2"]
I0216 15:41:35.490296   90872 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0216 15:41:35.490303   90872 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0216 15:41:35.590805] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:35.691753] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:35.792161] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:35.893259] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:35.994332] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.094622] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.195718] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.296113] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:36.370370   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:36.370420   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:36.370423] processVolumesInUse for node node="mynode"
I0216 15:41:36.370457   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:36.370456] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:36.370553] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:36.370562] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:36.370573] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:36.370589] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:36.370600] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:36.370606] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:36.370614] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:36.370620] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:36.397192] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:36.397217] Starting reconciling attached volumes still attached
E0216 15:41:36.397227   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.498302] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.599446] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.700608] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.801712] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:36.901969] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.003149] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.103305] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.204505] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.311627] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:37.371507   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:37.371564   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:37.371565   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:37.371718] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:37.371773] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:37.371803] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:37.371834] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:37.371846] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:37.371853] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:37.371903] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:37.371915] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:37.371929] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:37.371937] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:37.412769] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:37.412853] Starting reconciling attached volumes still attached
E0216 15:41:37.412898   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.514025] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.615151] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.716260] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.817719] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:37.918189] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.019361] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.120858] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.222792] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.323219] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:38.372705   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:38.372769   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:38.372940] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:38.372999] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:38.373019] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:38.373026] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:38.373055] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:38.373062] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:38.373072] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:38.373080] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:38.373086] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:38.373094] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:38.373403   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:38.424253] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:38.424321] Starting reconciling attached volumes still attached
E0216 15:41:38.424382   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.525639] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.626781] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.727878] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.828991] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:38.929715] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.030826] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.131885] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.233314] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.334392] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:39.373705   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:39.373705   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:39.373773   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:39.373938] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:39.374049] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:39.374110] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:39.374118] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:39.374128] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:39.374134] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:39.374143] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:39.374149] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:39.374166] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:39.374190] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0216 15:41:39.435461] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:39.435505] Starting reconciling attached volumes still attached
E0216 15:41:39.435541   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.536672] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.637836] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.738082] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.838566] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:39.939962] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.041339] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.142223] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.242374] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.343438] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:40.374898   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:40.374909   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:40.375051] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:40.375135] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:40.375158] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:40.375165] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:40.375175] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:40.375182] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:40.375189] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:40.375202] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:40.375218] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:40.375230] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:40.377713   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:40.444044] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:40.444067] Starting reconciling attached volumes still attached
E0216 15:41:40.444089   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.545190] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.646298] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.747382] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.856346] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:40.957471] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.064469] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.165563] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.266696] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.366907] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:41.375891   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:41.376117] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:41.376153] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:41.376185] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:41.376195] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:41.376204] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:41.376210] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:41.376218] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:41.376226] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:41.376232] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:41.376246] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0216 15:41:41.377187   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:41.380124   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:41.468029] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:41.468089] Starting reconciling attached volumes still attached
E0216 15:41:41.468125   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.569166] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.670221] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.771330] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.872520] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:41.973647] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.074707] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.175161] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.276249] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.377103] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:42.377129   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:42.377209] processVolumesInUse for node node="mynode"
I0216 15:41:42.377226   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0216 15:41:42.377256] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:42.377271] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:42.377281] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:42.377290] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:42.377307] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:42.377320] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:42.377326] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:42.377332] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:42.377338] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:42.380585   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:42.477438] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:42.477498] Starting reconciling attached volumes still attached
E0216 15:41:42.477519   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.578585] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.679693] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.780793] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.881898] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:42.982975] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.084140] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.187143] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.288255] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:43.378142   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:43.378162   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:43.378224] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:43.378269] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:43.378306] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:43.378315] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:43.378325] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:43.378342] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:43.378382] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:43.378391] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:43.378400] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:43.378407] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:43.381000   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:43.389311] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.490482] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:43.490543] Starting reconciling attached volumes still attached
E0216 15:41:43.490582   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.591675] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.692759] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.793625] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.894464] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:43.994626] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.097252] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.198443] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.299550] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:44.379256   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:44.379256   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:44.379289] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:44.379307] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:44.379316] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:44.379330] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:44.379337] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:44.379348] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:44.379356] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:44.379365] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:44.379376] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:44.379382] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:44.381305   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:44.400646] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.501718] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0216 15:41:44.501744] Starting reconciling attached volumes still attached
E0216 15:41:44.501763   90872 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.602835] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.703976] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.805168] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:44.906259] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:45.007343] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:45.107681] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:45.208072] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0216 15:41:45.309211] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0216 15:41:45.380258   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.380285   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:45.380377] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:45.380414] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0216 15:41:45.380426] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:45.380436] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:45.380445] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:45.380451] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:45.380485] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:45.380492] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:45.380505] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:45.380524] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:45.381464   90872 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0216 15:41:45.410290] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    attach_detach_controller_test.go:704: Expected detach operation not found, node:mynode-1, volume: vol-zero-va, tries: 10
I0216 15:41:45.504764   90872 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/CSI_migration_with_empty_volume_name
I0216 15:41:45.519235   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.519285   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.519241   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.519250   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.519291   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.519272   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.519272   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.606694   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0216 15:41:45.618611   90872 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0216 15:41:45.618637   90872 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0216 15:41:45.703187   90872 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0216 15:41:45.720998   90872 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721008   90872 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721054   90872 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721052   90872 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721002   90872 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721067   90872 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.720998   90872 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721073   90872 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721089   90872 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.720998   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721108   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.721064   90872 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.724924   90872 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.724929   90872 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.754702   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.754783   90872 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.766740   90872 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.766742   90872 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.766764   90872 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.766766   90872 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:45.766787] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0216 15:41:45.766835] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0216 15:41:45.766832   90872 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0216 15:41:45.766845] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0216 15:41:45.766853] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:45.766867] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0216 15:41:45.766898] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:45.766905] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0216 15:41:45.766915] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0216 15:41:45.766923] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0216 15:41:45.766930] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0216 15:41:45.812781   90872 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0216 15:41:45.812823] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0216 15:41:45.812877] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0216 15:41:45.812905] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0216 15:41:45.812919] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0216 15:41:45.812939] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0216 15:41:45.812970] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:45.812985] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0216 15:41:45.812995] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:45.813003] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:45.813012] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:45.813021] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0216 15:41:45.813029] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:45.813037] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:45.813045] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:45.813052] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0216 15:41:45.813068] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:45.813074] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0216 15:41:45.813080] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0216 15:41:45.813088] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0216 15:41:45.813097] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0216 15:41:45.813105] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0216 15:41:45.815454] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/ attachState="Detached"
    attach_detach_controller.go:424: I0216 15:41:45.815467] Populating DesiredStateOfworld
I0216 15:41:45.829011   90872 watch.go:142] "Stopping fake watcher"
--- FAIL: TestCtest_ADC_VolumeAttachmentRecovery (34.25s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_attached (1.10s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_unattached (1.10s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/Scheduled_Pod_with_migrated_PV (0.10s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/Deleted_Pod_with_migrated_PV (0.10s)
    --- FAIL: TestCtest_ADC_VolumeAttachmentRecovery/Empty_volume_name (10.11s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/Nil_pod_name_(should_be_ignored) (1.10s)
    --- FAIL: TestCtest_ADC_VolumeAttachmentRecovery/Invalid_node_name_characters (10.12s)
    --- FAIL: TestCtest_ADC_VolumeAttachmentRecovery/Zero-length_VA_name (10.17s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/CSI_migration_with_empty_volume_name (0.31s)
I0216 15:41:45.829035   90872 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.829049   90872 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.829050   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.829063   90872 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.829066   90872 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.829066   90872 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:41:45.829073   90872 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
FAIL
coverage: 1.2% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/volume/attachdetach	43.420s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/cache	3.308s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/attachdetach/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/attachdetach/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestVolumesInUseMetricCollection

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:38]: Matched PodSpec config: {test_fixture.json [volumes-in-use podspec] spec [pods] {[{metric-test-volume-name {nil nil nil nil nil nil nil nil nil &PersistentVolumeClaimVolumeSource{ClaimName:metric-test-pvc,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> metric-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:41:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:41:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"nodeName":"metric-test-host","volumes":[{"name":"metric-test-volume-name","persistentVolumeClaim":{"claimName":"metric-test-pvc"}}]})[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/16 15:41:15 [DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{metric-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil &PersistentVolumeClaimVolumeSource{ClaimName:metric-test-pvc,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> metric-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{metric-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil &PersistentVolumeClaimVolumeSource{ClaimName:metric-test-pvc,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> metric-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:44]: PodSpec JSON: [{"volumes":[{"name":"metric-test-volume-name","hostPath":{"path":"/etc/kubernetes"},"persistentVolumeClaim":{"claimName":"metric-test-pvc"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"nodeName":"metric-test-host"}]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:49]: Number of PodSpec configs: 1
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:57]: Matched PVC config: {test_fixture.json [volumes-in-use pvc] spec [persistentvolumeclaims] {[ReadOnlyMany ReadWriteOnce] nil {map[] map[storage:{{2 9} {<nil>} 2G DecimalSI}]} test-metric-pv-1 <nil> <nil> nil nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumeclaims]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumeclaims], int=1)[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumeclaims]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:41:15 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumeclaims
FAIL	k8s.io/kubernetes/pkg/controller/volume/attachdetach/metrics	0.818s
=== RUN   TestCtestFindAndAddActivePods_FindAndRemoveDeletedPods

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:42]: get default configs: {test_fixture.json [default pod spec for RBD volume] spec [pods] {[{dswp-test-volume-name {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:41:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:41:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"nodeName":"dswp-test-host","volumes":[{"name":"dswp-test-volume-name","rbd":{"image":"dswp-test-fake-device","monitors":null}}]})[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/16 15:41:15 [DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:53]: New Json Test Configs: [{"volumes":[{"name":"dswp-test-volume-name","hostPath":{"path":"/etc/kubernetes"},"rbd":{"monitors":null,"image":"dswp-test-fake-device"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"nodeName":"dswp-test-host"}]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:54]: Num of Test Cases: 1
Running 0 th test case.
{[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    desired_state_of_world_populator.go:158: I0216 15:41:15.996928] Removing pod from dsw because it does not exist in pod informer podName="dswp-test/dswp-test-pod-9b162565-ee44-4e95-90b7-55521d38af63" podUID="dswp-test-pod-uid-170a83f7-832b-46b9-8b95-88266c530c5d"
    desired_state_of_world_populator.go:158: I0216 15:41:15.997184] Removing pod from dsw because it does not exist in pod informer podName="dswp-test/dswp-test-pod-9b162565-ee44-4e95-90b7-55521d38af63" podUID="dswp-test-pod-uid-170a83f7-832b-46b9-8b95-88266c530c5d"
    ctest_desired_state_of_world_populator_test.go:179: VolumeExists("fake-plugin/dswp-test-fake-device") failed. Expected: <false> Actual: <true>
--- FAIL: TestCtestFindAndAddActivePods_FindAndRemoveDeletedPods (0.00s)
=== RUN   TestCtestFindAndRemoveNonattachableVolumes

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:195]: get default configs: {test_fixture.json [default pod spec for CSI volume] spec [pods] {[{dswp-test-volume-name {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:41:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:41:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"nodeName":"dswp-test-host","volumes":[{"csi":{"driver":"dswp-test-fake-csi-driver"},"name":"dswp-test-volume-name"}]})[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/16 15:41:15 [DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:206]: New Json Test Configs: [{"volumes":[{"name":"dswp-test-volume-name","hostPath":{"path":"/etc/kubernetes"},"csi":{"driver":"dswp-test-fake-csi-driver"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"nodeName":"dswp-test-host"}]
[DEBUG-CTEST 2026-02-16 15:41:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:207]: Num of Test Cases: 1
Running 0 th test case.
{[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    desired_state_of_world_populator.go:168: I0216 15:41:15.998451] Volume changes from attachable to non-attachable volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998464] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:168: I0216 15:41:15.998474] Volume changes from attachable to non-attachable volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998481] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:168: I0216 15:41:15.998552] Volume changes from attachable to non-attachable volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998561] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:168: I0216 15:41:15.998568] Volume changes from attachable to non-attachable volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998576] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:168: I0216 15:41:15.998636] Volume changes from attachable to non-attachable volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998645] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:168: I0216 15:41:15.998651] Volume changes from attachable to non-attachable volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998656] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:168: I0216 15:41:15.998661] Volume changes from attachable to non-attachable volumeName="fake-plugin/extra-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0216 15:41:15.998668] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-a7c74822-50d9-494f-930b-1f5b4680fd68" volumeName="fake-plugin/extra-fake-csi-driver"

==================== CTEST END ======================
--- PASS: TestCtestFindAndRemoveNonattachableVolumes (0.00s)
FAIL
coverage: 1.0% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/volume/attachdetach/populator	2.785s
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/reconciler	3.452s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/statusupdater	3.117s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/attachdetach/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/util	2.227s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/common		coverage: 0.0% of statements
/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b3745/ephemeral.test flag redefined: v
panic: /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b3745/ephemeral.test flag redefined: v

goroutine 1 [running]:
flag.(*FlagSet).Var(0x140001d0070, {0x106facbe8, 0x108eabf30}, {0x1063ba448, 0x1}, {0x105fcf87a, 0x22})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/flag/flag.go:1028 +0x2d4
k8s.io/klog/v2.InitFlags.func1(0x14000261a70?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/k8s.io/klog/v2/klog.go:447 +0x3c
flag.(*FlagSet).VisitAll(0x0?, 0x14000067dd8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/flag/flag.go:458 +0x48
k8s.io/klog/v2.InitFlags(0x0?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/k8s.io/klog/v2/klog.go:446 +0x44
k8s.io/kubernetes/pkg/controller/volume/ephemeral.init.0()
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/ephemeral/controller_test.go:56 +0x20
FAIL	k8s.io/kubernetes/pkg/controller/volume/ephemeral	0.745s
	k8s.io/kubernetes/pkg/controller/volume/ephemeral/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/ephemeral/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/ephemeral/metrics		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/controller/volume/events	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/expand	1.722s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/persistentvolume	2.217s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/metrics		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/options		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/testing		coverage: 0.0% of statements
=== RUN   TestCtestIsDeletionCandidateLackDeleteTimeAndFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:54]: Number of test cases: 5
Running 0 th test case.
{pv lacks delete time and finalizer 0x140008c1408 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc lacks delete time and finalizer 0x140003292c0 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv empty finalizer 0x140008c1688  false}
Running 3 th test case.
{pvc empty finalizer 0x140003294a0  false}
Running 4 th test case.
{service object with pv finalizer 0x140008c1908 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateLackDeleteTimeAndFinalizer (0.00s)
=== RUN   TestCtestIsDeletionCandidateLackDeleteTime

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:108]: Number of test cases: 5
Running 0 th test case.
{pv lacks delete time 0x140008c1b88 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc lacks delete time 0x14000329680 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv empty finalizer after setting delete time 0x140008f2008  false}
Running 3 th test case.
{pvc empty finalizer after setting delete time 0x14000329860  false}
Running 4 th test case.
{service object with pvc finalizer 0x140008f2288 kubernetes.io/pvc-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateLackDeleteTime (0.00s)
=== RUN   TestCtestIsDeletionCandidateLackFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:162]: Number of test cases: 5
Running 0 th test case.
{pv lacks finalizer 0x140008f2508 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc lacks finalizer 0x14000329a40 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv with empty finalizer string 0x140008f2788  false}
Running 3 th test case.
{pvc with empty finalizer string 0x14000329c20  false}
Running 4 th test case.
{service object with pv finalizer 0x140008f2a08 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateLackFinalizer (0.00s)
=== RUN   TestCtestIsDeletionCandidateSuccess

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:222]: Number of test cases: 5
Running 0 th test case.
{pv is to delete 0x140008f2c88 kubernetes.io/pv-protection true}
Running 1 th test case.
{pvc is to delete 0x140008f4000 kubernetes.io/pvc-protection true}
Running 2 th test case.
{pv missing finalizer but has delete time 0x140008f2f08 kubernetes.io/pv-protection false}
Running 3 th test case.
{pvc missing delete time but has finalizer 0x140008f41e0 kubernetes.io/pvc-protection false}
Running 4 th test case.
{service object with correct finalizer 0x140008f3188 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateSuccess (0.00s)
=== RUN   TestCtestNeedToAddFinalizerHasDeleteTimeAndFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:278]: Number of test cases: 5
Running 0 th test case.
{pv has delete time and finalizer 0x140008f3408 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc has delete time and finalizer 0x140008f43c0 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv has delete time but empty finalizer list 0x140008f3688 kubernetes.io/pv-protection true}
    ctest_utils_test.go:283: pv has delete time but empty finalizer list
Running 3 th test case.
{pvc has delete time but empty finalizer list 0x140008f45a0 kubernetes.io/pvc-protection true}
    ctest_utils_test.go:283: pvc has delete time but empty finalizer list
Running 4 th test case.
{service object with delete time and finalizer 0x140008f3908 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- FAIL: TestCtestNeedToAddFinalizerHasDeleteTimeAndFinalizer (0.00s)
=== RUN   TestCtestNeedToAddFinalizerHasDeleteTime

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:332]: Number of test cases: 5
Running 0 th test case.
{pv has delete 0x140008f3b88 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc has delete 0x140008f4780 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv has delete but empty finalizer string 0x140008f6008  true}
Running 3 th test case.
{pvc has delete but empty finalizer string 0x140008f4960  true}
Running 4 th test case.
{service with delete time 0x140008f6288 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestNeedToAddFinalizerHasDeleteTime (0.00s)
=== RUN   TestCtestNeedToAddFinalizerHasFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:390]: Number of test cases: 5
Running 0 th test case.
{pv has finalizer 0x140008f6508 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc has finalizer 0x140008f4b40 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv has unrelated finalizer 0x140008f6788 kubernetes.io/pv-protection true}
Running 3 th test case.
{pvc has unrelated finalizer 0x140008f4d20 kubernetes.io/pvc-protection true}
Running 4 th test case.
{service object with finalizer 0x140008f6a08 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestNeedToAddFinalizerHasFinalizer (0.00s)
=== RUN   TestCtestNeedToAddFinalizerSuccess

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:439]: Number of test cases: 5
Running 0 th test case.
{pv needs add finalizer 0x140008f6c88 kubernetes.io/pv-protection true}
Running 1 th test case.
{pvc needs add finalizer 0x140008f4f00 kubernetes.io/pvc-protection true}
Running 2 th test case.
{pv with empty finalizer string 0x140008f6f08  true}
Running 3 th test case.
{pvc with empty finalizer string 0x140008f50e0  true}
Running 4 th test case.
{service object without delete time or finalizer 0x140008f7188 kubernetes.io/pv-protection true}

==================== CTEST END ======================
--- PASS: TestCtestNeedToAddFinalizerSuccess (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/volume/protectionutil	3.304s
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/pvcprotection	3.123s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestPVProtectionController

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/pvprotection/ctest_pv_protection_controller_test.go:41]: Base config item: {test_fixture.json [basic pv] finalizers [persistentvolumes] &PersistentVolume{ObjectMeta:{default-pv      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PersistentVolumeSpec{Capacity:ResourceList{},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:nil,Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[],ClaimRef:nil,PersistentVolumeReclaimPolicy:,StorageClassName:,MountOptions:[],VolumeMode:nil,NodeAffinity:nil,VolumeAttributesClassName:nil,},Status:PersistentVolumeStatus{Phase:,Message:,Reason:,LastPhaseTransitionTime:<nil>,},}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-16 15:41:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-16 15:41:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:41:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-16 15:41:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:41:32 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/controller/volume/pvprotection	0.793s
=== RUN   TestCtestSELinuxWarningController_Sync
[DEBUG-CTEST 2026-02-16 15:41:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:166]: Running SELinuxWarningController sync tests, total: 8
=== RUN   TestCtestSELinuxWarningController_Sync/existing_pod_with_no_volumes
[DEBUG-CTEST 2026-02-16 15:41:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 0: existing pod with no volumes
    selinux_warning_controller.go:348: I0216 15:41:34.166028] Starting SELinux warning controller
    selinux_warning_controller.go:418: I0216 15:41:34.166159] Syncing pod pod="ns1/pod1"
    selinux_warning_controller.go:441: I0216 15:41:34.166290] skipping not found volume pod="ns1/pod1" volume="vol1"
  I0216 15:41:34.166315   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.166349   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/existing_pod_with_unbound_PVC
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 1: existing pod with unbound PVC
    selinux_warning_controller.go:418: I0216 15:41:34.268497] Syncing pod pod="ns1/pod1"
    selinux_warning_controller.go:348: I0216 15:41:34.268601] Starting SELinux warning controller
    selinux_warning_controller.go:441: I0216 15:41:34.268667] skipping not found volume pod="ns1/pod1" volume="vol1"
  I0216 15:41:34.268728   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.268794   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/existing_pod_with_fully_bound_PVC
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 2: existing pod with fully bound PVC
    selinux_warning_controller.go:348: I0216 15:41:34.370278] Starting SELinux warning controller
    selinux_warning_controller.go:418: I0216 15:41:34.370354] Syncing pod pod="ns1/pod1"
  I0216 15:41:34.370365   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.370378   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
    selinux_warning_controller.go:494: I0216 15:41:34.370450] Syncing pod volume pod="ns1/pod1" volume="pv1" label=":::s0:c1,c2" uniqueVolumeName="fake-plugin/pv1" changePolicy="MountOption" csiDriver="ebs.csi.aws.com"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_nil_security_context
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 3: pod with nil security context
    selinux_warning_controller.go:348: I0216 15:41:34.471414] Starting SELinux warning controller
    selinux_warning_controller.go:418: I0216 15:41:34.471529] Syncing pod pod="ns1/pod-nil-sc"
  I0216 15:41:34.471581   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
    selinux_warning_controller.go:441: I0216 15:41:34.471575] skipping not found volume pod="ns1/pod-nil-sc" volume="vol1"
  I0216 15:41:34.471630   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
    ctest_selinux_warning_controller_test.go:242: got unexpected events: map[]
    ctest_selinux_warning_controller_test.go:243: missing events: map[Normal SELinuxLabelMissing SELinuxLabel missing on pod pod-nil-sc:{}]
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_empty_SELinux_level
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 4: pod with empty SELinux level
    selinux_warning_controller.go:348: I0216 15:41:34.572674] Starting SELinux warning controller
    selinux_warning_controller.go:418: I0216 15:41:34.572762] Syncing pod pod="ns1/pod-empty-level"
  I0216 15:41:34.572782   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.572797   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
    selinux_warning_controller.go:441: I0216 15:41:34.572803] skipping not found volume pod="ns1/pod-empty-level" volume="vol1"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_unknown_SELinuxChangePolicy_(invalid_pointer)
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 5: pod with unknown SELinuxChangePolicy (invalid pointer)
    selinux_warning_controller.go:418: I0216 15:41:34.674919] Syncing pod pod="ns1/pod-unknown-policy"
    selinux_warning_controller.go:441: I0216 15:41:34.674976] skipping not found volume pod="ns1/pod-unknown-policy" volume="vol1"
    selinux_warning_controller.go:348: I0216 15:41:34.675009] Starting SELinux warning controller
    ctest_selinux_warning_controller_test.go:242: got unexpected events: map[]
    ctest_selinux_warning_controller_test.go:243: missing events: map[Warning SELinuxChangePolicyInvalid SELinuxChangePolicy "UnknownPolicy" is not supported:{}]
  I0216 15:41:34.675122   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.675168   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/PVC_missing_volume_source_(nil_PV)
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 6: PVC missing volume source (nil PV)
    selinux_warning_controller.go:418: I0216 15:41:34.776251] Syncing pod pod="ns1/pod-missing-pv"
    selinux_warning_controller.go:441: I0216 15:41:34.776305] skipping not found volume pod="ns1/pod-missing-pv" volume="vol1"
    selinux_warning_controller.go:348: I0216 15:41:34.776363] Starting SELinux warning controller
  I0216 15:41:34.776516   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.776544   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_duplicate_volumeMount_names
[DEBUG-CTEST 2026-02-16 15:41:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 7: pod with duplicate volumeMount names
    selinux_warning_controller.go:348: I0216 15:41:34.877888] Starting SELinux warning controller
    selinux_warning_controller.go:418: I0216 15:41:34.878001] Syncing pod pod="ns1/pod-dup"
    selinux_warning_controller.go:494: I0216 15:41:34.878048] Syncing pod volume pod="ns1/pod-dup" volume="pvdup" label=":::s0:c1,c2" uniqueVolumeName="fake-plugin/pvdup" changePolicy="MountOption" csiDriver="ebs.csi.aws.com"
    selinux_warning_controller.go:441: I0216 15:41:34.878059] skipping not found volume pod="ns1/pod-dup" volume="vol1"
    ctest_selinux_warning_controller_test.go:224: expected error, got nil
  I0216 15:41:34.878243   91109 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0216 15:41:34.878265   91109 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
--- FAIL: TestCtestSELinuxWarningController_Sync (0.88s)
    --- PASS: TestCtestSELinuxWarningController_Sync/existing_pod_with_no_volumes (0.17s)
    --- PASS: TestCtestSELinuxWarningController_Sync/existing_pod_with_unbound_PVC (0.10s)
    --- PASS: TestCtestSELinuxWarningController_Sync/existing_pod_with_fully_bound_PVC (0.10s)
    --- FAIL: TestCtestSELinuxWarningController_Sync/pod_with_nil_security_context (0.10s)
    --- PASS: TestCtestSELinuxWarningController_Sync/pod_with_empty_SELinux_level (0.10s)
    --- FAIL: TestCtestSELinuxWarningController_Sync/pod_with_unknown_SELinuxChangePolicy_(invalid_pointer) (0.10s)
    --- PASS: TestCtestSELinuxWarningController_Sync/PVC_missing_volume_source_(nil_PV) (0.10s)
    --- FAIL: TestCtestSELinuxWarningController_Sync/pod_with_duplicate_volumeMount_names (0.10s)
FAIL
coverage: 1.1% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controller/volume/selinuxwarning	5.123s
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/selinuxwarning/cache	2.966s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/selinuxwarning/translator	3.832s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controller/volume/vacprotection	4.023s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 1.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controlplane	2.486s	coverage: 1.3% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 1.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controlplane/apiserver	3.601s	coverage: 1.2% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controlplane/apiserver/admission	5.570s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestCompleteForServiceAccount
=== RUN   TestCtestCompleteForServiceAccount/endpoint_and_key_file
I0216 15:41:55.872956   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-652545000.sock
I0216 15:41:55.873104   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-652545000.sock
=== RUN   TestCtestCompleteForServiceAccount/max_token_expiration_breaching_acceptable_values
I0216 15:41:56.434112   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-876952000.sock
I0216 15:41:56.434140   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-876952000.sock
=== RUN   TestCtestCompleteForServiceAccount/path_to_a_signing_key_provided
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_endpoint_expiration
I0216 15:41:57.015772   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-435137000.sock
I0216 15:41:57.015805   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-435137000.sock
I0216 15:41:57.017795   91228 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_local_smaller_expirations
I0216 15:41:57.466553   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-18693000.sock
I0216 15:41:57.466593   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-18693000.sock
I0216 15:41:57.467475   91228 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_want_larger_than_signer_can_provide
I0216 15:41:58.134992   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-467720000.sock
I0216 15:41:58.135066   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-467720000.sock
I0216 15:41:58.137242   91228 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_but_return_smaller_than_acceptable_max_token_exp
I0216 15:41:58.552551   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-137565000.sock
I0216 15:41:58.552569   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-137565000.sock
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_getting_metadata
I0216 15:41:58.553779   91228 keycache.go:72] "Key cache shutting down"
I0216 15:41:59.061494   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-553452000.sock
I0216 15:41:59.061513   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-553452000.sock
I0216 15:41:59.070913   91228 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_creating_plugin_(during_initial_fetch)
I0216 15:41:59.665076   91228 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-71132000.sock
I0216 15:41:59.665174   91228 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-71132000.sock
=== RUN   TestCtestCompleteForServiceAccount/max_token_expiration_at_upper_limit
--- PASS: TestCtestCompleteForServiceAccount (4.09s)
    --- PASS: TestCtestCompleteForServiceAccount/endpoint_and_key_file (0.22s)
    --- PASS: TestCtestCompleteForServiceAccount/max_token_expiration_breaching_acceptable_values (0.56s)
    --- PASS: TestCtestCompleteForServiceAccount/path_to_a_signing_key_provided (0.00s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_endpoint_expiration (0.58s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_local_smaller_expirations (0.45s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_want_larger_than_signer_can_provide (0.67s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_but_return_smaller_than_acceptable_max_token_exp (0.42s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_getting_metadata (0.52s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_creating_plugin_(during_initial_fetch) (0.60s)
    --- PASS: TestCtestCompleteForServiceAccount/max_token_expiration_at_upper_limit (0.00s)
=== RUN   TestCtestValidateAPIPriorityAndFairness
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/all=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/beta=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/ga=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/ga=true
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta1=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta2=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1=false
    ctest_validation_test.go:103: Expected no error, but got: "--runtime-config=flowcontrol.apiserver.k8s.io/v1=false conflicts with --enable-priority-and-fairness=true"
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true,flowcontrol.apiserver.k8s.io/v1=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/#00
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/unknown=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/beta=maybe
=== RUN   TestCtestValidateAPIPriorityAndFairness/invalid-format
--- FAIL: TestCtestValidateAPIPriorityAndFairness (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/all=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/beta=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/ga=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/ga=true (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta1=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta2=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true (0.00s)
    --- FAIL: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true,flowcontrol.apiserver.k8s.io/v1=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/#00 (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/unknown=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/beta=maybe (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/invalid-format (0.00s)
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerCAFile_set
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertiseIP_set
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertisePort_set
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_empty_peerCAFile
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_both_peerCAFile_and_peerAdvertiseIP
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/invalid_peerAdvertisePort_(non-numeric)
    ctest_validation_test.go:175: Expected error message to contain: "--peer-advertise-port requires a numeric value", but got: ""
--- FAIL: TestCtestValidateUnknownVersionInteroperabilityProxy (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerCAFile_set (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertiseIP_set (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertisePort_set (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_empty_peerCAFile (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_both_peerCAFile_and_peerAdvertiseIP (0.00s)
    --- FAIL: TestCtestValidateUnknownVersionInteroperabilityProxy/invalid_peerAdvertisePort_(non-numeric) (0.00s)
=== RUN   TestCtestValidateOptions
=== RUN   TestCtestValidateOptions/validate_master_count_equal_0
=== RUN   TestCtestValidateOptions/validate_token_request_enable_not_attempted
=== RUN   TestCtestValidateOptions/nil_options
=== RUN   TestCtestValidateOptions/empty_Options_struct
--- FAIL: TestCtestValidateOptions (0.00s)
    --- PASS: TestCtestValidateOptions/validate_master_count_equal_0 (0.00s)
    --- PASS: TestCtestValidateOptions/validate_token_request_enable_not_attempted (0.00s)
    --- PASS: TestCtestValidateOptions/nil_options (0.00s)
    --- FAIL: TestCtestValidateOptions/empty_Options_struct (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x78 pc=0x103226b68]

goroutine 279 [running]:
testing.tRunner.func1.2({0x1045b8be0, 0x1064b4560})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1045b8be0?, 0x1064b4560?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/apiserver/pkg/server/options.(*ServerRunOptions).Validate(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apiserver/pkg/server/options/server_run_options.go:193 +0x98
k8s.io/kubernetes/pkg/controlplane/apiserver/options.(*Options).Validate(0x140002734a0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/options/validation.go:142 +0x94
k8s.io/kubernetes/pkg/controlplane/apiserver/options.TestCtestValidateOptions.func1(0x14000b32fc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/options/ctest_validation_test.go:254 +0x34
testing.tRunner(0x14000b32fc0, 0x14000b387e0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 275
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controlplane/apiserver/options	6.465s
?   	k8s.io/kubernetes/pkg/controlplane/apiserver/samples	[no test files]
	k8s.io/kubernetes/pkg/controlplane/apiserver/samples/generic		coverage: 0.0% of statements
=== RUN   TestCtestDefaultOffAdmissionPlugins

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/samples/generic/server/ctest_admission_test.go:92]: get default configs: {test_fixture.json [default admission plugins] plugins [] [LimitRanger DefaultStorageClass PersistentVolumeClaimResize StorageObjectInUseProtection Priority TaintNodesByCondition RuntimeClass DefaultIngressClass PodSecurity PodTopologyLabels]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "plugins" in requested fixtures
2026/02/16 15:41:57 [DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:41:57 Mode: 1
2026/02/16 15:41:57 Base JSON size: 204 bytes
2026/02/16 15:41:57 Number of external values: 0
2026/02/16 15:41:57 [DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:41:57 [DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=["LimitRanger","DefaultStorageClass","PersistentVolumeClaimResize","StorageObjectInUseProtection","Priority","TaintNodesByCondition","RuntimeClass","DefaultIngressClass","PodSecurity","PodTopologyLabels"])[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/samples/generic/server/ctest_admission_test.go:40]: New Json Test Configs: [[LimitRanger DefaultStorageClass PersistentVolumeClaimResize StorageObjectInUseProtection Priority TaintNodesByCondition RuntimeClass DefaultIngressClass PodSecurity PodTopologyLabels]]
[DEBUG-CTEST 2026-02-16 15:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/samples/generic/server/ctest_admission_test.go:41]: Num of Test Cases: 1
Running 0 th test case.
Plugin list: [LimitRanger DefaultStorageClass PersistentVolumeClaimResize StorageObjectInUseProtection Priority TaintNodesByCondition RuntimeClass DefaultIngressClass PodSecurity PodTopologyLabels]

==================== CTEST END ======================
--- PASS: TestCtestDefaultOffAdmissionPlugins (0.00s)
PASS
coverage: 1.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controlplane/apiserver/samples/generic/server	5.953s	coverage: 1.3% of statements in ./...
	k8s.io/kubernetes/pkg/controlplane/apiserver/samples/generic/server/testing		coverage: 0.0% of statements
=== RUN   TestCtest_Controller

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtest_Controller/lease_not_expired
[DEBUG-CTEST 2026-02-16 15:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/apiserverleasegc/ctest_gc_controller_test.go:51]: Matched config for lease not expired : {test_fixture.json [lease not expired] spec [leases] &Lease{ObjectMeta:{kube-apiserver-12345  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[apiserver.kubernetes.io/identity:kube-apiserver] map[] [] [] []},Spec:LeaseSpec{HolderIdentity:*kube-apiserver-12345,LeaseDurationSeconds:*10,AcquireTime:<nil>,RenewTime:2026-02-16 15:42:04.963819 -0600 CST m=+0.033197084,LeaseTransitions:nil,Strategy:nil,PreferredHolder:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[leases]
[DEBUG-CTEST 2026-02-16 15:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[leases], int=1)[DEBUG-CTEST 2026-02-16 15:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [leases]
[DEBUG-CTEST 2026-02-16 15:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:42:04 load all fixtures failed: requested fixture keys not found in test_fixtures.json: leases
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/apiserverleasegc	0.897s
=== RUN   TestCtestWriteClientCAsEdge
=== RUN   TestCtestWriteClientCAsEdge/nil_clusterAuthInfo_(no_data)
=== RUN   TestCtestWriteClientCAsEdge/empty_CA_provider_(zero_length)
    ctest_cluster_authentication_trust_controller_test.go:143: empty CA provider (zero length): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s"&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{client-ca-file: ,},BinaryData:map[string][]byte{},Immutable:nil,}",
          }
=== RUN   TestCtestWriteClientCAsEdge/extremely_long_header_slice_(edge_size)
    ctest_cluster_authentication_trust_controller_test.go:143: extremely long header slice (edge size): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s"&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{requestheader-username-headers: [\"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"...,
          }
=== RUN   TestCtestWriteClientCAsEdge/requestheader_UID_without_feature_gate_(should_be_dropped)
    ctest_cluster_authentication_trust_controller_test.go:143: requestheader UID without feature gate (should be dropped): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s"&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}",
          }
=== RUN   TestCtestWriteClientCAsEdge/requestheader_UID_with_feature_gate_(should_be_added)
    ctest_cluster_authentication_trust_controller_test.go:143: requestheader UID with feature gate (should be added): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s`&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{requestheader-uid-headers: ["uid-123"],},BinaryData:map[string][]byte{},Immutable:nil,}`,
          }
--- FAIL: TestCtestWriteClientCAsEdge (0.00s)
    --- PASS: TestCtestWriteClientCAsEdge/nil_clusterAuthInfo_(no_data) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/empty_CA_provider_(zero_length) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/extremely_long_header_slice_(edge_size) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/requestheader_UID_without_feature_gate_(should_be_dropped) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/requestheader_UID_with_feature_gate_(should_be_added) (0.00s)
FAIL
coverage: 0.6% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/clusterauthenticationtrust	6.423s
=== RUN   TestCtestHandleVersionUpdate
[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/crdregistration/ctest_crdregistration_controller_test.go:24]: Start test TestCtestHandleVersionUpdate
[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/crdregistration/ctest_crdregistration_controller_test.go:31]: get default configs: {test_fixture.json [starting crds] spec.versions [customresourcedefinitions] [&CustomResourceDefinition{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:CustomResourceDefinitionSpec{Group:group.com,Names:CustomResourceDefinitionNames{Plural:,Singular:,ShortNames:[],Kind:,ListKind:,Categories:[],},Scope:,Versions:[]CustomResourceDefinitionVersion{CustomResourceDefinitionVersion{Name:v1,Served:true,Storage:true,Schema:nil,Subresources:nil,AdditionalPrinterColumns:[]CustomResourceColumnDefinition{},Deprecated:false,DeprecationWarning:nil,SelectableFields:[]SelectableField{},},},Conversion:nil,PreserveUnknownFields:false,},Status:CustomResourceDefinitionStatus{Conditions:[]CustomResourceDefinitionCondition{},AcceptedNames:CustomResourceDefinitionNames{Plural:,Singular:,ShortNames:[],Kind:,ListKind:,Categories:[],},StoredVersions:[],},}]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-16 15:42:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:42:06 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/crdregistration	1.958s
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/controlplane/controller/defaultservicecidr	6.417s	coverage: 0.4% of statements in ./... [no tests to run]
=== RUN   TestCtestCreateOrUpdateMasterService
=== RUN   TestCtestCreateOrUpdateMasterService/service_does_not_exist
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:72]: Running create test: service does not exist
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:42:09 Mode: 1
2026/02/16 15:42:09 Base JSON size: 170 bytes
2026/02/16 15:42:09 Number of external values: 23
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].protocol: TCP → TCP
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 5432
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 5432
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → tcp
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [KEEP] type: ClusterIP (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].name: foo (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 6379
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 6379
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 3306
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 3306
2026/02/16 15:42:09   [KEEP] ports[0].name: foo (missing in external)
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [KEEP] type: ClusterIP (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].name: foo (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].protocol: TCP → TCP
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 3100
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 3100
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [KEEP] type: ClusterIP (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 5432
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 5432
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → db-service
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → LoadBalancer
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 80
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → LoadBalancer
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → tcp
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 5432
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 5432
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8848
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8848
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [KEEP] type: ClusterIP (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 3306
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 3306
2026/02/16 15:42:09   [KEEP] ports[0].name: foo (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → NodePort
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 9090
2026/02/16 15:42:09   [KEEP] ports[0].name: foo (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 6379
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 6379
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → redis-service
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 80
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → result-service
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8081
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → NodePort
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 4444
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → web
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 4444
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → NodePort
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → NodePort
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → web
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 4444
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 4444
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → web
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 4444
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 4444
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → NodePort
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → http
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] sessionAffinity: None (missing in external)
2026/02/16 15:42:09   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/16 15:42:09   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/16 15:42:09   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/16 15:42:09   [OVERRIDE] ports[0].targetPort: 8080 → 80
2026/02/16 15:42:09   [OVERRIDE] ports[0].name: foo → vote-service
2026/02/16 15:42:09   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/16 15:42:09   [OVERRIDE] type: ClusterIP → NodePort
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=23)
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"clusterIP":"1.2.3.4","ipFamilyPolicy":"SingleStack","ports":[{"name":"foo","port":8080,"protocol":"TCP","targetPort":8080}],"sessionAffinity":"None","type":"ClusterIP"})[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032ed30 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032ed30 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 2
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032efd0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032efd0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 3
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032f290 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032f290 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 4
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032f530 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032f530 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 5
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032f7d0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032f7d0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 6
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 3100 {0 3100 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032fa70 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 3100 {0 3100 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032fa70 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 7
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{db-service TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032fd30 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{db-service TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400032fd30 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 8
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b4090 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b4090 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 9
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x140006b4330 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x140006b4330 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 10
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 80 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x140006b45e0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 80 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x140006b45e0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 11
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b48a0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b48a0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 12
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b4b40 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b4b40 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 13
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8848 {0 8848 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b4de0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8848 {0 8848 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b4de0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 14
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b5080 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b5080 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 15
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 8080 {0 9090 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5320 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 8080 {0 9090 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5320 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 16
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{redis-service TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b55c0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{redis-service TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006b55c0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 17
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{result-service TCP <nil> 8081 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5860 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{result-service TCP <nil> 8081 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5860 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 18
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5b00 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5b00 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 19
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5da0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x140006b5da0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 20
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400065c0c0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400065c0c0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 21
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400065c3b0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400065c3b0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 22
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400065c690 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400065c690 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 23
2026/02/16 15:42:09 [DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{vote-service TCP <nil> 8080 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400065c930 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{vote-service TCP <nil> 8080 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400065c930 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 23
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:88]: New Json Test Configs: [{"ports":[{"name":"tcp","protocol":"TCP","port":5432,"targetPort":5432}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":6379,"targetPort":6379}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":3306,"targetPort":3306}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":3100,"targetPort":3100}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"db-service","protocol":"TCP","port":5432,"targetPort":5432}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"LoadBalancer","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":80,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"LoadBalancer","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"tcp","protocol":"TCP","port":5432,"targetPort":5432}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8848,"targetPort":8848}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":3306,"targetPort":3306}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":8080,"targetPort":9090}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"redis-service","protocol":"TCP","port":6379,"targetPort":6379}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"result-service","protocol":"TCP","port":8081,"targetPort":80}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"web","protocol":"TCP","port":4444,"targetPort":4444}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"web","protocol":"TCP","port":4444,"targetPort":4444}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"web","protocol":"TCP","port":4444,"targetPort":4444}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"vote-service","protocol":"TCP","port":8080,"targetPort":80}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"}]
    ctest_controller_test.go:112: case "service does not exist": expected create spec:
        v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"tcp", Protocol:"TCP", AppProtocol:(*string)(nil), Port:5432, TargetPort:intstr.IntOrString{Type:0, IntVal:5432, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(0x1400032ed30), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}
        got:
        v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"foo", Protocol:"TCP", AppProtocol:(*string)(nil), Port:8080, TargetPort:intstr.IntOrString{Type:0, IntVal:8080, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(0x1400065ccf0), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}
=== RUN   TestCtestCreateOrUpdateMasterService/empty_service_ports_(edge_case)
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:72]: Running create test: empty service ports (edge case)
    ctest_controller_test.go:116: case "empty service ports (edge case)": no create expected, yet saw: [{{default create /v1, Resource=services }  &Service{ObjectMeta:{empty-ports  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[component:apiserver provider:kubernetes] map[] [] [] []},Spec:ServiceSpec{Ports:[]ServicePort{},Selector:map[string]string{},ClusterIP:1.2.3.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[],IPFamilies:[],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,TrafficDistribution:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},} {{ } []  }}]
=== RUN   TestCtestCreateOrUpdateMasterService/service_definition_wrong_port
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:199]: Running reconcile test: service definition wrong port
  W0216 15:42:09.881489   91323 controller.go:218] Resetting master service "foo" to &v1.Service{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"foo", Protocol:"TCP", AppProtocol:(*string)(nil), Port:8080, TargetPort:intstr.IntOrString{Type:0, IntVal:8080, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(nil), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}, Status:v1.ServiceStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}, Conditions:[]v1.Condition(nil)}}
=== RUN   TestCtestCreateOrUpdateMasterService/service_definition_missing_port_(edge_case)
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:199]: Running reconcile test: service definition missing port (edge case)
  W0216 15:42:09.881551   91323 controller.go:218] Resetting master service "foo" to &v1.Service{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"foo", Protocol:"TCP", AppProtocol:(*string)(nil), Port:8080, TargetPort:intstr.IntOrString{Type:0, IntVal:8080, StrVal:""}, NodePort:0}, v1.ServicePort{Name:"baz", Protocol:"TCP", AppProtocol:(*string)(nil), Port:1000, TargetPort:intstr.IntOrString{Type:0, IntVal:1000, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(nil), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}, Status:v1.ServiceStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}, Conditions:[]v1.Condition(nil)}}
=== RUN   TestCtestCreateOrUpdateMasterService/service_definition_wrong_port,_no_expected_update
[DEBUG-CTEST 2026-02-16 15:42:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:267]: Running non‑reconcile test: service definition wrong port, no expected update
--- FAIL: TestCtestCreateOrUpdateMasterService (0.00s)
    --- FAIL: TestCtestCreateOrUpdateMasterService/service_does_not_exist (0.00s)
    --- FAIL: TestCtestCreateOrUpdateMasterService/empty_service_ports_(edge_case) (0.00s)
    --- PASS: TestCtestCreateOrUpdateMasterService/service_definition_wrong_port (0.00s)
    --- PASS: TestCtestCreateOrUpdateMasterService/service_definition_missing_port_(edge_case) (0.00s)
    --- PASS: TestCtestCreateOrUpdateMasterService/service_definition_wrong_port,_no_expected_update (0.00s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/kubernetesservice	5.684s
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/empty
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/single_candidate
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_different_emulation_versions
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_different_binary_versions
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_same_binary_versions,_different_creation_timestamps
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/nil_candidate_in_slice
--- FAIL: TestCtestPickBestLeaderOldestEmulationVersion (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/empty (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/single_candidate (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_different_emulation_versions (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_different_binary_versions (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_same_binary_versions,_different_creation_timestamps (0.00s)
    --- FAIL: TestCtestPickBestLeaderOldestEmulationVersion/nil_candidate_in_slice (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x138 pc=0x105adbcc0]

goroutine 13 [running]:
testing.tRunner.func1.2({0x106144340, 0x106ce3690})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x106144340?, 0x106ce3690?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/controlplane/controller/leaderelection.validLeaseCandidateForOldestEmulationVersion(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/leaderelection/election.go:66 +0x90
k8s.io/kubernetes/pkg/controlplane/controller/leaderelection.pickBestLeaderOldestEmulationVersion({0x140002baed0, 0x2, 0x14000113f38?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/leaderelection/election.go:32 +0x110
k8s.io/kubernetes/pkg/controlplane/controller/leaderelection.TestCtestPickBestLeaderOldestEmulationVersion.func1(0x140003c1c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/leaderelection/ctest_election_test.go:208 +0x30
testing.tRunner(0x140003c1c00, 0x1400044c5c0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 7
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/leaderelection	5.364s
=== RUN   TestCtestSyncConfigMap

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-16 15:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-16 15:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-16 15:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:42:11 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/legacytokentracking	4.000s
=== RUN   TestCtest_Controller

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/systemnamespaces/ctest_system_namespaces_controller_test.go:31]: get default configs: {test_fixture.json [system namespaces list] systemNamespaces [namespaces] [kube-system kube-public kube-node-lease default]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[namespaces]
[DEBUG-CTEST 2026-02-16 15:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[namespaces], int=1)[DEBUG-CTEST 2026-02-16 15:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "systemNamespaces" in requested fixtures
[DEBUG-CTEST 2026-02-16 15:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/systemnamespaces/ctest_system_namespaces_controller_test.go:36]: Failed to generate config: %v mode-combination failed: no values found for field "systemNamespaces" in requested fixtures
    ctest_system_namespaces_controller_test.go:37: config generation error: mode-combination failed: no values found for field "systemNamespaces" in requested fixtures
--- FAIL: TestCtest_Controller (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/systemnamespaces	6.093s
=== RUN   TestCtestEndpointsAdapterGet
=== RUN   TestCtestEndpointsAdapterGet/single-existing-endpoints
=== RUN   TestCtestEndpointsAdapterGet/endpoints_exists,_endpointslice_does_not
=== RUN   TestCtestEndpointsAdapterGet/endpointslice_exists,_endpoints_does_not
=== RUN   TestCtestEndpointsAdapterGet/wrong-namespace
=== RUN   TestCtestEndpointsAdapterGet/wrong-name
=== RUN   TestCtestEndpointsAdapterGet/empty-namespace
=== RUN   TestCtestEndpointsAdapterGet/empty-name
=== RUN   TestCtestEndpointsAdapterGet/nil-initial-state
--- PASS: TestCtestEndpointsAdapterGet (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/single-existing-endpoints (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/endpoints_exists,_endpointslice_does_not (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/endpointslice_exists,_endpoints_does_not (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/wrong-namespace (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/wrong-name (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/empty-namespace (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/empty-name (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/nil-initial-state (0.00s)
=== RUN   TestCtestEndpointsAdapterCreate
=== RUN   TestCtestEndpointsAdapterCreate/single-endpoint
=== RUN   TestCtestEndpointsAdapterCreate/nil-endpoints-param
    ctest_endpointsadapter_test.go:205: Expected error: endpoints parameter is nil, got: object does not implement the Object interfaces
    ctest_endpointsadapter_test.go:211: unexpected error in side effects: expected 0 creates got 1
=== RUN   TestCtestEndpointsAdapterCreate/empty-namespace-param
    ctest_endpointsadapter_test.go:205: Expected error: namespace parameter is empty, got: request namespace does not match object namespace, request: "" object: "testing"
    ctest_endpointsadapter_test.go:211: unexpected error in side effects: [expected 0 creates got 1, expected create 0 to be:
        <nil>
        got:
        &v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"testing", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"endpointslice.kubernetes.io/skip-mirror":"true"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Subsets:[]v1.EndpointSubset{v1.EndpointSubset{Addresses:[]v1.EndpointAddress{v1.EndpointAddress{IP:"10.1.2.3", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x14000a288c0)}, v1.EndpointAddress{IP:"10.1.2.4", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x14000a28930)}}, NotReadyAddresses:[]v1.EndpointAddress(nil), Ports:[]v1.EndpointPort{v1.EndpointPort{Name:"port-0", Port:80, Protocol:"TCP", AppProtocol:(*string)(nil)}}}}}
        ]
=== RUN   TestCtestEndpointsAdapterCreate/single-endpoint-partial-ipv6
=== RUN   TestCtestEndpointsAdapterCreate/single-endpoint-full-ipv6
=== RUN   TestCtestEndpointsAdapterCreate/existing-endpoints
=== RUN   TestCtestEndpointsAdapterCreate/existing-endpointslice-incorrect
=== RUN   TestCtestEndpointsAdapterCreate/existing-endpointslice-correct
=== RUN   TestCtestEndpointsAdapterCreate/empty-endpoints-subsets
    ctest_endpointsadapter_test.go:211: unexpected error in side effects: [expected 1 creates got 2, expected create 1 to be:
        <nil>
        got:
        &v1.EndpointSlice{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"empty", GenerateName:"", Namespace:"testing", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"kubernetes.io/service-name":"empty"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, AddressType:"IPv4", Endpoints:[]v1.Endpoint(nil), Ports:[]v1.EndpointPort(nil)}
        ]
--- FAIL: TestCtestEndpointsAdapterCreate (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/single-endpoint (0.00s)
    --- FAIL: TestCtestEndpointsAdapterCreate/nil-endpoints-param (0.00s)
    --- FAIL: TestCtestEndpointsAdapterCreate/empty-namespace-param (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/single-endpoint-partial-ipv6 (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/single-endpoint-full-ipv6 (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/existing-endpoints (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/existing-endpointslice-incorrect (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/existing-endpointslice-correct (0.00s)
    --- FAIL: TestCtestEndpointsAdapterCreate/empty-endpoints-subsets (0.00s)
=== RUN   TestCtestEndpointsAdapterUpdate
=== RUN   TestCtestEndpointsAdapterUpdate/existing-endpointslice-replaced-with-updated-ipv4-address-type
=== RUN   TestCtestEndpointsAdapterUpdate/add-ports-and-ips
=== RUN   TestCtestEndpointsAdapterUpdate/endpointslice-correct-endpoints-wrong
=== RUN   TestCtestEndpointsAdapterUpdate/wrong-endpoints
=== RUN   TestCtestEndpointsAdapterUpdate/missing-endpoints
=== RUN   TestCtestEndpointsAdapterUpdate/missing-endpointslice
=== RUN   TestCtestEndpointsAdapterUpdate/nil-endpoints-param
    ctest_endpointsadapter_test.go:335: Expected error: endpoints parameter is nil, got: object does not implement the Object interfaces
    ctest_endpointsadapter_test.go:341: unexpected error in side effects: expected 0 updates got 1
=== RUN   TestCtestEndpointsAdapterUpdate/endpoints-correct-endpointslice-wrong
=== RUN   TestCtestEndpointsAdapterUpdate/empty-namespace
    ctest_endpointsadapter_test.go:335: Expected error: namespace parameter is empty, got: request namespace does not match object namespace, request: "" object: "testing"
    ctest_endpointsadapter_test.go:341: unexpected error in side effects: [expected 0 updates got 1, expected update 0 to be:
        <nil>
        got:
        &v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"testing", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"endpointslice.kubernetes.io/skip-mirror":"true"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Subsets:[]v1.EndpointSubset{v1.EndpointSubset{Addresses:[]v1.EndpointAddress{v1.EndpointAddress{IP:"10.1.2.3", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x14000387500)}, v1.EndpointAddress{IP:"10.1.2.4", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x14000387570)}}, NotReadyAddresses:[]v1.EndpointAddress(nil), Ports:[]v1.EndpointPort{v1.EndpointPort{Name:"port-0", Port:80, Protocol:"TCP", AppProtocol:(*string)(nil)}}}}}
        ]
=== RUN   TestCtestEndpointsAdapterUpdate/empty-endpoints-subsets
    ctest_endpointsadapter_test.go:335: Expected error: <nil>, got: endpoints "empty" not found
    ctest_endpointsadapter_test.go:338: Expected endpoints: &Endpoints{ObjectMeta:{empty  testing    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Subsets:[]EndpointSubset{},}, got: &Endpoints{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Subsets:[]EndpointSubset{},}
=== RUN   TestCtestEndpointsAdapterUpdate/single-existing-endpoints-no-change
--- FAIL: TestCtestEndpointsAdapterUpdate (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/existing-endpointslice-replaced-with-updated-ipv4-address-type (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/add-ports-and-ips (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/endpointslice-correct-endpoints-wrong (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/wrong-endpoints (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/missing-endpoints (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/missing-endpointslice (0.00s)
    --- FAIL: TestCtestEndpointsAdapterUpdate/nil-endpoints-param (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/endpoints-correct-endpointslice-wrong (0.00s)
    --- FAIL: TestCtestEndpointsAdapterUpdate/empty-namespace (0.00s)
    --- FAIL: TestCtestEndpointsAdapterUpdate/empty-endpoints-subsets (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/single-existing-endpoints-no-change (0.00s)
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-no-change
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-change
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/missing-endpointslice
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/nil-endpoints-param
--- FAIL: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints (0.00s)
    --- PASS: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-no-change (0.00s)
    --- PASS: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-change (0.00s)
    --- PASS: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/missing-endpointslice (0.00s)
    --- FAIL: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/nil-endpoints-param (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x28 pc=0x107e4d024]

goroutine 234 [running]:
testing.tRunner.func1.2({0x108a1dd00, 0x10aa46f70})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x108a1dd00?, 0x10aa46f70?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/controlplane/reconcilers.endpointSliceFromEndpoints(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/reconcilers/endpointsadapter.go:120 +0xa4
k8s.io/kubernetes/pkg/controlplane/reconcilers.(*EndpointsAdapter).EnsureEndpointSliceFromEndpoints(0x140004d5ed8, {0x107ec0c23, 0x7}, 0x10984e626?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/reconcilers/endpointsadapter.go:84 +0xa4
k8s.io/kubernetes/pkg/controlplane/reconcilers.TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints.func1(0x14000103880)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/reconcilers/ctest_endpointsadapter_test.go:413 +0x2d8
testing.tRunner(0x14000103880, 0x1400059d180)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 230
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controlplane/reconcilers	1.359s
?   	k8s.io/kubernetes/pkg/controlplane/storageversionhashdata	[no test files]
=== RUN   TestCtestURLsMatch
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:18]: Start TestCtestURLsMatch
    ctest_keyring_test.go:52: Expected match result of  and  to be false, but was true
    ctest_keyring_test.go:52: Expected match result of * and  to be false, but was true
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:56]: End TestCtestURLsMatch
--- FAIL: TestCtestURLsMatch (0.00s)
=== RUN   TestCtestDockerKeyringForGlob
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:60]: Start TestCtestDockerKeyringForGlob
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:122]: End TestCtestDockerKeyringForGlob
--- PASS: TestCtestDockerKeyringForGlob (0.00s)
=== RUN   TestCtestKeyringMiss
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:126]: Start TestCtestKeyringMiss
    ctest_keyring_test.go:162: Expected not to find URL , but found
    ctest_keyring_test.go:162: Expected not to find URL invalid, but found
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:165]: End TestCtestKeyringMiss
--- FAIL: TestCtestKeyringMiss (0.00s)
=== RUN   TestCtestKeyringMissWithDockerHubCredentials
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:169]: Start TestCtestKeyringMissWithDockerHubCredentials
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:193]: End TestCtestKeyringMissWithDockerHubCredentials
--- PASS: TestCtestKeyringMissWithDockerHubCredentials (0.00s)
=== RUN   TestCtestKeyringHitWithUnqualifiedDockerHub
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:197]: Start TestCtestKeyringHitWithUnqualifiedDockerHub
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:236]: End TestCtestKeyringHitWithUnqualifiedDockerHub
--- PASS: TestCtestKeyringHitWithUnqualifiedDockerHub (0.00s)
=== RUN   TestCtestKeyringHitWithUnqualifiedLibraryDockerHub
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:240]: Start TestCtestKeyringHitWithUnqualifiedLibraryDockerHub
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:279]: End TestCtestKeyringHitWithUnqualifiedLibraryDockerHub
--- PASS: TestCtestKeyringHitWithUnqualifiedLibraryDockerHub (0.00s)
=== RUN   TestCtestKeyringHitWithQualifiedDockerHub
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:283]: Start TestCtestKeyringHitWithQualifiedDockerHub
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:322]: End TestCtestKeyringHitWithQualifiedDockerHub
--- PASS: TestCtestKeyringHitWithQualifiedDockerHub (0.00s)
=== RUN   TestCtestIsDefaultRegistryMatch
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:326]: Start TestCtestIsDefaultRegistryMatch
    ctest_keyring_test.go:343: Expected 'registry.k8s.io:8080' to be false, got true
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:347]: End TestCtestIsDefaultRegistryMatch
--- FAIL: TestCtestIsDefaultRegistryMatch (0.00s)
=== RUN   TestCtestProvidersDockerKeyring
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:351]: Start TestCtestProvidersDockerKeyring
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:376]: End TestCtestProvidersDockerKeyring
--- PASS: TestCtestProvidersDockerKeyring (0.00s)
=== RUN   TestCtestDockerKeyringLookup
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:380]: Start TestCtestDockerKeyringLookup
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:435]: End TestCtestDockerKeyringLookup
--- PASS: TestCtestDockerKeyringLookup (0.00s)
=== RUN   TestCtestIssue3797
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:442]: Start TestCtestIssue3797
[DEBUG-CTEST 2026-02-16 15:42:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:481]: End TestCtestIssue3797
--- PASS: TestCtestIssue3797 (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/credentialprovider	2.041s
=== RUN   TestCtestReadCredentialProviderConfig
=== RUN   TestCtestReadCredentialProviderConfig/empty_directory_with_no_JSON_or_YAML_files
=== RUN   TestCtestReadCredentialProviderConfig/directory_with_unsupported_file_extensions
=== RUN   TestCtestReadCredentialProviderConfig/config_with_1_plugin_and_1_image_matcher
=== RUN   TestCtestReadCredentialProviderConfig/directory_with_a_valid_config_and_an_empty_file
    ctest_config_test.go:173: error decoding config "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/config-dir3739196251/config-001.yaml": Object 'Kind' is missing in ''
--- FAIL: TestCtestReadCredentialProviderConfig (0.00s)
    --- PASS: TestCtestReadCredentialProviderConfig/empty_directory_with_no_JSON_or_YAML_files (0.00s)
    --- PASS: TestCtestReadCredentialProviderConfig/directory_with_unsupported_file_extensions (0.00s)
    --- PASS: TestCtestReadCredentialProviderConfig/config_with_1_plugin_and_1_image_matcher (0.00s)
    --- FAIL: TestCtestReadCredentialProviderConfig/directory_with_a_valid_config_and_an_empty_file (0.00s)
=== RUN   TestCtestValidateCredentialProviderConfig
=== RUN   TestCtestValidateCredentialProviderConfig/nil_config
--- FAIL: TestCtestValidateCredentialProviderConfig (0.00s)
    --- FAIL: TestCtestValidateCredentialProviderConfig/nil_config (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x28 pc=0x1051f935c]

goroutine 148 [running]:
testing.tRunner.func1.2({0x105d6bbc0, 0x107ab9630})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x105d6bbc0?, 0x107ab9630?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/credentialprovider/plugin.validateCredentialProviderConfig(0x0, 0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/plugin/config.go:149 +0x9c
k8s.io/kubernetes/pkg/credentialprovider/plugin.TestCtestValidateCredentialProviderConfig.func1(0x14000485dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/plugin/ctest_config_test.go:390 +0x2c
testing.tRunner(0x14000485dc0, 0x140004a2d40)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 147
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/credentialprovider/plugin	2.731s
=== RUN   TestCtestMakeDockerKeyring
[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/secrets/ctest_secrets_test.go:281]: Start TestCtestMakeDockerKeyring
Running 0 th test case: with .dockerconfigjson and auth field
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[secrets]
[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[secrets], int=1)[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "type" in requested fixtures
2026/02/16 15:42:20 [DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/16 15:42:20 Mode: 1
2026/02/16 15:42:20 Base JSON size: 213 bytes
2026/02/16 15:42:20 Number of external values: 0
2026/02/16 15:42:20 [DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/16 15:42:20 [DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"data":{".dockerconfigjson":"eyJhdXRocyI6eyJ0ZXN0LnJlZ2lzdHJ5LmlvIjp7ImF1dGgiOiJkWE5sY2pwd1lYTnpkMjl5WkE9PSJ9fX0="},"metadata":{"name":"s1","namespace":"ns1","uid":"uid1"},"type":"kubernetes.io/dockerconfigjson"})[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/secrets/ctest_secrets_test.go:302]: No config objects generated for with .dockerconfigjson and auth field
--- FAIL: TestCtestMakeDockerKeyring (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x18 pc=0x105925fb0]

goroutine 11 [running]:
testing.tRunner.func1.2({0x1063cf2a0, 0x107f102b0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1063cf2a0?, 0x107f102b0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/credentialprovider/secrets.TestCtestMakeDockerKeyring(0x14000456e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/secrets/ctest_secrets_test.go:311 +0x1080
testing.tRunner(0x14000456e00, 0x1067e08e8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/credentialprovider/secrets	1.826s
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/features	3.655s	coverage: 0.1% of statements in ./... [no tests to run]
=== RUN   TestCtestExtractFieldPathAsString
--- FAIL: TestCtestExtractFieldPathAsString (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x102eb5560]

goroutine 21 [running]:
testing.tRunner.func1.2({0x1030e1640, 0x10348d670})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1030e1640?, 0x10348d670?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/api/core/v1.(*Pod).GetName(0x102f07769?)
	<autogenerated>:1
k8s.io/kubernetes/pkg/fieldpath.ExtractFieldPathAsString({0x103183c00?, 0x0?}, {0x102f07769, 0xd})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/fieldpath/fieldpath.go:85 +0x760
k8s.io/kubernetes/pkg/fieldpath.TestCtestExtractFieldPathAsString(0x1400009b500)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/fieldpath/ctest_fieldpath_test.go:167 +0x644
testing.tRunner(0x1400009b500, 0x10318cea8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/fieldpath	2.047s
?   	k8s.io/kubernetes/pkg/generated	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/generated/openapi	3.597s	coverage: 0.2% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/generated/openapi/cmd/models-schema		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/admission		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/admission/exclusion		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/authenticator		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/authorizer		coverage: 0.0% of statements
=== RUN   TestCtestIsValidAuthorizationMode

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubeapiserver/authorizer/modes/ctest_modes_test.go:34]: Number of test cases: 14
Running 0 th test case: authzMode="" expected=false
Running 1 th test case: authzMode="rBAC" expected=false
Running 2 th test case: authzMode="falsy value" expected=false
Running 3 th test case: authzMode="RBAC" expected=true
Running 4 th test case: authzMode="ABAC" expected=true
Running 5 th test case: authzMode="Webhook" expected=true
Running 6 th test case: authzMode="AlwaysAllow" expected=true
Running 7 th test case: authzMode="AlwaysDeny" expected=true
Running 8 th test case: authzMode=" " expected=false
Running 9 th test case: authzMode="rbac" expected=false
Running 10 th test case: authzMode="RBAC " expected=false
Running 11 th test case: authzMode="RBAC\n" expected=false
Running 12 th test case: authzMode="\x00" expected=false
Running 13 th test case: authzMode="\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" expected=false

==================== CTEST END ======================
--- PASS: TestCtestIsValidAuthorizationMode (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubeapiserver/authorizer/modes	4.476s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestComputeEnabledAdmission
=== RUN   TestCtestComputeEnabledAdmission/matches
=== RUN   TestCtestComputeEnabledAdmission/choose_one
=== RUN   TestCtestComputeEnabledAdmission/enabled_not_in_all
    ctest_admission_test.go:69: expected enabled [], got [three]
=== RUN   TestCtestComputeEnabledAdmission/empty_all_slice
    ctest_admission_test.go:69: expected enabled [], got [one]
=== RUN   TestCtestComputeEnabledAdmission/nil_enabled_slice
    ctest_admission_test.go:69: expected enabled [], got []
=== RUN   TestCtestComputeEnabledAdmission/duplicate_entries_in_all
--- FAIL: TestCtestComputeEnabledAdmission (0.00s)
    --- PASS: TestCtestComputeEnabledAdmission/matches (0.00s)
    --- PASS: TestCtestComputeEnabledAdmission/choose_one (0.00s)
    --- FAIL: TestCtestComputeEnabledAdmission/enabled_not_in_all (0.00s)
    --- FAIL: TestCtestComputeEnabledAdmission/empty_all_slice (0.00s)
    --- FAIL: TestCtestComputeEnabledAdmission/nil_enabled_slice (0.00s)
    --- PASS: TestCtestComputeEnabledAdmission/duplicate_entries_in_all (0.00s)
=== RUN   TestCtestAdmissionOptionsAddFlags
--- PASS: TestCtestAdmissionOptionsAddFlags (0.00s)
=== RUN   TestCtestAuthzValidate
=== RUN   TestCtestAuthzValidate/Unknown_modes_should_return_errors
=== RUN   TestCtestAuthzValidate/At_least_one_authorizationMode_is_necessary
=== RUN   TestCtestAuthzValidate/ModeAlwaysAllow_specified_more_than_once
=== RUN   TestCtestAuthzValidate/ModeAlwaysAllow_and_ModeAlwaysDeny_should_return_without_authorizationPolicyFile
=== RUN   TestCtestAuthzValidate/ModeABAC_requires_a_policy_file
=== RUN   TestCtestAuthzValidate/Authorization_Policy_file_cannot_be_used_without_ModeABAC
=== RUN   TestCtestAuthzValidate/ModeABAC_should_not_error_if_a_valid_policy_path_is_provided
=== RUN   TestCtestAuthzValidate/ModeWebhook_requires_a_config_file
=== RUN   TestCtestAuthzValidate/Cannot_provide_webhook_config_file_without_ModeWebhook
=== RUN   TestCtestAuthzValidate/ModeWebhook_should_not_error_if_a_valid_config_file_is_provided
=== RUN   TestCtestAuthzValidate/ModeWebhook_should_error_if_an_invalid_number_of_webhook_retry_attempts_is_provided
=== RUN   TestCtestAuthzValidate/Empty_string_mode_should_be_rejected
=== RUN   TestCtestAuthzValidate/Duplicate_mode_with_extra_whitespace
    ctest_authorization_test.go:186: expected to find error: has mode specified more than once, but got: authorization-mode " AlwaysAllow " is not a valid mode
=== RUN   TestCtestAuthzValidate/Nil_webhook_retry_backoff_with_ModeWebhook_should_error
    ctest_authorization_test.go:182: should return an error
=== RUN   TestCtestAuthzValidate/Negative_steps_in_webhook_retry_backoff
=== RUN   TestCtestAuthzValidate/Zero_duration_but_positive_steps_in_webhook_retry_backoff
    ctest_authorization_test.go:182: should return an error
=== RUN   TestCtestAuthzValidate/PolicyFile_path_does_not_exist_(simulated)
    ctest_authorization_test.go:182: should return an error
=== RUN   TestCtestAuthzValidate/WebhookConfigFile_empty_string_with_ModeWebhook
--- FAIL: TestCtestAuthzValidate (0.00s)
    --- PASS: TestCtestAuthzValidate/Unknown_modes_should_return_errors (0.00s)
    --- PASS: TestCtestAuthzValidate/At_least_one_authorizationMode_is_necessary (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeAlwaysAllow_specified_more_than_once (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeAlwaysAllow_and_ModeAlwaysDeny_should_return_without_authorizationPolicyFile (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeABAC_requires_a_policy_file (0.00s)
    --- PASS: TestCtestAuthzValidate/Authorization_Policy_file_cannot_be_used_without_ModeABAC (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeABAC_should_not_error_if_a_valid_policy_path_is_provided (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeWebhook_requires_a_config_file (0.00s)
    --- PASS: TestCtestAuthzValidate/Cannot_provide_webhook_config_file_without_ModeWebhook (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeWebhook_should_not_error_if_a_valid_config_file_is_provided (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeWebhook_should_error_if_an_invalid_number_of_webhook_retry_attempts_is_provided (0.00s)
    --- PASS: TestCtestAuthzValidate/Empty_string_mode_should_be_rejected (0.00s)
    --- FAIL: TestCtestAuthzValidate/Duplicate_mode_with_extra_whitespace (0.00s)
    --- FAIL: TestCtestAuthzValidate/Nil_webhook_retry_backoff_with_ModeWebhook_should_error (0.00s)
    --- PASS: TestCtestAuthzValidate/Negative_steps_in_webhook_retry_backoff (0.00s)
    --- FAIL: TestCtestAuthzValidate/Zero_duration_but_positive_steps_in_webhook_retry_backoff (0.00s)
    --- FAIL: TestCtestAuthzValidate/PolicyFile_path_does_not_exist_(simulated) (0.00s)
    --- PASS: TestCtestAuthzValidate/WebhookConfigFile_empty_string_with_ModeWebhook (0.00s)
FAIL
coverage: 0.9% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubeapiserver/options	2.875s
?   	k8s.io/kubernetes/pkg/kubectl	[no test files]
=== RUN   TestCtestConvertObject

==================== CTEST START ====================
=== RUN   TestCtestConvertObject/apps_deployment_to_extensions_deployment_apiVersion:_extensions/v1beta1_(field_#0)
=== RUN   TestCtestConvertObject/extensions_deployment_to_apps_deployment_apiVersion:_apps/v1beta2_(field_#0)
=== RUN   TestCtestConvertObject/v1_HPA_to_v2beta1_HPA_apiVersion:_autoscaling/v2beta1_(field_#0)
=== RUN   TestCtestConvertObject/v1_HPA_to_v2beta1_HPA_name:_cpu_(field_#1)
=== RUN   TestCtestConvertObject/v1_HPA_to_v2beta1_HPA_targetAverageUtilization:_50_(field_#2)
=== RUN   TestCtestConvertObject/v2beta1_HPA_to_v1_HPA_apiVersion:_autoscaling/v1_(field_#0)
=== RUN   TestCtestConvertObject/v2beta1_HPA_to_v1_HPA_targetCPUUtilizationPercentage:_50_(field_#1)
=== RUN   TestCtestConvertObject/v1beta1_Ingress_to_extensions_Ingress_apiVersion:_extensions/v1beta1_(field_#0)
=== RUN   TestCtestConvertObject/converting_multiple_including_service_to_neworking.k8s.io/v1_apiVersion:_networking.k8s.io/v1_(field_#0)
=== RUN   TestCtestConvertObject/empty_output_version_(should_fallback_to_original)__(field_#0)
=== RUN   TestCtestConvertObject/non‑existent_file_path__(field_#0)
error: the path "nonexistent/path.yaml" does not exist
FAIL	k8s.io/kubernetes/pkg/kubectl/cmd/convert	0.927s
=== RUN   TestCtestNewActiveDeadlineHandler

==================== CTEST START ====================
    ctest_active_deadline_test.go:60: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/ctest_active_deadline_test.go:60
        	Error:      	An error is expected but got nil.
        	Test:       	TestCtestNewActiveDeadlineHandler

==================== CTEST END ======================
--- FAIL: TestCtestNewActiveDeadlineHandler (0.00s)
=== RUN   TestCtestActiveDeadlineHandler

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:42:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/ctest_active_deadline_test.go:76]: get default configs: {test_fixture.json [default pod spec] activeDeadlineSeconds [pods deployments statefulsets daemonsets replicasets] {[] [] [] []  <nil> 0x140008cb010  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:42:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-16 15:42:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:42:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:42:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:42:39 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/kubelet	1.882s
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/allocation	4.063s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtest_stateCheckpoint_storeState
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0Ki
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0.0
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0.0Ki
--- PASS: TestCtest_stateCheckpoint_storeState (1.49s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Ei (0.04s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1n (0.06s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1M (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03k (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03M (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Ki (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Ti (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Gi (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Ti (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Pi (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100G (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100T (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Ei (0.08s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024n (0.03s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024P (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Ki (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Pi (0.06s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Ei (0.02s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700n (0.02s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000M (0.06s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000G (0.09s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0.0 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0.0Ki (0.01s)
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/allocation/state	3.293s	coverage: 0.4% of statements in ./...
=== RUN   TestCtestComponentConfigSetup

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:42:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/config/ctest_register_test.go:28]: get default configs: {test_fixture.json [component config package verification] AllowedTags [] {kubelet kubelet.config.k8s.io kubelet.config.k8s.io/__internal 0x10196d500 map[] map[v1.NodeConfigSource:true v1.TracingConfiguration:true v1.TypeMeta:true v1.Taint:true v1.LoggingConfiguration:true v1.Duration:true v1.Time:true] map[]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:42:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:312]: failed to marshal HardcodedConfig to JSON
    ctest_register_test.go:33: failed to generate config objects: failed to marshal HardcodedConfig to JSON: json: unsupported type: func(*runtime.Scheme) error
--- FAIL: TestCtestComponentConfigSetup (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/apis/config	5.446s
	k8s.io/kubernetes/pkg/kubelet/apis/config/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/apis/config/scheme	2.986s	coverage: 0.7% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/apis/config/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/apis/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/apis/config/v1beta1	2.147s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestValidateReservedMemoryConfiguration
--- FAIL: TestCtestValidateReservedMemoryConfiguration (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x560 pc=0x104df09c4]

goroutine 82 [running]:
testing.tRunner.func1.2({0x105ef1560, 0x107aaccc0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x105ef1560?, 0x107aaccc0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/apis/config/validation.validateReservedMemoryConfiguration(0x14000327208?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/config/validation/validation_reserved_memory.go:29 +0x94
k8s.io/kubernetes/pkg/kubelet/apis/config/validation.TestCtestValidateReservedMemoryConfiguration(0x140004b9c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/config/validation/ctest_validation_reserved_memory_test.go:172 +0x114c
testing.tRunner(0x140004b9c00, 0x10631eda8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/apis/config/validation	0.683s
	k8s.io/kubernetes/pkg/kubelet/apis/grpc		coverage: 0.0% of statements
=== RUN   TestCtestListPodResourcesV1alpha1
[DEBUG-CTEST 2026-02-16 15:42:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/ctest_server_v1alpha1_test.go:20]: Start TestCtestListPodResourcesV1alpha1
Running test case #0: no pods
=== RUN   TestCtestListPodResourcesV1alpha1/no_pods
Running test case #1: pod without devices
=== RUN   TestCtestListPodResourcesV1alpha1/pod_without_devices
Running test case #2: pod with devices
=== RUN   TestCtestListPodResourcesV1alpha1/pod_with_devices
Running test case #3: pod with nil containers slice
=== RUN   TestCtestListPodResourcesV1alpha1/pod_with_nil_containers_slice
Running test case #4: pod with empty container name
=== RUN   TestCtestListPodResourcesV1alpha1/pod_with_empty_container_name
    mock.go:351: 
        
        mock: Unexpected Method Call
        -----------------------------
        
        GetDevices(string,string)
        		0: "pod-uid"
        		1: ""
        
        The closest call I have is: 
        
        GetDevices(string,string)
        		0: "pod-uid"
        		1: "container-name"
        
        
        Diff: 0: PASS:  (string=pod-uid) == (string=pod-uid)
        	1: FAIL:  (string=) != (string=container-name)
        at: [/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/testing/devices_provider.go:89 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/server_v1alpha1.go:74 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/ctest_server_v1alpha1_test.go:248]
Running test case #5: devices with empty resource name
=== RUN   TestCtestListPodResourcesV1alpha1/devices_with_empty_resource_name
Running test case #6: nil pods slice with devices
=== RUN   TestCtestListPodResourcesV1alpha1/nil_pods_slice_with_devices
[DEBUG-CTEST 2026-02-16 15:42:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/ctest_server_v1alpha1_test.go:257]: End TestCtestListPodResourcesV1alpha1
--- FAIL: TestCtestListPodResourcesV1alpha1 (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/no_pods (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/pod_without_devices (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/pod_with_devices (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/pod_with_nil_containers_slice (0.00s)
    --- FAIL: TestCtestListPodResourcesV1alpha1/pod_with_empty_container_name (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/devices_with_empty_resource_name (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/nil_pods_slice_with_devices (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/apis/podresources	5.635s
	k8s.io/kubernetes/pkg/kubelet/apis/podresources/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/cadvisor		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/cadvisor/testing		coverage: 0.0% of statements
=== RUN   TestCtestAddressesToHostnamesAndIPs
=== RUN   TestCtestAddressesToHostnamesAndIPs/empty
=== RUN   TestCtestAddressesToHostnamesAndIPs/ignore_empty_values
=== RUN   TestCtestAddressesToHostnamesAndIPs/ignore_invalid_IPs
=== RUN   TestCtestAddressesToHostnamesAndIPs/dedupe_values
=== RUN   TestCtestAddressesToHostnamesAndIPs/order_values
=== RUN   TestCtestAddressesToHostnamesAndIPs/handle_IP_and_DNS_hostnames
=== RUN   TestCtestAddressesToHostnamesAndIPs/unknown_address_type
=== RUN   TestCtestAddressesToHostnamesAndIPs/whitespace_address
    ctest_kubelet_test.go:127: addressesToHostnamesAndIPs() gotDNSNames = [   ], want []
=== RUN   TestCtestAddressesToHostnamesAndIPs/duplicate_many_entries
--- FAIL: TestCtestAddressesToHostnamesAndIPs (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/empty (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/ignore_empty_values (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/ignore_invalid_IPs (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/dedupe_values (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/order_values (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/handle_IP_and_DNS_hostnames (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/unknown_address_type (0.00s)
    --- FAIL: TestCtestAddressesToHostnamesAndIPs/whitespace_address (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/duplicate_many_entries (0.00s)
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_enabled
W0216 15:42:52.352575   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_disabled
W0216 15:42:52.352903   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_enabled
W0216 15:42:52.352969   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_disabled
W0216 15:42:52.353045   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_enabled
W0216 15:42:52.353100   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_disabled
W0216 15:42:52.353169   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_enabled
W0216 15:42:52.353233   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_disabled
W0216 15:42:52.353291   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/unknown_address_type
W0216 15:42:52.353361   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/empty_address_with_valid_type
W0216 15:42:52.353421   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/malformed_IP_address
W0216 15:42:52.353472   91605 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
--- PASS: TestCtestNewCertificateManagerConfigGetTemplate (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/unknown_address_type (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/empty_address_with_valid_type (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/malformed_IP_address (0.00s)
FAIL
coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/certificate	4.720s
=== RUN   TestCtestLoadClientConfig
=== RUN   TestCtestLoadClientConfig/non‑existent_kubeconfig_path
=== RUN   TestCtestLoadClientConfig/bootstrap_path_is_non‑yaml_file
E0216 15:42:51.926030   91603 bootstrap.go:256] "Unhandled Error" err="unable to load TLS certificates from existing bootstrap client config read from /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestLoadClientConfig2996741088/001/valid-kubeconfig1119982437: data does not contain any valid RSA or ECDSA certificates" logger="UnhandledError"
=== RUN   TestCtestLoadClientConfig/certDir_does_not_exist
    ctest_bootstrap_test.go:99: expected error but got nil
--- FAIL: TestCtestLoadClientConfig (0.01s)
    --- PASS: TestCtestLoadClientConfig/non‑existent_kubeconfig_path (0.00s)
    --- PASS: TestCtestLoadClientConfig/bootstrap_path_is_non‑yaml_file (0.01s)
    --- FAIL: TestCtestLoadClientConfig/certDir_does_not_exist (0.00s)
=== RUN   TestCtestLoadRESTClientConfig
=== RUN   TestCtestLoadRESTClientConfig/valid_config
=== RUN   TestCtestLoadRESTClientConfig/malformed_config
=== RUN   TestCtestLoadRESTClientConfig/non‑existent_file
--- PASS: TestCtestLoadRESTClientConfig (0.00s)
    --- PASS: TestCtestLoadRESTClientConfig/valid_config (0.00s)
    --- PASS: TestCtestLoadRESTClientConfig/malformed_config (0.00s)
    --- PASS: TestCtestLoadRESTClientConfig/non‑existent_file (0.00s)
=== RUN   TestCtestRequestNodeCertificate
    ctest_bootstrap_test.go:201: expected error when node name is empty
--- FAIL: TestCtestRequestNodeCertificate (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/certificate/bootstrap	4.374s
=== RUN   TestCtestCheckpointManager
    ctest_checkpoint_manager_test.go:80: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/checkpointmanager/ctest_checkpoint_manager_test.go:80
        	Error:      	Received unexpected error:
        	            	checkpoint is corrupted
        	Test:       	TestCtestCheckpointManager
    ctest_checkpoint_manager_test.go:83: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/checkpointmanager/ctest_checkpoint_manager_test.go:83
        	Error:      	Not equal: 
        	            	expected: []*checkpointmanager.PortMapping{}
        	            	actual  : []*checkpointmanager.PortMapping(nil)
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1,3 +1,2 @@
        	            	-([]*checkpointmanager.PortMapping) {
        	            	-}
        	            	+([]*checkpointmanager.PortMapping) <nil>
        	            	 
        	Test:       	TestCtestCheckpointManager
    ctest_checkpoint_manager_test.go:114: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/checkpointmanager/ctest_checkpoint_manager_test.go:114
        	Error:      	Error message not equal:
        	            	expected: "checkpoint is corrupted"
        	            	actual  : "invalid character '\\u0084' looking for beginning of value"
        	Test:       	TestCtestCheckpointManager
--- FAIL: TestCtestCheckpointManager (0.00s)
FAIL
coverage: 0.0% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/checkpointmanager	3.829s
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/checksum		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/errors		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/testing/example_checkpoint_formats/v1		coverage: 0.0% of statements
=== RUN   TestCtestMakeTransportInvalid
=== RUN   TestCtestMakeTransportInvalid/invalid_cert_files
=== RUN   TestCtestMakeTransportInvalid/empty_cert_fields
    ctest_kubelet_client_test.go:47: expected error but got none
    ctest_kubelet_client_test.go:50: expected nil transport on error
--- FAIL: TestCtestMakeTransportInvalid (0.00s)
    --- PASS: TestCtestMakeTransportInvalid/invalid_cert_files (0.00s)
    --- FAIL: TestCtestMakeTransportInvalid/empty_cert_fields (0.00s)
=== RUN   TestCtestMakeTransportValid
=== RUN   TestCtestMakeTransportValid/valid_config_with_explicit_port
=== RUN   TestCtestMakeTransportValid/valid_config_with_zero_port_(should_default)
--- PASS: TestCtestMakeTransportValid (0.00s)
    --- PASS: TestCtestMakeTransportValid/valid_config_with_explicit_port (0.00s)
    --- PASS: TestCtestMakeTransportValid/valid_config_with_zero_port_(should_default) (0.00s)
=== RUN   TestCtestMakeTransportWithLookUp
=== RUN   TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer
=== PAUSE TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer
=== RUN   TestCtestMakeTransportWithLookUp/lookup_returns_error
=== PAUSE TestCtestMakeTransportWithLookUp/lookup_returns_error
=== RUN   TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
=== PAUSE TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
=== RUN   TestCtestMakeTransportWithLookUp/no_lookup_closure_provided
=== PAUSE TestCtestMakeTransportWithLookUp/no_lookup_closure_provided
=== CONT  TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer
=== CONT  TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
=== CONT  TestCtestMakeTransportWithLookUp/no_lookup_closure_provided
=== CONT  TestCtestMakeTransportWithLookUp/lookup_returns_error
=== NAME  TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
    ctest_kubelet_client_test.go:161: expected error but got none
--- FAIL: TestCtestMakeTransportWithLookUp (0.00s)
    --- PASS: TestCtestMakeTransportWithLookUp/lookup_returns_error (0.00s)
    --- FAIL: TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid) (0.00s)
    --- PASS: TestCtestMakeTransportWithLookUp/no_lookup_closure_provided (0.01s)
    --- PASS: TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer (0.01s)
=== RUN   TestCtestMakeInsecureTransport
=== RUN   TestCtestMakeInsecureTransport/valid_insecure_transport
=== RUN   TestCtestMakeInsecureTransport/zero_port_(should_default)
--- PASS: TestCtestMakeInsecureTransport (0.00s)
    --- PASS: TestCtestMakeInsecureTransport/valid_insecure_transport (0.00s)
    --- PASS: TestCtestMakeInsecureTransport/zero_port_(should_default) (0.00s)
=== RUN   TestCtestNewNodeConnectionInfoGetter
=== RUN   TestCtestNewNodeConnectionInfoGetter/valid_config
=== PAUSE TestCtestNewNodeConnectionInfoGetter/valid_config
=== RUN   TestCtestNewNodeConnectionInfoGetter/invalid_cert_file
=== PAUSE TestCtestNewNodeConnectionInfoGetter/invalid_cert_file
=== RUN   TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types
=== PAUSE TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types
=== RUN   TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
=== PAUSE TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
=== RUN   TestCtestNewNodeConnectionInfoGetter/zero_port
=== PAUSE TestCtestNewNodeConnectionInfoGetter/zero_port
=== CONT  TestCtestNewNodeConnectionInfoGetter/valid_config
=== CONT  TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
=== CONT  TestCtestNewNodeConnectionInfoGetter/zero_port
=== CONT  TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types
=== CONT  TestCtestNewNodeConnectionInfoGetter/invalid_cert_file
=== NAME  TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
    ctest_kubelet_client_test.go:300: expected error but got none
--- FAIL: TestCtestNewNodeConnectionInfoGetter (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/valid_config (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/invalid_cert_file (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/zero_port (0.00s)
    --- FAIL: TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter (0.00s)
=== RUN   TestCtestGetConnectionInfo
=== RUN   TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred
=== PAUSE TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred
=== RUN   TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal)
=== PAUSE TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal)
=== RUN   TestCtestGetConnectionInfo/node_not_found
=== PAUSE TestCtestGetConnectionInfo/node_not_found
=== RUN   TestCtestGetConnectionInfo/node_with_no_addresses
=== PAUSE TestCtestGetConnectionInfo/node_with_no_addresses
=== RUN   TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address)
=== PAUSE TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address)
=== CONT  TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred
=== CONT  TestCtestGetConnectionInfo/node_with_no_addresses
=== CONT  TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address)
=== CONT  TestCtestGetConnectionInfo/node_not_found
=== CONT  TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal)
--- PASS: TestCtestGetConnectionInfo (0.00s)
    --- PASS: TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred (0.00s)
    --- PASS: TestCtestGetConnectionInfo/node_with_no_addresses (0.00s)
    --- PASS: TestCtestGetConnectionInfo/node_not_found (0.00s)
    --- PASS: TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address) (0.00s)
    --- PASS: TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal) (0.00s)
FAIL
coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/client	3.106s
=== RUN   TestCtestLazyInformerManager_ensureManagerSet
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_unavailable
    clustertrustbundle_manager.go:422: I0216 15:42:58.353050] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1alpha1
    clustertrustbundle_manager.go:428: I0216 15:42:58.353654] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1alpha1"
    clustertrustbundle_manager.go:435: I0216 15:42:58.353751] Waiting for ClusterTrustBundle informer to sync
I0216 15:42:58.353871   91679 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0216 15:42:58.353889   91679 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0216 15:42:58.353898   91679 envvar.go:172] "Feature gate default state" feature="InOrderInformers" enabled=true
I0216 15:42:58.353902   91679 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0216 15:42:58.353907   91679 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0216 15:42:58.354132   91679 reflector.go:358] "Starting reflector" type="*v1alpha1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.354168   91679 reflector.go:404] "Listing and watching" type="*v1alpha1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.354623   91679 reflector.go:436] "Caches populated" type="*v1alpha1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0216 15:42:58.454990] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_an_unhandled_version
I0216 15:42:58.455071   91679 watch.go:218] "Stopping fake watcher"
I0216 15:42:58.455091   91679 reflector.go:364] "Stopping reflector" type="*v1alpha1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:422: I0216 15:42:58.455102] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1beta1
    clustertrustbundle_manager.go:428: I0216 15:42:58.455175] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0216 15:42:58.455190] Waiting for ClusterTrustBundle informer to sync
I0216 15:42:58.455225   91679 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.455232   91679 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.455294   91679 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0216 15:42:58.555295] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1_-_currently_unhandled
I0216 15:42:58.555405   91679 watch.go:218] "Stopping fake watcher"
    clustertrustbundle_manager.go:422: I0216 15:42:58.555396] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
I0216 15:42:58.555429   91679 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery_but_beta_API_shard_discovered
    clustertrustbundle_manager.go:428: I0216 15:42:58.555509] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0216 15:42:58.555523] Waiting for ClusterTrustBundle informer to sync
I0216 15:42:58.555594   91679 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.555599   91679 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.555634   91679 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0216 15:42:58.656262] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_alpha_and_beta_-_prefer_beta
    clustertrustbundle_manager.go:428: I0216 15:42:58.656358] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0216 15:42:58.656373] Waiting for ClusterTrustBundle informer to sync
I0216 15:42:58.656371   91679 watch.go:218] "Stopping fake watcher"
I0216 15:42:58.656409   91679 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.656421   91679 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.656433   91679 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.656462   91679 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0216 15:42:58.758892] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_multiple_handled_and_unhandled_versions_-_prefer_the_most-GA_handled_version
I0216 15:42:58.758971   91679 watch.go:218] "Stopping fake watcher"
I0216 15:42:58.759040   91679 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:428: I0216 15:42:58.759148] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
I0216 15:42:58.759232   91679 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:435: I0216 15:42:58.759229] Waiting for ClusterTrustBundle informer to sync
I0216 15:42:58.759241   91679 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.759336   91679 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0216 15:42:58.859529] ClusterTrustBundle informer synced
I0216 15:42:58.859606   91679 watch.go:218] "Stopping fake watcher"
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_empty_GV_list
I0216 15:42:58.859656   91679 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:422: I0216 15:42:58.859753] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_duplicate_GVs
    clustertrustbundle_manager.go:428: I0216 15:42:58.859929] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0216 15:42:58.859952] Waiting for ClusterTrustBundle informer to sync
I0216 15:42:58.860017   91679 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.860031   91679 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0216 15:42:58.860131   91679 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0216 15:42:58.961052] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/non‑nil_error_with_nil_GV_slice
I0216 15:42:58.961159   91679 watch.go:218] "Stopping fake watcher"
I0216 15:42:58.961179   91679 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
--- PASS: TestCtestLazyInformerManager_ensureManagerSet (0.67s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_unavailable (0.06s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1alpha1 (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_an_unhandled_version (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1beta1 (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1_-_currently_unhandled (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery_but_beta_API_shard_discovered (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_alpha_and_beta_-_prefer_beta (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_multiple_handled_and_unhandled_versions_-_prefer_the_most-GA_handled_version (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_empty_GV_list (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_duplicate_GVs (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/non‑nil_error_with_nil_GV_slice (0.00s)
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/clustertrustbundle	3.649s	coverage: 0.7% of statements in ./...
=== RUN   TestCtestParseQOSReserved
[DEBUG-CTEST 2026-02-16 15:42:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/ctest_cgroup_manager_test.go:13]: Starting TestCtestParseQOSReserved
[DEBUG-CTEST 2026-02-16 15:42:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/ctest_cgroup_manager_test.go:77]: Number of test cases: 13
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
Running 5 th test case.
Running 6 th test case.
Running 7 th test case.
    ctest_cgroup_manager_test.go:82: Unexpected success, input: map[], expected: <nil>, actual: &map[], err: <nil>
    ctest_cgroup_manager_test.go:90: Unexpected result, input: map[], expected: <nil>, actual: &map[], err: <nil>
Running 8 th test case.
Running 9 th test case.
Running 10 th test case.
Running 11 th test case.
Running 12 th test case.
--- FAIL: TestCtestParseQOSReserved (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/cm	3.330s
=== RUN   TestCtestAdmissionErrors

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:15]: Starting TestCtestAdmissionErrors
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:59]: Number of test cases: 6
Running 0 th test case.
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: <nil>
Running 1 th test case.
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: Not an AdmissionError error
Running 2 th test case.
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: Is an AdmissionError error
Running 3 th test case.
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: 
Running 4 th test case.
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: 
Running 5 th test case.
[DEBUG-CTEST 2026-02-16 15:42:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: 

==================== CTEST END ======================
--- PASS: TestCtestAdmissionErrors (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/admission	3.988s	coverage: 0.8% of statements in ./...
=== RUN   TestCtestContainerMapCloneUnshared
Running edge case: clone empty ContainerMap
--- PASS: TestCtestContainerMapCloneUnshared (0.00s)
=== RUN   TestCtestContainerMap
Running test case for podUID="fakePodUID", containerNames=[fakeContainerName-1 fakeContainerName-2], containerIDs=[fakeContainerID-1 fakeContainerName-2]
Running test case for podUID="", containerNames=[], containerIDs=[]
Running test case for podUID="dupPodUID", containerNames=[dupContainer-1 dupContainer-2], containerIDs=[dupID dupID]
Running test case for podUID="nilSlicePodUID", containerNames=[], containerIDs=[]
--- PASS: TestCtestContainerMap (0.00s)
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/containermap	1.487s	coverage: 0.0% of statements in ./...
=== RUN   TestCtestPolicyDefaultsAvailable
=== RUN   TestCtestPolicyDefaultsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyDefaultsAvailable/full-pcpus-only
=== RUN   TestCtestPolicyDefaultsAvailable/#00
=== RUN   TestCtestPolicyDefaultsAvailable/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
--- PASS: TestCtestPolicyDefaultsAvailable (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/full-pcpus-only (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (0.00s)
=== RUN   TestCtestPolicyOptionsAvailable
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01
=== RUN   TestCtestPolicyOptionsAvailable/align-by-socket
=== RUN   TestCtestPolicyOptionsAvailable/align-by-socket#01
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa#01
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores#01
=== RUN   TestCtestPolicyOptionsAvailable/strict-cpu-reservation
=== RUN   TestCtestPolicyOptionsAvailable/strict-cpu-reservation#01
=== RUN   TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache
=== RUN   TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache#01
=== RUN   TestCtestPolicyOptionsAvailable/#00
=== RUN   TestCtestPolicyOptionsAvailable/bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
--- PASS: TestCtestPolicyOptionsAvailable (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/align-by-socket (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/align-by-socket#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/strict-cpu-reservation (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/strict-cpu-reservation#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb (0.00s)
=== RUN   TestCtestPolicyOptionsAlwaysAvailableOnceGA
=== RUN   TestCtestPolicyOptionsAlwaysAvailableOnceGA/full-pcpus-only
--- PASS: TestCtestPolicyOptionsAlwaysAvailableOnceGA (0.00s)
    --- PASS: TestCtestPolicyOptionsAlwaysAvailableOnceGA/full-pcpus-only (0.00s)
=== RUN   TestCtestValidateStaticPolicyOptions
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_not_enabled
I0216 15:43:03.052690   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="single-numa-node"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_topology_manager_single_numa_node
I0216 15:43:03.052827   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="single-numa-node"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_num_sockets_>_num_numa
I0216 15:43:03.052856   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_None_policy
I0216 15:43:03.052887   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_best-effort_policy
I0216 15:43:03.052981   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_restricted_policy
I0216 15:43:03.053057   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Empty_policy_map
I0216 15:43:03.053145   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="single-numa-node"
=== RUN   TestCtestValidateStaticPolicyOptions/Invalid_boolean_value
I0216 15:43:03.053213   91720 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
    ctest_policy_options_test.go:240: testCase "Invalid boolean value" failed, got false expected true
--- FAIL: TestCtestValidateStaticPolicyOptions (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_not_enabled (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_topology_manager_single_numa_node (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_num_sockets_>_num_numa (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_None_policy (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_best-effort_policy (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_restricted_policy (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Empty_policy_map (0.00s)
    --- FAIL: TestCtestValidateStaticPolicyOptions/Invalid_boolean_value (0.00s)
=== RUN   TestCtestPolicyOptionsCompatibility
=== RUN   TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_set_to_true_only
=== RUN   TestCtestPolicyOptionsCompatibility/DistributeCPUsAcrossCores_set_to_true_only
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_StrictCPUReservation_set_to_true
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_FullPCPUsOnly_set_to_true
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_AlignBySocket_set_to_true
=== RUN   TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_and_DistributeCPUsAcrossCores_options_can_not_coexist
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossCores_options_can_not_coexist
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossNUMA_options_can_not_coexist
=== RUN   TestCtestPolicyOptionsCompatibility/Empty_policy_options_map
=== RUN   TestCtestPolicyOptionsCompatibility/Unknown_option_with_feature_gate_enabled
=== RUN   TestCtestPolicyOptionsCompatibility/Valid_option_but_feature_gate_disabled
    ctest_policy_options_test.go:354: testCase "Valid option but feature gate disabled" failed, got false expected true
--- FAIL: TestCtestPolicyOptionsCompatibility (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_set_to_true_only (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/DistributeCPUsAcrossCores_set_to_true_only (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_StrictCPUReservation_set_to_true (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_FullPCPUsOnly_set_to_true (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_AlignBySocket_set_to_true (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_and_DistributeCPUsAcrossCores_options_can_not_coexist (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossCores_options_can_not_coexist (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossNUMA_options_can_not_coexist (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/Empty_policy_options_map (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/Unknown_option_with_feature_gate_enabled (0.00s)
    --- FAIL: TestCtestPolicyOptionsCompatibility/Valid_option_but_feature_gate_disabled (0.00s)
FAIL
coverage: 0.7% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager	2.824s
=== RUN   TestCtestClone
=== Start TestCtestClone ===
Running test case #0: non‑empty map
Running test case #1: empty map
Running test case #2: nil map
    ctest_state_test.go:46: test case "nil map" failed: expected map[], got map[]
--- FAIL: TestCtestClone (0.00s)
FAIL
coverage: 0.0% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/state	1.197s
	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/state/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/topology	1.362s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/devicemanager	2.955s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/cm/devicemanager/checkpoint		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/cm/devicemanager/plugin/v1beta1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/dra	2.216s	coverage: 0.6% of statements in ./... [no tests to run]
=== RUN   TestCtestGRPCConnIsReused
--- PASS: TestCtestGRPCConnIsReused (0.00s)
=== RUN   TestCtestGRPCConnUsableAfterIdle
    ctest_dra_plugin_test.go:89: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/plugin/ctest_dra_plugin_test.go:89
        	Error:      	Received unexpected error:
        	            	listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCConnUsableAfterIdle4204941631/001/dra.sock: bind: invalid argument
        	Test:       	TestCtestGRPCConnUsableAfterIdle
--- FAIL: TestCtestGRPCConnUsableAfterIdle (0.00s)
=== RUN   TestCtestGetDRAPlugin
==== Starting TestGetDRAPlugin ====
=== RUN   TestCtestGetDRAPlugin/driver-name_is_empty
=== RUN   TestCtestGetDRAPlugin/driver_name_not_found_in_the_list
=== RUN   TestCtestGetDRAPlugin/plugin_exists
=== RUN   TestCtestGetDRAPlugin/driver_name_whitespace_only
=== RUN   TestCtestGetDRAPlugin/driver_name_with_special_characters
==== Finished TestGetDRAPlugin ====
--- PASS: TestCtestGetDRAPlugin (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver-name_is_empty (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver_name_not_found_in_the_list (0.00s)
    --- PASS: TestCtestGetDRAPlugin/plugin_exists (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver_name_whitespace_only (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver_name_with_special_characters (0.00s)
=== RUN   TestCtestGRPCMethods
==== Starting TestGRPCMethods ====
=== RUN   TestCtestGRPCMethods/v1beta1
=== RUN   TestCtestGRPCMethods/v1
=== RUN   TestCtestGRPCMethods/mismatch
=== RUN   TestCtestGRPCMethods/internal-error
    ctest_dra_plugin_test.go:230: listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCMethodsinternal-error757650552/001/dra.sock: bind: invalid argument
=== RUN   TestCtestGRPCMethods/empty-service
    ctest_dra_plugin_test.go:230: listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCMethodsempty-service2056782434/001/dra.sock: bind: invalid argument
=== RUN   TestCtestGRPCMethods/empty-chosen-service
    ctest_dra_plugin_test.go:230: listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCMethodsempty-chosen-service1637914644/001/dra.sock: bind: invalid argument
==== Finished TestGRPCMethods ====
--- FAIL: TestCtestGRPCMethods (0.01s)
    --- PASS: TestCtestGRPCMethods/v1beta1 (0.01s)
    --- PASS: TestCtestGRPCMethods/v1 (0.00s)
    --- PASS: TestCtestGRPCMethods/mismatch (0.00s)
    --- FAIL: TestCtestGRPCMethods/internal-error (0.00s)
    --- FAIL: TestCtestGRPCMethods/empty-service (0.00s)
    --- FAIL: TestCtestGRPCMethods/empty-chosen-service (0.00s)
=== RUN   TestCtestPlugin_WatchResources
--- PASS: TestCtestPlugin_WatchResources (0.00s)
=== RUN   TestCtestRegistrationHandler
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/plugin/ctest_registration_test.go:45]: get default configs: {test_fixture.json [registration handler slice spec] nodeName [resourceslices] { { 0 0} 0x14000715680 nil <nil> [] <nil> []}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceslices]
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceslices], int=1)[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceslices]
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:43:14 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceslices
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/dra/plugin	0.806s
=== RUN   TestCtestCheckpointGetOrCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:199]: Number of test cases: 10
Running 0 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: new-checkpoint
=== RUN   TestCtestCheckpointGetOrCreate/new-checkpoint
Running 1 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: single-claim-info-state
=== RUN   TestCtestCheckpointGetOrCreate/single-claim-info-state
Running 2 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: claim-info-state-with-multiple-devices
=== RUN   TestCtestCheckpointGetOrCreate/claim-info-state-with-multiple-devices
Running 3 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: two-claim-info-states
=== RUN   TestCtestCheckpointGetOrCreate/two-claim-info-states
Running 4 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: incorrect-checksum
=== RUN   TestCtestCheckpointGetOrCreate/incorrect-checksum
Running 5 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: invalid-JSON
=== RUN   TestCtestCheckpointGetOrCreate/invalid-JSON
Running 6 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: upgraded-structure
=== RUN   TestCtestCheckpointGetOrCreate/upgraded-structure
Running 7 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: upgraded-structure-and-version
=== RUN   TestCtestCheckpointGetOrCreate/upgraded-structure-and-version
Running 8 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: empty-data-field
=== RUN   TestCtestCheckpointGetOrCreate/empty-data-field
    ctest_checkpointer_test.go:223: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:223
        	Error:      	An error is expected but got nil.
        	Test:       	TestCtestCheckpointGetOrCreate/empty-data-field
Running 9 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: missing-claimuid
=== RUN   TestCtestCheckpointGetOrCreate/missing-claimuid
    ctest_checkpointer_test.go:223: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:223
        	Error:      	Error "failed to get checkpoint dramanager_checkpoint_test: checkpoint is corrupted" does not contain "missing required field"
        	Test:       	TestCtestCheckpointGetOrCreate/missing-claimuid

==================== CTEST END ======================
--- FAIL: TestCtestCheckpointGetOrCreate (0.28s)
    --- PASS: TestCtestCheckpointGetOrCreate/new-checkpoint (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/single-claim-info-state (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/claim-info-state-with-multiple-devices (0.10s)
    --- PASS: TestCtestCheckpointGetOrCreate/two-claim-info-states (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/incorrect-checksum (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/invalid-JSON (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/upgraded-structure (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/upgraded-structure-and-version (0.03s)
    --- FAIL: TestCtestCheckpointGetOrCreate/empty-data-field (0.01s)
    --- FAIL: TestCtestCheckpointGetOrCreate/missing-claimuid (0.02s)
=== RUN   TestCtestCheckpointStateStore

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:346]: Number of test cases: 4
Running 0 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: single-claim-info-state
=== RUN   TestCtestCheckpointStateStore/single-claim-info-state
Running 1 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: claim-info-state-with-multiple-devices
=== RUN   TestCtestCheckpointStateStore/claim-info-state-with-multiple-devices
Running 2 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: two-claim-info-states
=== RUN   TestCtestCheckpointStateStore/two-claim-info-states
Running 3 th test case.
[DEBUG-CTEST 2026-02-16 15:43:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: empty-claim-info-list
=== RUN   TestCtestCheckpointStateStore/empty-claim-info-list
    ctest_checkpointer_test.go:372: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:372
        	Error:      	Not equal: 
        	            	expected: "{\"Data\":\"{\\\"kind\\\":\\\"DRACheckpoint\\\",\\\"apiVersion\\\":\\\"checkpoint.dra.kubelet.k8s.io/v1\\\",\\\"ClaimInfoStateList\\\":[]}\", \"Checksum\":0}"
        	            	actual  : "{\"Data\":\"{\\\"kind\\\":\\\"DRACheckpoint\\\",\\\"apiVersion\\\":\\\"checkpoint.dra.kubelet.k8s.io/v1\\\",\\\"ClaimInfoStateList\\\":[]}\",\"Checksum\":2361995144}"
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1 +1 @@
        	            	-{"Data":"{\"kind\":\"DRACheckpoint\",\"apiVersion\":\"checkpoint.dra.kubelet.k8s.io/v1\",\"ClaimInfoStateList\":[]}", "Checksum":0}
        	            	+{"Data":"{\"kind\":\"DRACheckpoint\",\"apiVersion\":\"checkpoint.dra.kubelet.k8s.io/v1\",\"ClaimInfoStateList\":[]}","Checksum":2361995144}
        	Test:       	TestCtestCheckpointStateStore/empty-claim-info-list

==================== CTEST END ======================
--- FAIL: TestCtestCheckpointStateStore (0.05s)
    --- PASS: TestCtestCheckpointStateStore/single-claim-info-state (0.01s)
    --- PASS: TestCtestCheckpointStateStore/claim-info-state-with-multiple-devices (0.02s)
    --- PASS: TestCtestCheckpointStateStore/two-claim-info-states (0.02s)
    --- FAIL: TestCtestCheckpointStateStore/empty-claim-info-list (0.01s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/dra/state	3.379s
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/memorymanager	3.849s	coverage: 0.7% of statements in ./... [no tests to run]
=== RUN   TestCtestCheckpointStateRestore
=== RUN   TestCtestCheckpointStateRestore/Restore_non-existing_checkpoint
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:14.944797] Memory Manager state checkpoint: Initializing new in-memory state store
=== RUN   TestCtestCheckpointStateRestore/Restore_valid_checkpoint
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:14.970526] Memory Manager state checkpoint: Initializing new in-memory state store
    state_mem.go:77: I0216 15:43:14.970678] Memory Manager state checkpoint: Updated machine memory state
    state_mem.go:99: I0216 15:43:14.970759] Memory Manager state checkpoint: Updated Memory assignments assignments=<state.ContainerMemoryAssignments | len:1>: 
                pod:
                  container1:
                  - numaAffinity:
                    - 0
                    size: 512
                    type: memory
    state_checkpoint.go:85: I0216 15:43:14.970769] Memory Manager state checkpoint: State checkpoint: restored state from checkpoint
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_checksum
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:14.984336] Memory Manager state checkpoint: Initializing new in-memory state store
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_JSON
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:15.007613] Memory Manager state checkpoint: Initializing new in-memory state store
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:15.020284] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test2787121505/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: checksum missing"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:15.030818] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: json: cannot unmarshal number -100 into Go struct field Block.entries.size of type uint64, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test2787121505/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: invalid memory size"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:15.041996] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test2787121505/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: invalid memory size"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0216 15:43:15.053254] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test2787121505/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: memory size exceeds node capacity"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size
--- FAIL: TestCtestCheckpointStateRestore (0.11s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_non-existing_checkpoint (0.02s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_valid_checkpoint (0.01s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_checksum (0.01s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_JSON (0.02s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field (0.01s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size (0.01s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment (0.01s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size (0.01s)
FAIL
coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/memorymanager/state	3.451s
?   	k8s.io/kubernetes/pkg/kubelet/cm/resourceupdates	[no test files]
	k8s.io/kubernetes/pkg/kubelet/cm/testing		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/kubelet/container
# [k8s.io/kubernetes/pkg/kubelet/container]
pkg/kubelet/container/ctest_ref_test.go:73:6: (*testing.common).Errorf format %s has arg i of wrong type int
pkg/kubelet/container/ctest_ref_test.go:78:5: (*testing.common).Errorf format %s has arg i of wrong type int
pkg/kubelet/container/ctest_ref_test.go:82:5: (*testing.common).Errorf format %s has arg i of wrong type int
=== RUN   TestCtestNewFakeManager

==================== CTEST START ====================
  I0216 15:43:25.618825   91864 fake_topology_manager.go:33] "NewFakeManager"

==================== CTEST END ======================
--- PASS: TestCtestNewFakeManager (0.00s)
=== RUN   TestCtestFakeGetAffinity

==================== CTEST START ====================
  I0216 15:43:25.618974   91864 fake_topology_manager.go:55] "GetAffinity" podUID="0aafa4c4-38e8-11e9-bcb1-a4bf01040474" containerName="nginx"
  I0216 15:43:25.619020   91864 fake_topology_manager.go:55] "GetAffinity" podUID="11111111-2222-3333-4444-555555555555" containerName=""
  I0216 15:43:25.619025   91864 fake_topology_manager.go:55] "GetAffinity" podUID="" containerName="busybox"
  I0216 15:43:25.619028   91864 fake_topology_manager.go:55] "GetAffinity" podUID="uid-\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" containerName="container-\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"

==================== CTEST END ======================
--- PASS: TestCtestFakeGetAffinity (0.00s)
=== RUN   TestCtestFakeRemoveContainer

==================== CTEST START ====================
  I0216 15:43:25.619059   91864 fake_topology_manager.go:76] "RemoveContainer" containerID="nginx"
  I0216 15:43:25.619063   91864 fake_topology_manager.go:76] "RemoveContainer" containerID="Busy_Box"
  I0216 15:43:25.619066   91864 fake_topology_manager.go:76] "RemoveContainer" containerID=""
  I0216 15:43:25.619069   91864 fake_topology_manager.go:76] "RemoveContainer" containerID="!@#$%^&*()_+"
  I0216 15:43:25.619072   91864 fake_topology_manager.go:76] "RemoveContainer" containerID="\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"

==================== CTEST END ======================
--- PASS: TestCtestFakeRemoveContainer (0.00s)
=== RUN   TestCtestFakeAdmit

==================== CTEST START ====================
  I0216 15:43:25.619125   91864 fake_topology_manager.go:81] "Topology Admit Handler"
  I0216 15:43:25.619130   91864 fake_topology_manager.go:81] "Topology Admit Handler"
  I0216 15:43:25.619133   91864 fake_topology_manager.go:81] "Topology Admit Handler"
  I0216 15:43:25.619137   91864 fake_topology_manager.go:81] "Topology Admit Handler"
  I0216 15:43:25.619140   91864 fake_topology_manager.go:81] "Topology Admit Handler"

==================== CTEST END ======================
--- PASS: TestCtestFakeAdmit (0.00s)
=== RUN   TestCtestPolicyBestEffortMerge

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_best_effort_test.go:12]: Running TestPolicyBestEffortMerge
  I0216 15:43:25.619224   91864 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0216 15:43:25.619230   91864 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0216 15:43:25.619246   91864 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0216 15:43:25.619251   91864 policy.go:87] "Hint Provider has no preference for NUMA affinity with resource" resource="resource"
  I0216 15:43:25.619257   91864 policy.go:93] "Hint Provider has no possible NUMA affinities for resource" resource="resource"

==================== CTEST END ======================
--- PASS: TestCtestPolicyBestEffortMerge (0.00s)
=== RUN   TestCtestPolicyBestEffortMergeClosestNUMA

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_best_effort_test.go:29]: Running TestPolicyBestEffortMergeClosestNUMA
  I0216 15:43:25.619317   91864 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0216 15:43:25.619323   91864 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0216 15:43:25.619340   91864 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0216 15:43:25.619345   91864 policy.go:87] "Hint Provider has no preference for NUMA affinity with resource" resource="resource"
  I0216 15:43:25.619351   91864 policy.go:93] "Hint Provider has no possible NUMA affinities for resource" resource="resource"

==================== CTEST END ======================
--- PASS: TestCtestPolicyBestEffortMergeClosestNUMA (0.00s)
=== RUN   TestCtestPolicyNoneName
--- PASS: TestCtestPolicyNoneName (0.00s)
=== RUN   TestCtestPolicyNoneCanAdmitPodResult
--- PASS: TestCtestPolicyNoneCanAdmitPodResult (0.00s)
=== RUN   TestCtestPolicyNoneMerge
--- PASS: TestCtestPolicyNoneMerge (0.00s)
=== RUN   TestCtestNewTopologyManagerOptions
=== RUN   TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_PreferClosestNUMA_set_to_true
=== RUN   TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_MaxAllowableNUMANodes_set_to_12
  I0216 15:43:25.619527   91864 policy_options.go:95] "WARNING: the value of max-allowable-numa-nodes is more than the default recommended value" max-allowable-numa-nodes=12 defaultMaxAllowableNUMANodes=8
=== RUN   TestCtestNewTopologyManagerOptions/fail_to_set_option_when_TopologyManagerPolicyBetaOptions_feature_gate_is_not_set
=== RUN   TestCtestNewTopologyManagerOptions/return_empty_TopologyManagerOptions_(default_values)
=== RUN   TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_PreferClosestNUMANodes
=== RUN   TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_MaxAllowableNUMANodes
=== RUN   TestCtestNewTopologyManagerOptions/test_beta_options_success
    ctest_policy_options_test.go:228: Unexpected error: unsupported topologymanager option: "fancy-new-option" (true)
=== RUN   TestCtestNewTopologyManagerOptions/test_beta_options_fail
=== RUN   TestCtestNewTopologyManagerOptions/test_alpha_options_success
    ctest_policy_options_test.go:228: Unexpected error: unsupported topologymanager option: "fancy-alpha-option" (true)
=== RUN   TestCtestNewTopologyManagerOptions/test_alpha_options_fail
=== RUN   TestCtestNewTopologyManagerOptions/empty_policyOptions_map_with_no_feature_gate
=== RUN   TestCtestNewTopologyManagerOptions/nil_policyOptions_map_(should_behave_like_empty)
=== RUN   TestCtestNewTopologyManagerOptions/unknown_option_key_should_be_ignored
    ctest_policy_options_test.go:228: Unexpected error: unknown Topology Manager Policy option: "unknownOption"
=== RUN   TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_empty_string
=== RUN   TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_empty_string
=== RUN   TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_negative_value
    ctest_policy_options_test.go:228: Unexpected error: the minimum value of "max-allowable-numa-nodes" should not be less than 8
=== RUN   TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_very_large_integer
  I0216 15:43:25.619830   91864 policy_options.go:95] "WARNING: the value of max-allowable-numa-nodes is more than the default recommended value" max-allowable-numa-nodes=1000000 defaultMaxAllowableNUMANodes=8
=== RUN   TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_case-insensitive_true
=== RUN   TestCtestNewTopologyManagerOptions/beta_option_present_while_beta_feature_disabled_but_alpha_enabled_(should_fail)
    ctest_policy_options_test.go:223: Unexpected error. Got: unsupported topologymanager option: "fancy-new-option" (true), want error containing: topology manager policy beta-level options not enabled,
=== RUN   TestCtestNewTopologyManagerOptions/both_beta_and_alpha_options_present,_both_feature_gates_enabled_(both_succeed)
    ctest_policy_options_test.go:228: Unexpected error: unsupported topologymanager option: "fancy-new-option" (true)
--- FAIL: TestCtestNewTopologyManagerOptions (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_PreferClosestNUMA_set_to_true (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_MaxAllowableNUMANodes_set_to_12 (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/fail_to_set_option_when_TopologyManagerPolicyBetaOptions_feature_gate_is_not_set (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/return_empty_TopologyManagerOptions_(default_values) (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_PreferClosestNUMANodes (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_MaxAllowableNUMANodes (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/test_beta_options_success (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/test_beta_options_fail (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/test_alpha_options_success (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/test_alpha_options_fail (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/empty_policyOptions_map_with_no_feature_gate (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/nil_policyOptions_map_(should_behave_like_empty) (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/unknown_option_key_should_be_ignored (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_empty_string (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_empty_string (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_negative_value (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_very_large_integer (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_case-insensitive_true (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/beta_option_present_while_beta_feature_disabled_but_alpha_enabled_(should_fail) (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/both_beta_and_alpha_options_present,_both_feature_gates_enabled_(both_succeed) (0.00s)
=== RUN   TestCtestPolicyDefaultsAvailable
=== RUN   TestCtestPolicyDefaultsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyDefaultsAvailable/prefer-closest-numa-nodes
=== RUN   TestCtestPolicyDefaultsAvailable/max-allowable-numa-nodes
=== RUN   TestCtestPolicyDefaultsAvailable/#00
=== RUN   TestCtestPolicyDefaultsAvailable/___
=== RUN   TestCtestPolicyDefaultsAvailable/PREFER-CLOSEST-NUMA-NODES
--- PASS: TestCtestPolicyDefaultsAvailable (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/prefer-closest-numa-nodes (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/max-allowable-numa-nodes (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/___ (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/PREFER-CLOSEST-NUMA-NODES (0.00s)
=== RUN   TestCtestPolicyOptionsAvailable
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01
=== RUN   TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes
=== RUN   TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#01
=== RUN   TestCtestPolicyOptionsAvailable/fancy-alpha-option
=== RUN   TestCtestPolicyOptionsAvailable/fancy-alpha-option#01
=== RUN   TestCtestPolicyOptionsAvailable/fancy-new-option
=== RUN   TestCtestPolicyOptionsAvailable/fancy-new-option#01
=== RUN   TestCtestPolicyOptionsAvailable/#00
=== RUN   TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#02
--- PASS: TestCtestPolicyOptionsAvailable (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-alpha-option (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-alpha-option#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-new-option (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-new-option#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#02 (0.00s)
=== RUN   TestCtestPolicySingleNumaNodeCanAdmitPodResult

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:43:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_single_numa_node_test.go:13]: Running TestCtestPolicySingleNumaNodeCanAdmitPodResult
Running 0 th test case: Preferred is set to false in topology hints
Running 1 th test case: Preferred true with empty affinity mask (edge case)
    ctest_policy_single_numa_node_test.go:44: Test case "Preferred true with empty affinity mask (edge case)": expected result false, got true
Running 2 th test case: Preferred true with nil affinity (edge case)
    ctest_policy_single_numa_node_test.go:44: Test case "Preferred true with nil affinity (edge case)": expected result false, got true

==================== CTEST END ======================
--- FAIL: TestCtestPolicySingleNumaNodeCanAdmitPodResult (0.00s)
=== RUN   TestCtestPolicySingleNumaNodeFilterHints

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:43:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_single_numa_node_test.go:52]: Running TestCtestPolicySingleNumaNodeFilterHints
Running 0 th test case: filter empty resources
Running 1 th test case: filter hints with nil socket mask 1/2
Running 2 th test case: filter hints with nil socket mask 2/2
Running 3 th test case: filter hints with empty resource socket mask
Running 4 th test case: filter hints with wide sockemask
Running 5 th test case: filter nil resources (edge case)
Running 6 th test case: filter resources with nil inner slice (edge case)

==================== CTEST END ======================
--- PASS: TestCtestPolicySingleNumaNodeFilterHints (0.00s)
=== RUN   TestCtestContainerCalculateAffinity

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_scope_container_test.go:143]: Number of test cases: 6
Running 0 th test case.
{No hint providers [] []}
  I0216 15:43:25.620765   91864 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 1 th test case.
{HintProvider returns empty non-nil map[string][]TopologyHint [0x140003f8040] [map[]]}
  I0216 15:43:25.620884   91864 scope_container.go:83] "TopologyHints" hints={} pod="" containerName=""
  I0216 15:43:25.620895   91864 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 2 th test case.
{HintProvider returns -nil map[string][]TopologyHint from provider [0x140003f8050] [map[resource:[]]]}
  I0216 15:43:25.620911   91864 scope_container.go:83] "TopologyHints" hints={"resource":null} pod="" containerName=""
  I0216 15:43:25.620923   91864 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 3 th test case.
{Assorted HintProviders [0x140003f8058 0x140003f8068 0x140003f8070] [map[resource-1/A:[{0x140002da1f8 true} {0x140002da200 false}] resource-1/B:[{0x140002da208 true} {0x140002da210 false}]] map[resource-2/A:[{0x140002da218 true} {0x140002da220 false}] resource-2/B:[{0x140002da228 true} {0x140002da230 false}]] map[resource-3:[]]]}
  I0216 15:43:25.620932   91864 scope_container.go:83] "TopologyHints" hints={"resource-1/A":[{"NUMANodeAffinity":1,"Preferred":true},{"NUMANodeAffinity":3,"Preferred":false}],"resource-1/B":[{"NUMANodeAffinity":2,"Preferred":true},{"NUMANodeAffinity":6,"Preferred":false}]} pod="" containerName=""
  I0216 15:43:25.620938   91864 scope_container.go:83] "TopologyHints" hints={"resource-2/A":[{"NUMANodeAffinity":4,"Preferred":true},{"NUMANodeAffinity":24,"Preferred":false}],"resource-2/B":[{"NUMANodeAffinity":4,"Preferred":true},{"NUMANodeAffinity":24,"Preferred":false}]} pod="" containerName=""
  I0216 15:43:25.620943   91864 scope_container.go:83] "TopologyHints" hints={"resource-3":null} pod="" containerName=""
  I0216 15:43:25.620947   91864 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 4 th test case.
{HintProvider returns map with empty resource name [0x140003f8078] [map[:[{0x140002da240 true}]]]}
  I0216 15:43:25.620964   91864 scope_container.go:83] "TopologyHints" hints={"":[{"NUMANodeAffinity":1,"Preferred":true}]} pod="" containerName=""
  I0216 15:43:25.620968   91864 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 5 th test case.
{nil HintProvider in slice [<nil>] [map[]]}
--- FAIL: TestCtestContainerCalculateAffinity (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x28 pc=0x10723c2ac]

goroutine 133 [running]:
testing.tRunner.func1.2({0x107d647c0, 0x10996cf40})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x107d647c0?, 0x10996cf40?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).accumulateProvidersHints(0x1400066d918?, 0x140003c5b08, 0x14000696ea0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/scope_container.go:81 +0x17c
k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).calculateAffinity(0x1400066dcf8, 0x140003c5b08, 0x14000696ea0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/scope_container.go:89 +0x98
k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.TestCtestContainerCalculateAffinity(0x140004a2700)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_scope_container_test.go:157 +0x1114
testing.tRunner(0x140004a2700, 0x1081a2248)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/topologymanager	1.001s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/cm/topologymanager/bitmask	2.099s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/cm/util		coverage: 0.0% of statements
=== RUN   TestCtestNewSourceApiserver_UpdatesAndMultiplePods

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:29]: get default configs: {test_fixture.json [apiserver pods] containers [pods] {[] [] [{ image/placeholder [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:43:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:43:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"image":"image/placeholder","name":"","resources":{}}]})[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:37]: Skipping test execution. No new config objs found.
--- PASS: TestCtestNewSourceApiserver_UpdatesAndMultiplePods (0.00s)
=== RUN   TestCtestNewSourceApiserver_TwoNamespacesSameName

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:170]: get default configs: {test_fixture.json [apiserver two namespaces] containers [pods] {[] [] [{ image/placeholder [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/16 15:43:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:43:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"image":"image/placeholder","name":"","resources":{}}]})[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:178]: Skipping test execution. No new config objs found.
--- PASS: TestCtestNewSourceApiserver_TwoNamespacesSameName (0.00s)
=== RUN   TestCtestDecodeSinglePod

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_common_test.go:36]: get default configs: {test_fixture.json [decode single pod] spec [pods deployments statefulsets daemonsets replicasets] &Pod{ObjectMeta:{test  mynamespace  12345  0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:image,Image:test/image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[],},Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,AppArmorProfile:nil,SupplementalGroupsPolicy:nil,SELinuxChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:1.2.3.4,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:1.2.3.4,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-16 15:43:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/16 15:43:27 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/kubelet/config	0.742s
=== RUN   TestCtestCacheBasedConfigMapManager

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/configmap/ctest_configmap_manager_test.go:32]: get default configs: {test_fixture.json [cache based configmap manager test] containerEnvConfigMaps [pods] {[{[s1] []} {[] [s20]}] [s2]}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "containerEnvConfigMaps" in requested fixtures
2026/02/16 15:43:29 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/16 15:43:29 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={})[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:43:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/configmap/ctest_configmap_manager_test.go:41]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestCacheBasedConfigMapManager (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/configmap	3.418s	coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/container [build failed]
	k8s.io/kubernetes/pkg/kubelet/container/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/envvars	1.127s	coverage: 0.2% of statements in ./... [no tests to run]
?   	k8s.io/kubernetes/pkg/kubelet/events	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/eviction	2.595s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/eviction/api		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/images	5.346s	coverage: 0.8% of statements in ./... [no tests to run]
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/there_are_no_records_about_the_image_being_pulled
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/there_are_records_about_the_image_being_pulled
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_no_records
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_with_records
--- PASS: TestCtestNeverVerifyPreloadedPullPolicy (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/there_are_no_records_about_the_image_being_pulled (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/there_are_records_about_the_image_being_pulled (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_no_records (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_with_records (0.00s)
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_not_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_appears_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_appears_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_in_the_middle
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_non-segment_wildcard_middle
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_path_segment_in_the_middle
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_only_wildcard_segment
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_ends_with_a_'/'
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_empty
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_asterisk
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_tag
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_digest
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_whitespace
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_different_repo_wildcard
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_org_wildcard
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_repo_wildcard
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_no_records
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_records
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_duplicate_entries,_image_not_in_list
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_very_long_pattern_(max_length_255_chars)
    ctest_image_pull_policies_test.go:236: wanted error: false, got: failed to parse as an image name: repository name must not be more than 255 characters
--- FAIL: TestCtestNewNeverVerifyAllowListedPullPolicy (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_not_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_appears_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_appears_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_in_the_middle (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_non-segment_wildcard_middle (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_path_segment_in_the_middle (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_only_wildcard_segment (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_ends_with_a_'/' (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_empty (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_asterisk (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_tag (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_digest (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_whitespace (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_different_repo_wildcard (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_org_wildcard (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_repo_wildcard (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_no_records (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_records (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_duplicate_entries,_image_not_in_list (0.00s)
    --- FAIL: TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_very_long_pattern_(max_length_255_chars) (0.00s)
FAIL
coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/images/pullmanager	4.853s
testing: warning: no tests to run
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/kubeletconfig/configfiles	5.474s	coverage: 0.7% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/kubeletconfig/util/codec		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/kubeletconfig/util/test		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/kubelet/nodeshutdown.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/kubelet/metrics/collectors.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtestConvertToKubeContainerImageSpec
=== TestCtestConvertToKubeContainerImageSpec start ===
Running 7 test cases.
Case #0: input=id:"test"
Case #1: input=id:"test"  spec:{}
Case #2: input=id:"test"  spec:{}
Case #3: input=id:"test"  spec:{annotations:{key:"kubernetes.io/os"  value:"linux"}  annotations:{key:"kubernetes.io/runtimehandler"  value:"handler"}}
Case #4: input=
Case #5: input=id:"test"  spec:{annotations:{key:""  value:"emptykey"}}
Case #6: input=<nil>
--- FAIL: TestCtestConvertToKubeContainerImageSpec (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x68 pc=0x1062a5998]

goroutine 147 [running]:
testing.tRunner.func1.2({0x107080d20, 0x109924780})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x107080d20?, 0x109924780?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/kuberuntime.toKubeContainerImageSpec(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/convert.go:33 +0xa8
k8s.io/kubernetes/pkg/kubelet/kuberuntime.TestCtestConvertToKubeContainerImageSpec(0x14000602c40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/ctest_convert_test.go:129 +0x5e0
testing.tRunner(0x14000602c40, 0x10766d408)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/kuberuntime	1.795s
=== RUN   TestCtestPodSandboxChanged

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:43:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
=== RUN   TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_network_namespace_mismatch
=== RUN   TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_no_IP
=== RUN   TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_with_IP
=== RUN   TestCtestPodSandboxChanged/Pod_with_nil_status
--- FAIL: TestCtestPodSandboxChanged (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_network_namespace_mismatch (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_no_IP (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_with_IP (0.00s)
    --- FAIL: TestCtestPodSandboxChanged/Pod_with_nil_status (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x80 pc=0x104cd8270]

goroutine 48 [running]:
testing.tRunner.func1.2({0x1057b4aa0, 0x107354a80})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1057b4aa0?, 0x107354a80?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/kuberuntime/util.PodSandboxChanged(0x14000445208, 0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/util/util.go:33 +0xc0
k8s.io/kubernetes/pkg/kubelet/kuberuntime/util.TestCtestPodSandboxChanged.func1(0x14000685dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/util/ctest_util_test.go:170 +0x2c
testing.tRunner(0x14000685dc0, 0x14000385d70)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 44
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/kuberuntime/util	0.695s
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/lifecycle	5.169s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/logs	5.344s	coverage: 0.5% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/metrics	2.045s	coverage: 0.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/kubelet/metrics/collectors [build failed]
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/network/dns	3.923s	coverage: 0.5% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/kubelet/nodeshutdown [build failed]
?   	k8s.io/kubernetes/pkg/kubelet/nodeshutdown/systemd	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/nodestatus	2.323s	coverage: 0.6% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/oom		coverage: 0.0% of statements
=== RUN   TestCtestEventedPLEG_getPodIPs
[DEBUG-CTEST 2026-02-16 15:44:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/pleg/ctest_evented_test.go:169]: Starting TestEventedPLEG_getPodIPs with 8 cases
Running test case #0: status ips is not empty
=== RUN   TestCtestEventedPLEG_getPodIPs/status_ips_is_not_empty
Running test case #1: status ips is empty and SandboxStatuses has PodSandboxState_SANDBOX_READY state
=== RUN   TestCtestEventedPLEG_getPodIPs/status_ips_is_empty_and_SandboxStatuses_has_PodSandboxState_SANDBOX_READY_state
Running test case #2: status and cache ips are empty
=== RUN   TestCtestEventedPLEG_getPodIPs/status_and_cache_ips_are_empty
Running test case #3: sandbox state is no PodSandboxState_SANDBOX_READY
=== RUN   TestCtestEventedPLEG_getPodIPs/sandbox_state_is_no_PodSandboxState_SANDBOX_READY
Running test case #4: nil status
=== RUN   TestCtestEventedPLEG_getPodIPs/nil_status
--- FAIL: TestCtestEventedPLEG_getPodIPs (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/status_ips_is_not_empty (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/status_ips_is_empty_and_SandboxStatuses_has_PodSandboxState_SANDBOX_READY_state (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/status_and_cache_ips_are_empty (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/sandbox_state_is_no_PodSandboxState_SANDBOX_READY (0.00s)
    --- FAIL: TestCtestEventedPLEG_getPodIPs/nil_status (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x38 pc=0x103a12530]

goroutine 85 [running]:
testing.tRunner.func1.2({0x1044ee840, 0x1060c7f90})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1044ee840?, 0x1060c7f90?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).getPodIPs(0x140000a3f08?, {0x103a7a41b?, 0x63?}, 0x66b?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/pleg/evented.go:298 +0x90
k8s.io/kubernetes/pkg/kubelet/pleg.TestCtestEventedPLEG_getPodIPs.func1(0x14000205dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/pleg/ctest_evented_test.go:177 +0x38
testing.tRunner(0x14000205dc0, 0x1400027fec0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 48
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/pleg	1.178s
testing: warning: no tests to run
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager	3.121s	coverage: 0.6% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/cache	2.472s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/metrics	2.636s	coverage: 0.1% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/kubelet/prober/results.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtestOperationExecutor_RegisterPlugin_ConcurrentRegisterPlugin
--- PASS: TestCtestOperationExecutor_RegisterPlugin_ConcurrentRegisterPlugin (0.00s)
=== RUN   TestCtestOperationExecutor_RegisterPlugin_SerialRegisterPlugin
--- PASS: TestCtestOperationExecutor_RegisterPlugin_SerialRegisterPlugin (5.00s)
=== RUN   TestCtestOperationExecutor_UnregisterPlugin_ConcurrentUnregisterPlugin
--- PASS: TestCtestOperationExecutor_UnregisterPlugin_ConcurrentUnregisterPlugin (0.00s)
=== RUN   TestCtestOperationExecutor_UnregisterPlugin_SerialUnregisterPlugin
--- PASS: TestCtestOperationExecutor_UnregisterPlugin_SerialUnregisterPlugin (5.00s)
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor	14.171s	coverage: 0.3% of statements in ./...
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/pluginwatcher	4.882s	coverage: 0.5% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/pluginmanager/pluginwatcher/example_plugin_apis/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/pluginmanager/pluginwatcher/example_plugin_apis/v1beta2		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/reconciler	4.211s	coverage: 0.5% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/kubelet/pod.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4181/pod.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2004203946/000012.o -lresolv -O2 -g -O2 -g -framework CoreFoundation -framework CoreFoundation -framework Security
ld: can't write to output file: $WORK/b4181/pod.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/kubelet/pod [build failed]
	k8s.io/kubernetes/pkg/kubelet/pod/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/podcertificate	3.266s	coverage: 0.5% of statements in ./... [no tests to run]
=== RUN   TestCtestGetPodsToPreempt

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/ctest_preemption_test.go:192]: Number of test cases: 16
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
    ctest_preemption_test.go:202: equal pods and resources requirements: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 3 th test case.
Running 4 th test case.
    ctest_preemption_test.go:202: choose between bestEffort and burstable: expected [&Pod{ObjectMeta:{ bestEffort     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:bestEffort-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ bestEffort     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:bestEffort-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 5 th test case.
    ctest_preemption_test.go:202: choose between burstable and guaranteed: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 6 th test case.
    ctest_preemption_test.go:202: choose lower request burstable if it meets requirements: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 7 th test case.
    ctest_preemption_test.go:202: choose higher request burstable if lower does not meet requirements: expected [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 8 th test case.
    ctest_preemption_test.go:202: multiple pods required: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 9 th test case.
    ctest_preemption_test.go:202: evict guaranteed when we have to, and dont evict the extra burstable: expected [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ high-request-guaranteed     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-guaranteed-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ high-request-guaranteed     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-guaranteed-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 10 th test case.
    ctest_preemption_test.go:202: evict cluster critical pod for node critical pod: expected [&Pod{ObjectMeta:{ cluster-critical kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:cluster-critical-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:system-cluster-critical,Priority:*2000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ cluster-critical kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:cluster-critical-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:system-cluster-critical,Priority:*2000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 11 th test case.
Running 12 th test case.
--- FAIL: TestCtestGetPodsToPreempt (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0xb8 pc=0x10114c5fc]

goroutine 76 [running]:
testing.tRunner.func1.2({0x1039a8260, 0x105673050})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1039a8260?, 0x105673050?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/types.GetPodSource(0x14000067198?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:106 +0x8c
k8s.io/kubernetes/pkg/kubelet/types.IsStaticPod(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:155
k8s.io/kubernetes/pkg/kubelet/types.IsCriticalPod(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:161 +0xf8
k8s.io/kubernetes/pkg/kubelet/types.Preemptable(0x0, 0x140007cc008)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:176 +0x98
k8s.io/kubernetes/pkg/kubelet/preemption.sortPodsByQOS(0x0, {0x1400052aee8, 0x1, 0x14000067270?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/preemption.go:248 +0x180
k8s.io/kubernetes/pkg/kubelet/preemption.getPodsToPreempt(0x103e0eb38?, {0x1400052aee8?, 0x102eb910e?, 0x19?}, {0x140005414c0, 0x3, 0x4})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/preemption.go:134 +0xa4
k8s.io/kubernetes/pkg/kubelet/preemption.TestCtestGetPodsToPreempt(0x14000505a40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/ctest_preemption_test.go:196 +0x1820
testing.tRunner(0x14000505a40, 0x103e01778)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/preemption	0.735s
# k8s.io/kubernetes/pkg/kubelet/server/stats.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtestAddRemovePodsWithRestartableInitContainer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:42]: matched config: {test_fixture.json [restartable init container probes] initContainers [pods] {[] [{restartable-init  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] &Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} &Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} &Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} nil    nil false false false}] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initContainers" in requested fixtures
2026/02/16 15:44:22 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/16 15:44:22 Base JSON size: 402 bytes
2026/02/16 15:44:22 Number of external values: 0
2026/02/16 15:44:22 BASE DATA (type: map[string]interface {}):
{
  "containers": null,
  "initContainers": [
    {
      "livenessProbe": {
        "exec": {},
        "failureThreshold": 3,
        "periodSeconds": 1,
        "successThreshold": 1,
        "timeoutSeconds": 1
      },
      "name": "restartable-init",
      "readinessProbe": {
        "exec": {},
        "failureThreshold": 3,
        "periodSeconds": 1,
        "successThreshold": 1,
        "timeoutSeconds": 1
      },
      "resources": {},
      "startupProbe": {
        "exec": {},
        "failureThreshold": 3,
        "periodSeconds": 1,
        "successThreshold": 1,
        "timeoutSeconds": 1
      }
    }
  ]
}
2026/02/16 15:44:22 
=== UNION COMPLETE ===
2026/02/16 15:44:22 Generated 0 result(s)
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"initContainers":[{"livenessProbe":{"exec":{},"failureThreshold":3,"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1},"name":"restartable-init","readinessProbe":{"exec":{},"failureThreshold":3,"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1},"resources":{},"startupProbe":{"exec":{},"failureThreshold":3,"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1}}]})[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:50]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestAddRemovePodsWithRestartableInitContainer (0.00s)
=== RUN   TestCtestUpdatePodStatusWithInitContainers

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:189]: matched config: {test_fixture.json [init container spec without probes] initContainers [pods] {[] [{not_started_container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {started_container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {terminated_container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initContainers" in requested fixtures
2026/02/16 15:44:22 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/16 15:44:22 Base JSON size: 177 bytes
2026/02/16 15:44:22 Number of external values: 0
2026/02/16 15:44:22 BASE DATA (type: map[string]interface {}):
{
  "containers": null,
  "initContainers": [
    {
      "name": "not_started_container",
      "resources": {}
    },
    {
      "name": "started_container",
      "resources": {}
    },
    {
      "name": "terminated_container",
      "resources": {}
    }
  ]
}
2026/02/16 15:44:22 
=== UNION COMPLETE ===
2026/02/16 15:44:22 Generated 0 result(s)
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"initContainers":[{"name":"not_started_container","resources":{}},{"name":"started_container","resources":{}},{"name":"terminated_container","resources":{}}]})[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-16 15:44:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:197]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestUpdatePodStatusWithInitContainers (0.00s)
=== RUN   TestCtestTCPPortExhaustion
    ctest_scale_test.go:33: skipping TCP port exhaustion tests
--- SKIP: TestCtestTCPPortExhaustion (0.00s)
PASS
coverage: 0.8% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/prober	4.178s	coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/prober/results [build failed]
	k8s.io/kubernetes/pkg/kubelet/prober/testing		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/kubelet/server.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4209/server.test~: no space left on device
# k8s.io/kubernetes/pkg/kubelet/qos.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4199/qos.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/kubelet/qos [build failed]
# k8s.io/kubernetes/pkg/kubelet/secret.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4206/secret.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2750192638/000012.o -lresolv -framework CoreFoundation -framework Security -O2 -g -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4206/secret.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

=== RUN   TestCtestLookupRuntimeHandler
=== RUN   TestCtestLookupRuntimeHandler/""->""(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"native"->""(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"sandbox"->"kata-containers"(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"phantom"->""(err:true)
=== RUN   TestCtestLookupRuntimeHandler/"nil"->""(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"->""(err:true)
=== RUN   TestCtestLookupRuntimeHandler/"!@#$%^&*()"->""(err:true)
--- PASS: TestCtestLookupRuntimeHandler (0.10s)
    --- PASS: TestCtestLookupRuntimeHandler/""->""(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"native"->""(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"sandbox"->"kata-containers"(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"phantom"->""(err:true) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"nil"->""(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"->""(err:true) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"!@#$%^&*()"->""(err:true) (0.00s)
PASS
coverage: 0.7% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/runtimeclass	2.176s	coverage: 0.7% of statements in ./...
	k8s.io/kubernetes/pkg/kubelet/runtimeclass/testing		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/secret [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/server [build failed]
	k8s.io/kubernetes/pkg/kubelet/server/metrics		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/server/stats [build failed]
	k8s.io/kubernetes/pkg/kubelet/server/stats/testing		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/kubelet/stats.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
k8s.io/kubernetes/pkg/kubelet/stats/pidlimit: mkdir /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b4219/: no space left on device
FAIL	k8s.io/kubernetes/pkg/kubelet/stats [build failed]
k8s.io/kubernetes/pkg/kubelet/userns: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b4233/vet.cfg: no space left on device
k8s.io/kubernetes/pkg/kubelet/util: mkdir /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b4236/: no space left on device
# k8s.io/kubernetes/pkg/kubelet/token.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4228/token.test~: no space left on device
# k8s.io/kubernetes/pkg/kubelet/sysctl.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: cannot create /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-665358623/000000.o: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-665358623/000000.o: no space left on device
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: cannot create /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-665358623/000010.o: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-665358623/000010.o: no space left on device
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: cannot create /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-665358623/000011.o: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-665358623/000011.o: no space left on device
# k8s.io/kubernetes/pkg/kubelet/types.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: cannot create /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3120926061/000000.o: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3120926061/000000.o: no space left on device
# k8s.io/kubernetes/pkg/kubelet/status.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: cannot create /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-891060390/000012.o: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-891060390/000012.o: no space left on device
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: cannot create /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-891060390/000004.o: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-891060390/000004.o: no space left on device
FAIL	k8s.io/kubernetes/pkg/kubelet/status [build failed]
	k8s.io/kubernetes/pkg/kubelet/status/testing		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/sysctl [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/token [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/types [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/userns [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/util [build failed]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/util/cache	2.490s	coverage: 0.2% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/util/env	2.428s	coverage: 0.0% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/kubelet/util/sliceutils.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4258/sliceutils.test~: no space left on device
# k8s.io/kubernetes/pkg/kubelet/util/manager.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4252/manager.test~: no space left on device
=== RUN   TestCtestPod

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:68]: Number of test cases: 5
Running 0 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_empty_case
Running 1 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_normal_case
Running 2 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: nil_pod_case
Running 3 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_long_name_case
Running 4 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_special_chars_case
    ctest_pod_test.go:74: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:74
        	Error:      	Not equal: 
        	            	expected: "pod!@#$%^&*()_+_ns!@#$ (uid!@#$)"
        	            	actual  : "pod!@#$%^&*()_+_ns!@#$(uid!@#$)"
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1 +1 @@
        	            	-pod!@#$%^&*()_+_ns!@#$ (uid!@#$)
        	            	+pod!@#$%^&*()_+_ns!@#$(uid!@#$)
        	Test:       	TestCtestPod
        	Messages:   	Failed to test: field_special_chars_case

==================== CTEST END ======================
--- FAIL: TestCtestPod (0.00s)
=== RUN   TestCtestPodAndPodDesc

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:120]: Number of test cases: 4
Running 0 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_empty_case
Running 1 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_normal_case
Running 2 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_long_strings_case
Running 3 th test case.
[DEBUG-CTEST 2026-02-16 15:44:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_special_chars_case

==================== CTEST END ======================
--- PASS: TestCtestPodAndPodDesc (0.00s)
FAIL
coverage: 0.8% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/util/format	4.683s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b4249/_cover_.out: no space left on device
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/kubelet/util/ioutils	0.833s
FAIL	k8s.io/kubernetes/pkg/kubelet/util/manager [build failed]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/kubelet/util/queue	1.878s	coverage: 0.0% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/kubelet/util/sliceutils [build failed]
=== RUN   TestCtestFileStore
    ctest_filestore_test.go:44: test case:  {id1 data1 false}
    ctest_filestore_test.go:44: test case:  {id2 data2 false}
    ctest_filestore_test.go:44: test case:  {/id1 data1 true}
    ctest_filestore_test.go:44: test case:  {.id1 data1 true}
    ctest_filestore_test.go:44: test case:  {    data2 true}
    ctest_filestore_test.go:44: test case:  {___ data2 true}
    ctest_filestore_test.go:44: test case:  { emptykey true}
    ctest_filestore_test.go:44: test case:  {aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa longkey false}
    ctest_filestore_test.go:44: test case:  {valid-key-!$&'()*+,;= specialchars false}
    ctest_filestore_test.go:53: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/store/ctest_filestore_test.go:53
        	Error:      	Received unexpected error:
        	            	invalid key: "valid-key-!$&'()*+,;="
        	Test:       	TestCtestFileStore
--- FAIL: TestCtestFileStore (0.03s)
FAIL
coverage: 0.0% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/util/store	1.910s
=== RUN   TestCtestIsSwapEnabled
=== RUN   TestCtestIsSwapEnabled/empty
=== RUN   TestCtestIsSwapEnabled/with_swap_enabled,_one_partition
I0216 15:44:50.726972   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428	0		-2
 >
=== RUN   TestCtestIsSwapEnabled/with_swap_enabled,_2_partitions
I0216 15:44:50.727062   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428	0		-2
	/dev/zram0              partition	8388604		0		100
 >
=== RUN   TestCtestIsSwapEnabled/empty_lines
=== RUN   TestCtestIsSwapEnabled/missing_header_line
=== RUN   TestCtestIsSwapEnabled/malformed_entry,_not_enough_columns
I0216 15:44:50.727086   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428
 >
    ctest_swap_util_test.go:108: expected false, got true
=== RUN   TestCtestIsSwapEnabled/non‑numeric_size_field
I0216 15:44:50.727140   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	NaN	0		-2
 >
    ctest_swap_util_test.go:108: expected false, got true
=== RUN   TestCtestIsSwapEnabled/negative_size_field
I0216 15:44:50.727164   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	-1024	0		-2
 >
    ctest_swap_util_test.go:108: expected false, got true
=== RUN   TestCtestIsSwapEnabled/zero_size_but_valid_entry
I0216 15:44:50.727177   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/zero-swap          partition	0	0		-1
 >
=== RUN   TestCtestIsSwapEnabled/extremely_large_size_value
I0216 15:44:50.727186   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/huge-swap          partition	18446744073709551615	0		-1
 >
=== RUN   TestCtestIsSwapEnabled/duplicate_entries
I0216 15:44:50.727212   92404 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428	0		-2
	/dev/dm-1               partition	33554428	0		-2
 >
=== RUN   TestCtestIsSwapEnabled/header_only,_no_entries
--- FAIL: TestCtestIsSwapEnabled (0.00s)
    --- PASS: TestCtestIsSwapEnabled/empty (0.00s)
    --- PASS: TestCtestIsSwapEnabled/with_swap_enabled,_one_partition (0.00s)
    --- PASS: TestCtestIsSwapEnabled/with_swap_enabled,_2_partitions (0.00s)
    --- PASS: TestCtestIsSwapEnabled/empty_lines (0.00s)
    --- PASS: TestCtestIsSwapEnabled/missing_header_line (0.00s)
    --- FAIL: TestCtestIsSwapEnabled/malformed_entry,_not_enough_columns (0.00s)
    --- FAIL: TestCtestIsSwapEnabled/non‑numeric_size_field (0.00s)
    --- FAIL: TestCtestIsSwapEnabled/negative_size_field (0.00s)
    --- PASS: TestCtestIsSwapEnabled/zero_size_but_valid_entry (0.00s)
    --- PASS: TestCtestIsSwapEnabled/extremely_large_size_value (0.00s)
    --- PASS: TestCtestIsSwapEnabled/duplicate_entries (0.00s)
    --- PASS: TestCtestIsSwapEnabled/header_only,_no_entries (0.00s)
FAIL
coverage: 0.0% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/kubelet/util/swap	1.979s
# k8s.io/kubernetes/pkg/kubelet/volumemanager/populator.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/kubelet/volumemanager/cache.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4270/cache.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-757664742/000012.o -lresolv -O2 -g -O2 -g -framework CoreFoundation -framework CoreFoundation -framework Security
ld: can't write to output file: $WORK/b4270/cache.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

# k8s.io/kubernetes/pkg/kubelet/volumemanager.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4267/volumemanager.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-107057655/000012.o -lresolv -O2 -g -O2 -g -framework CoreFoundation -framework CoreFoundation -framework Security
ld: can't write to output file: $WORK/b4267/volumemanager.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/kubelet/volumemanager [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/volumemanager/cache [build failed]
# k8s.io/kubernetes/pkg/kubelet/volumemanager/metrics.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4273/metrics.test~: no space left on device
# k8s.io/kubernetes/pkg/kubelet/volumemanager/reconciler.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4279/reconciler.test~: no space left on device
# k8s.io/kubernetes/pkg/probe/exec
# [k8s.io/kubernetes/pkg/probe/exec]
pkg/probe/exec/ctest_exec_test.go:103:4: (*testing.common).Errorf format %v reads arg #4, but call has 3 args
pkg/probe/exec/ctest_exec_test.go:106:4: (*testing.common).Errorf format %v reads arg #3, but call has 2 args
pkg/probe/exec/ctest_exec_test.go:109:4: (*testing.common).Errorf format %s reads arg #2, but call has 1 arg
pkg/probe/exec/ctest_exec_test.go:112:4: (*testing.common).Errorf format %q reads arg #4, but call has 3 args
FAIL	k8s.io/kubernetes/pkg/kubelet/volumemanager/metrics [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/volumemanager/populator [build failed]
FAIL	k8s.io/kubernetes/pkg/kubelet/volumemanager/reconciler [build failed]
	k8s.io/kubernetes/pkg/kubelet/watchdog		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/kubelet/winstats	[no test files]
	k8s.io/kubernetes/pkg/kubemark		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/probe/http.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/probe/tcp.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4305/tcp.test~: no space left on device
# k8s.io/kubernetes/pkg/printers.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4286/printers.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/printers [build failed]
testing: warning: no tests to run
PASS
coverage: 1.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/printers/internalversion	3.059s	coverage: 1.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/printers/storage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/probe	2.027s	coverage: 0.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/probe/exec [build failed]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/probe/grpc	1.684s	coverage: 0.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/probe/http [build failed]
FAIL	k8s.io/kubernetes/pkg/probe/tcp [build failed]
# k8s.io/kubernetes/pkg/proxy/apis/config/validation.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4323/validation.test~: no space left on device
# k8s.io/kubernetes/pkg/proxy/apis/config/scheme.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4317/scheme.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2469314323/000012.o -lresolv -framework CoreFoundation -framework Security -O2 -g -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4317/scheme.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

# k8s.io/kubernetes/pkg/proxy.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4308/proxy.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-40710738/000012.o -lresolv -O2 -g -O2 -g -framework CoreFoundation -framework CoreFoundation -framework Security
ld: can't write to output file: $WORK/b4308/proxy.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/proxy [build failed]
?   	k8s.io/kubernetes/pkg/proxy/apis	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/proxy/apis/config	0.923s	coverage: 0.2% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/proxy/apis/config/fuzzer		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/proxy/apis/config/scheme [build failed]
=== RUN   TestCtestDefaultsKubeProxyConfiguration
=== Starting TestCtestDefaultsKubeProxyConfiguration ===
Number of test cases: 3
Running test case #0: empty-config
=== RUN   TestCtestDefaultsKubeProxyConfiguration/empty-config
Running test case #1: metrics and healthz address with no port
=== RUN   TestCtestDefaultsKubeProxyConfiguration/metrics_and_healthz_address_with_no_port
Running test case #2: zero duration fields should default
=== RUN   TestCtestDefaultsKubeProxyConfiguration/zero_duration_fields_should_default
=== Finished TestCtestDefaultsKubeProxyConfiguration ===
--- PASS: TestCtestDefaultsKubeProxyConfiguration (0.00s)
    --- PASS: TestCtestDefaultsKubeProxyConfiguration/empty-config (0.00s)
    --- PASS: TestCtestDefaultsKubeProxyConfiguration/metrics_and_healthz_address_with_no_port (0.00s)
    --- PASS: TestCtestDefaultsKubeProxyConfiguration/zero_duration_fields_should_default (0.00s)
PASS
coverage: 0.6% of statements in ./...
ok  	k8s.io/kubernetes/pkg/proxy/apis/config/v1alpha1	2.586s	coverage: 0.6% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/proxy/apis/config/validation [build failed]
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/proxy/config	2.183s	coverage: 0.4% of statements in ./... [no tests to run]
?   	k8s.io/kubernetes/pkg/proxy/conntrack	[no test files]
# k8s.io/kubernetes/pkg/quota/v1/install.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/proxy/healthcheck	2.796s	coverage: 0.5% of statements in ./... [no tests to run]
?   	k8s.io/kubernetes/pkg/proxy/iptables	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/ipset	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/ipset/testing	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/testing	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/util	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/util/testing	[no test files]
	k8s.io/kubernetes/pkg/proxy/kubemark		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/proxy/metaproxier		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/proxy/metrics		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/proxy/nftables	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/proxy/runner	1.903s	coverage: 0.0% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/proxy/util.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4359/util.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/proxy/util [build failed]
	k8s.io/kubernetes/pkg/proxy/util/nfacct		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/proxy/util/testing		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/proxy/winkernel	[no test files]
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:27]: Running MatchingResources with edge cases
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/invalid-empty-string
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case invalid-empty-string
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/whitespace-string
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case whitespace-string
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/duplicate-supported
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case duplicate-supported
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/very-long-string
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case very-long-string
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/unsupported-resources
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case unsupported-resources
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/supported-resources
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case supported-resources
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/empty-list
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case empty-list
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/nil-list
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case nil-list

==================== CTEST END ======================
--- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/invalid-empty-string (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/whitespace-string (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/duplicate-supported (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/very-long-string (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/unsupported-resources (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/supported-resources (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/empty-list (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/nil-list (0.00s)
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:126]: Running Handles with edge cases
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/create
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case create
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/update
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case update
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/delete
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case delete
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/connect
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case connect
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/create-subresource
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case create-subresource
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/update-subresource
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case update-subresource
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/nil-attributes
[DEBUG-CTEST 2026-02-16 15:45:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case nil-attributes
--- FAIL: TestCtestPersistentVolumeClaimEvaluatorHandles (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/create (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/update (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/delete (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/connect (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/create-subresource (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/update-subresource (0.00s)
    --- FAIL: TestCtestPersistentVolumeClaimEvaluatorHandles/nil-attributes (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x50 pc=0x105651c6c]

goroutine 90 [running]:
testing.tRunner.func1.2({0x106157940, 0x107cf0370})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x106157940?, 0x107cf0370?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/quota/v1/evaluator/core.(*pvcEvaluator).Handles(0x1065842f8?, {0x0, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/persistent_volume_claims.go:97 +0x9c
k8s.io/kubernetes/pkg/quota/v1/evaluator/core.TestCtestPersistentVolumeClaimEvaluatorHandles.func1(0x1400050ba40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:198 +0x12c
testing.tRunner(0x1400050ba40, 0x140007a8180)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 80
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/quota/v1/evaluator/core	0.677s
FAIL	k8s.io/kubernetes/pkg/quota/v1/install [build failed]
?   	k8s.io/kubernetes/pkg/registry	[no test files]
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy [build failed]
# k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy/storage [build failed]
# k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtestAuthorization
=== RUN   TestCtestAuthorization/superuser
=== RUN   TestCtestAuthorization/superuser/create
=== RUN   TestCtestAuthorization/superuser/update
=== RUN   TestCtestAuthorization/authorized
=== RUN   TestCtestAuthorization/authorized/create
=== RUN   TestCtestAuthorization/authorized/update
=== RUN   TestCtestAuthorization/denied
=== RUN   TestCtestAuthorization/denied/create
=== RUN   TestCtestAuthorization/denied/update
=== RUN   TestCtestAuthorization/deny_but_relevant_fields_not_updated
=== RUN   TestCtestAuthorization/deny_but_relevant_fields_not_updated/create
=== RUN   TestCtestAuthorization/deny_but_relevant_fields_not_updated/update
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef/create
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef/update
=== RUN   TestCtestAuthorization/unable_to_resolve_param
=== RUN   TestCtestAuthorization/unable_to_resolve_param/create
=== RUN   TestCtestAuthorization/unable_to_resolve_param/update
=== RUN   TestCtestAuthorization/unable_to_get_policy
=== RUN   TestCtestAuthorization/unable_to_get_policy/create
=== RUN   TestCtestAuthorization/unable_to_get_policy/update
=== RUN   TestCtestAuthorization/nil_userInfo
=== RUN   TestCtestAuthorization/nil_userInfo/create
=== RUN   TestCtestAuthorization/nil_userInfo/update
=== RUN   TestCtestAuthorization/auth_returns_error
=== RUN   TestCtestAuthorization/auth_returns_error/create
=== RUN   TestCtestAuthorization/auth_returns_error/update
=== RUN   TestCtestAuthorization/policyGetter_returns_nil_without_error
=== RUN   TestCtestAuthorization/policyGetter_returns_nil_without_error/create
--- FAIL: TestCtestAuthorization (0.00s)
    --- PASS: TestCtestAuthorization/superuser (0.00s)
        --- PASS: TestCtestAuthorization/superuser/create (0.00s)
        --- PASS: TestCtestAuthorization/superuser/update (0.00s)
    --- PASS: TestCtestAuthorization/authorized (0.00s)
        --- PASS: TestCtestAuthorization/authorized/create (0.00s)
        --- PASS: TestCtestAuthorization/authorized/update (0.00s)
    --- PASS: TestCtestAuthorization/denied (0.00s)
        --- PASS: TestCtestAuthorization/denied/create (0.00s)
        --- PASS: TestCtestAuthorization/denied/update (0.00s)
    --- PASS: TestCtestAuthorization/deny_but_relevant_fields_not_updated (0.00s)
        --- PASS: TestCtestAuthorization/deny_but_relevant_fields_not_updated/create (0.00s)
        --- PASS: TestCtestAuthorization/deny_but_relevant_fields_not_updated/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_parse_paramRef (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_parse_paramRef/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_parse_paramRef/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_resolve_param (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_resolve_param/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_resolve_param/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_get_policy (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_get_policy/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_get_policy/update (0.00s)
    --- PASS: TestCtestAuthorization/nil_userInfo (0.00s)
        --- PASS: TestCtestAuthorization/nil_userInfo/create (0.00s)
        --- PASS: TestCtestAuthorization/nil_userInfo/update (0.00s)
    --- PASS: TestCtestAuthorization/auth_returns_error (0.00s)
        --- PASS: TestCtestAuthorization/auth_returns_error/create (0.00s)
        --- PASS: TestCtestAuthorization/auth_returns_error/update (0.00s)
    --- FAIL: TestCtestAuthorization/policyGetter_returns_nil_without_error (0.00s)
        --- FAIL: TestCtestAuthorization/policyGetter_returns_nil_without_error/create (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x108 pc=0x10795d828]

goroutine 108 [running]:
testing.tRunner.func1.2({0x10843af80, 0x109fd0d10})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10843af80?, 0x109fd0d10?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.(*mutatingAdmissionPolicyBindingStrategy).authorize(0x1400044ee10, {0x10888b828, 0x14000728030}, 0x14000719180)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/authz.go:82 +0x3d8
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.(*mutatingAdmissionPolicyBindingStrategy).authorizeCreate(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/authz.go:38
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.(*mutatingAdmissionPolicyBindingStrategy).Validate(0x1400044ee10, {0x10888b828, 0x14000728030}, {0x10886a738?, 0x14000719180})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/strategy.go:88 +0x240
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.TestCtestAuthorization.func29.1(0x14000602c40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/ctest_authz_test.go:251 +0x190
testing.tRunner(0x14000602c40, 0x140004ed800)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 107
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding	0.740s
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/storage [build failed]
	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingwebhookconfiguration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingwebhookconfiguration/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/admissionregistration/resolver		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/admissionregistration/rest		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy [build failed]
# k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/admissionregistration/validatingwebhookconfiguration.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/admissionregistration/validatingwebhookconfiguration/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/apiserverinternal/storageversion/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4395/storage.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy/storage [build failed]
# k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4398/validatingadmissionpolicybinding.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1371166088/000012.o -lresolv -O2 -g -framework CoreFoundation -framework Security -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4398/validatingadmissionpolicybinding.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingwebhookconfiguration [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingwebhookconfiguration/storage [build failed]
	k8s.io/kubernetes/pkg/registry/apiserverinternal/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/apiserverinternal/storageversion	1.275s	coverage: 0.4% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/apiserverinternal/storageversion/storage [build failed]
# k8s.io/kubernetes/pkg/registry/apps/controllerrevision/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/apps/deployment/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/apps/daemonset/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/apps/replicaset.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/apps/daemonset.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4423/daemonset.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1098913882/000012.o -lresolv -O2 -g -framework CoreFoundation -framework Security -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4423/daemonset.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

# k8s.io/kubernetes/pkg/registry/apps/controllerrevision.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4417/controllerrevision.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-859472452/000012.o -lresolv -O2 -g -framework CoreFoundation -framework Security -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4417/controllerrevision.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/registry/apps/controllerrevision [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/apps/controllerrevision/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/apps/daemonset [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/apps/daemonset/storage [build failed]
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/apps/deployment	1.900s	coverage: 0.4% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/apps/deployment/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/apps/replicaset [build failed]
# k8s.io/kubernetes/pkg/registry/apps/statefulset/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4445/storage.test~: no space left on device
testing: warning: no tests to run
PASS
coverage: 1.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b4438/_cover_.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/apps/replicaset/storage	2.335s
	k8s.io/kubernetes/pkg/registry/apps/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
ok  	k8s.io/kubernetes/pkg/registry/apps/statefulset	1.815s	coverage: 0.4% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/apps/statefulset/storage [build failed]
	k8s.io/kubernetes/pkg/registry/authentication/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authentication/selfsubjectreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authentication/tokenreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/localsubjectaccessreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/selfsubjectaccessreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/selfsubjectrulesreview		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/batch/cronjob/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/autoscaling/horizontalpodautoscaler/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/autoscaling/horizontalpodautoscaler.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/batch/job.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/authorization/subjectaccessreview	2.276s	coverage: 0.3% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/authorization/util	2.458s	coverage: 0.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/autoscaling/horizontalpodautoscaler [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/autoscaling/horizontalpodautoscaler/storage [build failed]
	k8s.io/kubernetes/pkg/registry/autoscaling/rest		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/batch/job/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/certificates/certificates/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/batch/cronjob.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4468/cronjob.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/batch/cronjob [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/batch/cronjob/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/batch/job [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/batch/job/storage [build failed]
	k8s.io/kubernetes/pkg/registry/batch/rest		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/certificates/clustertrustbundle/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/certificates/podcertificaterequest.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/certificates/podcertificaterequest/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/endpoint/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/configmap/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/componentstatus.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/configmap.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/endpoint.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/event/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtestDeclarativeValidateForDeclarative
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
--- FAIL: TestCtestDeclarativeValidateForDeclarative (0.03s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
=== RUN   TestCtestValidateUpdateForDeclarative
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
--- FAIL: TestCtestValidateUpdateForDeclarative (0.04s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status (0.01s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
FAIL
coverage: 1.4% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/registry/certificates/certificates	4.725s
FAIL	k8s.io/kubernetes/pkg/registry/certificates/certificates/storage [build failed]
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/certificates/clustertrustbundle	1.437s	coverage: 0.4% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/certificates/clustertrustbundle/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/certificates/podcertificaterequest [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/certificates/podcertificaterequest/storage [build failed]
	k8s.io/kubernetes/pkg/registry/certificates/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/lease		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/lease/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/leasecandidate		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/leasecandidate/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/rest		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/componentstatus [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/configmap [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/configmap/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/endpoint [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/endpoint/storage [build failed]
# k8s.io/kubernetes/pkg/registry/core/namespace/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/node.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/namespace.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/limitrange/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/node/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/persistentvolume/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/persistentvolumeclaim/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/pod/rest.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/persistentvolumeclaim.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/pod.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 1.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/core/event	3.364s	coverage: 1.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/core/event/storage [build failed]
	k8s.io/kubernetes/pkg/registry/core/limitrange		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/limitrange/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/namespace [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/namespace/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/node [build failed]
	k8s.io/kubernetes/pkg/registry/core/node/rest		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/node/storage [build failed]
# k8s.io/kubernetes/pkg/registry/core/podtemplate.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/pod/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/rest.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/podtemplate/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/resourcequota/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/replicationcontroller/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/replicationcontroller.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 1.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/core/persistentvolume	3.328s	coverage: 1.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/core/persistentvolume/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/persistentvolumeclaim [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/persistentvolumeclaim/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/pod [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/pod/rest [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/pod/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/podtemplate [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/podtemplate/storage [build failed]
?   	k8s.io/kubernetes/pkg/registry/core/rangeallocation	[no test files]
FAIL	k8s.io/kubernetes/pkg/registry/core/replicationcontroller [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/replicationcontroller/storage [build failed]
# k8s.io/kubernetes/pkg/registry/core/secret/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/service.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/service/allocator/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/resourcequota.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4577/resourcequota.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3098549173/000012.o -lresolv -framework CoreFoundation -framework Security -O2 -g -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4577/resourcequota.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/registry/core/resourcequota [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/resourcequota/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/rest [build failed]
# k8s.io/kubernetes/pkg/registry/core/service/ipallocator/controller.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/service/ipallocator/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/secret.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4586/secret.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/core/secret [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/secret/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/service [build failed]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/core/service/allocator	1.274s	coverage: 0.0% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/core/service/allocator/storage [build failed]
# k8s.io/kubernetes/pkg/registry/core/service/portallocator/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/service/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/discovery/endpointslice.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/serviceaccount.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/core/serviceaccount/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtest_isNotContained
=== RUN   TestCtest_isNotContained/ipv4_not_contained_nor_overlapping
=== RUN   TestCtest_isNotContained/ipv4_not_contained_but_contains
=== RUN   TestCtest_isNotContained/ipv4_not_contained_but_matches_existing_one
=== RUN   TestCtest_isNotContained/ipv4_contained_but_matches_existing_one
=== RUN   TestCtest_isNotContained/empty_prefix_list_=>_not_contained
=== RUN   TestCtest_isNotContained/prefix_/0_with_empty_list
=== RUN   TestCtest_isNotContained/prefix_exactly_matches_larger_existing_prefix
=== RUN   TestCtest_isNotContained/ipv6_not_contained
=== RUN   TestCtest_isNotContained/ipv6_contained_by_larger_prefix
=== RUN   TestCtest_isNotContained/duplicate_prefixes_in_list
--- PASS: TestCtest_isNotContained (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_not_contained_nor_overlapping (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_not_contained_but_contains (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_not_contained_but_matches_existing_one (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_contained_but_matches_existing_one (0.00s)
    --- PASS: TestCtest_isNotContained/empty_prefix_list_=>_not_contained (0.00s)
    --- PASS: TestCtest_isNotContained/prefix_/0_with_empty_list (0.00s)
    --- PASS: TestCtest_isNotContained/prefix_exactly_matches_larger_existing_prefix (0.00s)
    --- PASS: TestCtest_isNotContained/ipv6_not_contained (0.00s)
    --- PASS: TestCtest_isNotContained/ipv6_contained_by_larger_prefix (0.00s)
    --- PASS: TestCtest_isNotContained/duplicate_prefixes_in_list (0.00s)
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/core/service/ipallocator	3.142s	coverage: 0.5% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/registry/core/service/ipallocator/controller [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/service/ipallocator/storage [build failed]
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/core/service/portallocator	2.126s	coverage: 0.2% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/registry/core/service/portallocator/controller.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4614/controller.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/core/service/portallocator/controller [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/service/portallocator/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/service/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/serviceaccount [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/core/serviceaccount/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/discovery/endpointslice [build failed]
	k8s.io/kubernetes/pkg/registry/discovery/endpointslice/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/discovery/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/events/rest		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/networking/ingress/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/flowcontrol/rest.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/flowcontrol/prioritylevelconfiguration.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4641/prioritylevelconfiguration.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1188071381/000012.o -lresolv -framework CoreFoundation -framework Security -O2 -g -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4641/prioritylevelconfiguration.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

# k8s.io/kubernetes/pkg/registry/networking/networkpolicy.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/networking/networkpolicy/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
ok  	k8s.io/kubernetes/pkg/registry/flowcontrol/ensurer	3.404s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/registry/flowcontrol/flowschema		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/flowcontrol/flowschema/storage		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/flowcontrol/prioritylevelconfiguration [build failed]
	k8s.io/kubernetes/pkg/registry/flowcontrol/prioritylevelconfiguration/storage		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/flowcontrol/rest [build failed]
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/networking/ingress	1.687s	coverage: 0.4% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/networking/ingress/storage [build failed]
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
ok  	k8s.io/kubernetes/pkg/registry/networking/ingressclass	1.771s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/registry/networking/ingressclass/storage		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/networking/servicecidr.test
WriteFile trivial.c failed: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1479787811/trivial.c: no space left on device
# k8s.io/kubernetes/pkg/registry/networking/ipaddress.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4658/ipaddress.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/networking/ipaddress [build failed]
	k8s.io/kubernetes/pkg/registry/networking/ipaddress/storage		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/networking/networkpolicy [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/networking/networkpolicy/storage [build failed]
	k8s.io/kubernetes/pkg/registry/networking/rest		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/networking/servicecidr [build failed]
	k8s.io/kubernetes/pkg/registry/networking/servicecidr/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/node/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/node/runtimeclass		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/node/runtimeclass/storage		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/policy/poddisruptionbudget/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/rbac/clusterrole/policybased.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/rbac.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/policy/poddisruptionbudget.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/policy/poddisruptionbudget [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/policy/poddisruptionbudget/storage [build failed]
	k8s.io/kubernetes/pkg/registry/policy/rest		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/rbac [build failed]
	k8s.io/kubernetes/pkg/registry/rbac/clusterrole		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/rbac/clusterrole/policybased [build failed]
	k8s.io/kubernetes/pkg/registry/rbac/clusterrole/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrolebinding		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrolebinding/policybased		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrolebinding/storage		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/rbac/rest.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4694/rest.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2237212049/000012.o -lresolv -framework CoreFoundation -framework Security -O2 -g -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4694/rest.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/registry/rbac/rest [build failed]
	k8s.io/kubernetes/pkg/registry/rbac/role		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/rbac/role/policybased	1.664s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/registry/rbac/role/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/rolebinding		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/rolebinding/policybased		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/rolebinding/storage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/rbac/validation	1.805s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/registry/registrytest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/resource		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/resource/resourceclaim.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/devicetaintrule.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/deviceclass.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/resource/deviceclass [build failed]
# k8s.io/kubernetes/pkg/registry/resource/resourceclaim/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/devicetaintrule/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/resourceslice.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/resourceclaimtemplate.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/resourceclaimtemplate/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/resource/deviceclass/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4713/storage.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/resource/deviceclass/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/devicetaintrule [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/devicetaintrule/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceclaim [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceclaim/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceclaimtemplate [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceclaimtemplate/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceslice [build failed]
# k8s.io/kubernetes/pkg/registry/resource/resourceslice/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/csidriver/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceslice/storage [build failed]
# k8s.io/kubernetes/pkg/registry/scheduling/priorityclass/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
	k8s.io/kubernetes/pkg/registry/resource/rest		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/scheduling/priorityclass.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/scheduling/priorityclass [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/scheduling/priorityclass/storage [build failed]
	k8s.io/kubernetes/pkg/registry/scheduling/rest		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/registry/storage/storageclass.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/csinode/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/csistoragecapacity/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/csistoragecapacity.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/csidriver.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4748/csidriver.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/storage/csidriver [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/csidriver/storage [build failed]
# k8s.io/kubernetes/pkg/registry/storage/volumeattributesclass/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/volumeattachment/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/storageclass/storage.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/registry/storage/volumeattributesclass.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/registry/storage/csinode	2.756s	coverage: 0.4% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/registry/storage/csinode/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/csistoragecapacity [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/csistoragecapacity/storage [build failed]
	k8s.io/kubernetes/pkg/registry/storage/rest		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/storage/storageclass [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/storageclass/storage [build failed]
# k8s.io/kubernetes/pkg/registry/storage/volumeattachment.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4773/volumeattachment.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/registry/storage/volumeattachment [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/volumeattachment/storage [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/volumeattributesclass [build failed]
FAIL	k8s.io/kubernetes/pkg/registry/storage/volumeattributesclass/storage [build failed]
	k8s.io/kubernetes/pkg/registry/storagemigration/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/storagemigration/storagemigration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/storagemigration/storagemigration/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/testapigroup/carp		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/testapigroup/carp/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/testapigroup/rest		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/scheduler.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/backend/cache.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/backend/api_dispatcher.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/routes	3.516s	coverage: 0.5% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/scheduler [build failed]
=== RUN   TestCtestPluginsNames
=== RUN   TestCtestPluginsNames/empty
=== RUN   TestCtestPluginsNames/with_duplicates
=== RUN   TestCtestPluginsNames/nil_plugin_sets
=== RUN   TestCtestPluginsNames/empty_enabled_slices
=== RUN   TestCtestPluginsNames/multiple_distinct_plugins
=== RUN   TestCtestPluginsNames/nil_plugins_pointer
    ctest_types_test.go:91: plugins mismatch (-want +got):
          []string(
        - 	{},
        + 	nil,
          )
--- FAIL: TestCtestPluginsNames (0.00s)
    --- PASS: TestCtestPluginsNames/empty (0.00s)
    --- PASS: TestCtestPluginsNames/with_duplicates (0.00s)
    --- PASS: TestCtestPluginsNames/nil_plugin_sets (0.00s)
    --- PASS: TestCtestPluginsNames/empty_enabled_slices (0.00s)
    --- PASS: TestCtestPluginsNames/multiple_distinct_plugins (0.00s)
    --- FAIL: TestCtestPluginsNames/nil_plugins_pointer (0.00s)
FAIL
coverage: 0.2% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/scheduler/apis/config	2.241s
	k8s.io/kubernetes/pkg/scheduler/apis/config/latest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.4% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/apis/config/scheme	1.863s	coverage: 0.4% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/apis/config/testing		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/scheduler/apis/config/testing/defaults	[no test files]
=== RUN   TestCtestApplyFeatureGates
=== RUN   TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_disabled
W0216 15:47:08.156777   93538 feature_gate.go:352] Setting GA feature gate DynamicResourceAllocation=false. It will be removed in a future release.
=== RUN   TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_enabled
W0216 15:47:08.157210   93538 feature_gate.go:352] Setting GA feature gate DynamicResourceAllocation=true. It will be removed in a future release.
=== RUN   TestCtestApplyFeatureGates/No_feature_gates_set_(edge_case)
--- PASS: TestCtestApplyFeatureGates (0.00s)
    --- PASS: TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_disabled (0.00s)
    --- PASS: TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_enabled (0.00s)
    --- PASS: TestCtestApplyFeatureGates/No_feature_gates_set_(edge_case) (0.00s)
=== RUN   TestCtestMergePlugins
=== RUN   TestCtestMergePlugins/AppendCustomPlugin
=== RUN   TestCtestMergePlugins/InsertAfterDefaultPlugins2
=== RUN   TestCtestMergePlugins/ApplyNilCustomPlugin
=== RUN   TestCtestMergePlugins/BothNil_(edge_case)
=== RUN   TestCtestMergePlugins/EmptyCustom_(edge_case)
=== RUN   TestCtestMergePlugins/NilFilterCustom_(edge_case)
=== RUN   TestCtestMergePlugins/CustomWithOnlyDisabled_(edge_case)
    ctest_default_plugins_test.go:256: plugins mismatch (-want +got):
          &v1.Plugins{
          	PreEnqueue: {},
          	QueueSort:  {},
          	PreFilter:  {},
          	Filter: v1.PluginSet{
        - 		Enabled:  []v1.Plugin{{Name: "DefaultPlugin1"}, {Name: "DefaultPlugin2"}},
        + 		Enabled:  nil,
          		Disabled: {{Name: "*"}},
          	},
          	PostFilter: {},
          	PreScore:   {},
          	... // 7 identical fields
          }
--- FAIL: TestCtestMergePlugins (0.00s)
    --- PASS: TestCtestMergePlugins/AppendCustomPlugin (0.00s)
    --- PASS: TestCtestMergePlugins/InsertAfterDefaultPlugins2 (0.00s)
    --- PASS: TestCtestMergePlugins/ApplyNilCustomPlugin (0.00s)
    --- PASS: TestCtestMergePlugins/BothNil_(edge_case) (0.00s)
    --- PASS: TestCtestMergePlugins/EmptyCustom_(edge_case) (0.00s)
    --- PASS: TestCtestMergePlugins/NilFilterCustom_(edge_case) (0.00s)
    --- FAIL: TestCtestMergePlugins/CustomWithOnlyDisabled_(edge_case) (0.00s)
FAIL
coverage: 0.4% of statements in ./...
FAIL	k8s.io/kubernetes/pkg/scheduler/apis/config/v1	2.516s
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/apis/config/validation	1.441s	coverage: 0.3% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/backend/api_cache		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/backend/api_dispatcher [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/backend/cache [build failed]
# k8s.io/kubernetes/pkg/scheduler/framework.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/backend/queue.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/api_calls.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/autoscaler_contract.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/backend/cache/debugger.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4825/debugger.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-1840657077/000012.o -lresolv -O2 -g -O2 -g -framework CoreFoundation -framework CoreFoundation -framework Security
ld: can't write to output file: $WORK/b4825/debugger.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/scheduler/backend/cache/debugger [build failed]
	k8s.io/kubernetes/pkg/scheduler/backend/cache/fake		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/backend/heap	0.983s	coverage: 0.1% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/scheduler/backend/queue [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/api_calls [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/autoscaler_contract [build failed]
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/framework/parallelize	1.145s	coverage: 0.1% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultpreemption.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultbinder.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultbinder [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultpreemption [build failed]
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/dynamicresources.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/dynamicresources [build failed]
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/helper.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/imagelocality.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeaffinity.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/interpodaffinity.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodename.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/dynamicresources/extended.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: /Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: rewriting uuid failed: write $WORK/b4857/extended.test~: no space left on device
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/dynamicresources/extended [build failed]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins/feature		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/helper [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/imagelocality [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/interpodaffinity [build failed]
?   	k8s.io/kubernetes/pkg/scheduler/framework/plugins/names	[no test files]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeaffinity [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodename [build failed]
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodevolumelimits.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeunschedulable.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/noderesources.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeports.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeports [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/noderesources [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeunschedulable [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodevolumelimits [build failed]
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/tainttoleration.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/schedulinggates.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/podtopologyspread.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/podtopologyspread [build failed]
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumerestrictions.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumezone.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumebinding.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/profile.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/runtime.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/framework/preemption.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/util.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/scheduler/util/assumecache.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.5% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/queuesort	3.129s	coverage: 0.5% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/schedulinggates [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/tainttoleration [build failed]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins/testing		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumebinding [build failed]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumebinding/metrics		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumerestrictions [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumezone [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/preemption [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/framework/runtime [build failed]
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/metrics	1.191s	coverage: 0.1% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements in ./...
ok  	k8s.io/kubernetes/pkg/scheduler/metrics/resources	1.505s	coverage: 0.3% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/scheduler/profile [build failed]
	k8s.io/kubernetes/pkg/scheduler/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/scheduler/testing/framework		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/util [build failed]
FAIL	k8s.io/kubernetes/pkg/scheduler/util/assumecache [build failed]
?   	k8s.io/kubernetes/pkg/security	[no test files]
# k8s.io/kubernetes/pkg/serviceaccount.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/security/apparmor.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/security/apparmor [build failed]
# k8s.io/kubernetes/pkg/securitycontext.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/securitycontext [build failed]
FAIL	k8s.io/kubernetes/pkg/serviceaccount [build failed]
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b4950/_cover_.out: no space left on device
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/serviceaccount/externaljwt/metrics	0.607s
# k8s.io/kubernetes/pkg/util/flag.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/serviceaccount/externaljwt/plugin.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b4953/plugin.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2397788538/000012.o -lresolv -framework CoreFoundation -framework Security -O2 -g -O2 -g -framework CoreFoundation
ld: can't write to output file: $WORK/b4953/plugin.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/pkg/serviceaccount/externaljwt/plugin [build failed]
	k8s.io/kubernetes/pkg/serviceaccount/externaljwt/plugin/testing/v1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/async	0.917s	coverage: 0.0% of statements in ./... [no tests to run]
?   	k8s.io/kubernetes/pkg/util/bandwidth	[no test files]
	k8s.io/kubernetes/pkg/util/coverage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/env	1.240s	coverage: 0.0% of statements in ./... [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/filesystem	1.636s	coverage: 0.0% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/util/flag [build failed]
	k8s.io/kubernetes/pkg/util/flock		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/goroutinemap	1.805s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/util/goroutinemap/exponentialbackoff		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/hash	2.018s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/util/interrupt		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/util/iptables	[no test files]
?   	k8s.io/kubernetes/pkg/util/iptables/testing	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/kernel	2.084s	coverage: 0.0% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/util/pod.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.1% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/labels	2.334s	coverage: 0.1% of statements in ./... [no tests to run]
# k8s.io/kubernetes/pkg/util/taints.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/util/tolerations.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/csi/fake
vet: failed to export analysis facts: open $WORK/b5030/vet.out: no space left on device
# k8s.io/kubernetes/pkg/volume.test
link: mkdir /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-204412149: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/node	2.880s	coverage: 0.2% of statements in ./... [no tests to run]
	k8s.io/kubernetes/pkg/util/oom		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/pkg/util/parsers	1.863s	coverage: 0.0% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/util/pod [build failed]
	k8s.io/kubernetes/pkg/util/procfs		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5006/_cover_.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/util/removeall	0.908s
	k8s.io/kubernetes/pkg/util/rlimit		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5010/_cover_.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/util/slice	0.917s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5013/_cover_.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/util/tail	0.707s
FAIL	k8s.io/kubernetes/pkg/util/taints [build failed]
FAIL	k8s.io/kubernetes/pkg/util/tolerations [build failed]
FAIL	k8s.io/kubernetes/pkg/volume [build failed]
# k8s.io/kubernetes/pkg/volume/csi/nodeinfomanager.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/configmap.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/configmap [build failed]
# k8s.io/kubernetes/pkg/volume/csimigration.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/csi.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/csi [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/csi/fake [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/csi/nodeinfomanager [build failed]
	k8s.io/kubernetes/pkg/volume/csi/testing		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/volume/csimigration [build failed]
# k8s.io/kubernetes/pkg/volume/fc.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/downwardapi.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/downwardapi [build failed]
	k8s.io/kubernetes/pkg/volume/emptydir		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/volume/fc [build failed]
# k8s.io/kubernetes/pkg/volume/local.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/flexvolume.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/flexvolume [build failed]
# k8s.io/kubernetes/pkg/volume/nfs.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/portworx.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/hostpath.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/image.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/git_repo.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/git_repo [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/hostpath [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/image [build failed]
# k8s.io/kubernetes/pkg/volume/iscsi.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/iscsi [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/local [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/nfs [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/portworx [build failed]
# k8s.io/kubernetes/pkg/volume/projected.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/projected [build failed]
# k8s.io/kubernetes/pkg/volume/secret.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/secret [build failed]
	k8s.io/kubernetes/pkg/volume/testing		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/volume/util.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/util [build failed]
	k8s.io/kubernetes/pkg/volume/util/fs		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/volume/util/fsquota		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/volume/util/fsquota/common	[no test files]
=== RUN   TestCtestGetFileType
    ctest_hostutil_test.go:153: [0-Directory Test] unexpected error: volume/util/hostutil on this platform is not supported
--- FAIL: TestCtestGetFileType (0.00s)
FAIL
coverage: 0.0% of statements in ./...
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/util/hostutil	0.805s
# k8s.io/kubernetes/pkg/volume/util/operationexecutor.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/util/recyclerclient.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/pkg/volume/validation.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/alwayspullimages.test
link: mkdir /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-3241395007: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/certificates/approval.test
link: mkdir /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-4081656718: no space left on device
testing: warning: no tests to run
PASS
coverage: 0.2% of statements in ./...
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
ok  	k8s.io/kubernetes/pkg/volume/util/nestedpendingoperations	1.674s	coverage: 0.2% of statements in ./... [no tests to run]
FAIL	k8s.io/kubernetes/pkg/volume/util/operationexecutor [build failed]
FAIL	k8s.io/kubernetes/pkg/volume/util/recyclerclient [build failed]
	k8s.io/kubernetes/pkg/volume/util/subpath		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
error generating coverage report: creating meta-data file /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5102/gocoverdir/tmp.covmeta.06061d63c6f02a8aaabf893d25c887c51771278458032908000: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5102/gocoverdir/tmp.covmeta.06061d63c6f02a8aaabf893d25c887c51771278458032908000: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/util/types	0.426s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5105/_cover_.out: no space left on device
FAIL	k8s.io/kubernetes/pkg/volume/util/volumepathhandler	0.592s
FAIL	k8s.io/kubernetes/pkg/volume/validation [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/admit.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/admit [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/alwayspullimages [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/antiaffinity.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/antiaffinity [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/approval [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/certificates/ctbattest.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/ctbattest [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/certificates/signing.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/signing [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/certificates/subjectrestriction.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/subjectrestriction [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/defaulttolerationseconds.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/defaulttolerationseconds [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/deny.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/deny [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/eventratelimit.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit [build failed]
	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/install		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/v1alpha1		coverage: 0.0% of statements
# k8s.io/kubernetes/plugin/pkg/admission/gc.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/validation.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/validation [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/extendedresourcetoleration.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/extendedresourcetoleration [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/gc [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/namespace/exists.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/namespace/autoprovision.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/limitranger.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/imagepolicy.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/imagepolicy [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/limitranger [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/namespace/autoprovision [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/namespace/exists [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/noderestriction.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/network/defaultingressclass.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/network/defaultingressclass [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/network/denyserviceexternalips.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/network/denyserviceexternalips [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/noderestriction [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/podnodeselector.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/nodetaint.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: running clang failed: exit status 1
/usr/bin/clang -arch arm64 -Wl,-S -Wl,-x -o $WORK/b5175/nodetaint.test -Qunused-arguments /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/go.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000000.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000001.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000002.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000003.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000004.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000005.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000006.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000007.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000008.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000009.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000010.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000011.o /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-link-2310299800/000012.o -lresolv -O2 -g -O2 -g -framework CoreFoundation -framework CoreFoundation -framework Security
ld: can't write to output file: $WORK/b5175/nodetaint.test, errno=28 for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

FAIL	k8s.io/kubernetes/plugin/pkg/admission/nodetaint [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/podnodeselector [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction [build failed]
	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/install		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/v1alpha1		coverage: 0.0% of statements
# k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/priority.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/podtopologylabels.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/podtopologylabels [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/priority [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/runtimeclass.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/security/podsecurity.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/resourcequota.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/resourcequota [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/runtimeclass [build failed]
?   	k8s.io/kubernetes/plugin/pkg/admission/security	[no test files]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/security/podsecurity [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/storage/persistentvolume/resize.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/serviceaccount.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/serviceaccount [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/admission/storage/persistentvolume/resize [build failed]
# k8s.io/kubernetes/plugin/pkg/admission/storage/storageclass/setdefault.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/storage/storageclass/setdefault [build failed]
# k8s.io/kubernetes/plugin/pkg/auth/authorizer/node.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
# k8s.io/kubernetes/plugin/pkg/admission/storage/storageobjectinuseprotection.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
FAIL	k8s.io/kubernetes/plugin/pkg/admission/storage/storageobjectinuseprotection [build failed]
?   	k8s.io/kubernetes/plugin/pkg/auth	[no test files]
# k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac/bootstrappolicy.test
/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/pkg/tool/darwin_arm64/link: mapping output file failed: no space left on device
=== RUN   TestCtestTokenAuthenticator
[DEBUG] Starting TestCtestTokenAuthenticator
[DEBUG] Running sub‑test: valid token
[DEBUG] Running sub‑test: valid token with extra group
[DEBUG] Running sub‑test: invalid group
[DEBUG] Running sub‑test: invalid secret name
[DEBUG] Running sub‑test: no usage
[DEBUG] Running sub‑test: wrong token
[DEBUG] Running sub‑test: deleted token
[DEBUG] Running sub‑test: expired token
[DEBUG] Running sub‑test: not expired token
[DEBUG] Running sub‑test: token id wrong length
[DEBUG] Running sub‑test: empty token
[DEBUG] Running sub‑test: missing secret part
[DEBUG] Running sub‑test: extra delimiter
[DEBUG] Running sub‑test: secret with non‑base64 characters in token secret
[DEBUG] Completed TestCtestTokenAuthenticator
--- PASS: TestCtestTokenAuthenticator (0.00s)
PASS
coverage: 0.3% of statements in ./...
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
ok  	k8s.io/kubernetes/plugin/pkg/auth/authenticator/token/bootstrap	1.140s	coverage: 0.3% of statements in ./...
?   	k8s.io/kubernetes/plugin/pkg/auth/authorizer	[no test files]
FAIL	k8s.io/kubernetes/plugin/pkg/auth/authorizer/node [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac [build failed]
FAIL	k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac/bootstrappolicy [build failed]
testing: warning: no tests to run
PASS
error generating coverage report: creating meta-data file /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5236/gocoverdir/tmp.covmeta.d3ae648ad4a939d3463625b5af361afe1771278466596258000: open /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5236/gocoverdir/tmp.covmeta.d3ae648ad4a939d3463625b5af361afe1771278466596258000: no space left on device
FAIL	k8s.io/kubernetes/third_party/forked/golang/expansion	0.461s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
error generating coverage report: write /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build642273491/b5238/_cover_.out: no space left on device
error: saving coverage profile: write /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out: no space left on device
FAIL	k8s.io/kubernetes/third_party/forked/golang/net	0.453s
	k8s.io/kubernetes/third_party/forked/gonum/graph		coverage: 0.0% of statements
	k8s.io/kubernetes/third_party/forked/gonum/graph/internal/linear		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements in ./...
ok  	k8s.io/kubernetes/third_party/forked/gonum/graph/simple	1.291s	coverage: 0.0% of statements in ./... [no tests to run]
	k8s.io/kubernetes/third_party/forked/gonum/graph/traverse		coverage: 0.0% of statements
?   	k8s.io/kubernetes/third_party/forked/gotestsum/junitxml	[no test files]
	k8s.io/kubernetes/third_party/forked/libcontainer/apparmor		coverage: 0.0% of statements
	k8s.io/kubernetes/third_party/forked/libcontainer/utils		coverage: 0.0% of statements
	k8s.io/kubernetes/third_party/forked/vishhstress		coverage: 0.0% of statements

📊 Test summary:
🧪 Total tests : 2319
✅ Passed     : 1905
❌ Failed     : 360
⚠ Skipped    : 54
⏱ Time        : 685 seconds
🧪 Coverage   : /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_coverage_20260216T153624.out
📄 HTML log   : /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/logs/ctest_unit_logs_20260216T153623.html
