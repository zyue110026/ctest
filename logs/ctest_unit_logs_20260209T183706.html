	k8s.io/kubernetes/build/pause/windows/wincat		coverage: 0.0% of statements
=== RUN   TestCtestServerOverride

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:21]: matched config: {test_fixture.json [ETCD default override] env [] {               }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:37:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:37:27 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:28]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:21]: matched config: {test_fixture.json [ETCD servers and override set] env [] {  ETCDServers ETCDServersOverrides            }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:37:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:37:27 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"ETCDServers","ETCDServersOverride":"ETCDServersOverrides","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:28]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestServerOverride (0.01s)
=== RUN   TestCtestStorageOptions

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:97]: matched config: {test_fixture.json [storage options supplied] env [] {             StorageBackend StorageMediaType 1s}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:37:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:37:27 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"1s","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"StorageBackend","StorageMediaType":"StorageMediaType"})[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:104]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:97]: matched config: {test_fixture.json [storage options not supplied] env [] {               }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:37:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:37:27 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:104]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestStorageOptions (0.01s)
=== RUN   TestCtestTLSFlags

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:188]: matched config: {test_fixture.json [mTLS enabled] env [] {  https://127.0.0.1:2379  CAKey CACert CACertPath APIServerKey APIServerCert APIServerCertPath APIServerKeyPath ETCDKey ETCDCert   }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:37:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:37:27 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"APIServerCert","APIServerCertPath":"APIServerCertPath","APIServerKey":"APIServerKey","APIServerKeyPath":"APIServerKeyPath","CACert":"CACert","CACertPath":"CACertPath","CAKey":"CAKey","CompactionInterval":"","ETCDCert":"ETCDCert","ETCDKey":"ETCDKey","ETCDServers":"https://127.0.0.1:2379","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:195]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:188]: matched config: {test_fixture.json [mTLS disabled] env [] {               }}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:37:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:37:27 
=== COMPLETE: Generated 27 results ===
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServerCert":"","APIServerCertPath":"","APIServerKey":"","APIServerKeyPath":"","CACert":"","CACertPath":"","CAKey":"","CompactionInterval":"","ETCDCert":"","ETCDKey":"","ETCDServers":"","ETCDServersOverride":"","KubeAPIServerRunAsUser":"","KubeHome":"","StorageBackend":"","StorageMediaType":""})[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_etcd_test.go:195]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestTLSFlags (0.01s)
=== RUN   TestCtestEncryptionProviderFlag
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_set
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_not_set
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_large_payload
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
=== RUN   TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_malformed_base64
    configure_helper_test.go:122: Start kubernetes api-server
        WARNING: ALL of ETCD_APISERVER_CA_KEY, ETCD_APISERVER_CA_CERT, ETCD_APISERVER_SERVER_KEY, ETCD_APISERVER_SERVER_CERT, ETCD_APISERVER_CLIENT_KEY and ETCD_APISERVER_CLIENT_CERT are missing, mTLS between etcd server and kube-apiserver is not enabled.
--- PASS: TestCtestEncryptionProviderFlag (1.63s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_set (0.43s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_is_not_set (0.26s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_large_payload (0.49s)
    --- PASS: TestCtestEncryptionProviderFlag/ENCRYPTION_PROVIDER_CONFIG_malformed_base64 (0.45s)
=== RUN   TestCtestKMSIntegration
[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_kms_test.go:142]: Loading hard‑coded volume config
[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/gce/gci/ctest_apiserver_kms_test.go:150]: got default config: {test_fixture.json [CLOUD_KMS_INTEGRATION set] volumes [pods deployments statefulsets daemonsets replicasets] {kmssocket {&HostPathVolumeSource{Path:/var/run/kmsplugin,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:37:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:37:29 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/cluster/gce/gci	2.855s
	k8s.io/kubernetes/cluster/gce/gci/mounter		coverage: 0.0% of statements
# k8s.io/kubernetes/cmd/kube-controller-manager/app
# [k8s.io/kubernetes/cmd/kube-controller-manager/app]
cmd/kube-controller-manager/app/ctest_core_test.go:139:3: fmt.Printf format %d has arg i of wrong type string
=== RUN   TestCtestMigrate
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:38]: Starting TestCtestMigrate with edge cases
[DEBUG-CTEST 2026-02-09 18:37:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:77]: Total migrations to run: 11
Running migration 0: v3-v3-up
=== RUN   TestCtestMigrate/v3-v3-up
  I0209 18:37:27.277728   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:37:27.278257   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:37:27.278275   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server v3-v3-up-0: [/usr/local/bin/etcd-3.0.17 --name v3-v3-up-0 --initial-cluster v3-v3-up-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-v3-v3-up-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 1: oldest-newest-up
=== RUN   TestCtestMigrate/oldest-newest-up
  I0209 18:37:47.324681   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:37:47.331495   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:37:47.331512   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server oldest-newest-up-0: [/usr/local/bin/etcd-3.0.17 --name oldest-newest-up-0 --initial-cluster oldest-newest-up-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-oldest-newest-up-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 2: v3-v3-up-with-additional-client-url
=== RUN   TestCtestMigrate/v3-v3-up-with-additional-client-url
  I0209 18:38:07.379401   79926 data_dir.go:46] data directory '/tmp/etcd-data-dir-v3-v3-up-with-additional-client-url-0' does not exist, creating it
  I0209 18:38:07.379940   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:38:07.382870   79926 data_dir.go:68] data directory '/tmp/etcd-data-dir-v3-v3-up-with-additional-client-url-0' is empty, writing target version '3.0.17/etcd3' to version.txt
  I0209 18:38:07.383913   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:38:07.383920   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server v3-v3-up-with-additional-client-url-0: [/usr/local/bin/etcd-3.0.17 --name v3-v3-up-with-additional-client-url-0 --initial-cluster v3-v3-up-with-additional-client-url-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-v3-v3-up-with-additional-client-url-0 --listen-client-urls http://127.0.0.1:2379,http://10.128.0.1:2379 --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 3: ha-v3-v3-up
=== RUN   TestCtestMigrate/ha-v3-v3-up
  I0209 18:38:27.422095   79926 data_dir.go:46] data directory '/tmp/etcd-data-dir-ha-v3-v3-up-2' does not exist, creating it
  I0209 18:38:27.422314   79926 data_dir.go:46] data directory '/tmp/etcd-data-dir-ha-v3-v3-up-1' does not exist, creating it
  I0209 18:38:27.422088   79926 data_dir.go:46] data directory '/tmp/etcd-data-dir-ha-v3-v3-up-0' does not exist, creating it
  I0209 18:38:27.424211   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:38:27.424155   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:38:27.424170   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:38:27.429116   79926 data_dir.go:68] data directory '/tmp/etcd-data-dir-ha-v3-v3-up-2' is empty, writing target version '3.0.17/etcd3' to version.txt
  I0209 18:38:27.429128   79926 data_dir.go:68] data directory '/tmp/etcd-data-dir-ha-v3-v3-up-0' is empty, writing target version '3.0.17/etcd3' to version.txt
  I0209 18:38:27.429178   79926 data_dir.go:68] data directory '/tmp/etcd-data-dir-ha-v3-v3-up-1' is empty, writing target version '3.0.17/etcd3' to version.txt
  I0209 18:38:27.429570   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:38:27.429589   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:38:27.429595   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
  I0209 18:38:27.429603   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:38:27.429608   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
  I0209 18:38:27.429577   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server ha-v3-v3-up-2: [/usr/local/bin/etcd-3.0.17 --name ha-v3-v3-up-2 --initial-cluster ha-v3-v3-up-0=https://127.0.0.1:2380,ha-v3-v3-up-1=https://127.0.0.1:12380,ha-v3-v3-up-2=https://127.0.0.1:22380 --debug --data-dir /tmp/etcd-data-dir-ha-v3-v3-up-2 --listen-client-urls  --advertise-client-urls http://127.0.0.1:22379 --listen-peer-urls https://127.0.0.1:22380 --initial-advertise-peer-urls https://127.0.0.1:22380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
Starting server ha-v3-v3-up-0: [/usr/local/bin/etcd-3.0.17 --name ha-v3-v3-up-0 --initial-cluster ha-v3-v3-up-0=https://127.0.0.1:2380,ha-v3-v3-up-1=https://127.0.0.1:12380,ha-v3-v3-up-2=https://127.0.0.1:22380 --debug --data-dir /tmp/etcd-data-dir-ha-v3-v3-up-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
Starting server ha-v3-v3-up-1: [/usr/local/bin/etcd-3.0.17 --name ha-v3-v3-up-1 --initial-cluster ha-v3-v3-up-0=https://127.0.0.1:2380,ha-v3-v3-up-1=https://127.0.0.1:12380,ha-v3-v3-up-2=https://127.0.0.1:22380 --debug --data-dir /tmp/etcd-data-dir-ha-v3-v3-up-1 --listen-client-urls  --advertise-client-urls http://127.0.0.1:12379 --listen-peer-urls https://127.0.0.1:12380 --initial-advertise-peer-urls https://127.0.0.1:12380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 4: v3-v3-down
=== RUN   TestCtestMigrate/v3-v3-down
  I0209 18:38:47.467105   79926 data_dir.go:46] data directory '/tmp/etcd-data-dir-v3-v3-down-0' does not exist, creating it
  I0209 18:38:47.469945   79926 migrator.go:66] Starting migration to 3.1.12/etcd3
  I0209 18:38:47.475402   79926 data_dir.go:68] data directory '/tmp/etcd-data-dir-v3-v3-down-0' is empty, writing target version '3.1.12/etcd3' to version.txt
  I0209 18:38:47.475735   79926 migrator.go:87] Converging current version '3.1.12/etcd3' to target version '3.1.12/etcd3'
  I0209 18:38:47.475742   79926 migrator.go:91] current version '3.1.12/etcd3' equals or is one minor version previous of target version '3.1.12/etcd3' - migration complete
Starting server v3-v3-down-0: [/usr/local/bin/etcd-3.1.12 --name v3-v3-down-0 --initial-cluster v3-v3-down-0=https://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-v3-v3-down-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls https://127.0.0.1:2380 --initial-advertise-peer-urls https://127.0.0.1:2380 --peer-client-cert-auth --peer-trusted-ca-file=/tmp/certs/test.crt --peer-cert-file=/tmp/certs/test.crt --peer-key-file=/tmp/certs/test.key]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.1.12: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 5: zero-member-count
=== RUN   TestCtestMigrate/zero-member-count
Running migration 6: negative-member-count
=== RUN   TestCtestMigrate/negative-member-count
Running migration 7: empty-protocol
=== RUN   TestCtestMigrate/empty-protocol
  I0209 18:39:07.506082   79926 data_dir.go:46] data directory '/tmp/etcd-data-dir-empty-protocol-0' does not exist, creating it
  I0209 18:39:07.507298   79926 migrator.go:66] Starting migration to 3.0.17/etcd3
  I0209 18:39:07.510052   79926 data_dir.go:68] data directory '/tmp/etcd-data-dir-empty-protocol-0' is empty, writing target version '3.0.17/etcd3' to version.txt
  I0209 18:39:07.510878   79926 migrator.go:87] Converging current version '3.0.17/etcd3' to target version '3.0.17/etcd3'
  I0209 18:39:07.511025   79926 migrator.go:91] current version '3.0.17/etcd3' equals or is one minor version previous of target version '3.0.17/etcd3' - migration complete
Starting server empty-protocol-0: [/usr/local/bin/etcd-3.0.17 --name empty-protocol-0 --initial-cluster empty-protocol-0=://127.0.0.1:2380 --debug --data-dir /tmp/etcd-data-dir-empty-protocol-0 --listen-client-urls  --advertise-client-urls http://127.0.0.1:2379 --listen-peer-urls ://127.0.0.1:2380 --initial-advertise-peer-urls ://127.0.0.1:2380]
    ctest_integration_test.go:125: Failed to start server: fork/exec /usr/local/bin/etcd-3.0.17: no such file or directory
    ctest_integration_test.go:134: failed to write text value: context deadline exceeded
    ctest_integration_test.go:146: Stop server failed: cannot stop EtcdMigrateServer that has not been started
Running migration 8: malformed-start-version
=== RUN   TestCtestMigrate/malformed-start-version
--- FAIL: TestCtestMigrate (120.39s)
    --- FAIL: TestCtestMigrate/v3-v3-up (20.04s)
    --- FAIL: TestCtestMigrate/oldest-newest-up (20.05s)
    --- FAIL: TestCtestMigrate/v3-v3-up-with-additional-client-url (20.04s)
    --- FAIL: TestCtestMigrate/ha-v3-v3-up (20.04s)
    --- FAIL: TestCtestMigrate/v3-v3-down (20.04s)
    --- PASS: TestCtestMigrate/zero-member-count (0.00s)
    --- PASS: TestCtestMigrate/negative-member-count (0.00s)
    --- FAIL: TestCtestMigrate/empty-protocol (20.09s)
    --- FAIL: TestCtestMigrate/malformed-start-version (0.02s)
panic: malformed version file, expected <major>.<minor>.<patch>/<storage> but got invalid-version [recovered]
	panic: malformed version file, expected <major>.<minor>.<patch>/<storage> but got invalid-version

goroutine 60 [running]:
testing.tRunner.func1.2({0x1050153e0, 0x140009aeeb0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1050153e0?, 0x140009aeeb0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cluster/images/etcd/migrate.mustParseEtcdVersionPair(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:384
k8s.io/kubernetes/cluster/images/etcd/migrate.TestCtestMigrate.func1.1(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:87
k8s.io/kubernetes/cluster/images/etcd/migrate.TestCtestMigrate.func1(0x140002b28c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cluster/images/etcd/migrate/ctest_integration_test.go:88 +0x7cc
testing.tRunner(0x140002b28c0, 0x14000595680)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 31
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cluster/images/etcd/migrate	121.167s
	k8s.io/kubernetes/cluster/images/etcd-version-monitor		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/clicheck		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/cloud-controller-manager		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/dependencycheck		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/dependencyverifier		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/fieldnamedocscheck		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/gendocs		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/genkubedocs	1.139s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/genman		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/genswaggertypedocs		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/genutils	1.276s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/genyaml		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/gotemplate	0.519s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/import-boss	0.267s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/importverifier		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kube-apiserver		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kube-apiserver/app		coverage: 0.0% of statements
=== RUN   TestCtestGetServiceIPAndRanges
Starting TestCtestGetServiceIPAndRanges with 24 cases
Running case #0: ""
W0209 18:38:41.697962   82497 options.go:369] No CIDR for service cluster IPs specified. Default value which was 10.0.0.0/24 is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.
Running case #1: "192.0.2.1/24"
Running case #2: "192.0.2.1/24,192.168.128.0/17"
Running case #3: "192.0.2.1/24,2001:db2:1:3:4::1/112"
Running case #4: "2001:db2:1:3:4::1/112,192.0.2.1/24"
Running case #5: "192.0.2.1/30,192.168.128.0/17"
Running case #6: "192.0.2.1/33,192.168.128.0/17"
Running case #7: "192.0.2.1/24,192.168.128.0/33"
Running case #8: "2001:db2:1:3:4::1/129,192.0.2.1/24"
Running case #9: "192.0.2.1/24,2001:db2:1:3:4::1/129"
Running case #10: "192.0.2.1,192.168.128.0/17"
Running case #11: "192.0.2.1/24,192.168.128.1"
Running case #12: "2001:db2:1:3:4::1,192.0.2.1/24"
Running case #13: "192.0.2.1/24,2001:db2:1:3:4::1"
Running case #14: "bad.ip.range,192.168.0.2/24"
Running case #15: "192.168.0.2/24,bad.ip.range"
Running case #16: " 192.0.2.1/24"
    ctest_completion_test.go:51: case 16: expected apiServerServiceIP: 192.0.2.1, got: <nil>
    ctest_completion_test.go:55: case 16: expected primaryServiceIPRange: 192.0.2.0/24, got: <nil>
    ctest_completion_test.go:63: case 16: expected err to be: false, but it was true
Running case #17: "192.0.2.1/24 "
    ctest_completion_test.go:51: case 17: expected apiServerServiceIP: 192.0.2.1, got: <nil>
    ctest_completion_test.go:55: case 17: expected primaryServiceIPRange: 192.0.2.0/24, got: <nil>
    ctest_completion_test.go:63: case 17: expected err to be: false, but it was true
Running case #18: "\t192.0.2.1/24\n"
    ctest_completion_test.go:51: case 18: expected apiServerServiceIP: 192.0.2.1, got: <nil>
    ctest_completion_test.go:55: case 18: expected primaryServiceIPRange: 192.0.2.0/24, got: <nil>
    ctest_completion_test.go:63: case 18: expected err to be: false, but it was true
Running case #19: ","
Running case #20: "192.0.2.1/24,,192.168.128.0/17"
Running case #21: "192.0.2.1/24, "
Running case #22: "invalid"
Running case #23: "192.0.2.1/24,2001:db2:1:3:4::1/112,10.0.0.0/8"
Completed TestCtestGetServiceIPAndRanges
--- FAIL: TestCtestGetServiceIPAndRanges (0.00s)
=== RUN   TestCtestClusterServiceIPRange
=== RUN   TestCtestClusterServiceIPRange/no_service_cidr
W0209 18:38:41.698382   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/only_secondary_service_cidr
W0209 18:38:41.698469   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v4-v4
W0209 18:38:41.698698   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v6-v6
W0209 18:38:41.698852   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/service_cidr_is_too_big
W0209 18:38:41.699053   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enbled
W0209 18:38:41.699172   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699234   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled
W0209 18:38:41.699340   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699359   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled#01
W0209 18:38:41.699400   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699417   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enabled
W0209 18:38:41.699451   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699473   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_gate_enabled
W0209 18:38:41.699509   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699524   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_and_gate_enabled
W0209 18:38:41.699557   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699574   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big
W0209 18:38:41.699610   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from true to false
W0209 18:38:41.699678   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_but_only_ipallocator_gate_enabled
W0209 18:38:41.699706   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699724   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_gate_enabled
W0209 18:38:41.699751   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.699766   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
=== RUN   TestCtestClusterServiceIPRange/more_than_two_entries
W0209 18:38:41.699793   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from true to false
W0209 18:38:41.699842   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_primary
W0209 18:38:41.699921   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_primary,_class_E_range
W0209 18:38:41.700006   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_v4-v6_dual_stack
W0209 18:38:41.700105   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/valid_v6-v4_dual_stack
W0209 18:38:41.700183   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/malformed_primary_CIDR
W0209 18:38:41.700253   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/malformed_secondary_CIDR
    ctest_validation_test.go:171: expected errors, no errors found
W0209 18:38:41.700349   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:MultiCIDRServiceAllocator from false to true
=== RUN   TestCtestClusterServiceIPRange/empty_CIDRs_with_gates_enabled_(should_succeed)
W0209 18:38:41.700381   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
W0209 18:38:41.700400   82497 feature_gate.go:352] Setting GA feature gate MultiCIDRServiceAllocator=true. It will be removed in a future release.
    ctest_validation_test.go:167: expected no errors, errors found [--service-cluster-ip-range must contain at least one valid cidr]
W0209 18:38:41.700425   82497 feature_gate.go:568] SetEmulationVersion will change already queried feature:DisableAllocatorDualWrite from false to true
--- FAIL: TestCtestClusterServiceIPRange (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/no_service_cidr (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/only_secondary_service_cidr (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v4-v4 (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/primary_and_secondary_are_provided_but_not_dual_stack_v6-v6 (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_is_too_big (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enbled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_only_ipallocator_gate_enabled#01 (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv4_is_too_big_but_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_but_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/service_cidr_IPv6_is_too_big_and_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_but_only_ipallocator_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/dual-stack_secondary_cidr_too_big_gate_enabled (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/more_than_two_entries (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_primary (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_primary,_class_E_range (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_v4-v6_dual_stack (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/valid_v6-v4_dual_stack (0.00s)
    --- PASS: TestCtestClusterServiceIPRange/malformed_primary_CIDR (0.00s)
    --- FAIL: TestCtestClusterServiceIPRange/malformed_secondary_CIDR (0.00s)
    --- FAIL: TestCtestClusterServiceIPRange/empty_CIDRs_with_gates_enabled_(should_succeed) (0.00s)
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv4_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv6_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families#01
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv4_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv6_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families#01
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/none_endpoint_reconciler_-_wrong_IP_families
=== RUN   TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/empty_endpoint_reconciler_type_with_matching_families
--- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv4_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_IPv6_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/master_endpoint_reconciler_-_wrong_IP_families#01 (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv4_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_IPv6_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/lease_endpoint_reconciler_-_wrong_IP_families#01 (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/none_endpoint_reconciler_-_wrong_IP_families (0.00s)
    --- PASS: TestCtestValidatePublicIPServiceClusterIPRangeIPFamilies/empty_endpoint_reconciler_type_with_matching_families (0.00s)
=== RUN   TestCtestValidateServiceNodePort
=== RUN   TestCtestValidateServiceNodePort/validate_port_less_than_0
=== RUN   TestCtestValidateServiceNodePort/validate_port_more_than_65535
=== RUN   TestCtestValidateServiceNodePort/validate_port_equal_0
=== RUN   TestCtestValidateServiceNodePort/validate_port_less_than_base
=== RUN   TestCtestValidateServiceNodePort/validate_port_minus_base_more_than_size
=== RUN   TestCtestValidateServiceNodePort/validate_success
=== RUN   TestCtestValidateServiceNodePort/zero_base_and_size
=== RUN   TestCtestValidateServiceNodePort/negative_base
--- PASS: TestCtestValidateServiceNodePort (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_less_than_0 (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_more_than_65535 (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_equal_0 (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_less_than_base (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_port_minus_base_more_than_size (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/validate_success (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/zero_base_and_size (0.00s)
    --- PASS: TestCtestValidateServiceNodePort/negative_base (0.00s)
FAIL
coverage: 52.6% of statements
FAIL	k8s.io/kubernetes/cmd/kube-apiserver/app/options	0.787s
	k8s.io/kubernetes/cmd/kube-apiserver/app/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kube-controller-manager		coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/cmd/kube-controller-manager/app [build failed]
	k8s.io/kubernetes/cmd/kube-controller-manager/app/config		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kube-controller-manager/app/options	0.794s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kube-controller-manager/app/testing		coverage: 0.0% of statements
?   	k8s.io/kubernetes/cmd/kube-controller-manager/names	[no test files]
	k8s.io/kubernetes/cmd/kube-proxy		coverage: 0.0% of statements
=== RUN   TestCtest_detectNodeIPs
=== RUN   TestCtest_detectNodeIPs/Bind_address_IPv4_unicast_address_and_no_Node_object
=== RUN   TestCtest_detectNodeIPs/Bind_address_IPv6_unicast_address_and_no_Node_object
=== RUN   TestCtest_detectNodeIPs/No_Valid_IP_found_and_no_bind_address
    server.go:684: I0209 18:39:03.279829] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/No_Valid_IP_found_and_unspecified_bind_address
    server.go:684: I0209 18:39:03.279906] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv4_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv4_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv6_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv6_InternalIP_set
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_primary_IPv4
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_primary_IPv6
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_IPv4
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_IPv6
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv4
    server.go:684: I0209 18:39:03.280087] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv6
    server.go:684: I0209 18:39:03.280112] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Invalid_bind_address,_fallback_to_defaults
    server.go:684: I0209 18:39:03.280142] Can't determine this node's IP, assuming loopback; if this is incorrect, please set the --bind-address flag
=== RUN   TestCtest_detectNodeIPs/Empty_bind_address_with_IPv4_node_IPs_present
--- PASS: TestCtest_detectNodeIPs (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_IPv4_unicast_address_and_no_Node_object (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_IPv6_unicast_address_and_no_Node_object (0.00s)
    --- PASS: TestCtest_detectNodeIPs/No_Valid_IP_found_and_no_bind_address (0.00s)
    --- PASS: TestCtest_detectNodeIPs/No_Valid_IP_found_and_unspecified_bind_address (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv4_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv4_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_0.0.0.0_and_node_with_IPv6_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Bind_address_::_and_node_with_IPv6_InternalIP_set (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_primary_IPv4 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_primary_IPv6 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_IPv4 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_IPv6 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv4 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Dual_stack,_override_primary_family,_IPv6 (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Invalid_bind_address,_fallback_to_defaults (0.00s)
    --- PASS: TestCtest_detectNodeIPs/Empty_bind_address_with_IPv4_node_IPs_present (0.00s)
=== RUN   TestCtest_checkBadConfig
=== RUN   TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_single-stack_config
=== RUN   TestCtest_checkBadConfig/dual-stack_NodePortAddresses_with_dual-stack_config
=== RUN   TestCtest_checkBadConfig/empty_NodePortAddresses
=== RUN   TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_dual-stack_config
=== RUN   TestCtest_checkBadConfig/wrong-single-stack_NodePortAddresses
=== RUN   TestCtest_checkBadConfig/nil_Config_should_error
--- FAIL: TestCtest_checkBadConfig (0.00s)
    --- PASS: TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_single-stack_config (0.00s)
    --- PASS: TestCtest_checkBadConfig/dual-stack_NodePortAddresses_with_dual-stack_config (0.00s)
    --- PASS: TestCtest_checkBadConfig/empty_NodePortAddresses (0.00s)
    --- PASS: TestCtest_checkBadConfig/single-stack_NodePortAddresses_with_dual-stack_config (0.00s)
    --- PASS: TestCtest_checkBadConfig/wrong-single-stack_NodePortAddresses (0.00s)
    --- FAIL: TestCtest_checkBadConfig/nil_Config_should_error (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x298 pc=0x1062b9334]

goroutine 184 [running]:
testing.tRunner.func1.2({0x1069abc60, 0x10828dc40})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1069abc60?, 0x10828dc40?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kube-proxy/app.checkBadConfig(0x14000513c30)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-proxy/app/server.go:295 +0xb4
k8s.io/kubernetes/cmd/kube-proxy/app.TestCtest_checkBadConfig.func1(0x14000186e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-proxy/app/ctest_server_test.go:281 +0x2c
testing.tRunner(0x14000186e00, 0x140006e1dd0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 178
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kube-proxy/app	1.440s
	k8s.io/kubernetes/cmd/kube-scheduler		coverage: 0.0% of statements
=== RUN   TestCtestSetup
=== RUN   TestCtestSetup/component_configuration_v1_with_only_scheduler_name_configured
W0209 18:39:12.298416   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:12.514830   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:12.723810   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:12.723823   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:12.723834   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/default_config
W0209 18:39:12.732597   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:12.852960   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:13.205263   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:13.205276   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:13.205292   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/component_configuration_v1
W0209 18:39:13.208081   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:13.507578   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:13.779609   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:13.779621   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:13.779628   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/out-of-tree_component_configuration_v1
W0209 18:39:13.781424   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:14.366907   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:14.546999   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:14.547013   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:14.547022   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_CLI_args,_along_with_--config_arg
W0209 18:39:14.548507   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:15.102439   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:15.369094   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:15.369105   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:15.369111   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_CLI_args,_without_--config_arg
W0209 18:39:15.371041   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:15.607795   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:16.198117   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:16.198131   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:16.198157   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_settings_specified_by_ComponentConfig_only
W0209 18:39:16.201462   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:16.312637   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:17.041199   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:17.041214   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:17.041221   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/leader_election_settings_specified_by_CLI_args_and_ComponentConfig
W0209 18:39:17.042550   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:17.931024   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:18.146384   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:18.146397   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:18.146403   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/emulated_version_out_of_range
W0209 18:39:18.150822   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
=== RUN   TestCtestSetup/default_feature_gates_at_binary_version
W0209 18:39:18.151386   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:18.434826   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:18.727858   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:18.727872   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:18.727883   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/default_feature_gates_at_emulated_version
W0209 18:39:18.730931   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:19.444761   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:20.201485   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:20.201502   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:20.201514   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/set_feature_gates_at_emulated_version
W0209 18:39:20.203466   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
W0209 18:39:20.204068   82664 registry.go:412] component has alpha features enabled in emulated version, this is unsupported: features=[kubeB]
I0209 18:39:20.388343   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:20.742634   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:20.742646   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:20.742651   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
=== RUN   TestCtestSetup/cannot_set_locked_feature_gate
W0209 18:39:20.745109   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
=== RUN   TestCtestSetup/non‑existent_config_file_should_error
W0209 18:39:20.745541   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:20.942111   82664 serving.go:386] Generated self-signed cert in-memory
=== RUN   TestCtestSetup/malformed_yaml_config_should_error
W0209 18:39:20.943269   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:21.258560   82664 serving.go:386] Generated self-signed cert in-memory
=== RUN   TestCtestSetup/invalid_feature‑gate_name_should_error
W0209 18:39:21.260576   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
=== RUN   TestCtestSetup/negative_lease_duration_should_error
W0209 18:39:21.260841   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:21.716319   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:21.975007   82664 authentication.go:368] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0209 18:39:21.975020   82664 authentication.go:392] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0209 18:39:21.975236   82664 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
    ctest_server_test.go:500: expected Setup error, got nil
=== RUN   TestCtestSetup/missing_kubeconfig_and_no_config_should_error
W0209 18:39:21.985551   82664 registry.go:335] setting componentGlobalsRegistry in SetFallback. We recommend calling componentGlobalsRegistry.Set() right after parsing flags to avoid using feature gates before their final values are set by the flags.
I0209 18:39:22.597855   82664 serving.go:386] Generated self-signed cert in-memory
W0209 18:39:22.603213   82664 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
W0209 18:39:22.603243   82664 client_config.go:672] error creating inClusterConfig, falling back to default config: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined
--- FAIL: TestCtestSetup (10.32s)
    --- PASS: TestCtestSetup/component_configuration_v1_with_only_scheduler_name_configured (0.43s)
    --- PASS: TestCtestSetup/default_config (0.47s)
    --- PASS: TestCtestSetup/component_configuration_v1 (0.57s)
    --- PASS: TestCtestSetup/out-of-tree_component_configuration_v1 (0.77s)
    --- PASS: TestCtestSetup/leader_election_CLI_args,_along_with_--config_arg (0.82s)
    --- PASS: TestCtestSetup/leader_election_CLI_args,_without_--config_arg (0.83s)
    --- PASS: TestCtestSetup/leader_election_settings_specified_by_ComponentConfig_only (0.84s)
    --- PASS: TestCtestSetup/leader_election_settings_specified_by_CLI_args_and_ComponentConfig (1.11s)
    --- PASS: TestCtestSetup/emulated_version_out_of_range (0.00s)
    --- PASS: TestCtestSetup/default_feature_gates_at_binary_version (0.58s)
    --- PASS: TestCtestSetup/default_feature_gates_at_emulated_version (1.47s)
    --- PASS: TestCtestSetup/set_feature_gates_at_emulated_version (0.54s)
    --- PASS: TestCtestSetup/cannot_set_locked_feature_gate (0.00s)
    --- PASS: TestCtestSetup/non‑existent_config_file_should_error (0.20s)
    --- PASS: TestCtestSetup/malformed_yaml_config_should_error (0.32s)
    --- PASS: TestCtestSetup/invalid_feature‑gate_name_should_error (0.00s)
    --- FAIL: TestCtestSetup/negative_lease_duration_should_error (0.72s)
    --- PASS: TestCtestSetup/missing_kubeconfig_and_no_config_should_error (0.62s)
FAIL
coverage: 12.8% of statements
FAIL	k8s.io/kubernetes/cmd/kube-scheduler/app	11.449s
	k8s.io/kubernetes/cmd/kube-scheduler/app/config		coverage: 0.0% of statements
=== RUN   TestCtestLoadConfigFromFile
=== RUN   TestCtestLoadConfigFromFile/case_0:_Empty_scheduler_config_file_path
=== RUN   TestCtestLoadConfigFromFile/case_1:_Correct_scheduler_config
=== RUN   TestCtestLoadConfigFromFile/case_2:_Scheduler_config_with_decode_error
=== RUN   TestCtestLoadConfigFromFile/case_3:_Scheduler_config_version_too_old
=== RUN   TestCtestLoadConfigFromFile/case_4:_Empty_file_content
    ctest_configfile_test.go:115: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-scheduler/app/options/ctest_configfile_test.go:115
        	Error:      	Error "Object 'Kind' is missing in ''" does not contain "'apiVersion' is missing"
        	Test:       	TestCtestLoadConfigFromFile/case_4:_Empty_file_content
=== RUN   TestCtestLoadConfigFromFile/case_5:_Invalid_YAML_format
    ctest_configfile_test.go:115: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kube-scheduler/app/options/ctest_configfile_test.go:115
        	Error:      	Error "Object 'Kind' is missing in ':::'" does not contain "'apiVersion' is missing"
        	Test:       	TestCtestLoadConfigFromFile/case_5:_Invalid_YAML_format
--- FAIL: TestCtestLoadConfigFromFile (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_0:_Empty_scheduler_config_file_path (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_1:_Correct_scheduler_config (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_2:_Scheduler_config_with_decode_error (0.00s)
    --- PASS: TestCtestLoadConfigFromFile/case_3:_Scheduler_config_version_too_old (0.00s)
    --- FAIL: TestCtestLoadConfigFromFile/case_4:_Empty_file_content (0.00s)
    --- FAIL: TestCtestLoadConfigFromFile/case_5:_Invalid_YAML_format (0.00s)
FAIL
coverage: 4.8% of statements
FAIL	k8s.io/kubernetes/cmd/kube-scheduler/app/options	0.610s
	k8s.io/kubernetes/cmd/kube-scheduler/app/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app		coverage: 0.0% of statements
?   	k8s.io/kubernetes/cmd/kubeadm/app/apis/bootstraptoken	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/bootstraptoken/v1	0.294s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestAPIEndpointFromString
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:1234
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:-1
=== RUN   TestCtestAPIEndpointFromString/1.2.3.::1234
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:65536
=== RUN   TestCtestAPIEndpointFromString/1.2.3.456:1234
=== RUN   TestCtestAPIEndpointFromString/[::1]:1234
=== RUN   TestCtestAPIEndpointFromString/[::1]:-1
=== RUN   TestCtestAPIEndpointFromString/[::1]:65536
=== RUN   TestCtestAPIEndpointFromString/[::1:1234
=== RUN   TestCtestAPIEndpointFromString/[::g]:1234
=== RUN   TestCtestAPIEndpointFromString/#00
=== RUN   TestCtestAPIEndpointFromString/localhost
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:
    ctest_apiendpoint_test.go:39: expected error true, got false, error: <nil>
=== RUN   TestCtestAPIEndpointFromString/1.2.3.4:0
=== RUN   TestCtestAPIEndpointFromString/[::1]
=== RUN   TestCtestAPIEndpointFromString/[::1]:
    ctest_apiendpoint_test.go:39: expected error true, got false, error: <nil>
=== RUN   TestCtestAPIEndpointFromString/[::1]:0
--- FAIL: TestCtestAPIEndpointFromString (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:-1 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.::1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:65536 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.456:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:-1 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:65536 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::g]:1234 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/#00 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/localhost (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4 (0.00s)
    --- FAIL: TestCtestAPIEndpointFromString/1.2.3.4: (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/1.2.3.4:0 (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1] (0.00s)
    --- FAIL: TestCtestAPIEndpointFromString/[::1]: (0.00s)
    --- PASS: TestCtestAPIEndpointFromString/[::1]:0 (0.00s)
=== RUN   TestCtestString
=== RUN   TestCtestString/ipv4_and_port
=== RUN   TestCtestString/ipv6_and_port
=== RUN   TestCtestString/ipv4_zero_port
=== RUN   TestCtestString/ipv6_zero_port
=== RUN   TestCtestString/empty_address
--- PASS: TestCtestString (0.00s)
    --- PASS: TestCtestString/ipv4_and_port (0.00s)
    --- PASS: TestCtestString/ipv6_and_port (0.00s)
    --- PASS: TestCtestString/ipv4_zero_port (0.00s)
    --- PASS: TestCtestString/ipv6_zero_port (0.00s)
    --- PASS: TestCtestString/empty_address (0.00s)
=== RUN   TestCtestGetArgValue

==================== CTEST START ====================
Running 0 th test case: argument exists with non-empty value
=== RUN   TestCtestGetArgValue/argument_exists_with_non-empty_value
Running 1 th test case: argument exists with non-empty value (offset index)
=== RUN   TestCtestGetArgValue/argument_exists_with_non-empty_value_(offset_index)
Running 2 th test case: argument exists with empty value
=== RUN   TestCtestGetArgValue/argument_exists_with_empty_value
Running 3 th test case: argument does not exists
=== RUN   TestCtestGetArgValue/argument_does_not_exists
Running 4 th test case: empty args slice
=== RUN   TestCtestGetArgValue/empty_args_slice
Running 5 th test case: startIdx beyond slice length
=== RUN   TestCtestGetArgValue/startIdx_beyond_slice_length
    ctest_argument_test.go:94: expected index: -1, got: 0
    ctest_argument_test.go:97: expected value: , got: a1
Running 6 th test case: name empty string with match
=== RUN   TestCtestGetArgValue/name_empty_string_with_match
Running 7 th test case: multiple matching names, startIdx selects second
=== RUN   TestCtestGetArgValue/multiple_matching_names,_startIdx_selects_second

==================== CTEST END ======================
--- FAIL: TestCtestGetArgValue (0.00s)
    --- PASS: TestCtestGetArgValue/argument_exists_with_non-empty_value (0.00s)
    --- PASS: TestCtestGetArgValue/argument_exists_with_non-empty_value_(offset_index) (0.00s)
    --- PASS: TestCtestGetArgValue/argument_exists_with_empty_value (0.00s)
    --- PASS: TestCtestGetArgValue/argument_does_not_exists (0.00s)
    --- PASS: TestCtestGetArgValue/empty_args_slice (0.00s)
    --- FAIL: TestCtestGetArgValue/startIdx_beyond_slice_length (0.00s)
    --- PASS: TestCtestGetArgValue/name_empty_string_with_match (0.00s)
    --- PASS: TestCtestGetArgValue/multiple_matching_names,_startIdx_selects_second (0.00s)
=== RUN   TestCtestSetArgValues

==================== CTEST START ====================
Running 0 th test case: update 1 argument
=== RUN   TestCtestSetArgValues/update_1_argument
Running 1 th test case: update all arguments
=== RUN   TestCtestSetArgValues/update_all_arguments
Running 2 th test case: add new argument
=== RUN   TestCtestSetArgValues/add_new_argument
Running 3 th test case: no matching arguments, nArgs=0 (no change)
=== RUN   TestCtestSetArgValues/no_matching_arguments,_nArgs=0_(no_change)
    ctest_argument_test.go:179: expected args: []kubeadm.Arg{kubeadm.Arg{Name:"a", Value:"1"}, kubeadm.Arg{Name:"b", Value:"2"}}, got: []kubeadm.Arg{kubeadm.Arg{Name:"a", Value:"1"}, kubeadm.Arg{Name:"b", Value:"2"}, kubeadm.Arg{Name:"c", Value:"new"}}
Running 4 th test case: empty args slice, add new argument
=== RUN   TestCtestSetArgValues/empty_args_slice,_add_new_argument
Running 5 th test case: negative nArgs other than -1 (treated as all)
=== RUN   TestCtestSetArgValues/negative_nArgs_other_than_-1_(treated_as_all)
Running 6 th test case: multiple matching names, nArgs=2 updates first two
=== RUN   TestCtestSetArgValues/multiple_matching_names,_nArgs=2_updates_first_two
    ctest_argument_test.go:179: expected args: []kubeadm.Arg{kubeadm.Arg{Name:"dup", Value:"updated"}, kubeadm.Arg{Name:"dup", Value:"updated"}, kubeadm.Arg{Name:"dup", Value:"third"}}, got: []kubeadm.Arg{kubeadm.Arg{Name:"dup", Value:"first"}, kubeadm.Arg{Name:"dup", Value:"updated"}, kubeadm.Arg{Name:"dup", Value:"updated"}}

==================== CTEST END ======================
--- FAIL: TestCtestSetArgValues (0.00s)
    --- PASS: TestCtestSetArgValues/update_1_argument (0.00s)
    --- PASS: TestCtestSetArgValues/update_all_arguments (0.00s)
    --- PASS: TestCtestSetArgValues/add_new_argument (0.00s)
    --- FAIL: TestCtestSetArgValues/no_matching_arguments,_nArgs=0_(no_change) (0.00s)
    --- PASS: TestCtestSetArgValues/empty_args_slice,_add_new_argument (0.00s)
    --- PASS: TestCtestSetArgValues/negative_nArgs_other_than_-1_(treated_as_all) (0.00s)
    --- FAIL: TestCtestSetArgValues/multiple_matching_names,_nArgs=2_updates_first_two (0.00s)
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType

==================== CTEST START ====================
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_true,_return_ECDSA-P256
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_false,_return_the_default_RSA-2048
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_not_set,_return_the_field_value
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_and_field_are_not_set,_return_empty_string
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_empty,_expect_ECDSA-P256
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_invalid,_expect_ECDSA-P256
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_empty,_expect_default_RSA-2048
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_invalid,_expect_invalid_value
    ctest_types_test.go:108: expected result: unsupported-algo, got: RSA-2048
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_set,_expect_field_value
=== RUN   TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_empty,_expect_empty_string

==================== CTEST END ======================
--- FAIL: TestCtestClusterConfigurationEncryptionAlgorithmType (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_true,_return_ECDSA-P256 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_set_to_false,_return_the_default_RSA-2048 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_is_not_set,_return_the_field_value (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_and_field_are_not_set,_return_empty_string (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_empty,_expect_ECDSA-P256 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_true,_field_invalid,_expect_ECDSA-P256 (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_empty,_expect_default_RSA-2048 (0.00s)
    --- FAIL: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_false,_field_invalid,_expect_invalid_value (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_set,_expect_field_value (0.00s)
    --- PASS: TestCtestClusterConfigurationEncryptionAlgorithmType/feature_gate_map_nil,_field_empty,_expect_empty_string (0.00s)
FAIL
coverage: 6.6% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm	0.864s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/fuzzer	0.357s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme		coverage: 0.0% of statements
=== RUN   TestCtestConvertToArgs
[DEBUG-CTEST 2026-02-09 18:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:14]: Start TestCtestConvertToArgs
[DEBUG-CTEST 2026-02-09 18:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:40]: Number of test cases: 3
Running 0 th test case.
{nil map returns nil args map[] []}
=== RUN   TestCtestConvertToArgs/nil_map_returns_nil_args
Running 1 th test case.
{empty map returns nil args map[] []}
=== RUN   TestCtestConvertToArgs/empty_map_returns_nil_args
    ctest_conversion_test.go:47: expected args: []
        	 got: []
        	
Running 2 th test case.
{valid args are parsed (sorted) map[a:b c:d] [{a b} {c d}]}
=== RUN   TestCtestConvertToArgs/valid_args_are_parsed_(sorted)
--- FAIL: TestCtestConvertToArgs (0.00s)
    --- PASS: TestCtestConvertToArgs/nil_map_returns_nil_args (0.00s)
    --- FAIL: TestCtestConvertToArgs/empty_map_returns_nil_args (0.00s)
    --- PASS: TestCtestConvertToArgs/valid_args_are_parsed_(sorted) (0.00s)
=== RUN   TestCtestConvertFromArgs
[DEBUG-CTEST 2026-02-09 18:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:54]: Start TestCtestConvertFromArgs
[DEBUG-CTEST 2026-02-09 18:39:19 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta3/ctest_conversion_test.go:88]: Number of test cases: 4
Running 0 th test case.
{nil args return nil map [] map[]}
=== RUN   TestCtestConvertFromArgs/nil_args_return_nil_map
Running 1 th test case.
{empty args return nil map [] map[]}
=== RUN   TestCtestConvertFromArgs/empty_args_return_nil_map
    ctest_conversion_test.go:95: expected args: map[]
        	 got: map[]
        	
Running 2 th test case.
{valid args are parsed [{a b} {c d}] map[a:b c:d]}
=== RUN   TestCtestConvertFromArgs/valid_args_are_parsed
Running 3 th test case.
{duplicates are dropped [{a b} {c d1} {c d2}] map[a:b c:d2]}
=== RUN   TestCtestConvertFromArgs/duplicates_are_dropped
--- FAIL: TestCtestConvertFromArgs (0.00s)
    --- PASS: TestCtestConvertFromArgs/nil_args_return_nil_map (0.00s)
    --- FAIL: TestCtestConvertFromArgs/empty_args_return_nil_map (0.00s)
    --- PASS: TestCtestConvertFromArgs/valid_args_are_parsed (0.00s)
    --- PASS: TestCtestConvertFromArgs/duplicates_are_dropped (0.00s)
FAIL
coverage: 2.3% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta3	1.730s
	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/validation	1.191s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/apis/output		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/apis/output/fuzzer	1.154s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/apis/output/scheme		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app/apis/output/v1alpha3		coverage: 0.0% of statements
=== RUN   TestCtestNewCmdCompletion
--- PASS: TestCtestNewCmdCompletion (0.00s)
=== RUN   TestCtestRunCompletion
=== RUN   TestCtestRunCompletion/invalid:_missing_argument
=== RUN   TestCtestRunCompletion/invalid:_too_many_arguments
=== RUN   TestCtestRunCompletion/invalid:_unsupported_shell_name
=== RUN   TestCtestRunCompletion/invalid:_empty_string_argument
=== RUN   TestCtestRunCompletion/invalid:_excessively_long_shell_name
=== RUN   TestCtestRunCompletion/invalid:_special_characters_in_shell_name
=== RUN   TestCtestRunCompletion/valid:_test_shell_bash
=== RUN   TestCtestRunCompletion/valid:_test_shell_zsh
--- PASS: TestCtestRunCompletion (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_missing_argument (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_too_many_arguments (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_unsupported_shell_name (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_empty_string_argument (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_excessively_long_shell_name (0.00s)
    --- PASS: TestCtestRunCompletion/invalid:_special_characters_in_shell_name (0.00s)
    --- PASS: TestCtestRunCompletion/valid:_test_shell_bash (0.00s)
    --- PASS: TestCtestRunCompletion/valid:_test_shell_zsh (0.00s)
=== RUN   TestCtestNewJoinData

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "cfg" in requested fixtures
2026/02/09 18:39:43 [DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:39:43 Mode: 1
2026/02/09 18:39:43 Base JSON size: 783 bytes
2026/02/09 18:39:43 Number of external values: 0
2026/02/09 18:39:43 [DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:39:43 [DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"/etc/kubernetes/pki/ca.crt","ControlPlane":{"CertificateKey":"c39a18bae4a72e71b178661f437363da218a3efb83ddb03f1cd91d9ae1da41bd","LocalAPIEndpoint":{"AdvertiseAddress":"","BindPort":0}},"Discovery":{"BootstrapToken":{"APIServerEndpoint":"1.2.3.4:6443","CACertHashes":null,"Token":"abcdef.0123456789abcdef","UnsafeSkipCAVerification":true},"File":null,"TLSBootstrapToken":"abcdef.0123456789abcdef","Timeout":"5m0s"},"DryRun":false,"NodeRegistration":{"CRISocket":"unix://var/run/containerd/containerd.sock","IgnorePreflightErrors":["c","d"],"ImagePullSerial":true,"KubeletExtraArgs":null,"Name":"somename","Taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"}],"imagePullPolicy":"IfNotPresent"},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_join_test.go:79]: New Json Test Configs: 

==================== CTEST END ======================
=== RUN   TestCtestNewJoinData/fails_if_no_discovery_method_set
=== RUN   TestCtestNewJoinData/fails_if_both_file_and_bootstrap_discovery_methods_set
=== RUN   TestCtestNewJoinData/pass_if_file_discovery_is_set
=== RUN   TestCtestNewJoinData/pass_if_bootstrap_discovery_is_set
  W0209 18:39:43.552621   82906 join.go:382] [preflight] WARNING: More than one API server endpoint supplied on command line [1.2.3.4:6443 5.6.7.8:6443]. Using the first one.
=== RUN   TestCtestNewJoinData/--token_sets_TLSBootstrapToken_and_BootstrapToken.Token_if_unset
=== RUN   TestCtestNewJoinData/--token_doesn't_override_TLSBootstrapToken_and_BootstrapToken.Token_if_set
=== RUN   TestCtestNewJoinData/control_plane_setting_are_preserved_if_--control-plane_flag_is_set
=== RUN   TestCtestNewJoinData/control_plane_setting_are_cleaned_up_if_--control-plane_flag_is_not_set
  W0209 18:39:43.553109   82906 join.go:402] [preflight] WARNING: --control-plane is also required when passing control-plane related flags such as [certificate-key, apiserver-advertise-address, apiserver-bind-port]
    ctest_join_test.go:308: expected warning "WARNING: --control-plane is also required when passing control-plane", got ""
=== RUN   TestCtestNewJoinData/fails_if_invalid_preflight_checks_are_provided
=== RUN   TestCtestNewJoinData/Pass_with_config_from_file
=== RUN   TestCtestNewJoinData/--node-name_flags_override_config_from_file
=== RUN   TestCtestNewJoinData/fail_if_mixedArguments_are_passed
=== RUN   TestCtestNewJoinData/pre-flights_errors_from_CLI_args_only
=== RUN   TestCtestNewJoinData/pre-flights_errors_from_JoinConfiguration_only
=== RUN   TestCtestNewJoinData/pre-flights_errors_from_both_CLI_args_and_JoinConfiguration
=== RUN   TestCtestNewJoinData/warn_if_--control-plane_flag_is_not_set
  W0209 18:39:43.554593   82906 join.go:402] [preflight] WARNING: --control-plane is also required when passing control-plane related flags such as [certificate-key, apiserver-advertise-address, apiserver-bind-port]
    ctest_join_test.go:308: expected warning "WARNING: --control-plane is also required when passing control-plane", got ""
=== RUN   TestCtestNewJoinData/no_warn_if_--control-plane_flag_is_set
=== RUN   TestCtestNewJoinData/fails_if_config_file_is_invalid_yaml
--- FAIL: TestCtestNewJoinData (0.01s)
    --- PASS: TestCtestNewJoinData/fails_if_no_discovery_method_set (0.00s)
    --- PASS: TestCtestNewJoinData/fails_if_both_file_and_bootstrap_discovery_methods_set (0.00s)
    --- PASS: TestCtestNewJoinData/pass_if_file_discovery_is_set (0.00s)
    --- PASS: TestCtestNewJoinData/pass_if_bootstrap_discovery_is_set (0.00s)
    --- PASS: TestCtestNewJoinData/--token_sets_TLSBootstrapToken_and_BootstrapToken.Token_if_unset (0.00s)
    --- PASS: TestCtestNewJoinData/--token_doesn't_override_TLSBootstrapToken_and_BootstrapToken.Token_if_set (0.00s)
    --- PASS: TestCtestNewJoinData/control_plane_setting_are_preserved_if_--control-plane_flag_is_set (0.00s)
    --- FAIL: TestCtestNewJoinData/control_plane_setting_are_cleaned_up_if_--control-plane_flag_is_not_set (0.00s)
    --- PASS: TestCtestNewJoinData/fails_if_invalid_preflight_checks_are_provided (0.00s)
    --- PASS: TestCtestNewJoinData/Pass_with_config_from_file (0.00s)
    --- PASS: TestCtestNewJoinData/--node-name_flags_override_config_from_file (0.00s)
    --- PASS: TestCtestNewJoinData/fail_if_mixedArguments_are_passed (0.00s)
    --- PASS: TestCtestNewJoinData/pre-flights_errors_from_CLI_args_only (0.00s)
    --- PASS: TestCtestNewJoinData/pre-flights_errors_from_JoinConfiguration_only (0.00s)
    --- PASS: TestCtestNewJoinData/pre-flights_errors_from_both_CLI_args_and_JoinConfiguration (0.00s)
    --- FAIL: TestCtestNewJoinData/warn_if_--control-plane_flag_is_not_set (0.00s)
    --- PASS: TestCtestNewJoinData/no_warn_if_--control-plane_flag_is_set (0.00s)
    --- PASS: TestCtestNewJoinData/fails_if_config_file_is_invalid_yaml (0.00s)
=== RUN   TestCtestKubeConfigSubCommandsThatWritesToOut

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_kubeconfig_test.go:106]: get default configs: {test_fixture.json [default kubeadm config spec] kubeadmConfigSpec [pods] {1.2.3.4 1234}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "kubeadmConfigSpec" in requested fixtures
2026/02/09 18:39:43 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:43 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"AdvertiseAddress":"1.2.3.4","BindPort":1234})[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_kubeconfig_test.go:115]: Skipping test execution. No new configurations generated. 

==================== CTEST END ======================
--- PASS: TestCtestKubeConfigSubCommandsThatWritesToOut (0.00s)
=== RUN   TestCtestNewCmdVersion
--- PASS: TestCtestNewCmdVersion (0.00s)
=== RUN   TestCtestRunVersion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:99]: Running TestCtestRunVersion with 10 cases
Running 0 th test case: valid: run without flags
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run without flags  false false false}
=== RUN   TestCtestRunVersion/valid:_run_without_flags
Running 1 th test case: valid: run with flag 'short'
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run with flag 'short' short false false false}
=== RUN   TestCtestRunVersion/valid:_run_with_flag_'short'
Running 2 th test case: valid: run with flag 'yaml'
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run with flag 'yaml' yaml false true false}
=== RUN   TestCtestRunVersion/valid:_run_with_flag_'yaml'
Running 3 th test case: valid: run with flag 'json'
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {valid: run with flag 'json' json false false true}
=== RUN   TestCtestRunVersion/valid:_run_with_flag_'json'
Running 4 th test case: invalid: run with unsupported flag
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: run with unsupported flag unsupported-flag true false false}
=== RUN   TestCtestRunVersion/invalid:_run_with_unsupported_flag
Running 5 th test case: invalid: empty flag string (explicit)
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: empty flag string (explicit)  false false false}
=== RUN   TestCtestRunVersion/invalid:_empty_flag_string_(explicit)
    ctest_version_test.go:125: Test case "invalid: empty flag string (explicit)": RunVersion expected error: false, saw: true; invalid output format: unsupported-flag
Running 6 th test case: invalid: whitespace flag
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: whitespace flag   true false false}
=== RUN   TestCtestRunVersion/invalid:_whitespace_flag
Running 7 th test case: invalid: excessively long flag
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: excessively long flag                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  true false false}
=== RUN   TestCtestRunVersion/invalid:_excessively_long_flag
Running 8 th test case: invalid: numeric flag
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: numeric flag 12345 true false false}
=== RUN   TestCtestRunVersion/invalid:_numeric_flag
Running 9 th test case: invalid: flag with newline
[DEBUG-CTEST 2026-02-09 18:39:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/ctest_version_test.go:102]: {invalid: flag with newline json
 true false false}
=== RUN   TestCtestRunVersion/invalid:_flag_with_newline

==================== CTEST END ======================
--- FAIL: TestCtestRunVersion (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_without_flags (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_with_flag_'short' (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_with_flag_'yaml' (0.00s)
    --- PASS: TestCtestRunVersion/valid:_run_with_flag_'json' (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_run_with_unsupported_flag (0.00s)
    --- FAIL: TestCtestRunVersion/invalid:_empty_flag_string_(explicit) (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_whitespace_flag (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_excessively_long_flag (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_numeric_flag (0.00s)
    --- PASS: TestCtestRunVersion/invalid:_flag_with_newline (0.00s)
FAIL
coverage: 11.9% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd	1.562s
	k8s.io/kubernetes/cmd/kubeadm/app/cmd/alpha		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/options	0.478s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestSetKubernetesVersion

==================== CTEST START ====================
Running test case 0: empty version is processed
=== RUN   TestCtestSetKubernetesVersion/empty_version_is_processed
Running test case 1: default version is processed
=== RUN   TestCtestSetKubernetesVersion/default_version_is_processed
Running test case 2: any other version is skipped
=== RUN   TestCtestSetKubernetesVersion/any_other_version_is_skipped
Running test case 3: whitespace version is treated as non-empty
=== RUN   TestCtestSetKubernetesVersion/whitespace_version_is_treated_as_non-empty
Running test case 4: malformed version string is skipped
=== RUN   TestCtestSetKubernetesVersion/malformed_version_string_is_skipped
Running test case 5: very long version string is skipped
=== RUN   TestCtestSetKubernetesVersion/very_long_version_string_is_skipped
Running test case 6: pre-release version is skipped
=== RUN   TestCtestSetKubernetesVersion/pre-release_version_is_skipped
Running test case 7: zero version is processed as empty after trim
=== RUN   TestCtestSetKubernetesVersion/zero_version_is_processed_as_empty_after_trim

==================== CTEST END ======================
--- PASS: TestCtestSetKubernetesVersion (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/empty_version_is_processed (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/default_version_is_processed (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/any_other_version_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/whitespace_version_is_treated_as_non-empty (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/malformed_version_string_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/very_long_version_string_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/pre-release_version_is_skipped (0.00s)
    --- PASS: TestCtestSetKubernetesVersion/zero_version_is_processed_as_empty_after_trim (0.00s)
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases	2.761s	coverage: 100.0% of statements
=== RUN   TestCtestGetAddonPhaseFlags

==================== CTEST START ====================
Running 0 th test case: all
=== RUN   TestCtestGetAddonPhaseFlags/all
Running 1 th test case: kube-proxy
=== RUN   TestCtestGetAddonPhaseFlags/kube-proxy
Running 2 th test case: coredns
=== RUN   TestCtestGetAddonPhaseFlags/coredns
Running 3 th test case: invalid_name
=== RUN   TestCtestGetAddonPhaseFlags/invalid_name
Running 4 th test case: 
=== RUN   TestCtestGetAddonPhaseFlags/#00
Running 5 th test case:    
=== RUN   TestCtestGetAddonPhaseFlags/___
Running 6 th test case: unknown-addon
=== RUN   TestCtestGetAddonPhaseFlags/unknown-addon
Running 7 th test case: extremely-long-addon-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
=== RUN   TestCtestGetAddonPhaseFlags/extremely-long-addon-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

==================== CTEST END ======================
--- PASS: TestCtestGetAddonPhaseFlags (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/all (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/kube-proxy (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/coredns (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/invalid_name (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/#00 (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/___ (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/unknown-addon (0.00s)
    --- PASS: TestCtestGetAddonPhaseFlags/extremely-long-addon-name-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (0.00s)
PASS
coverage: 1.6% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/init	3.549s	coverage: 1.6% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join	2.062s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGetEtcdDataDir

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:39:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/phases/reset/ctest_removeetcdmember_test.go:119]: Prepared pod yaml configurations for test cases
[DEBUG-CTEST 2026-02-09 18:39:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/cmd/phases/reset/ctest_removeetcdmember_test.go:120]: Number of test cases: 7
=== RUN   TestCtestGetEtcdDataDir/no_manifest_written
  W0209 18:39:47.029475   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
    ctest_removeetcdmember_test.go:145: getEtcdDataDir failed
        no manifest written
        expected error: true
        	got: false
        error: <nil>
=== RUN   TestCtestGetEtcdDataDir/non-existent_file_returns_default_data_dir
  W0209 18:39:47.029863   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/return_etcd_data_dir
  W0209 18:39:47.030183   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
    ctest_removeetcdmember_test.go:145: getEtcdDataDir failed
        return etcd data dir
        expected error: false
        	got: true
        error: failed to unmarshal manifest for "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGetEtcdDataDirreturn_etcd_data_dir1161003111/001/etcd.yaml": failed to decode metadata: {}
        spec:
          containers: null
          volumes:
          - hostPath:
              path: /path/to/etcd
              type: DirectoryOrCreate
            name: etcd-data
          - hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
            name: etcd-certs
        status: {}
         into runtime.Object: Object 'Kind' is missing in 'metadata: {}
        spec:
          containers: null
          volumes:
          - hostPath:
              path: /path/to/etcd
              type: DirectoryOrCreate
            name: etcd-data
          - hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
            name: etcd-certs
        status: {}
        '
=== RUN   TestCtestGetEtcdDataDir/invalid_etcd_pod
  W0209 18:39:47.031360   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/etcd_pod_spec_without_data_volume
  W0209 18:39:47.031776   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
=== RUN   TestCtestGetEtcdDataDir/kubeconfig_file_doesn't_exist
  W0209 18:39:47.032111   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory
    ctest_removeetcdmember_test.go:145: getEtcdDataDir failed
        kubeconfig file doesn't exist
        expected error: false
        	got: true
        error: failed to unmarshal manifest for "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGetEtcdDataDirkubeconfig_file_doesnt_exist2476238270/001/etcd.yaml": failed to decode  into runtime.Object: Object 'Kind' is missing in ''
=== RUN   TestCtestGetEtcdDataDir/empty_pod_yaml
  W0209 18:39:47.032479   82947 removeetcdmember.go:105] [reset] No kubeadm config, using etcd pod spec to get data directory

==================== CTEST END ======================
--- FAIL: TestCtestGetEtcdDataDir (0.01s)
    --- FAIL: TestCtestGetEtcdDataDir/no_manifest_written (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/non-existent_file_returns_default_data_dir (0.00s)
    --- FAIL: TestCtestGetEtcdDataDir/return_etcd_data_dir (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/invalid_etcd_pod (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/etcd_pod_spec_without_data_volume (0.00s)
    --- FAIL: TestCtestGetEtcdDataDir/kubeconfig_file_doesn't_exist (0.00s)
    --- PASS: TestCtestGetEtcdDataDir/empty_pod_yaml (0.00s)
FAIL
coverage: 7.6% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/reset	3.023s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/upgrade	2.133s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/upgrade/apply	1.302s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/upgrade/node	0.540s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestComputePhaseRunFlags
=== RUN   TestCtestComputePhaseRunFlags/no_options_>_all_phases
=== RUN   TestCtestComputePhaseRunFlags/options_can_filter_phases
=== RUN   TestCtestComputePhaseRunFlags/options_can_filter_phases_-_hierarchy_is_considered
=== RUN   TestCtestComputePhaseRunFlags/options_can_skip_phases
=== RUN   TestCtestComputePhaseRunFlags/options_can_skip_phases_-_hierarchy_is_considered
=== RUN   TestCtestComputePhaseRunFlags/skip_options_have_higher_precedence_than_filter_options
=== RUN   TestCtestComputePhaseRunFlags/invalid_filter_option
=== RUN   TestCtestComputePhaseRunFlags/invalid_skip_option
=== RUN   TestCtestComputePhaseRunFlags/empty_filter_and_skip_slices_should_behave_like_no_options
=== RUN   TestCtestComputePhaseRunFlags/duplicate_filter_entries_are_ignored
=== RUN   TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_filter_entries_should_error
=== RUN   TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_skip_entries_should_error
--- PASS: TestCtestComputePhaseRunFlags (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/no_options_>_all_phases (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_filter_phases (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_filter_phases_-_hierarchy_is_considered (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_skip_phases (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/options_can_skip_phases_-_hierarchy_is_considered (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/skip_options_have_higher_precedence_than_filter_options (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/invalid_filter_option (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/invalid_skip_option (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/empty_filter_and_skip_slices_should_behave_like_no_options (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/duplicate_filter_entries_are_ignored (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_filter_entries_should_error (0.00s)
    --- PASS: TestCtestComputePhaseRunFlags/mixed_valid_and_invalid_skip_entries_should_error (0.00s)
=== RUN   TestCtestRunOrderAndConditions
=== RUN   TestCtestRunOrderAndConditions/Run_respect_runCondition
=== RUN   TestCtestRunOrderAndConditions/Run_takes_options_into_account
=== RUN   TestCtestRunOrderAndConditions/explicit_empty_filters_and_skips_behave_as_default
--- PASS: TestCtestRunOrderAndConditions (0.00s)
    --- PASS: TestCtestRunOrderAndConditions/Run_respect_runCondition (0.00s)
    --- PASS: TestCtestRunOrderAndConditions/Run_takes_options_into_account (0.00s)
    --- PASS: TestCtestRunOrderAndConditions/explicit_empty_filters_and_skips_behave_as_default (0.00s)
=== RUN   TestCtestRunHandleErrors
=== RUN   TestCtestRunHandleErrors/no_errors
=== RUN   TestCtestRunHandleErrors/run_fails
=== RUN   TestCtestRunHandleErrors/run_condition_fails
=== RUN   TestCtestRunHandleErrors/empty_filters_should_succeed_(no_errors)
    ctest_runner_test.go:199: Unexpected error: error execution phase bar: run fails
--- FAIL: TestCtestRunHandleErrors (0.00s)
    --- PASS: TestCtestRunHandleErrors/no_errors (0.00s)
    --- PASS: TestCtestRunHandleErrors/run_fails (0.00s)
    --- PASS: TestCtestRunHandleErrors/run_condition_fails (0.00s)
    --- FAIL: TestCtestRunHandleErrors/empty_filters_should_succeed_(no_errors) (0.00s)
=== RUN   TestCtestBindToCommandArgRequirements
=== RUN   TestCtestBindToCommandArgRequirements/leaf_command,_no_defined_args,_follow_parent
=== RUN   TestCtestBindToCommandArgRequirements/container_cmd_expect_none,_custom_arg_check_for_leaf
    ctest_runner_test.go:283: cmd didn't have phase foo baz empty subcommand
--- FAIL: TestCtestBindToCommandArgRequirements (0.00s)
    --- PASS: TestCtestBindToCommandArgRequirements/leaf_command,_no_defined_args,_follow_parent (0.00s)
    --- FAIL: TestCtestBindToCommandArgRequirements/container_cmd_expect_none,_custom_arg_check_for_leaf (0.00s)
FAIL
coverage: 78.2% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/workflow	0.873s
=== RUN   TestCtestNewApplyData
=== RUN   TestCtestNewApplyData/fails_if_no_upgrade_version_set
=== RUN   TestCtestNewApplyData/fails_if_invalid_preflight_checks_are_provided
=== RUN   TestCtestNewApplyData/fails_if_kubeconfig_file_doesn't_exists
=== RUN   TestCtestNewApplyData/fails_if_unknown_flag_is_provided
    ctest_apply_test.go:138: newApplyData returned unexpected error, expected: unknown flag, got couldn't create a Kubernetes client from file "/etc/kubernetes/admin.conf": failed to load admin kubeconfig: open /etc/kubernetes/admin.conf: no such file or directory
=== RUN   TestCtestNewApplyData/fails_if_config_file_is_empty
    ctest_apply_test.go:138: newApplyData returned unexpected error, expected: failed to read config file, got no UpgradeConfiguration found in the supplied config
--- FAIL: TestCtestNewApplyData (0.01s)
    --- PASS: TestCtestNewApplyData/fails_if_no_upgrade_version_set (0.00s)
    --- PASS: TestCtestNewApplyData/fails_if_invalid_preflight_checks_are_provided (0.00s)
    --- PASS: TestCtestNewApplyData/fails_if_kubeconfig_file_doesn't_exists (0.00s)
    --- FAIL: TestCtestNewApplyData/fails_if_unknown_flag_is_provided (0.00s)
    --- FAIL: TestCtestNewApplyData/fails_if_config_file_is_empty (0.00s)
=== RUN   TestCtestRunDiff
=== RUN   TestCtestRunDiff/valid:_run_diff_with_empty_config_path_on_valid_manifest_path
=== RUN   TestCtestRunDiff/valid:_run_diff_on_valid_manifest_path
=== RUN   TestCtestRunDiff/invalid:_missing_config_file
=== RUN   TestCtestRunDiff/invalid:_valid_config_but_bad_manifest_path
=== RUN   TestCtestRunDiff/invalid:_badly_formatted_version_as_argument
=== RUN   TestCtestRunDiff/invalid:_empty_manifest_path_with_flag_set
=== RUN   TestCtestRunDiff/invalid:_extremely_long_manifest_path
=== RUN   TestCtestRunDiff/invalid:_nil_args_slice
--- PASS: TestCtestRunDiff (0.02s)
    --- PASS: TestCtestRunDiff/valid:_run_diff_with_empty_config_path_on_valid_manifest_path (0.01s)
    --- PASS: TestCtestRunDiff/valid:_run_diff_on_valid_manifest_path (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_missing_config_file (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_valid_config_but_bad_manifest_path (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_badly_formatted_version_as_argument (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_empty_manifest_path_with_flag_set (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_extremely_long_manifest_path (0.00s)
    --- PASS: TestCtestRunDiff/invalid:_nil_args_slice (0.00s)
=== RUN   TestCtestValidateManifests
=== RUN   TestCtestValidateManifests/valid:_valid_manifest_path
=== RUN   TestCtestValidateManifests/invalid:_one_is_empty_path
=== RUN   TestCtestValidateManifests/invalid:_manifest_path_is_directory
=== RUN   TestCtestValidateManifests/invalid:_manifest_path_does_not_exist
=== RUN   TestCtestValidateManifests/invalid:_duplicate_manifest_paths
    ctest_diff_test.go:216: expected error: true, saw: false, error: <nil>
=== RUN   TestCtestValidateManifests/invalid:_path_with_spaces
=== RUN   TestCtestValidateManifests/invalid:_nil_args_slice
    ctest_diff_test.go:216: expected error: true, saw: false, error: <nil>
=== RUN   TestCtestValidateManifests/invalid:_relative_parent_path_traversal
=== RUN   TestCtestValidateManifests/invalid:_empty_args_slice
    ctest_diff_test.go:216: expected error: true, saw: false, error: <nil>
--- FAIL: TestCtestValidateManifests (0.00s)
    --- PASS: TestCtestValidateManifests/valid:_valid_manifest_path (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_one_is_empty_path (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_manifest_path_is_directory (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_manifest_path_does_not_exist (0.00s)
    --- FAIL: TestCtestValidateManifests/invalid:_duplicate_manifest_paths (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_path_with_spaces (0.00s)
    --- FAIL: TestCtestValidateManifests/invalid:_nil_args_slice (0.00s)
    --- PASS: TestCtestValidateManifests/invalid:_relative_parent_path_traversal (0.00s)
    --- FAIL: TestCtestValidateManifests/invalid:_empty_args_slice (0.00s)
=== RUN   TestCtestSortedSliceFromStringStringArrayMap
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/the_returned_slice_should_be_alphabetically_sorted_based_on_the_string_keys_in_the_map
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/the_int_value_should_not_affect_this_func
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/slice_with_4_keys_and_different_values
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/this_should_work_for_version_numbers_as_well;_and_the_lowest_version_should_come_first
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/empty_map_returns_empty_slice
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/nil_map_returns_empty_slice
=== RUN   TestCtestSortedSliceFromStringStringArrayMap/map_with_empty_slice_values
--- PASS: TestCtestSortedSliceFromStringStringArrayMap (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/the_returned_slice_should_be_alphabetically_sorted_based_on_the_string_keys_in_the_map (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/the_int_value_should_not_affect_this_func (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/slice_with_4_keys_and_different_values (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/this_should_work_for_version_numbers_as_well;_and_the_lowest_version_should_come_first (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/empty_map_returns_empty_slice (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/nil_map_returns_empty_slice (0.00s)
    --- PASS: TestCtestSortedSliceFromStringStringArrayMap/map_with_empty_slice_values (0.00s)
FAIL
coverage: 22.2% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade	0.739s
=== RUN   TestCtestValidateExactArgNumber
=== RUN   TestCtestValidateExactArgNumber/one_arg_given_and_one_arg_expected
=== RUN   TestCtestValidateExactArgNumber/two_args_given_and_two_args_expected
=== RUN   TestCtestValidateExactArgNumber/too_few_supplied_args
=== RUN   TestCtestValidateExactArgNumber/too_few_non-empty_args
=== RUN   TestCtestValidateExactArgNumber/too_many_args
=== RUN   TestCtestValidateExactArgNumber/nil_args_slice
=== RUN   TestCtestValidateExactArgNumber/nil_supportedArgs_slice
=== RUN   TestCtestValidateExactArgNumber/args_with_whitespace
    ctest_cmdutil_test.go:78: failed ValidateExactArgNumber:
        	expected error: true
        	  actual error: false
=== RUN   TestCtestValidateExactArgNumber/supportedArgs_contains_empty_string
    ctest_cmdutil_test.go:78: failed ValidateExactArgNumber:
        	expected error: true
        	  actual error: false
--- FAIL: TestCtestValidateExactArgNumber (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/one_arg_given_and_one_arg_expected (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/two_args_given_and_two_args_expected (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/too_few_supplied_args (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/too_few_non-empty_args (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/too_many_args (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/nil_args_slice (0.00s)
    --- PASS: TestCtestValidateExactArgNumber/nil_supportedArgs_slice (0.00s)
    --- FAIL: TestCtestValidateExactArgNumber/args_with_whitespace (0.00s)
    --- FAIL: TestCtestValidateExactArgNumber/supportedArgs_contains_empty_string (0.00s)
=== RUN   TestCtestGetKubeConfigPath
=== RUN   TestCtestGetKubeConfigPath/provide_an_empty_value
=== RUN   TestCtestGetKubeConfigPath/provide_a_non-empty_value
=== RUN   TestCtestGetKubeConfigPath/provide_whitespace_only
=== RUN   TestCtestGetKubeConfigPath/provide_path_with_tilde_(user_home_expansion_not_performed)
--- PASS: TestCtestGetKubeConfigPath (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_an_empty_value (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_a_non-empty_value (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_whitespace_only (0.00s)
    --- PASS: TestCtestGetKubeConfigPath/provide_path_with_tilde_(user_home_expansion_not_performed) (0.00s)
=== RUN   TestCtestValueFromFlagsOrConfig
=== RUN   TestCtestValueFromFlagsOrConfig/string:_config_is_overridden_by_the_flag
=== RUN   TestCtestValueFromFlagsOrConfig/bool:_config_is_overridden_by_the_flag
=== RUN   TestCtestValueFromFlagsOrConfig/nil_bool_is_converted_to_false
=== RUN   TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(string)
=== RUN   TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(bool_true)
=== RUN   TestCtestValueFromFlagsOrConfig/flag_provided_with_zero_value_for_int_(unsupported_type)
    ctest_cmdutil_test.go:194: failed to set the value of the flag 0
--- FAIL: TestCtestValueFromFlagsOrConfig (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/string:_config_is_overridden_by_the_flag (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/bool:_config_is_overridden_by_the_flag (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/nil_bool_is_converted_to_false (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(string) (0.00s)
    --- PASS: TestCtestValueFromFlagsOrConfig/no_flag_provided,_config_retained_(bool_true) (0.00s)
    --- FAIL: TestCtestValueFromFlagsOrConfig/flag_provided_with_zero_value_for_int_(unsupported_type) (0.00s)
=== RUN   TestCtestGetJoinCommand
=== RUN   TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_token
=== RUN   TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_empty_token
=== RUN   TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_very_long_token
=== RUN   TestCtestGetJoinCommand/Error_to_load_kubeconfig
=== RUN   TestCtestGetJoinCommand/Error_to_get_default_cluster_config
=== RUN   TestCtestGetJoinCommand/Error_when_CA_certificate_is_invalid
=== RUN   TestCtestGetJoinCommand/Error_when_CA_certificate_file_path_is_invalid
=== RUN   TestCtestGetJoinCommand/Error_when_CA_certificate_is_missing
--- PASS: TestCtestGetJoinCommand (0.02s)
    --- PASS: TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_token (0.00s)
    --- PASS: TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_empty_token (0.01s)
    --- PASS: TestCtestGetJoinCommand/Success_with_valid_kubeconfig_and_very_long_token (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_to_load_kubeconfig (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_to_get_default_cluster_config (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_when_CA_certificate_is_invalid (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_when_CA_certificate_file_path_is_invalid (0.00s)
    --- PASS: TestCtestGetJoinCommand/Error_when_CA_certificate_is_missing (0.00s)
FAIL
coverage: 54.8% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/cmd/util	1.335s
=== RUN   TestCtestChecksumForConfigMap

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:39:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/componentconfigs/ctest_checksums_test.go:264]: matched config: {test_fixture.json [checksum case 1] data [configmaps] {{{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} <nil> map[foo:bar] map[bar:[98 97 122]]} sha256:c8f8b724728a6d6684106e5e64e94ce811c9965d19dd44dd073cf86cf43bc238}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:39:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:39:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:39:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:39:56 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs	0.626s
testing: warning: no tests to run
PASS
coverage: 6.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/constants	0.828s	coverage: 6.0% of statements [no tests to run]
=== RUN   TestCtestFor

==================== CTEST START ====================
=== RUN   TestCtestFor/default_Discovery
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [default Discovery] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/file_Discovery_with_a_path
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [file Discovery with a path] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> 0x140007008d0  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":{"KubeConfigPath":"notnil"},"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/file_Discovery_with_an_url
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [file Discovery with an url] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> 0x14000701200  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":{"KubeConfigPath":"https://localhost"},"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/BootstrapTokenDiscovery
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [BootstrapTokenDiscovery] discovery [pods] {{ } false {  [] [] []  <nil>}  {0x140003dedc0 <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":{"APIServerEndpoint":"","CACertHashes":null,"Token":"foo.bar@foobar","UnsafeSkipCAVerification":false},"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/file_Discovery_empty_path
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [file Discovery empty path] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> 0x1400087e520  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":{"KubeConfigPath":""},"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/BootstrapToken_empty_token
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [BootstrapToken empty token] discovery [pods] {{ } false {  [] [] []  <nil>}  {0x140003df1c0 <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":{"APIServerEndpoint":"","CACertHashes":null,"Token":"","UnsafeSkipCAVerification":false},"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.
=== RUN   TestCtestFor/BootstrapToken_nil_discovery
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:71]: Matched config item: {test_fixture.json [BootstrapToken nil discovery] discovery [pods] {{ } false {  [] [] []  <nil>}  {<nil> <nil>  nil} <nil> [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "discovery" in requested fixtures
2026/02/09 18:39:57 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:39:57 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"CACertPath":"","ControlPlane":null,"Discovery":{"BootstrapToken":null,"File":null,"TLSBootstrapToken":"","Timeout":null},"DryRun":false,"NodeRegistration":{"CRISocket":"","IgnorePreflightErrors":null,"ImagePullSerial":null,"KubeletExtraArgs":null,"Name":"","Taints":null},"Patches":null,"SkipPhases":null,"Timeouts":null})[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:39:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/discovery/ctest_discovery_test.go:80]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestFor (0.01s)
    --- PASS: TestCtestFor/default_Discovery (0.00s)
    --- PASS: TestCtestFor/file_Discovery_with_a_path (0.00s)
    --- PASS: TestCtestFor/file_Discovery_with_an_url (0.00s)
    --- PASS: TestCtestFor/BootstrapTokenDiscovery (0.00s)
    --- PASS: TestCtestFor/file_Discovery_empty_path (0.00s)
    --- PASS: TestCtestFor/BootstrapToken_empty_token (0.00s)
    --- PASS: TestCtestFor/BootstrapToken_nil_discovery (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/discovery	1.488s	coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app/discovery/file		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/app/discovery/https		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/discovery/token	1.867s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestKnownFeatures
--- PASS: TestCtestKnownFeatures (0.00s)
=== RUN   TestCtestNewFeatureGate
=== RUN   TestCtestNewFeatureGate/invalidValue
=== RUN   TestCtestNewFeatureGate/feature1=true,invalidValue
=== RUN   TestCtestNewFeatureGate/feature1=notABoolean
=== RUN   TestCtestNewFeatureGate/feature1=true,feature2=notABoolean
=== RUN   TestCtestNewFeatureGate/unknownFeature=false
=== RUN   TestCtestNewFeatureGate/feature1=true,unknownFeature=false
=== RUN   TestCtestNewFeatureGate/deprecated=true
W0209 18:39:57.956753   83111 features.go:161] Setting deprecated feature gate deprecated=true. It will be removed in a future release.
=== RUN   TestCtestNewFeatureGate/feature1=true
=== RUN   TestCtestNewFeatureGate/feature1=true,feature2=false
=== RUN   TestCtestNewFeatureGate/#00
=== RUN   TestCtestNewFeatureGate/feature1=true,
    ctest_features_test.go:134: NewFeatureGate didn't fail when expected
=== RUN   TestCtestNewFeatureGate/feature1=true,feature1=false
=== RUN   TestCtestNewFeatureGate/feature1=
=== RUN   TestCtestNewFeatureGate/_feature1_=_true_
    ctest_features_test.go:134: NewFeatureGate didn't fail when expected
=== RUN   TestCtestNewFeatureGate/feature1=true,feature2=false,deprecated=true
W0209 18:39:57.957986   83111 features.go:161] Setting deprecated feature gate deprecated=true. It will be removed in a future release.
--- FAIL: TestCtestNewFeatureGate (0.00s)
    --- PASS: TestCtestNewFeatureGate/invalidValue (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,invalidValue (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=notABoolean (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature2=notABoolean (0.00s)
    --- PASS: TestCtestNewFeatureGate/unknownFeature=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,unknownFeature=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/deprecated=true (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature2=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/#00 (0.00s)
    --- FAIL: TestCtestNewFeatureGate/feature1=true, (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature1=false (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1= (0.00s)
    --- FAIL: TestCtestNewFeatureGate/_feature1_=_true_ (0.00s)
    --- PASS: TestCtestNewFeatureGate/feature1=true,feature2=false,deprecated=true (0.00s)
=== RUN   TestCtestValidateVersion
=== RUN   TestCtestValidateVersion/no_min_version
=== RUN   TestCtestValidateVersion/min_version_but_correct_value_given
=== RUN   TestCtestValidateVersion/min_version_and_incorrect_value_given
=== RUN   TestCtestValidateVersion/invalid_version_format
=== RUN   TestCtestValidateVersion/empty_version_string_(treated_as_no_version)
    ctest_features_test.go:209: ValidateVersion didn't fail when expected
=== RUN   TestCtestValidateVersion/feature_not_defined_in_FeatureList
    ctest_features_test.go:209: ValidateVersion didn't fail when expected
=== RUN   TestCtestValidateVersion/nil_feature_map
--- FAIL: TestCtestValidateVersion (0.00s)
    --- PASS: TestCtestValidateVersion/no_min_version (0.00s)
    --- PASS: TestCtestValidateVersion/min_version_but_correct_value_given (0.00s)
    --- PASS: TestCtestValidateVersion/min_version_and_incorrect_value_given (0.00s)
    --- PASS: TestCtestValidateVersion/invalid_version_format (0.00s)
    --- FAIL: TestCtestValidateVersion/empty_version_string_(treated_as_no_version) (0.00s)
    --- FAIL: TestCtestValidateVersion/feature_not_defined_in_FeatureList (0.00s)
    --- PASS: TestCtestValidateVersion/nil_feature_map (0.00s)
=== RUN   TestCtestEnabledDefaults
--- PASS: TestCtestEnabledDefaults (0.00s)
=== RUN   TestCtestCheckDeprecatedFlags
=== RUN   TestCtestCheckDeprecatedFlags/deprecated_feature
=== RUN   TestCtestCheckDeprecatedFlags/valid_feature
=== RUN   TestCtestCheckDeprecatedFlags/invalid_feature
=== RUN   TestCtestCheckDeprecatedFlags/nil_feature_map
=== RUN   TestCtestCheckDeprecatedFlags/empty_feature_map
--- PASS: TestCtestCheckDeprecatedFlags (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/deprecated_feature (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/valid_feature (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/invalid_feature (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/nil_feature_map (0.00s)
    --- PASS: TestCtestCheckDeprecatedFlags/empty_feature_map (0.00s)
=== RUN   TestCtestSupports
=== RUN   TestCtestSupports/the_feature_is_not_supported
=== RUN   TestCtestSupports/the_feature_is_supported
=== RUN   TestCtestSupports/empty_feature_name
--- PASS: TestCtestSupports (0.00s)
    --- PASS: TestCtestSupports/the_feature_is_not_supported (0.00s)
    --- PASS: TestCtestSupports/the_feature_is_supported (0.00s)
    --- PASS: TestCtestSupports/empty_feature_name (0.00s)
FAIL
coverage: 98.2% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/features	1.707s
=== RUN   TestCtestGetKubernetesImage

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ClusterConfiguration" in requested fixtures
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:07 Mode: 1
2026/02/09 18:40:07 Base JSON size: 763 bytes
2026/02/09 18:40:07 Number of external values: 0
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServer":{"CertSANs":null,"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null,"TimeoutForControlPlane":null},"CACertificateValidityPeriod":null,"CIImageRepository":"","CIKubernetesVersion":"","CertificateValidityPeriod":null,"CertificatesDir":"","ClusterName":"","ComponentConfigs":null,"ControlPlaneEndpoint":"","ControllerManager":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null},"DNS":{"Disabled":false,"ImageRepository":"","ImageTag":""},"EncryptionAlgorithm":"","Etcd":{"External":null,"Local":null},"FeatureGates":null,"ImageRepository":"registry.k8s.io","KubernetesVersion":"v1.99.0","Networking":{"DNSDomain":"","PodSubnet":"","ServiceSubnet":""},"Proxy":{"Disabled":false},"Scheduler":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null}})[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:31]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:32]: Num of Test Cases: 0
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:35]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetKubernetesImage (0.00s)
=== RUN   TestCtestGetEtcdImage

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ClusterConfiguration" in requested fixtures
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:07 Mode: 1
2026/02/09 18:40:07 Base JSON size: 878 bytes
2026/02/09 18:40:07 Number of external values: 0
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServer":{"CertSANs":null,"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null,"TimeoutForControlPlane":null},"CACertificateValidityPeriod":null,"CIImageRepository":"","CIKubernetesVersion":"","CertificateValidityPeriod":null,"CertificatesDir":"","ClusterName":"","ComponentConfigs":null,"ControlPlaneEndpoint":"","ControllerManager":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null},"DNS":{"Disabled":false,"ImageRepository":"","ImageTag":""},"EncryptionAlgorithm":"","Etcd":{"External":null,"Local":{"DataDir":"","ExtraArgs":null,"ExtraEnvs":null,"ImageRepository":"","ImageTag":"","PeerCertSANs":null,"ServerCertSANs":null}},"FeatureGates":null,"ImageRepository":"real.repo","KubernetesVersion":"v1.99.0","Networking":{"DNSDomain":"","PodSubnet":"","ServiceSubnet":""},"Proxy":{"Disabled":false},"Scheduler":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null}})[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:72]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:73]: Num of Test Cases: 0
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:76]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetEtcdImage (0.00s)
=== RUN   TestCtestGetPauseImage

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ClusterConfiguration" in requested fixtures
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:07 Mode: 1
2026/02/09 18:40:07 Base JSON size: 750 bytes
2026/02/09 18:40:07 Number of external values: 0
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:07 [DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"APIServer":{"CertSANs":null,"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null,"TimeoutForControlPlane":null},"CACertificateValidityPeriod":null,"CIImageRepository":"","CIKubernetesVersion":"","CertificateValidityPeriod":null,"CertificatesDir":"","ClusterName":"","ComponentConfigs":null,"ControlPlaneEndpoint":"","ControllerManager":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null},"DNS":{"Disabled":false,"ImageRepository":"","ImageTag":""},"EncryptionAlgorithm":"","Etcd":{"External":null,"Local":null},"FeatureGates":null,"ImageRepository":"test.repo","KubernetesVersion":"","Networking":{"DNSDomain":"","PodSubnet":"","ServiceSubnet":""},"Proxy":{"Disabled":false},"Scheduler":{"ExtraArgs":null,"ExtraEnvs":null,"ExtraVolumes":null}})[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:115]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:116]: Num of Test Cases: 0
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/images/ctest_images_test.go:119]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetPauseImage (0.00s)
=== RUN   TestCtestGetAllImages

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[deployments statefulsets daemonsets replicasets pods]
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[deployments statefulsets daemonsets replicasets pods], int=5)[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:40:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:40:07 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/images	0.843s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/addons/dns	1.723s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/addons/proxy	1.305s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#01
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
=== RUN   TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#02
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace

==================== CTEST END ======================
--- PASS: TestCtestCreateBootstrapConfigMapIfNotExists (0.01s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#01 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/successful_case_should_have_no_error#02 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/if_configmap_already_exists,_return_error#02 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_error_should_be_returned#02 (0.00s)
    --- PASS: TestCtestCreateBootstrapConfigMapIfNotExists/unexpected_nil_error_should_be_treated_as_success#02 (0.00s)
=== RUN   TestCtestCreateClusterInfoRBACRules

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/bootstraptoken/clusterinfo/ctest_clusterinfo_test.go:135]: Generating hardcoded RBAC role from fixture
[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/bootstraptoken/clusterinfo/ctest_clusterinfo_test.go:142]: Matched config item: {test_fixture.json [default rbac role] rules [roles rolebindings] {{ } {kubeadm:bootstrap-signer-clusterinfo  kube-public    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[get] [] [Secret] [cluster-info] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[roles rolebindings]
[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[roles rolebindings], int=2)[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [rolebindings]
[DEBUG-CTEST 2026-02-09 18:40:08 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:40:08 load all fixtures failed: requested fixture keys not found in test_fixtures.json: rolebindings
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/bootstraptoken/clusterinfo	1.576s
=== RUN   TestCtestAllowBootstrapTokensToPostCSRs

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/bootstraptoken/node/ctest_tlsbootstrap_test.go:27]: get default configs: {test_fixture.json [existing binding]  [clusterrolebindings] &ClusterRoleBinding{ObjectMeta:{kubeadm:kubelet-bootstrap      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Subjects:[]Subject{Subject{Kind:Group,APIGroup:,Name:system:bootstrappers:kubeadm:default-node-token,Namespace:,},},RoleRef:RoleRef{APIGroup:rbac.authorization.k8s.io,Kind:ClusterRole,Name:system:node-bootstrapper,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[clusterrolebindings]
[DEBUG-CTEST 2026-02-09 18:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[clusterrolebindings], int=1)[DEBUG-CTEST 2026-02-09 18:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [clusterrolebindings]
[DEBUG-CTEST 2026-02-09 18:40:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:40:09 load all fixtures failed: requested fixture keys not found in test_fixtures.json: clusterrolebindings
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/bootstraptoken/node	2.503s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/certs	1.939s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/certs/renewal	3.817s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/empty_paths_should_return_no_volumes
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/only_ca_path_present
    ctest_volumes_test.go:82: Unexpected volumes.
        Expected: []
        Actual:   [{etcd-certs-0 {&HostPathVolumeSource{Path:/var/lib/certs/etcd,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    ctest_volumes_test.go:85: Unexpected volume mounts.
        Expected: []
        Actual:   [{etcd-certs-0 true <nil> /var/lib/certs/etcd  <nil> }]
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/ca_and_cert_in_same_dir,_key_outside
    ctest_volumes_test.go:82: Unexpected volumes.
        Expected: [{etcd-certs-0 {&HostPathVolumeSource{Path:/var/lib/certs/etcd,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
        Actual:   [{etcd-certs-0 {&HostPathVolumeSource{Path:/outside,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {etcd-certs-1 {&HostPathVolumeSource{Path:/var/lib/certs/etcd,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    ctest_volumes_test.go:85: Unexpected volume mounts.
        Expected: [{etcd-certs-0 true <nil> /var/lib/certs/etcd  <nil> }]
        Actual:   [{etcd-certs-0 true <nil> /outside  <nil> } {etcd-certs-1 true <nil> /var/lib/certs/etcd  <nil> }]
=== RUN   TestCtestGetEtcdCertVolumes_EdgeCases/certs_located_in_kubeadm_certificates_dir_should_be_ignored
--- FAIL: TestCtestGetEtcdCertVolumes_EdgeCases (0.00s)
    --- PASS: TestCtestGetEtcdCertVolumes_EdgeCases/empty_paths_should_return_no_volumes (0.00s)
    --- FAIL: TestCtestGetEtcdCertVolumes_EdgeCases/only_ca_path_present (0.00s)
    --- FAIL: TestCtestGetEtcdCertVolumes_EdgeCases/ca_and_cert_in_same_dir,_key_outside (0.00s)
    --- PASS: TestCtestGetEtcdCertVolumes_EdgeCases/certs_located_in_kubeadm_certificates_dir_should_be_ignored (0.00s)
=== RUN   TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases
=== RUN   TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases/nil_config_should_produce_empty_maps
--- FAIL: TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases (0.00s)
    --- FAIL: TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases/nil_config_should_produce_empty_maps (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x1c0 pc=0x101e3c488]

goroutine 43 [running]:
testing.tRunner.func1.2({0x1023975e0, 0x1035dc5c0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1023975e0?, 0x1035dc5c0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane.getHostPathVolumesForTheControlPlane(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/controlplane/volumes.go:54 +0x168
k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane.TestCtestGetHostPathVolumesForTheControlPlane_EdgeCases.func1(0x1400038bc00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/controlplane/ctest_volumes_test.go:183 +0x28
testing.tRunner(0x1400038bc00, 0x1400026de60)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 42
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/controlplane	2.885s
testing: warning: no tests to run
PASS
coverage: 1.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/copycerts	1.389s	coverage: 1.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/etcd	0.999s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/phases/kubeconfig	1.669s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestBuildKubeletArgs

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/ctest_flags_test.go:114]: Number of test cases: 6
Running test case 0: hostname override
Running test case 1: register with taints
Running test case 2: pause image is set
Running test case 3: nil nodeRegOpts and empty criSocket
--- FAIL: TestCtestBuildKubeletArgs (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x8 pc=0x1067ef150]

goroutine 101 [running]:
testing.tRunner.func1.2({0x107036f80, 0x1094eee80})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x107036f80?, 0x1094eee80?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.GetNodeNameAndHostname(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/flags.go:53 +0xa0
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.buildKubeletArgsCommon({0x0, {0x0, 0x0}, 0x0, {0x106c83928, 0x0}})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/flags.go:110 +0x388
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.buildKubeletArgs(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/flags.go:140
k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet.TestCtestBuildKubeletArgs(0x14000583180)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/kubelet/ctest_flags_test.go:117 +0x634
testing.tRunner(0x14000583180, 0x10757c6a0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/kubelet	1.365s
=== RUN   TestCtestMarkControlPlane

==================== CTEST UNION MODE START ====================
[DEBUG-CTEST 2026-02-09 18:40:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/markcontrolplane/ctest_markcontrolplane_test.go:40]: get default configs: {test_fixture.json [markcontrolplane base configs] node [nodes] [{[] [] [] {"metadata":{"labels":{"node-role.kubernetes.io/control-plane":"","node.kubernetes.io/exclude-from-external-load-balancers":""}}}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] {"spec":{"taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"}]}}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] {}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [{node.cloudprovider.kubernetes.io/uninitialized  NoSchedule <nil>}] [] {}} {[node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [{node.cloudprovider.kubernetes.io/uninitialized  NoSchedule <nil>}] [{node-role.kubernetes.io/control-plane  NoSchedule <nil>}] {"spec":{"taints":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/control-plane"},{"effect":"NoSchedule","key":"node.cloudprovider.kubernetes.io/uninitialized"}]}}}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-09 18:40:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-09 18:40:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-09 18:40:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:40:34 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/markcontrolplane	2.965s
=== RUN   TestCtestAnnotateCRISocket

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:40:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/patchnode/ctest_patchnode_test.go:33]: get default configs: {test_fixture.json [default node] annotations [nodes] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} { []  false [] nil } {map[] map[]  [] [] {{0}} {          nil} [] [] [] nil [] nil}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-09 18:40:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-09 18:40:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-09 18:40:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:40:33 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/patchnode	2.127s
=== RUN   TestCtestEnforceVersionPolicies
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/upgrade/ctest_policy_test.go:265]: Running EnforceVersionPolicies tests, total: 23
=== RUN   TestCtestEnforceVersionPolicies/minor_upgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.5
=== RUN   TestCtestEnforceVersionPolicies/major_upgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.1
=== RUN   TestCtestEnforceVersionPolicies/downgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/same_version_upgrade
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/new_version_must_be_higher_than_v1.12.0
[upgrade/versions] Cluster version: v1.12.3
[upgrade/versions] kubeadm version: v1.12.3
=== RUN   TestCtestEnforceVersionPolicies/upgrading_two_minor_versions_in_one_go_is_not_supported
[upgrade/versions] Cluster version: v1.11.3
[upgrade/versions] kubeadm version: v1.13.0
=== RUN   TestCtestEnforceVersionPolicies/upgrading_with_n-3_kubelet_is_supported
[upgrade/versions] Cluster version: v1.14.3
[upgrade/versions] kubeadm version: v1.15.0
=== RUN   TestCtestEnforceVersionPolicies/upgrading_with_n-4_kubelet_is_not_supported
[upgrade/versions] Cluster version: v1.14.3
[upgrade/versions] kubeadm version: v1.15.0
=== RUN   TestCtestEnforceVersionPolicies/downgrading_two_minor_versions_in_one_go_is_not_supported
[upgrade/versions] Cluster version: 1.6.0
[upgrade/versions] kubeadm version: 1.4.0
=== RUN   TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._However,_patch_version_skews_may_be_forced
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._Trying_to_upgrade_k8s_to_a_higher_minor_version_than_kubeadm_itself_should_never_be_supported
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.3.3
=== RUN   TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version.
[upgrade/versions] Cluster version: v1.13.0
[upgrade/versions] kubeadm version: v1.13.0
=== RUN   TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version._This_may_be_forced_through_though.
[upgrade/versions] Cluster version: v1.14.0
[upgrade/versions] kubeadm version: v1.14.0
=== RUN   TestCtestEnforceVersionPolicies/experimental_upgrades_supported_if_the_flag_is_set
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-beta.1
=== RUN   TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-rc.1
=== RUN   TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set#01
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-rc.1
=== RUN   TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_experimental_version_if_they_haven't_opted_into_that
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-beta.1
=== RUN   TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_release_candidate_version_if_they_haven't_opted_into_that
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0-rc.1
=== RUN   TestCtestEnforceVersionPolicies/the_user_can't_use_a_newer_minor_version_of_kubeadm_to_upgrade_an_older_version_of_kubeadm
[upgrade/versions] Cluster version: 1.3.3
[upgrade/versions] kubeadm version: 1.4.0
=== RUN   TestCtestEnforceVersionPolicies/build_release_supported_at_MinimumControlPlaneVersion
[upgrade/versions] Cluster version: 1.3.0
[upgrade/versions] kubeadm version: 1.3.0+build
=== RUN   TestCtestEnforceVersionPolicies/empty_version_strings
=== RUN   TestCtestEnforceVersionPolicies/malformed_version_strings
=== RUN   TestCtestEnforceVersionPolicies/nil_version_getter
--- PASS: TestCtestEnforceVersionPolicies (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/minor_upgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/major_upgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/downgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/same_version_upgrade (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/new_version_must_be_higher_than_v1.12.0 (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/upgrading_two_minor_versions_in_one_go_is_not_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/upgrading_with_n-3_kubelet_is_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/upgrading_with_n-4_kubelet_is_not_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/downgrading_two_minor_versions_in_one_go_is_not_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._However,_patch_version_skews_may_be_forced (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/kubeadm_version_must_be_higher_than_the_new_kube_version._Trying_to_upgrade_k8s_to_a_higher_minor_version_than_kubeadm_itself_should_never_be_supported (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version. (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_maximum_skew_between_the_cluster_version_and_the_kubelet_versions_should_be_three_minor_version._This_may_be_forced_through_though. (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/experimental_upgrades_supported_if_the_flag_is_set (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/release_candidate_upgrades_supported_if_the_flag_is_set#01 (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_experimental_version_if_they_haven't_opted_into_that (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_user_should_not_be_able_to_upgrade_to_an_release_candidate_version_if_they_haven't_opted_into_that (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/the_user_can't_use_a_newer_minor_version_of_kubeadm_to_upgrade_an_older_version_of_kubeadm (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/build_release_supported_at_MinimumControlPlaneVersion (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/empty_version_strings (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/malformed_version_strings (0.00s)
    --- PASS: TestCtestEnforceVersionPolicies/nil_version_getter (0.00s)
=== RUN   TestCtestWriteKubeletConfigFiles
  W0209 18:40:35.938090   83375 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config3178317308 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config3178317308/config.yaml
[dryrun] would read the flag --container-runtime-endpoint value from "kubeadm-flags.env", which is missing. Using default socket "unix:///var/run/containerd/containerd.sock" instead[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-upgrade-dryrun496108885/instance-config.yaml"
[dryrun] Would write file "/var/lib/kubelet/instance-config.yaml" with content:
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
[patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-upgrade-dryrun496108885/config.yaml"
[dryrun] Would write file "/var/lib/kubelet/config.yaml" with content:
containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock
  W0209 18:40:35.938580   83375 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config3005389948 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config3005389948/config.yaml
[dryrun] would read the flag --container-runtime-endpoint value from "kubeadm-flags.env", which is missing. Using default socket "unix:///var/run/containerd/containerd.sock" instead[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-upgrade-dryrun1367236679/instance-config.yaml"
[dryrun] Would write file "/var/lib/kubelet/instance-config.yaml" with content:
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
  W0209 18:40:35.938834   83375 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config2941052826 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config2941052826/config.yaml
[dryrun] would read the flag --container-runtime-endpoint value from "kubeadm-flags.env", which is missing. Using default socket "unix:///var/run/containerd/containerd.sock" instead[kubelet-start] Writing kubelet configuration to file "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-upgrade-dryrun222917725/instance-config.yaml"
[dryrun] Would write file "/var/lib/kubelet/instance-config.yaml" with content:
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
[patches] Reading patches from path "Bogus"
  W0209 18:40:35.939185   83375 postupgrade.go:116] Using temporary directory /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config492807283 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR
[dryrun] Would back up kubelet config file to /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestWriteKubeletConfigFiles3911081668/001/tmp/kubeadm-kubelet-config492807283/config.yaml
--- FAIL: TestCtestWriteKubeletConfigFiles (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x210 pc=0x106741720]

goroutine 106 [running]:
testing.tRunner.func1.2({0x10701a9e0, 0x10968ad90})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10701a9e0?, 0x10968ad90?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.WriteKubeletConfigFiles(0x0, {0x14000151980?, 0x5f?}, {0x0, 0x0}, 0x1, {0x1075bdcf8, 0x1400011c040})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/upgrade/postupgrade.go:132 +0x5e0
k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.TestCtestWriteKubeletConfigFiles(0x1400084afc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/upgrade/ctest_postupgrade_test.go:71 +0x2b8
testing.tRunner(0x1400084afc0, 0x1075a8ca8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade	4.586s
=== RUN   TestCtestUploadConfiguration
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/uploadconfig/ctest_uploadconfig_test.go:25]: Start TestCtestUploadConfiguration
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/phases/uploadconfig/ctest_uploadconfig_test.go:33]: Matched config: {test_fixture.json [default cluster config] kubernetesVersion [configmaps] &TypeMeta{Kind:,APIVersion:,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:40:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:40:35 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/phases/uploadconfig	3.766s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/preflight	0.617s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestArgumentsToCommand
=== RUN   TestCtestArgumentsToCommand/override_an_argument_from_the_base
=== RUN   TestCtestArgumentsToCommand/override_an_argument_from_the_base_and_add_duplicate
=== RUN   TestCtestArgumentsToCommand/override_all_duplicate_arguments_from_base
=== RUN   TestCtestArgumentsToCommand/add_an_argument_that_is_not_in_base
=== RUN   TestCtestArgumentsToCommand/allow_empty_strings_in_base
=== RUN   TestCtestArgumentsToCommand/allow_empty_strings_in_overrides
=== RUN   TestCtestArgumentsToCommand/empty_base_and_overrides
    ctest_arguments_test.go:153: failed ArgumentsToCommand:
        expected:
        []
        saw:
        []
=== RUN   TestCtestArgumentsToCommand/override_with_empty_value
=== RUN   TestCtestArgumentsToCommand/large_number_of_arguments
--- FAIL: TestCtestArgumentsToCommand (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_an_argument_from_the_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_an_argument_from_the_base_and_add_duplicate (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_all_duplicate_arguments_from_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/add_an_argument_that_is_not_in_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/allow_empty_strings_in_base (0.00s)
    --- PASS: TestCtestArgumentsToCommand/allow_empty_strings_in_overrides (0.00s)
    --- FAIL: TestCtestArgumentsToCommand/empty_base_and_overrides (0.00s)
    --- PASS: TestCtestArgumentsToCommand/override_with_empty_value (0.00s)
    --- PASS: TestCtestArgumentsToCommand/large_number_of_arguments (0.00s)
=== RUN   TestCtestArgumentsFromCommand
=== RUN   TestCtestArgumentsFromCommand/normal_case
=== RUN   TestCtestArgumentsFromCommand/test_that_feature-gates_is_working
=== RUN   TestCtestArgumentsFromCommand/test_that_a_binary_can_be_the_first_arg
=== RUN   TestCtestArgumentsFromCommand/allow_duplicate_args
=== RUN   TestCtestArgumentsFromCommand/empty_args_slice
=== RUN   TestCtestArgumentsFromCommand/only_binary_name,_no_flags
=== RUN   TestCtestArgumentsFromCommand/invalid_flag_without_equals
--- PASS: TestCtestArgumentsFromCommand (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/normal_case (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/test_that_feature-gates_is_working (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/test_that_a_binary_can_be_the_first_arg (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/allow_duplicate_args (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/empty_args_slice (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/only_binary_name,_no_flags (0.00s)
    --- PASS: TestCtestArgumentsFromCommand/invalid_flag_without_equals (0.00s)
=== RUN   TestCtestRoundtrip
=== RUN   TestCtestRoundtrip/normal_case
=== RUN   TestCtestRoundtrip/test_that_feature-gates_is_working
=== RUN   TestCtestRoundtrip/empty_args_slice
    ctest_arguments_test.go:277: failed TestRoundtrip:
        expected:
        []
        saw:
        []
--- FAIL: TestCtestRoundtrip (0.00s)
    --- PASS: TestCtestRoundtrip/normal_case (0.00s)
    --- PASS: TestCtestRoundtrip/test_that_feature-gates_is_working (0.00s)
    --- FAIL: TestCtestRoundtrip/empty_args_slice (0.00s)
=== RUN   TestCtestParseArgument
=== RUN   TestCtestParseArgument/arg_cannot_be_empty
=== RUN   TestCtestParseArgument/arg_must_contain_--_and_=
=== RUN   TestCtestParseArgument/arg_must_contain_--_and_=#01
=== RUN   TestCtestParseArgument/arg_must_contain_--
=== RUN   TestCtestParseArgument/arg_must_contain_a_key
=== RUN   TestCtestParseArgument/arg_can_contain_key_but_no_value
=== RUN   TestCtestParseArgument/simple_case
=== RUN   TestCtestParseArgument/keys/values_with_'-'_should_be_supported
=== RUN   TestCtestParseArgument/numbers_should_be_handled_correctly
=== RUN   TestCtestParseArgument/lists_should_be_handled_correctly
=== RUN   TestCtestParseArgument/more_than_one_'='_should_be_allowed
=== RUN   TestCtestParseArgument/just_double_dash_without_key
=== RUN   TestCtestParseArgument/key_with_spaces
--- PASS: TestCtestParseArgument (0.00s)
    --- PASS: TestCtestParseArgument/arg_cannot_be_empty (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_--_and_= (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_--_and_=#01 (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_-- (0.00s)
    --- PASS: TestCtestParseArgument/arg_must_contain_a_key (0.00s)
    --- PASS: TestCtestParseArgument/arg_can_contain_key_but_no_value (0.00s)
    --- PASS: TestCtestParseArgument/simple_case (0.00s)
    --- PASS: TestCtestParseArgument/keys/values_with_'-'_should_be_supported (0.00s)
    --- PASS: TestCtestParseArgument/numbers_should_be_handled_correctly (0.00s)
    --- PASS: TestCtestParseArgument/lists_should_be_handled_correctly (0.00s)
    --- PASS: TestCtestParseArgument/more_than_one_'='_should_be_allowed (0.00s)
    --- PASS: TestCtestParseArgument/just_double_dash_without_key (0.00s)
    --- PASS: TestCtestParseArgument/key_with_spaces (0.00s)
=== RUN   TestCtestGetControlPlaneEndpoint

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_endpoint_test.go:232]: Number of test cases: 19
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_if_fully_defined
  W0209 18:40:40.485700   83478 endpoint.go:56] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_if_fully_defined
  W0209 18:40:40.485732   83478 endpoint.go:56] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_if_fully_defined
  W0209 18:40:40.485744   83478 endpoint.go:56] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port
=== RUN   TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port
=== RUN   TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined
=== RUN   TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_BindPort
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(dns)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip4)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip6)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(port)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip4)
=== RUN   TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip6)
=== RUN   TestCtestGetControlPlaneEndpoint/empty_ControlPlaneEndpoint_and_AdvertiseAddress
=== RUN   TestCtestGetControlPlaneEndpoint/negative_BindPort
=== RUN   TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_with_whitespace
=== RUN   TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_without_host
--- PASS: TestCtestGetControlPlaneEndpoint (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_if_fully_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_if_fully_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_if_fully_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(dns)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_ControlPlaneEndpoint_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_defined_without_port (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv4)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/use_AdvertiseAddress_(ipv6)_+_BindPort_if_ControlPlaneEndpoint_is_not_defined (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_BindPort (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(dns) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip4) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(ip6) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_ControlPlaneEndpoint_(port) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip4) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/fail_if_invalid_AdvertiseAddress_(ip6) (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/empty_ControlPlaneEndpoint_and_AdvertiseAddress (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/negative_BindPort (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_with_whitespace (0.00s)
    --- PASS: TestCtestGetControlPlaneEndpoint/ControlPlaneEndpoint_without_host (0.00s)
=== RUN   TestCtestParsePort

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_endpoint_test.go:468]: Number of test cases: 9
=== RUN   TestCtestParsePort/valid_port
=== RUN   TestCtestParsePort/invalid_port_(not_a_number)
=== RUN   TestCtestParsePort/invalid_port_(<1)
=== RUN   TestCtestParsePort/invalid_port_(>65535)
=== RUN   TestCtestParsePort/empty_string
=== RUN   TestCtestParsePort/zero_port
=== RUN   TestCtestParsePort/max_valid_port
=== RUN   TestCtestParsePort/negative_zero_string
=== RUN   TestCtestParsePort/huge_number_overflow
--- PASS: TestCtestParsePort (0.00s)
    --- PASS: TestCtestParsePort/valid_port (0.00s)
    --- PASS: TestCtestParsePort/invalid_port_(not_a_number) (0.00s)
    --- PASS: TestCtestParsePort/invalid_port_(<1) (0.00s)
    --- PASS: TestCtestParsePort/invalid_port_(>65535) (0.00s)
    --- PASS: TestCtestParsePort/empty_string (0.00s)
    --- PASS: TestCtestParsePort/zero_port (0.00s)
    --- PASS: TestCtestParsePort/max_valid_port (0.00s)
    --- PASS: TestCtestParsePort/negative_zero_string (0.00s)
    --- PASS: TestCtestParsePort/huge_number_overflow (0.00s)
=== RUN   TestCtestMergeKubeadmEnvVars

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_env_test.go:91]: Number of test cases: 5
Running 0 th test case: normal case without duplicated env
=== RUN   TestCtestMergeKubeadmEnvVars/normal_case_without_duplicated_env
Running 1 th test case: extraEnv env take precedence over the proxyEnv
=== RUN   TestCtestMergeKubeadmEnvVars/extraEnv_env_take_precedence_over_the_proxyEnv
Running 2 th test case: both proxyEnv and extraEnv empty
=== RUN   TestCtestMergeKubeadmEnvVars/both_proxyEnv_and_extraEnv_empty
Running 3 th test case: proxyEnv has duplicate keys, extraEnv empty
=== RUN   TestCtestMergeKubeadmEnvVars/proxyEnv_has_duplicate_keys,_extraEnv_empty
    ctest_env_test.go:97: expected env: [{Dup First nil} {Dup Second nil}], got: [{Dup Second nil}]
Running 4 th test case: extraEnv overrides multiple proxyEnv entries
=== RUN   TestCtestMergeKubeadmEnvVars/extraEnv_overrides_multiple_proxyEnv_entries

==================== CTEST END ======================
--- FAIL: TestCtestMergeKubeadmEnvVars (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/normal_case_without_duplicated_env (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/extraEnv_env_take_precedence_over_the_proxyEnv (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/both_proxyEnv_and_extraEnv_empty (0.00s)
    --- FAIL: TestCtestMergeKubeadmEnvVars/proxyEnv_has_duplicate_keys,_extraEnv_empty (0.00s)
    --- PASS: TestCtestMergeKubeadmEnvVars/extraEnv_overrides_multiple_proxyEnv_entries (0.00s)
=== RUN   TestCtestGetProxyEnvVars

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_env_test.go:164]: Number of test cases: 4
Running 0 th test case: environment with lowercase proxy vars
=== RUN   TestCtestGetProxyEnvVars/environment_with_lowercase_proxy_vars
Running 1 th test case: environment with uppercase proxy vars
=== RUN   TestCtestGetProxyEnvVars/environment_with_uppercase_proxy_vars
Running 2 th test case: empty environment slice
=== RUN   TestCtestGetProxyEnvVars/empty_environment_slice
Running 3 th test case: environment with malformed entries and duplicates
=== RUN   TestCtestGetProxyEnvVars/environment_with_malformed_entries_and_duplicates
    ctest_env_test.go:170: expected env: [{{http_proxy  nil}} {{https_proxy  nil}} {{no_proxy p3 nil}} {{NO_PROXY override nil}}], got: [{{http_proxy p1 nil}} {{no_proxy p3 nil}} {{NO_PROXY override nil}}]

==================== CTEST END ======================
--- FAIL: TestCtestGetProxyEnvVars (0.00s)
    --- PASS: TestCtestGetProxyEnvVars/environment_with_lowercase_proxy_vars (0.00s)
    --- PASS: TestCtestGetProxyEnvVars/environment_with_uppercase_proxy_vars (0.00s)
    --- PASS: TestCtestGetProxyEnvVars/empty_environment_slice (0.00s)
    --- FAIL: TestCtestGetProxyEnvVars/environment_with_malformed_entries_and_duplicates (0.00s)
=== RUN   TestCtestMarshalUnmarshalYaml

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:26]: matched config: {test_fixture.json [marshal-unmarshal yaml pod] spec [pods] {[] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:40:40 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:40 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"restartPolicy":"Always"})[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:40:40 [DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:36]: New Json Test Configs: [{"volumes":[{"name":"config","hostPath":{"path":"/etc/kubernetes"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"restartPolicy":"Always"}]
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:37]: Num of test cases: 1
Running 0 th test case.
{[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    ctest_marshal_test.go:62: 
        apiVersion: v1
        kind: Pod
        metadata:
          labels:
            test: "yes"
          name: someName-3a101638-efe6-4ccd-bb84-674c3a46db09
          namespace: testNamespace-32a78034-8aed-468a-84a1-2ca613002c09
        spec:
          containers: null
          restartPolicy: Always
          volumes:
          - hostPath:
              path: /etc/kubernetes
            name: config
          - hostPath:
              path: /var/lib/kms/
              type: DirectoryOrCreate
            name: socket-dir
        status: {}

==================== CTEST END ======================
--- PASS: TestCtestMarshalUnmarshalYaml (0.00s)
=== RUN   TestCtestUnmarshalJson

==================== CTEST OVERRIDE ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:95]: matched config: {test_fixture.json [unmarshal json pod] spec [pods] {[] [] [] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:40:40 [DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:40 Mode: 1
2026/02/09 18:40:40 Base JSON size: 44 bytes
2026/02/09 18:40:40 Number of external values: 1
2026/02/09 18:40:40   [OVERRIDE] containers: <nil> → [map[args:[--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config] image:registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 name:barbican-kms resources:map[] volumeMounts:[map[mountPath:/etc/kubernetes/ name:config] map[mountPath:/kms/ name:socket-dir]]]]
2026/02/09 18:40:40   [KEEP] restartPolicy: Always (missing in external)
2026/02/09 18:40:40 [DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:40 [DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"restartPolicy":"Always"})[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:40:40 [DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[] [] [{barbican-kms registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[] [] [{barbican-kms registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:105]: New Json Test Configs: [{"containers":[{"name":"barbican-kms","image":"registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0","args":["--socketpath=/kms/kms.sock","--cloud-config=/etc/kubernetes/cloud-config"],"resources":{},"volumeMounts":[{"name":"config","mountPath":"/etc/kubernetes/"},{"name":"socket-dir","mountPath":"/kms/"}]}],"restartPolicy":"Always"}]
[DEBUG-CTEST 2026-02-09 18:40:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/ctest_marshal_test.go:106]: Num of test cases: 1
Running 0 th test case.
{[] [] [{barbican-kms registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] [] Always <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    ctest_marshal_test.go:128: 
        {
          "apiVersion": "v1",
          "kind": "Pod",
          "metadata": {
            "name": "someName",
            "namespace": "testNamespace",
            "labels": {
              "test": "yes"
            }
          },
          "spec": {
            "restartPolicy": "Always"
          }
        }

==================== CTEST END ======================
--- PASS: TestCtestUnmarshalJson (0.00s)
=== RUN   TestCtestKubernetesReleaseVersion
=== RUN   TestCtestKubernetesReleaseVersion/empty_input
=== RUN   TestCtestKubernetesReleaseVersion/label_as_input
  I0209 18:40:40.493723   83478 version.go:260] remote version is much newer: 1.0.0-placeholder-version; falling back to: stable-0.0
    ctest_version_test.go:57: expected error: false, got: true, error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:60: expected output: v1.0.0-placeholder-version, got: 
=== RUN   TestCtestKubernetesReleaseVersion/whitespace_only
=== RUN   TestCtestKubernetesReleaseVersion/unknown_label_without_fetcher_entry
--- FAIL: TestCtestKubernetesReleaseVersion (0.00s)
    --- PASS: TestCtestKubernetesReleaseVersion/empty_input (0.00s)
    --- FAIL: TestCtestKubernetesReleaseVersion/label_as_input (0.00s)
    --- PASS: TestCtestKubernetesReleaseVersion/whitespace_only (0.00s)
    --- PASS: TestCtestKubernetesReleaseVersion/unknown_label_without_fetcher_entry (0.00s)
=== RUN   TestCtestValidVersion
=== RUN   TestCtestValidVersion/v1.3.0
    ctest_version_test.go:83: Valid:  v1.3.0 v1.3.0 <nil>
=== RUN   TestCtestValidVersion/v1.4.0-alpha.0
    ctest_version_test.go:83: Valid:  v1.4.0-alpha.0 v1.4.0-alpha.0 <nil>
=== RUN   TestCtestValidVersion/v1.4.5
    ctest_version_test.go:83: Valid:  v1.4.5 v1.4.5 <nil>
=== RUN   TestCtestValidVersion/v1.4.0-beta.0
    ctest_version_test.go:83: Valid:  v1.4.0-beta.0 v1.4.0-beta.0 <nil>
=== RUN   TestCtestValidVersion/v2.0.0
    ctest_version_test.go:83: Valid:  v2.0.0 v2.0.0 <nil>
=== RUN   TestCtestValidVersion/v1.6.0-alpha.0.536+d60d9f3269288f
    ctest_version_test.go:83: Valid:  v1.6.0-alpha.0.536+d60d9f3269288f v1.6.0-alpha.0.536+d60d9f3269288f <nil>
=== RUN   TestCtestValidVersion/v1.5.0-alpha.0.1078+1044b6822497da-pull
    ctest_version_test.go:83: Valid:  v1.5.0-alpha.0.1078+1044b6822497da-pull v1.5.0-alpha.0.1078+1044b6822497da-pull <nil>
=== RUN   TestCtestValidVersion/v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci
    ctest_version_test.go:83: Valid:  v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci <nil>
=== RUN   TestCtestValidVersion/v1.6.1+coreos.0
    ctest_version_test.go:83: Valid:  v1.6.1+coreos.0 v1.6.1+coreos.0 <nil>
=== RUN   TestCtestValidVersion/1.7.1
    ctest_version_test.go:83: Valid:  1.7.1 v1.7.1 <nil>
=== RUN   TestCtestValidVersion/v9999.9999.9999-alpha.0+build.1234567890
    ctest_version_test.go:83: Valid:  v9999.9999.9999-alpha.0+build.1234567890 v9999.9999.9999-alpha.0+build.1234567890 <nil>
--- PASS: TestCtestValidVersion (0.00s)
    --- PASS: TestCtestValidVersion/v1.3.0 (0.00s)
    --- PASS: TestCtestValidVersion/v1.4.0-alpha.0 (0.00s)
    --- PASS: TestCtestValidVersion/v1.4.5 (0.00s)
    --- PASS: TestCtestValidVersion/v1.4.0-beta.0 (0.00s)
    --- PASS: TestCtestValidVersion/v2.0.0 (0.00s)
    --- PASS: TestCtestValidVersion/v1.6.0-alpha.0.536+d60d9f3269288f (0.00s)
    --- PASS: TestCtestValidVersion/v1.5.0-alpha.0.1078+1044b6822497da-pull (0.00s)
    --- PASS: TestCtestValidVersion/v1.5.0-alpha.1.822+49b9e32fad9f32-pull-gke-gci (0.00s)
    --- PASS: TestCtestValidVersion/v1.6.1+coreos.0 (0.00s)
    --- PASS: TestCtestValidVersion/1.7.1 (0.00s)
    --- PASS: TestCtestValidVersion/v9999.9999.9999-alpha.0+build.1234567890 (0.00s)
=== RUN   TestCtestInvalidVersion
=== RUN   TestCtestInvalidVersion/v1.3
    ctest_version_test.go:109: Invalid:  v1.3  version "v1.3" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/1.4
    ctest_version_test.go:109: Invalid:  1.4  version "1.4" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/b1.4.0
    ctest_version_test.go:109: Invalid:  b1.4.0  version "b1.4.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/c1.4.5+git
    ctest_version_test.go:109: Invalid:  c1.4.5+git  version "c1.4.5+git" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/something1.2
    ctest_version_test.go:109: Invalid:  something1.2  version "something1.2" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/#00
    ctest_version_test.go:109: Invalid:    invalid version ""
=== RUN   TestCtestInvalidVersion/v
    ctest_version_test.go:109: Invalid:  v  version "v" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/v1
    ctest_version_test.go:109: Invalid:  v1  version "v1" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestInvalidVersion/v1.2.3.4
    ctest_version_test.go:109: Invalid:  v1.2.3.4 v1.2.3.4 <nil>
    ctest_version_test.go:111: kubernetesReleaseVersion error expected for version "v1.2.3.4", but returned successfully
    ctest_version_test.go:114: kubernetesReleaseVersion should return empty string in case of error. Returned "v1.2.3.4" for version "v1.2.3.4"
--- FAIL: TestCtestInvalidVersion (0.00s)
    --- PASS: TestCtestInvalidVersion/v1.3 (0.00s)
    --- PASS: TestCtestInvalidVersion/1.4 (0.00s)
    --- PASS: TestCtestInvalidVersion/b1.4.0 (0.00s)
    --- PASS: TestCtestInvalidVersion/c1.4.5+git (0.00s)
    --- PASS: TestCtestInvalidVersion/something1.2 (0.00s)
    --- PASS: TestCtestInvalidVersion/#00 (0.00s)
    --- PASS: TestCtestInvalidVersion/v (0.00s)
    --- PASS: TestCtestInvalidVersion/v1 (0.00s)
    --- FAIL: TestCtestInvalidVersion/v1.2.3.4 (0.00s)
=== RUN   TestCtestValidConvenientForUserVersion
=== RUN   TestCtestValidConvenientForUserVersion/1.4.0
    ctest_version_test.go:130: Valid:  1.4.0 v1.4.0 <nil>
=== RUN   TestCtestValidConvenientForUserVersion/1.4.5+git
    ctest_version_test.go:130: Valid:  1.4.5+git v1.4.5+git <nil>
=== RUN   TestCtestValidConvenientForUserVersion/1.6.1_coreos.0
    ctest_version_test.go:130: Valid:  1.6.1_coreos.0 v1.6.1_coreos.0 <nil>
=== RUN   TestCtestValidConvenientForUserVersion/#00
    ctest_version_test.go:130: Valid:    invalid version ""
--- PASS: TestCtestValidConvenientForUserVersion (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/1.4.0 (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/1.4.5+git (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/1.6.1_coreos.0 (0.00s)
    --- PASS: TestCtestValidConvenientForUserVersion/#00 (0.00s)
=== RUN   TestCtestVersionFromNetwork
=== RUN   TestCtestVersionFromNetwork/latest
  I0209 18:40:40.494545   83478 version.go:260] remote version is much newer: v1.6.0-alpha.0; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "latest". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "latest": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/latest-1.3
  I0209 18:40:40.494583   83478 version.go:260] remote version is much newer: v1.3.11-beta.0; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "latest-1.3". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "latest-1.3": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/invalid-version
    ctest_version_test.go:179: Key: "invalid-version". Result: "", Error: version "invalid-version" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/nonexistent
    ctest_version_test.go:179: Key: "nonexistent". Result: "", Error: version "nonexistent" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/latest-1.5
  W0209 18:40:40.494807   83478 version.go:108] could not fetch a Kubernetes version from the internet: expected error
  W0209 18:40:40.494818   83478 version.go:109] falling back to the local client version: v0.0.0-master.0
    ctest_version_test.go:179: Key: "latest-1.5". Result: "v0.0.0-master.0", Error: <nil>
    ctest_version_test.go:186: unexpected result for "latest-1.5". Expected: "v1.0.0-placeholder-version" Actual: "v0.0.0-master.0"
=== RUN   TestCtestVersionFromNetwork/stable
    ctest_version_test.go:179: Key: "stable". Result: "", Error: remote version error: could not parse "stable-1" as version
    ctest_version_test.go:182: unexpected error for "stable": remote version error: could not parse "stable-1" as version
=== RUN   TestCtestVersionFromNetwork/stable-1
  I0209 18:40:40.494933   83478 version.go:260] remote version is much newer: v1.4.6; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "stable-1". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "stable-1": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestVersionFromNetwork/stable-1.3
  I0209 18:40:40.494994   83478 version.go:260] remote version is much newer: v1.3.10; falling back to: stable-0.0
    ctest_version_test.go:179: Key: "stable-1.3". Result: "", Error: version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
    ctest_version_test.go:182: unexpected error for "stable-1.3": version "stable-0.0" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
--- FAIL: TestCtestVersionFromNetwork (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/latest (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/latest-1.3 (0.00s)
    --- PASS: TestCtestVersionFromNetwork/invalid-version (0.00s)
    --- PASS: TestCtestVersionFromNetwork/nonexistent (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/latest-1.5 (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/stable (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/stable-1 (0.00s)
    --- FAIL: TestCtestVersionFromNetwork/stable-1.3 (0.00s)
=== RUN   TestCtestVersionToTag
=== RUN   TestCtestVersionToTag/input:/expected:
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "". Result: "". Expected: ""
=== RUN   TestCtestVersionToTag/input:v1.0.0/expected:v1.0.0
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v1.0.0". Result: "v1.0.0". Expected: "v1.0.0"
=== RUN   TestCtestVersionToTag/input:v10.1.2-alpha.1.100+0123456789abcdef+SOMETHING/expected:v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v10.1.2-alpha.1.100+0123456789abcdef+SOMETHING". Result: "v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING". Expected: "v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING"
=== RUN   TestCtestVersionToTag/input:v1,0!0+üñµ/expected:v1_0_0____
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v1,0!0+üñµ". Result: "v1_0_0____". Expected: "v1_0_0____"
=== RUN   TestCtestVersionToTag/input:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0/expected:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0
    ctest_version_test.go:207: kubernetesVersionToImageTag: Input: "v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0". Result: "v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0". Expected: "v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0"
--- PASS: TestCtestVersionToTag (0.00s)
    --- PASS: TestCtestVersionToTag/input:/expected: (0.00s)
    --- PASS: TestCtestVersionToTag/input:v1.0.0/expected:v1.0.0 (0.00s)
    --- PASS: TestCtestVersionToTag/input:v10.1.2-alpha.1.100+0123456789abcdef+SOMETHING/expected:v10.1.2-alpha.1.100_0123456789abcdef_SOMETHING (0.00s)
    --- PASS: TestCtestVersionToTag/input:v1,0!0+üñµ/expected:v1_0_0____ (0.00s)
    --- PASS: TestCtestVersionToTag/input:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0/expected:v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.v1.0 (0.00s)
=== RUN   TestCtestSplitVersion
=== RUN   TestCtestSplitVersion/input:v1.7.0/label:v1.7.0
=== RUN   TestCtestSplitVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/label:v1.8.0-alpha.2.1231+afabd012389d53a
=== RUN   TestCtestSplitVersion/input:release/v1.7.0/label:v1.7.0
=== RUN   TestCtestSplitVersion/input:release/latest-1.7/label:latest-1.7
=== RUN   TestCtestSplitVersion/input:ci/latest/label:latest
=== RUN   TestCtestSplitVersion/input:ci/latest-1.7/label:latest-1.7
=== RUN   TestCtestSplitVersion/input:unknown-1/label:unknown-1
=== RUN   TestCtestSplitVersion/input:unknown/latest-1/label:
=== RUN   TestCtestSplitVersion/input:/label:
=== RUN   TestCtestSplitVersion/input:ci//label:
=== RUN   TestCtestSplitVersion/input:ci/label:
    ctest_version_test.go:242: splitVersion: error expected for "ci", but result is "https://dl.k8s.io/release", "ci"
--- FAIL: TestCtestSplitVersion (0.00s)
    --- PASS: TestCtestSplitVersion/input:v1.7.0/label:v1.7.0 (0.00s)
    --- PASS: TestCtestSplitVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/label:v1.8.0-alpha.2.1231+afabd012389d53a (0.00s)
    --- PASS: TestCtestSplitVersion/input:release/v1.7.0/label:v1.7.0 (0.00s)
    --- PASS: TestCtestSplitVersion/input:release/latest-1.7/label:latest-1.7 (0.00s)
    --- PASS: TestCtestSplitVersion/input:ci/latest/label:latest (0.00s)
    --- PASS: TestCtestSplitVersion/input:ci/latest-1.7/label:latest-1.7 (0.00s)
    --- PASS: TestCtestSplitVersion/input:unknown-1/label:unknown-1 (0.00s)
    --- PASS: TestCtestSplitVersion/input:unknown/latest-1/label: (0.00s)
    --- PASS: TestCtestSplitVersion/input:/label: (0.00s)
    --- PASS: TestCtestSplitVersion/input:ci//label: (0.00s)
    --- FAIL: TestCtestSplitVersion/input:ci/label: (0.00s)
=== RUN   TestCtestKubernetesIsCIVersion
=== RUN   TestCtestKubernetesIsCIVersion/input:/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:v1.0.0/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "v1.0.0". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:release/v1.0.0/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "release/v1.0.0". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:ci/latest-1/expected:true
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci/latest-1". Result: true. Expected: true
=== RUN   TestCtestKubernetesIsCIVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:true
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a". Result: true. Expected: true
=== RUN   TestCtestKubernetesIsCIVersion/input:ci//expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci/". Result: false. Expected: false
=== RUN   TestCtestKubernetesIsCIVersion/input:ci/expected:false
    ctest_version_test.go:269: kubernetesIsCIVersion: Input: "ci". Result: false. Expected: false
--- PASS: TestCtestKubernetesIsCIVersion (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:/expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:v1.0.0/expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:release/v1.0.0/expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci/latest-1/expected:true (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:true (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci//expected:false (0.00s)
    --- PASS: TestCtestKubernetesIsCIVersion/input:ci/expected:false (0.00s)
=== RUN   TestCtestCIBuildVersion
=== RUN   TestCtestCIBuildVersion/input:v1.7.0/expected:v1.7.0
    ctest_version_test.go:303: Input: "v1.7.0". Result: "v1.7.0", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:release/v1.8.0/expected:v1.8.0
    ctest_version_test.go:303: Input: "release/v1.8.0". Result: "v1.8.0", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:1.4.0-beta.0/expected:v1.4.0-beta.0
    ctest_version_test.go:303: Input: "1.4.0-beta.0". Result: "v1.4.0-beta.0", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:release/0invalid/expected:
    ctest_version_test.go:303: Input: "release/0invalid". Result: "", Error: version "release/0invalid" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestCIBuildVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a
    ctest_version_test.go:303: Input: "ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a". Result: "v1.9.0-alpha.1.123+acbcbfd53bfa0a", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:ci/1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a
    ctest_version_test.go:303: Input: "ci/1.9.0-alpha.1.123+acbcbfd53bfa0a". Result: "v1.9.0-alpha.1.123+acbcbfd53bfa0a", Error: <nil>
=== RUN   TestCtestCIBuildVersion/input:ci/0invalid/expected:
    ctest_version_test.go:303: Input: "ci/0invalid". Result: "", Error: version "ci/0invalid" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestCIBuildVersion/input:0invalid/expected:
    ctest_version_test.go:303: Input: "0invalid". Result: "", Error: version "0invalid" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
=== RUN   TestCtestCIBuildVersion/input:ci/expected:
    ctest_version_test.go:303: Input: "ci". Result: "", Error: version "ci" doesn't match patterns for neither semantic version nor labels (stable, latest, ...)
--- PASS: TestCtestCIBuildVersion (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:v1.7.0/expected:v1.7.0 (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:release/v1.8.0/expected:v1.8.0 (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:1.4.0-beta.0/expected:v1.4.0-beta.0 (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:release/0invalid/expected: (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/v1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/1.9.0-alpha.1.123+acbcbfd53bfa0a/expected:v1.9.0-alpha.1.123+acbcbfd53bfa0a (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/0invalid/expected: (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:0invalid/expected: (0.00s)
    --- PASS: TestCtestCIBuildVersion/input:ci/expected: (0.00s)
=== RUN   TestCtestNormalizedBuildVersionVersion
=== RUN   TestCtestNormalizedBuildVersionVersion/input:v1.7.0/expected:v1.7.0
=== RUN   TestCtestNormalizedBuildVersionVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/expected:v1.8.0-alpha.2.1231+afabd012389d53a
=== RUN   TestCtestNormalizedBuildVersionVersion/input:1.7.0/expected:v1.7.0
=== RUN   TestCtestNormalizedBuildVersionVersion/input:unknown-1/expected:
=== RUN   TestCtestNormalizedBuildVersionVersion/input:/expected:
=== RUN   TestCtestNormalizedBuildVersionVersion/input:v/expected:
--- PASS: TestCtestNormalizedBuildVersionVersion (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:v1.7.0/expected:v1.7.0 (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:v1.8.0-alpha.2.1231+afabd012389d53a/expected:v1.8.0-alpha.2.1231+afabd012389d53a (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:1.7.0/expected:v1.7.0 (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:unknown-1/expected: (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:/expected: (0.00s)
    --- PASS: TestCtestNormalizedBuildVersionVersion/input:v/expected: (0.00s)
=== RUN   TestCtestKubeadmVersion
=== RUN   TestCtestKubeadmVersion/valid_version_with_label_and_metadata
=== RUN   TestCtestKubeadmVersion/valid_version_with_label_and_extra_metadata
=== RUN   TestCtestKubeadmVersion/valid_patch_version_with_label_and_extra_metadata
=== RUN   TestCtestKubeadmVersion/valid_version_with_label_extra
=== RUN   TestCtestKubeadmVersion/valid_patch_version_with_label
=== RUN   TestCtestKubeadmVersion/handle_version_with_partial_label
=== RUN   TestCtestKubeadmVersion/handle_version_missing_'v'
=== RUN   TestCtestKubeadmVersion/valid_version_without_label_and_metadata
=== RUN   TestCtestKubeadmVersion/valid_patch_version_without_label_and_metadata
=== RUN   TestCtestKubeadmVersion/invalid_version
=== RUN   TestCtestKubeadmVersion/invalid_version_with_stray_dash
=== RUN   TestCtestKubeadmVersion/invalid_version_without_patch_release
=== RUN   TestCtestKubeadmVersion/invalid_version_with_label_and_stray_dot
=== RUN   TestCtestKubeadmVersion/invalid_version_empty
=== RUN   TestCtestKubeadmVersion/invalid_version_malformed
--- PASS: TestCtestKubeadmVersion (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_with_label_and_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_with_label_and_extra_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_patch_version_with_label_and_extra_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_with_label_extra (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_patch_version_with_label (0.00s)
    --- PASS: TestCtestKubeadmVersion/handle_version_with_partial_label (0.00s)
    --- PASS: TestCtestKubeadmVersion/handle_version_missing_'v' (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_version_without_label_and_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/valid_patch_version_without_label_and_metadata (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_with_stray_dash (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_without_patch_release (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_with_label_and_stray_dot (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_empty (0.00s)
    --- PASS: TestCtestKubeadmVersion/invalid_version_malformed (0.00s)
=== RUN   TestCtestValidateStableVersion
=== RUN   TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[1]
  I0209 18:40:40.496787   83478 version.go:260] remote version is much newer: v1.12.0; falling back to: stable-1.11
=== RUN   TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[2]
  I0209 18:40:40.496815   83478 version.go:260] remote version is much newer: v2.0.0; falling back to: stable-1.11
=== RUN   TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[3]
  I0209 18:40:40.496842   83478 version.go:260] remote version is much newer: v2.1.5; falling back to: stable-1.11
=== RUN   TestCtestValidateStableVersion/valid:_return_the_remote_version_as_it_is_part_of_the_same_release
=== RUN   TestCtestValidateStableVersion/valid:_return_the_same_version
=== RUN   TestCtestValidateStableVersion/invalid:_client_version_is_empty
=== RUN   TestCtestValidateStableVersion/invalid:_error_parsing_the_remote_version
=== RUN   TestCtestValidateStableVersion/invalid:_error_parsing_the_client_version
=== RUN   TestCtestValidateStableVersion/invalid:_both_versions_empty
--- PASS: TestCtestValidateStableVersion (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[1] (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[2] (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_remote_version_is_newer;_return_stable_label_[3] (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_return_the_remote_version_as_it_is_part_of_the_same_release (0.00s)
    --- PASS: TestCtestValidateStableVersion/valid:_return_the_same_version (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_client_version_is_empty (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_error_parsing_the_remote_version (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_error_parsing_the_client_version (0.00s)
    --- PASS: TestCtestValidateStableVersion/invalid:_both_versions_empty (0.00s)
=== RUN   TestCtestFetchFromURL
=== RUN   TestCtestFetchFromURL/normal_success
=== RUN   TestCtestFetchFromURL/HTTP_error_status
=== RUN   TestCtestFetchFromURL/Request_timeout
=== RUN   TestCtestFetchFromURL/Server_never_responds_within_timeout
--- PASS: TestCtestFetchFromURL (0.40s)
    --- PASS: TestCtestFetchFromURL/normal_success (0.00s)
    --- PASS: TestCtestFetchFromURL/HTTP_error_status (0.00s)
    --- PASS: TestCtestFetchFromURL/Request_timeout (0.20s)
    --- PASS: TestCtestFetchFromURL/Server_never_responds_within_timeout (0.20s)
FAIL
coverage: 68.8% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util	0.977s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/apiclient	0.994s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/certs		coverage: 0.0% of statements
=== RUN   TestCtestDefaultTaintsMarshaling

==================== CTEST EXTEND ONLY START ====================
Running 0 th test case.
{test_fixture.json [uninitialized nodeRegistration] nodeRegistration.taints [] {{ } [] false {  [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "nodeRegistration.taints" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"localAPIEndpoint":{},"nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:38]: Skipping test execution. No new configurations generated.
Running 1 th test case.
{test_fixture.json [empty taints slice] nodeRegistration.taints [] {{ } [] false {  [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "nodeRegistration.taints" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"localAPIEndpoint":{},"nodeRegistration":{"taints":[]}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:38]: Skipping test execution. No new configurations generated.
Running 2 th test case.
{test_fixture.json [custom taints] nodeRegistration.taints [] {{ } [] false {  [{taint1   <nil>} {taint2   <nil>}] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "nodeRegistration.taints" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"localAPIEndpoint":{},"nodeRegistration":{"taints":[{"effect":"","key":"taint1"},{"effect":"","key":"taint2"}]}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:38]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestDefaultTaintsMarshaling (0.01s)
=== RUN   TestCtestBytesToInitConfiguration

==================== CTEST EXTEND ONLY START ====================
Running 0 th test case.
{test_fixture.json [default config is set correctly] initConfiguration [] {{InitConfiguration kubeadm.k8s.io/v1beta4} [] false {  [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","kind":"InitConfiguration","localAPIEndpoint":{},"nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:85]: Skipping test execution. No new configurations generated.
Running 1 th test case.
{test_fixture.json [partial config with custom values] initConfiguration [] {{InitConfiguration kubeadm.k8s.io/v1beta4} [] false {test-node unix:///var/run/containerd/containerd.sock [] [] []  <nil>} { 0}  [] <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","kind":"InitConfiguration","localAPIEndpoint":{},"nodeRegistration":{"criSocket":"unix:///var/run/containerd/containerd.sock","name":"test-node","taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:85]: Skipping test execution. No new configurations generated.
Running 2 th test case.
{test_fixture.json [invalid configuration type] upgradeConfiguration [] {{UpgradeConfiguration kubeadm.k8s.io/v1beta4} { <nil> <nil> <nil> <nil> <nil> <nil> [] <nil> <nil> []  <nil>} { 0} {<nil> <nil> <nil> [] [] <nil>  <nil>} { <nil> <nil> <nil> <nil> [] <nil>} <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "upgradeConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","apply":{},"diff":{},"kind":"UpgradeConfiguration","node":{},"plan":{}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_initconfiguration_test.go:85]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestBytesToInitConfiguration (0.01s)
=== RUN   TestCtestBytesToJoinConfiguration
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:29]: Start TestCtestBytesToJoinConfiguration
Running 0 th test case: Normal configuration
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [normal join config] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","caCertPath":"/some/cert.crt","discovery":{"bootstrapToken":{"apiServerEndpoint":"1.2.3.4:6443","caCertHashes":["aaaa"],"token":"abcdef.1234567890123456"},"tlsBootstrapToken":"abcdef.1234567890123456"},"kind":"JoinConfiguration","nodeRegistration":{"criSocket":"unix:///var/run/crio/crio.sock","name":"node-1","taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.
Running 1 th test case: Only contains Discovery configuration
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [discovery only join config] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","discovery":{"bootstrapToken":{"apiServerEndpoint":"1.2.3.4:6443","caCertHashes":["aaaa"],"token":"abcdef.1234567890123456"},"tlsBootstrapToken":"abcdef.1234567890123456"},"kind":"JoinConfiguration","nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.
Running 2 th test case: Edge – empty NodeRegistration
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [edge empty nodereg] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","discovery":{"bootstrapToken":{"apiServerEndpoint":"1.2.3.4:6443","caCertHashes":["aaaa"],"token":"abcdef.1234567890123456"},"tlsBootstrapToken":"abcdef.1234567890123456"},"kind":"JoinConfiguration","nodeRegistration":{"taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.
Running 3 th test case: Edge – nil Discovery
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:131]: matched fixture: {test_fixture.json [edge nil discovery] joinConfiguration [pods] &TypeMeta{Kind:JoinConfiguration,APIVersion:kubeadm.k8s.io/v1beta4,}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "joinConfiguration" in requested fixtures
2026/02/09 18:40:41 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:40:41 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"apiVersion":"kubeadm.k8s.io/v1beta4","caCertPath":"/some/cert.crt","discovery":{},"kind":"JoinConfiguration","nodeRegistration":{"criSocket":"unix:///var/run/crio/crio.sock","name":"node-1","taints":null}})[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:140]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestBytesToJoinConfiguration (0.00s)
=== RUN   TestCtestLoadJoinConfigurationFromFile
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:174]: Start TestCtestLoadJoinConfigurationFromFile
Running 0 th file test case: Config file does not exist
Running 1 th file test case: Valid kubeadm config
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "fileContent" in requested fixtures
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:41 Mode: 1
2026/02/09 18:40:41 Base JSON size: 244 bytes
2026/02/09 18:40:41 Number of external values: 0
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="\napiVersion: kubeadm.k8s.io/v1beta4\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: 1.2.3.4:6443\n    caCertHashes:\n    - aaaa\n    token: abcdef.1234567890123456\n  tlsBootstrapToken: abcdef.1234567890123456\nkind: JoinConfiguration")[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:248]: No content generated
Running 2 th file test case: Edge – empty file
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "fileContent" in requested fixtures
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:41 Mode: 1
2026/02/09 18:40:41 Base JSON size: 2 bytes
2026/02/09 18:40:41 Number of external values: 0
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="")[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:248]: No content generated
Running 3 th file test case: Edge – malformed yaml
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "fileContent" in requested fixtures
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:40:41 Mode: 1
2026/02/09 18:40:41 Base JSON size: 30 bytes
2026/02/09 18:40:41 Number of external values: 0
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:40:41 [DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="apiVersion: ???\n: malformed")[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_joinconfiguration_test.go:248]: No content generated

==================== CTEST END ======================
--- PASS: TestCtestLoadJoinConfigurationFromFile (0.00s)
=== RUN   TestCtestBytesToUpgradeConfiguration
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:26]: Starting TestCtestBytesToUpgradeConfiguration
=== RUN   TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_JSON
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: default config is set correctly format: JSON
=== RUN   TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_YAML
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: default config is set correctly format: YAML
=== RUN   TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_JSON
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: cfg has part of fields configured format: JSON
=== RUN   TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_YAML
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: cfg has part of fields configured format: YAML
=== RUN   TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_JSON
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: no UpgradeConfiguration found format: JSON
  W0209 18:40:41.915037   83503 upgradeconfiguration.go:42] [config] WARNING: Ignored configuration document with GroupVersionKind kubeadm.k8s.io/v1beta4, Kind=InitConfiguration
=== RUN   TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_YAML
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: no UpgradeConfiguration found format: YAML
  W0209 18:40:41.915133   83503 upgradeconfiguration.go:42] [config] WARNING: Ignored configuration document with GroupVersionKind kubeadm.k8s.io/v1beta4, Kind=InitConfiguration
=== RUN   TestCtestBytesToUpgradeConfiguration/nil_cfg_value_JSON
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: nil cfg value format: JSON
=== RUN   TestCtestBytesToUpgradeConfiguration/nil_cfg_value_YAML
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: nil cfg value format: YAML
=== RUN   TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_JSON
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: empty UpgradeConfiguration struct format: JSON
    ctest_upgradeconfiguration_test.go:147: failed BytesToUpgradeConfiguration:
        	expected error: false
        	actual error: invalid configuration for GroupVersionKind /, Kind=: kind and apiVersion is mandatory information that must be specified
=== RUN   TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_YAML
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:136]: Running subtest: empty UpgradeConfiguration struct format: YAML
    ctest_upgradeconfiguration_test.go:147: failed BytesToUpgradeConfiguration:
        	expected error: false
        	actual error: invalid configuration for GroupVersionKind /, Kind=: kind and apiVersion is mandatory information that must be specified
--- FAIL: TestCtestBytesToUpgradeConfiguration (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/default_config_is_set_correctly_YAML (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/cfg_has_part_of_fields_configured_YAML (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/no_UpgradeConfiguration_found_YAML (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/nil_cfg_value_JSON (0.00s)
    --- PASS: TestCtestBytesToUpgradeConfiguration/nil_cfg_value_YAML (0.00s)
    --- FAIL: TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_JSON (0.00s)
    --- FAIL: TestCtestBytesToUpgradeConfiguration/empty_UpgradeConfiguration_struct_YAML (0.00s)
=== RUN   TestCtestLoadUpgradeConfigurationFromFile
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:160]: Starting TestCtestLoadUpgradeConfigurationFromFile
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Config_file_does_not_exists
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Config file does not exists
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Valid_kubeadm_config
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Valid kubeadm config
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Empty_file
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Empty file
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Malformed_yaml
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Malformed yaml
=== RUN   TestCtestLoadUpgradeConfigurationFromFile/Wrong_kind
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:219]: Running subtest: Wrong kind
  W0209 18:40:41.917229   83503 upgradeconfiguration.go:42] [config] WARNING: Ignored configuration document with GroupVersionKind kubeadm.k8s.io/v1beta4, Kind=InitConfiguration
--- PASS: TestCtestLoadUpgradeConfigurationFromFile (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Config_file_does_not_exists (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Valid_kubeadm_config (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Empty_file (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Malformed_yaml (0.00s)
    --- PASS: TestCtestLoadUpgradeConfigurationFromFile/Wrong_kind (0.00s)
=== RUN   TestCtestDefaultedUpgradeConfiguration
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:240]: Starting TestCtestDefaultedUpgradeConfiguration
=== RUN   TestCtestDefaultedUpgradeConfiguration/config_is_empty
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:369]: Running subtest: config is empty
=== RUN   TestCtestDefaultedUpgradeConfiguration/config_has_some_fields_configured
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:369]: Running subtest: config has some fields configured
=== RUN   TestCtestDefaultedUpgradeConfiguration/nil_config_pointer
[DEBUG-CTEST 2026-02-09 18:40:41 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:369]: Running subtest: nil config pointer
--- FAIL: TestCtestDefaultedUpgradeConfiguration (0.00s)
    --- PASS: TestCtestDefaultedUpgradeConfiguration/config_is_empty (0.00s)
    --- PASS: TestCtestDefaultedUpgradeConfiguration/config_has_some_fields_configured (0.00s)
    --- FAIL: TestCtestDefaultedUpgradeConfiguration/nil_config_pointer (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0xe0 pc=0x1016d9640]

goroutine 107 [running]:
testing.tRunner.func1.2({0x102ca97e0, 0x1051dfbc0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x102ca97e0?, 0x1051dfbc0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4.SetDefaults_UpgradeConfiguration(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta4/defaults.go:276 +0x90
k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4.SetObjectDefaults_UpgradeConfiguration(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta4/zz_generated.defaults.go:93 +0x94
k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta4.RegisterDefaults.func5({0x1030aa0a0?, 0x0?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/apis/kubeadm/v1beta4/zz_generated.defaults.go:36 +0x78
k8s.io/apimachinery/pkg/runtime.(*Scheme).Default(0x140000a7cf8?, {0x103221260, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apimachinery/pkg/runtime/scheme.go:356 +0x94
k8s.io/kubernetes/cmd/kubeadm/app/util/config.DefaultedUpgradeConfiguration(0x0, {0x48?, 0x40?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/upgradeconfiguration.go:141 +0xb8
k8s.io/kubernetes/cmd/kubeadm/app/util/config.TestCtestDefaultedUpgradeConfiguration.func1(0x14000583a40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/ctest_upgradeconfiguration_test.go:370 +0xe4
testing.tRunner(0x14000583a40, 0x140005ec450)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 104
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/config	0.982s
=== RUN   TestCtestVerifyUnmarshalStrict
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:76]: Start strict unmarshal verification test
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:78]: total test cases: 22
=== RUN   TestCtestVerifyUnmarshalStrict/0-invalid_casesensitive_field.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_casesensitive_field.yaml ClusterConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/1-invalid_duplicate_field_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_clustercfg.yaml InitConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/2-invalid_duplicate_field_joincfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_joincfg.yaml JoinConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/3-invalid_duplicate_field_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/4-invalid_duplicate_field_kubeproxycfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_duplicate_field_kubeproxycfg.yaml KubeProxyConfiguration {kubeproxy.config.k8s.io v1alpha1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/5-invalid_unknown_field_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_clustercfg.yaml ClusterConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/6-invalid_unknown_field_initcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_initcfg.yaml InitConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/7-invalid_unknown_field_joincfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_joincfg.yaml JoinConfiguration {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/8-invalid_unknown_field_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/9-invalid_unknown_field_kubeproxycfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {invalid_unknown_field_kubeproxycfg.yaml KubeProxyConfiguration {kubeproxy.config.k8s.io v1alpha1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/10-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml ClusterConfiguration {someGroup v1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/11-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml SomeUnknownKind {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/12-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml ClusterConfiguration {kubeadm.k8s.io v1beta4} false}
=== RUN   TestCtestVerifyUnmarshalStrict/13-valid_initcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_initcfg.yaml InitConfiguration {kubeadm.k8s.io v1beta4} false}
=== RUN   TestCtestVerifyUnmarshalStrict/14-valid_joincfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_joincfg.yaml JoinConfiguration {kubeadm.k8s.io v1beta4} false}
=== RUN   TestCtestVerifyUnmarshalStrict/15-valid_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} false}
=== RUN   TestCtestVerifyUnmarshalStrict/16-valid_kubeproxycfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_kubeproxycfg.yaml KubeProxyConfiguration {kubeproxy.config.k8s.io v1alpha1} false}
=== RUN   TestCtestVerifyUnmarshalStrict/17-
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: { ClusterConfiguration {kubeadm.k8s.io v1beta4} true}
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:90]: expected read error: read testdata: is a directory
=== RUN   TestCtestVerifyUnmarshalStrict/18-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml  {kubeadm.k8s.io v1beta4} true}
=== RUN   TestCtestVerifyUnmarshalStrict/19-valid_clustercfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_clustercfg.yaml ClusterConfiguration { } true}
=== RUN   TestCtestVerifyUnmarshalStrict/20-valid_initcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_initcfg.yaml NonExistentKind {unknownGroup v1} true}
=== RUN   TestCtestVerifyUnmarshalStrict/21-valid_kubeletcfg.yaml
[DEBUG-CTEST 2026-02-09 18:40:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/config/strict/ctest_strict_test.go:82]: Running test case: {valid_kubeletcfg.yaml KubeletConfiguration {kubelet.config.k8s.io v1beta1} true}
    ctest_strict_test.go:100: expected error true, got false, error: <nil>

==================== CTEST END ======================
--- FAIL: TestCtestVerifyUnmarshalStrict (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/0-invalid_casesensitive_field.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/1-invalid_duplicate_field_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/2-invalid_duplicate_field_joincfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/3-invalid_duplicate_field_kubeletcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/4-invalid_duplicate_field_kubeproxycfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/5-invalid_unknown_field_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/6-invalid_unknown_field_initcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/7-invalid_unknown_field_joincfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/8-invalid_unknown_field_kubeletcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/9-invalid_unknown_field_kubeproxycfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/10-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/11-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/12-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/13-valid_initcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/14-valid_joincfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/15-valid_kubeletcfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/16-valid_kubeproxycfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/17- (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/18-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/19-valid_clustercfg.yaml (0.00s)
    --- PASS: TestCtestVerifyUnmarshalStrict/20-valid_initcfg.yaml (0.00s)
    --- FAIL: TestCtestVerifyUnmarshalStrict/21-valid_kubeletcfg.yaml (0.00s)
FAIL
coverage: 100.0% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/config/strict	1.405s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/crypto	0.277s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPrintDryRunFiles
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_empty
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_a_file_that_does_not_exist
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_a_readable_file
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_empty_and_PrintPath_empty_(both_empty)
=== RUN   TestCtestPrintDryRunFiles/RealPath_is_a_readable_file_with_non‑empty_PrintPath
--- PASS: TestCtestPrintDryRunFiles (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_empty (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_a_file_that_does_not_exist (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_a_readable_file (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_empty_and_PrintPath_empty_(both_empty) (0.00s)
    --- PASS: TestCtestPrintDryRunFiles/RealPath_is_a_readable_file_with_non‑empty_PrintPath (0.00s)
=== RUN   TestCtestNewFileToPrint
=== RUN   TestCtestNewFileToPrint/TestNewFileToPrint
=== RUN   TestCtestNewFileToPrint/TestNewFileToPrint#01
=== RUN   TestCtestNewFileToPrint/TestNewFileToPrint#02
--- PASS: TestCtestNewFileToPrint (0.00s)
    --- PASS: TestCtestNewFileToPrint/TestNewFileToPrint (0.00s)
    --- PASS: TestCtestNewFileToPrint/TestNewFileToPrint#01 (0.00s)
    --- PASS: TestCtestNewFileToPrint/TestNewFileToPrint#02 (0.00s)
PASS
coverage: 37.2% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/dryrun	1.451s	coverage: 37.2% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/errors	1.627s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/etcd	0.507s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestTagFromImage

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:40:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/image/ctest_image_test.go:39]: Total test cases: 22
Running test case input: "example.com/kindest/node:v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: "v1.17.0"
Running test case input: "example.com:3000/kindest/node" expected: ""
Running test case input: "example.com:3000/kindest/node:latest" expected: "latest"
Running test case input: "example.com:3000/kindest/node@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: ""
Running test case input: "" expected: ""
Running test case input: ":latest" expected: "latest"
Running test case input: "repo:tag@sha256:" expected: "tag"
    ctest_image_test.go:44: TagFromImage("repo:tag@sha256:") = "", expected "tag"
Running test case input: "kindest/node:latest" expected: "latest"
Running test case input: "kindest/node:v1.17.0" expected: "v1.17.0"
Running test case input: "example.com:3000/kindest/node:v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: "v1.17.0"
Running test case input: "kindest/node" expected: ""
Running test case input: "example.com:3000/kindest/node:v1.17.0" expected: "v1.17.0"
Running test case input: "repo:tag@invaliddigest" expected: "tag"
    ctest_image_test.go:44: TagFromImage("repo:tag@invaliddigest") = "", expected "tag"
Running test case input: "kindest/node:v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: "v1.17.0"
Running test case input: "kindest/node@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: ""
Running test case input: "example.com/kindest/node:latest" expected: "latest"
Running test case input: "example.com/kindest/node:v1.17.0" expected: "v1.17.0"
Running test case input: "example.com/kindest/node@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62" expected: ""
Running test case input: "repo@:digest" expected: ""
    ctest_image_test.go:44: TagFromImage("repo@:digest") = "digest", expected ""
Running test case input: "repo:" expected: ""
Running test case input: "repo@" expected: ""
Running test case input: "example.com/kindest/node" expected: ""

==================== CTEST END ======================
--- FAIL: TestCtestTagFromImage (0.00s)
FAIL
coverage: 75.0% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/image	0.536s
	k8s.io/kubernetes/cmd/kubeadm/app/util/initsystem		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/kubeconfig	1.071s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/output		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/patches	0.723s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 18.8% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/pkiutil	1.759s	coverage: 18.8% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/pkiutil/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/pubkeypin	1.314s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestNewContainerRuntime
=== RUN   TestCtestNewContainerRuntime/valid
=== RUN   TestCtestNewContainerRuntime/invalid:_new_runtime_service_fails
=== RUN   TestCtestNewContainerRuntime/invalid:_new_image_service_fails
=== RUN   TestCtestNewContainerRuntime/invalid:_both_services_fail
--- PASS: TestCtestNewContainerRuntime (0.00s)
    --- PASS: TestCtestNewContainerRuntime/valid (0.00s)
    --- PASS: TestCtestNewContainerRuntime/invalid:_new_runtime_service_fails (0.00s)
    --- PASS: TestCtestNewContainerRuntime/invalid:_new_image_service_fails (0.00s)
    --- PASS: TestCtestNewContainerRuntime/invalid:_both_services_fail (0.00s)
=== RUN   TestCtestIsRunning
=== RUN   TestCtestIsRunning/valid
=== RUN   TestCtestIsRunning/invalid:_runtime_status_fails
=== RUN   TestCtestIsRunning/invalid:_runtime_condition_status_not_'true'
=== RUN   TestCtestIsRunning/valid:_runtime_condition_type_does_not_match
=== RUN   TestCtestIsRunning/invalid:_nil_RuntimeStatus
    ctest_runtime_test.go:131: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/runtime/ctest_runtime_test.go:131
        	Error:      	Not equal: 
        	            	expected: true
        	            	actual  : false
        	Test:       	TestCtestIsRunning/invalid:_nil_RuntimeStatus
--- FAIL: TestCtestIsRunning (0.00s)
    --- PASS: TestCtestIsRunning/valid (0.00s)
    --- PASS: TestCtestIsRunning/invalid:_runtime_status_fails (0.00s)
    --- PASS: TestCtestIsRunning/invalid:_runtime_condition_status_not_'true' (0.00s)
    --- PASS: TestCtestIsRunning/valid:_runtime_condition_type_does_not_match (0.00s)
    --- FAIL: TestCtestIsRunning/invalid:_nil_RuntimeStatus (0.00s)
=== RUN   TestCtestListKubeContainers
=== RUN   TestCtestListKubeContainers/valid
=== RUN   TestCtestListKubeContainers/invalid:_list_pod_sandbox_fails
=== RUN   TestCtestListKubeContainers/valid:_empty_list_returns_empty_slice
--- PASS: TestCtestListKubeContainers (0.00s)
    --- PASS: TestCtestListKubeContainers/valid (0.00s)
    --- PASS: TestCtestListKubeContainers/invalid:_list_pod_sandbox_fails (0.00s)
    --- PASS: TestCtestListKubeContainers/valid:_empty_list_returns_empty_slice (0.00s)
=== RUN   TestCtestSandboxImage
=== RUN   TestCtestSandboxImage/valid
=== RUN   TestCtestSandboxImage/invalid:_runtime_status_fails
=== RUN   TestCtestSandboxImage/invalid:_no_config_JSON
=== RUN   TestCtestSandboxImage/invalid:_no_config
=== RUN   TestCtestSandboxImage/valid:_empty_sandboxImage_value
    ctest_runtime_test.go:246: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/app/util/runtime/ctest_runtime_test.go:246
        	Error:      	Not equal: 
        	            	expected: false
        	            	actual  : true
        	Test:       	TestCtestSandboxImage/valid:_empty_sandboxImage_value
--- FAIL: TestCtestSandboxImage (0.00s)
    --- PASS: TestCtestSandboxImage/valid (0.00s)
    --- PASS: TestCtestSandboxImage/invalid:_runtime_status_fails (0.00s)
    --- PASS: TestCtestSandboxImage/invalid:_no_config_JSON (0.00s)
    --- PASS: TestCtestSandboxImage/invalid:_no_config (0.00s)
    --- FAIL: TestCtestSandboxImage/valid:_empty_sandboxImage_value (0.00s)
=== RUN   TestCtestRemoveContainers
=== RUN   TestCtestRemoveContainers/valid
=== RUN   TestCtestRemoveContainers/valid:_two_containers
=== RUN   TestCtestRemoveContainers/invalid:_remove_pod_sandbox_fails
=== RUN   TestCtestRemoveContainers/invalid:_stop_pod_sandbox_fails
=== RUN   TestCtestRemoveContainers/valid:_nil_slice_(no_containers)
--- PASS: TestCtestRemoveContainers (0.00s)
    --- PASS: TestCtestRemoveContainers/valid (0.00s)
    --- PASS: TestCtestRemoveContainers/valid:_two_containers (0.00s)
    --- PASS: TestCtestRemoveContainers/invalid:_remove_pod_sandbox_fails (0.00s)
    --- PASS: TestCtestRemoveContainers/invalid:_stop_pod_sandbox_fails (0.00s)
    --- PASS: TestCtestRemoveContainers/valid:_nil_slice_(no_containers) (0.00s)
=== RUN   TestCtestPullImage
=== RUN   TestCtestPullImage/valid
=== RUN   TestCtestPullImage/invalid:_pull_image_fails
=== RUN   TestCtestPullImage/valid:_pull_image_succeeds_with_non‑empty_reference
--- PASS: TestCtestPullImage (0.00s)
    --- PASS: TestCtestPullImage/valid (0.00s)
    --- PASS: TestCtestPullImage/invalid:_pull_image_fails (0.00s)
    --- PASS: TestCtestPullImage/valid:_pull_image_succeeds_with_non‑empty_reference (0.00s)
=== RUN   TestCtestImageExists
=== RUN   TestCtestImageExists/valid
=== RUN   TestCtestImageExists/invalid:_image_status_fails
W0209 18:40:46.394645   83612 runtime.go:234] Failed to get image status, image: "", error: test
=== RUN   TestCtestImageExists/invalid:_nil_image_in_response
--- PASS: TestCtestImageExists (0.00s)
    --- PASS: TestCtestImageExists/valid (0.00s)
    --- PASS: TestCtestImageExists/invalid:_image_status_fails (0.00s)
    --- PASS: TestCtestImageExists/invalid:_nil_image_in_response (0.00s)
=== RUN   TestCtestIsExistingSocket
=== RUN   TestCtestIsExistingSocket/Valid_domain_socket_is_detected_as_such
=== RUN   TestCtestIsExistingSocket/Regular_file_is_not_a_domain_socket
=== RUN   TestCtestIsExistingSocket/Non_existent_socket_is_not_a_domain_socket
=== RUN   TestCtestIsExistingSocket/Empty_string_is_not_a_socket
--- PASS: TestCtestIsExistingSocket (0.01s)
    --- PASS: TestCtestIsExistingSocket/Valid_domain_socket_is_detected_as_such (0.01s)
    --- PASS: TestCtestIsExistingSocket/Regular_file_is_not_a_domain_socket (0.00s)
    --- PASS: TestCtestIsExistingSocket/Non_existent_socket_is_not_a_domain_socket (0.00s)
    --- PASS: TestCtestIsExistingSocket/Empty_string_is_not_a_socket (0.00s)
=== RUN   TestCtestDetectCRISocketImpl
=== RUN   TestCtestDetectCRISocketImpl/No_existing_sockets,_use_default
=== RUN   TestCtestDetectCRISocketImpl/One_valid_CRI_socket_leads_to_success
=== RUN   TestCtestDetectCRISocketImpl/Multiple_CRI_sockets_lead_to_an_error
=== RUN   TestCtestDetectCRISocketImpl/Invalid_socket_format_should_error
    ctest_runtime_test.go:504: detectCRISocketImpl returned unexpected result
        	Expected error: true
        	Got error: false
--- FAIL: TestCtestDetectCRISocketImpl (0.00s)
    --- PASS: TestCtestDetectCRISocketImpl/No_existing_sockets,_use_default (0.00s)
    --- PASS: TestCtestDetectCRISocketImpl/One_valid_CRI_socket_leads_to_success (0.00s)
    --- PASS: TestCtestDetectCRISocketImpl/Multiple_CRI_sockets_lead_to_an_error (0.00s)
    --- FAIL: TestCtestDetectCRISocketImpl/Invalid_socket_format_should_error (0.00s)
=== RUN   TestCtestPullImagesInParallel
=== RUN   TestCtestPullImagesInParallel/valid
=== RUN   TestCtestPullImagesInParallel/valid:_ifNotPresent_is_true
=== RUN   TestCtestPullImagesInParallel/invalid:_pull_fails
=== RUN   TestCtestPullImagesInParallel/invalid:_ifNotPresent_true_and_pull_fails
--- PASS: TestCtestPullImagesInParallel (0.00s)
    --- PASS: TestCtestPullImagesInParallel/valid (0.00s)
    --- PASS: TestCtestPullImagesInParallel/valid:_ifNotPresent_is_true (0.00s)
    --- PASS: TestCtestPullImagesInParallel/invalid:_pull_fails (0.00s)
    --- PASS: TestCtestPullImagesInParallel/invalid:_ifNotPresent_true_and_pull_fails (0.00s)
FAIL
coverage: 89.8% of statements
FAIL	k8s.io/kubernetes/cmd/kubeadm/app/util/runtime	0.573s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/kubeadm/app/util/staticpod	0.859s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/cmd/kubeadm/app/util/users		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/test		coverage: 0.0% of statements
=== RUN   TestCtestCmdCompletion

==================== CTEST START ====================
--- FAIL: TestCtestCmdCompletion (0.00s)
panic: the environment variable KUBEADM_PATH must point to the kubeadm binary path [recovered]
	panic: the environment variable KUBEADM_PATH must point to the kubeadm binary path

goroutine 23 [running]:
testing.tRunner.func1.2({0x1042993c0, 0x104816b90})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1042993c0?, 0x104816b90?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/cmd/kubeadm/test/cmd.getKubeadmPath(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/test/cmd/util.go:80
k8s.io/kubernetes/cmd/kubeadm/test/cmd.TestCtestCmdCompletion(0x1400049efc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/cmd/kubeadm/test/cmd/ctest_completion_test.go:12 +0x5b0
testing.tRunner(0x1400049efc0, 0x10480f650)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/cmd/kubeadm/test/cmd	0.659s
	k8s.io/kubernetes/cmd/kubeadm/test/kubeconfig		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubeadm/test/resources		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubectl		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubectl-convert		coverage: 0.0% of statements
	k8s.io/kubernetes/cmd/kubelet		coverage: 0.0% of statements
=== RUN   TestCtestValueOfAllocatableResources
    ctest_server_test.go:87: negative zero quantities: error expected
--- FAIL: TestCtestValueOfAllocatableResources (0.00s)
=== RUN   TestCtestMergeKubeletConfigurations
=== RUN   TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf
=== RUN   TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_subfield_override
=== RUN   TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_slices/lists
=== RUN   TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf.d
=== RUN   TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf
=== RUN   TestCtestMergeKubeletConfigurations/json_conversion_is_correct
=== RUN   TestCtestMergeKubeletConfigurations/invalid_drop-in_apiVersion
=== RUN   TestCtestMergeKubeletConfigurations/invalid_drop-in_kind
=== RUN   TestCtestMergeKubeletConfigurations/empty_drop-in_apiVersion/kind
=== RUN   TestCtestMergeKubeletConfigurations/identical_kubelet_config_and_drop-in_file
--- PASS: TestCtestMergeKubeletConfigurations (0.01s)
    --- PASS: TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_subfield_override (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/kubelet.conf.d_overrides_kubelet.conf_with_slices/lists (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf.d (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/cli_args_override_kubelet.conf (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/json_conversion_is_correct (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/invalid_drop-in_apiVersion (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/invalid_drop-in_kind (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/empty_drop-in_apiVersion/kind (0.00s)
    --- PASS: TestCtestMergeKubeletConfigurations/identical_kubelet_config_and_drop-in_file (0.00s)
FAIL
coverage: 9.3% of statements
FAIL	k8s.io/kubernetes/cmd/kubelet/app	0.992s
=== RUN   TestCtestRoundTrip
    ctest_options_test.go:101: default flags are eliminated: args: []
    ctest_options_test.go:101: default flag values round trip: args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address=127.0.0.1 --healthz-port=10248 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-high-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-low-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-service-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kernel-memcg-notification has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-content-type has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --local-storage-capacity-isolation has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-info-buffer-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-split-stream has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --logging-format has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --make-iptables-util-chains has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --manifest-url has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-open-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --maximum-dead-containers has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --maximum-dead-containers-per-container has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --memory-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --minimum-image-ttl-duration has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-max-images has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --oom-score-adj has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-cidr has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-max-pids has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pods-per-core has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --protect-kernel-defaults has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-node has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-memory has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-server-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runonce has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --streaming-connection-idle-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --sync-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-scope has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-stats-agg-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
    ctest_options_test.go:101: nil address does not fail for optional argument: args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address= --healthz-port=10248 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-high-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-low-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-service-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kernel-memcg-notification has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-content-type has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --local-storage-capacity-isolation has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-info-buffer-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-split-stream has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --logging-format has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --make-iptables-util-chains has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --manifest-url has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-open-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --maximum-dead-containers has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --maximum-dead-containers-per-container has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --memory-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --minimum-image-ttl-duration has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-max-images has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --oom-score-adj has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-cidr has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-max-pids has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pods-per-core has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --protect-kernel-defaults has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-node has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-memory has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-server-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runonce has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --streaming-connection-idle-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --sync-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-scope has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-stats-agg-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
    ctest_options_test.go:101: excessively long healthz bind address: args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address=                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 --healthz-port=10248 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
    ctest_options_test.go:107: excessively long healthz bind address: unexpected flag error: invalid argument "\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" for "--healthz-bind-address" flag: "\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" is not a valid IP address
    ctest_options_test.go:101: invalid numeric flag value (negative port): args: [--address=0.0.0.0 --anonymous-auth=true --authentication-token-webhook=false --authentication-token-webhook-cache-ttl=2m0s --authorization-mode=AlwaysAllow --authorization-webhook-cache-authorized-ttl=5m0s --authorization-webhook-cache-unauthorized-ttl=30s --bootstrap-kubeconfig= --cert-dir=/var/lib/kubelet/pki --cgroup-driver=cgroupfs --cgroup-root= --cgroups-per-qos=true --client-ca-file= --cloud-provider= --cluster-domain= --config= --config-dir= --container-log-max-files=5 --container-log-max-size=10Mi --container-runtime-endpoint=unix:///run/containerd/containerd.sock --contention-profiling=false --cpu-cfs-quota=true --cpu-cfs-quota-period=100ms --cpu-manager-policy=none --cpu-manager-reconcile-period=10s --enable-controller-attach-detach=true --enable-debugging-handlers=true --enable-server=true --enforce-node-allocatable=pods --event-burst=100 --event-qps=50 --eviction-max-pod-grace-period=0 --eviction-pressure-transition-period=5m0s --exit-on-lock-contention=false --experimental-allocatable-ignore-eviction=false --experimental-mounter-path= --fail-cgroupv1=false --fail-swap-on=true --file-check-frequency=20s --hairpin-mode=promiscuous-bridge --healthz-bind-address=127.0.0.1 --healthz-port=-1 --hostname-override= --http-check-frequency=20s --image-credential-provider-bin-dir= --image-credential-provider-config= --image-gc-high-threshold=85 --image-gc-low-threshold=80 --image-service-endpoint= --kernel-memcg-notification=false --kube-api-burst=100 --kube-api-content-type=application/vnd.kubernetes.protobuf --kube-api-qps=50 --kube-reserved-cgroup= --kubeconfig= --kubelet-cgroups= --local-storage-capacity-isolation=true --lock-file= --log-flush-frequency=5s --log-text-info-buffer-size=0 --log-text-split-stream=false --logging-format=text --make-iptables-util-chains=true --manifest-url= --max-open-files=1000000 --max-pods=110 --maximum-dead-containers=-1 --maximum-dead-containers-per-container=1 --memory-manager-policy=None --minimum-container-ttl-duration=0s --minimum-image-ttl-duration=2m0s --node-ip= --node-labels= --node-status-max-images=50 --node-status-update-frequency=10s --oom-score-adj=-999 --pod-cidr= --pod-infra-container-image= --pod-manifest-path= --pod-max-pids=-1 --pods-per-core=0 --port=10250 --protect-kernel-defaults=false --provider-id= --read-only-port=10255 --register-node=true --register-with-taints= --registry-burst=10 --registry-qps=5 --reserved-cpus= --reserved-memory= --resolv-conf=/etc/resolv.conf --root-dir=/var/lib/kubelet --rotate-certificates=false --rotate-server-certificates=false --runonce=false --runtime-cgroups= --runtime-request-timeout=2m0s --seccomp-default=false --serialize-image-pulls=true --streaming-connection-idle-timeout=4h0m0s --sync-frequency=1m0s --system-cgroups= --system-reserved-cgroup= --tls-cert-file= --tls-min-version= --tls-private-key-file= --topology-manager-policy=none --topology-manager-scope=container --v=0 --vmodule= --volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/ --volume-stats-agg-period=1m0s]
Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --anonymous-auth has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authentication-token-webhook-cache-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-authorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --authorization-webhook-cache-unauthorized-ttl has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroup-root has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cgroups-per-qos has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-log-max-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --contention-profiling has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-cfs-quota-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --cpu-manager-reconcile-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-controller-attach-detach has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-debugging-handlers has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enable-server has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --enforce-node-allocatable has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --event-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-max-pod-grace-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --eviction-pressure-transition-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --experimental-allocatable-ignore-eviction has been deprecated, will be removed in 1.25 or later.
Flag --experimental-mounter-path has been deprecated, will be removed in 1.25 or later. in favor of using CSI.
Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --file-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --hairpin-mode has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-bind-address has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --healthz-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --http-check-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-high-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-gc-low-threshold has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --image-service-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kernel-memcg-notification has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-content-type has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-api-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kube-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --kubelet-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --local-storage-capacity-isolation has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-info-buffer-size has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --log-text-split-stream has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --logging-format has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --make-iptables-util-chains has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --manifest-url has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-open-files has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --max-pods has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --maximum-dead-containers has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --maximum-dead-containers-per-container has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --memory-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Flag --minimum-image-ttl-duration has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-max-images has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --node-status-update-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --oom-score-adj has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-cidr has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.
Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-max-pids has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pods-per-core has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --protect-kernel-defaults has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --read-only-port has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-node has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --register-with-taints has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-burst has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --registry-qps has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-cpus has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --reserved-memory has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --resolv-conf has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --rotate-server-certificates has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runonce has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --runtime-request-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --serialize-image-pulls has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --streaming-connection-idle-timeout has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --sync-frequency has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-cgroups has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --system-reserved-cgroup has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-cert-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --tls-private-key-file has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-policy has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --topology-manager-scope has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-plugin-dir has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --volume-stats-agg-period has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
--- FAIL: TestCtestRoundTrip (0.00s)
=== RUN   TestCtestValidateKubeletFlags
=== RUN   TestCtestValidateKubeletFlags/Invalid_kubernetes.io_label
=== RUN   TestCtestValidateKubeletFlags/Valid_label_outside_of_kubernetes.io_and_k8s.io
=== RUN   TestCtestValidateKubeletFlags/Empty_label_list
=== RUN   TestCtestValidateKubeletFlags/Invalid_label
=== RUN   TestCtestValidateKubeletFlags/Empty_label_key
=== RUN   TestCtestValidateKubeletFlags/Label_key_with_invalid_characters
    ctest_options_test.go:186: ValidateKubeletFlags should have failed with labels: map[invalid/key:value]
=== RUN   TestCtestValidateKubeletFlags/Excessively_long_label_key_and_value
--- FAIL: TestCtestValidateKubeletFlags (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Invalid_kubernetes.io_label (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Valid_label_outside_of_kubernetes.io_and_k8s.io (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Empty_label_list (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Invalid_label (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Empty_label_key (0.00s)
    --- FAIL: TestCtestValidateKubeletFlags/Label_key_with_invalid_characters (0.00s)
    --- PASS: TestCtestValidateKubeletFlags/Excessively_long_label_key_and_value (0.00s)
FAIL
coverage: 89.0% of statements
FAIL	k8s.io/kubernetes/cmd/kubelet/app/options	0.492s
	k8s.io/kubernetes/cmd/kubemark		coverage: 0.0% of statements
=== RUN   TestCtestHollowNode
=== RUN   TestCtestHollowNode/kubelet
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0209 18:41:00.266694   83792 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
I0209 18:41:00.269601   83792 hollow_kubelet.go:152] Using /tmp/hollow-kubelet.3080354323 as root dir for hollow-kubelet
I0209 18:41:00.269881   83792 feature_gate.go:385] feature gates: {map[]}
I0209 18:41:00.270197   83792 logging.go:31] "[core] [Server #1]Server created\n"
I0209 18:41:00.270829   83792 log.go:25] "Connecting to runtime service" endpoint="unix:///tmp/kubelet_remote_5587194740633939459.sock"
I0209 18:41:00.270868   83792 logging.go:31] "[core] [Server #1 ListenSocket #2]ListenSocket created\n"
I0209 18:41:00.270950   83792 clientconn.go:1678] "[core] original dial target is: \"/tmp/kubelet_remote_5587194740633939459.sock\"\n"
I0209 18:41:00.270984   83792 clientconn.go:333] "[core] [Channel #3]Channel created\n"
I0209 18:41:00.271010   83792 logging.go:39] "[core] [Channel #3]parsed dial target is: resolver.Target{URL:url.URL{Scheme:\"passthrough\", Opaque:\"\", User:(*url.Userinfo)(nil), Host:\"\", Path:\"//tmp/kubelet_remote_5587194740633939459.sock\", RawPath:\"\", OmitHost:false, ForceQuery:false, RawQuery:\"\", Fragment:\"\", RawFragment:\"\"}}\n"
I0209 18:41:00.271019   83792 logging.go:39] "[core] [Channel #3]Channel authority set to \"localhost\"\n"
I0209 18:41:00.271223   83792 logging.go:39] "[core] [Channel #3]Resolver state updated: {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/tmp/kubelet_remote_5587194740633939459.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/tmp/kubelet_remote_5587194740633939459.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n} (resolver returned new addresses)\n"
I0209 18:41:00.271242   83792 logging.go:39] "[core] [Channel #3]Channel switches to new LB policy \"pick_first\"\n"
I0209 18:41:00.271294   83792 pickfirstleaf.go:279] "[pick-first-leaf-lb] [pick-first-leaf-lb 0x14000740990] Received new config {\n  \"shuffleAddressList\": false\n}, resolver state {\n  \"Addresses\": [\n    {\n      \"Addr\": \"/tmp/kubelet_remote_5587194740633939459.sock\",\n      \"ServerName\": \"\",\n      \"Attributes\": null,\n      \"BalancerAttributes\": null,\n      \"Metadata\": null\n    }\n  ],\n  \"Endpoints\": [\n    {\n      \"Addresses\": [\n        {\n          \"Addr\": \"/tmp/kubelet_remote_5587194740633939459.sock\",\n          \"ServerName\": \"\",\n          \"Attributes\": null,\n          \"BalancerAttributes\": null,\n          \"Metadata\": null\n        }\n      ],\n      \"Attributes\": null\n    }\n  ],\n  \"ServiceConfig\": null,\n  \"Attributes\": null\n}\n"
I0209 18:41:00.271306   83792 logging.go:39] "[core] [Channel #3]Channel Connectivity change to CONNECTING\n"
I0209 18:41:00.271324   83792 clientconn.go:857] "[core] [Channel #3 SubChannel #4]Subchannel created\n"
I0209 18:41:00.271337   83792 clientconn.go:333] "[core] [Channel #3]Channel exiting idle mode\n"
I0209 18:41:00.271346   83792 log.go:25] "Validating the CRI v1 API runtime version"
I0209 18:41:00.271389   83792 logging.go:39] "[core] [Channel #3 SubChannel #4]Subchannel Connectivity change to CONNECTING\n"
I0209 18:41:00.271411   83792 logging.go:39] "[core] [Channel #3 SubChannel #4]Subchannel picks a new address \"/tmp/kubelet_remote_5587194740633939459.sock\" to connect\n"
I0209 18:41:00.271729   83792 syscall_nonlinux.go:39] "[core] CPU time info is unavailable on non-linux environments.\n"
I0209 18:41:00.271820   83792 logging.go:39] "[core] [Channel #3 SubChannel #4]Subchannel Connectivity change to READY\n"
I0209 18:41:00.271839   83792 pickfirstleaf.go:633] "[pick-first-leaf-lb] [pick-first-leaf-lb 0x14000740990] SubConn 0x1400056d7c0 reported connectivity state READY and the health listener is disabled. Transitioning SubConn to READY.\n"
I0209 18:41:00.271844   83792 logging.go:39] "[core] [Channel #3]Channel Connectivity change to READY\n"
I0209 18:41:00.272430   83792 log.go:25] "Validated CRI v1 runtime API"
    server.go:1234: I0209 18:41:00.272531] Using root directory path="/tmp/hollow-kubelet.3080354323"
I0209 18:41:00.272848   83792 kubelet.go:475] "Attempting to sync node with API server"
I0209 18:41:00.272860   83792 kubelet.go:376] "Adding static pod path" path="/tmp/hollow-kubelet.3080354323/static-pods2010222722"
I0209 18:41:00.273005   83792 file.go:68] "Watching path" path="/tmp/hollow-kubelet.3080354323/static-pods2010222722"
E0209 18:41:00.273031   83792 file_unsupported.go:29] "Watching source file is unsupported in this build"
I0209 18:41:00.273034   83792 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.273040   83792 kubelet.go:387] "Adding apiserver pod source"
I0209 18:41:00.273044   83792 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.273075   83792 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
I0209 18:41:00.273306   83792 reflector.go:358] "Starting reflector" type="*v1.Service" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.273315   83792 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.273503   83792 state_mem.go:40] "Initialized new in-memory state store for pod resource information tracking"
I0209 18:41:00.273531   83792 config.go:292] "Setting pods for source" source="file"
I0209 18:41:00.273556   83792 state_mem.go:40] "Initialized new in-memory state store for pod resource information tracking"
I0209 18:41:00.273895   83792 state_mem.go:36] "Initialized new in-memory state store"
I0209 18:41:00.273998   83792 fake_topology_manager.go:33] "NewFakeManager"
    kuberuntime_manager.go:291: I0209 18:41:00.274428] Container runtime initialized containerRuntime="fakeRuntime" version="0.1.0" apiVersion="0.1.0"
I0209 18:41:00.274697   83792 kubelet.go:940] "Not starting ClusterTrustBundle informer because we are in static kubelet mode or the ClusterTrustBundleProjection featuregate is disabled"
I0209 18:41:00.274735   83792 kubelet.go:964] "Not starting PodCertificateRequest manager because we are in static kubelet mode or the PodCertificateProjection feature gate is disabled"
I0209 18:41:00.274839   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/empty-dir"
I0209 18:41:00.274853   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/git-repo"
I0209 18:41:00.274867   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/host-path"
I0209 18:41:00.274888   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/nfs"
I0209 18:41:00.274912   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/secret"
I0209 18:41:00.274934   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/iscsi"
I0209 18:41:00.274954   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/downward-api"
I0209 18:41:00.274974   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/fc"
I0209 18:41:00.274997   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/configmap"
I0209 18:41:00.275008   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/projected"
I0209 18:41:00.275035   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/portworx-volume"
I0209 18:41:00.275056   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/local-volume"
I0209 18:41:00.275195   83792 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:41:00.276007   83792 fake_topology_manager.go:33] "NewFakeManager"
E0209 18:41:00.276770   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0209 18:41:00.277822   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0209 18:41:00.277963   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0209 18:41:00.285110   83792 kubelet_pods.go:148] "user namespaces: user not found, using default mappings" user="kubelet"
I0209 18:41:00.285175   83792 userns_manager.go:157] "User namespace manager mapping" offset=1 length=65535 idsPerPod=65536
    server.go:1258: E0209 18:41:00.285266] Failed to set rlimit on max file handles err="SetRLimit unsupported in this platform"
    server.go:1262: I0209 18:41:00.285292] Started kubelet
I0209 18:41:00.285386   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={}
    ratelimit.go:56: I0209 18:41:00.285500] Setting rate limiting for endpoint service="podresources" qps=<rate.Limit>: 100 burstTokens=10
I0209 18:41:00.285553   83792 logging.go:31] "[core] [Server #7]Server created\n"
    server_v1.go:49: I0209 18:41:00.285561] podresources method="list" useActivePods=true
I0209 18:41:00.285611   83792 server.go:180] "Starting to listen" address="0.0.0.0" port=0
I0209 18:41:00.285644   83792 healthz.go:208] Installing health checkers for (/healthz): "ping","log","syncloop"
E0209 18:41:00.285737   83792 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
E0209 18:41:00.286187   83792 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:41:00.286293   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0209 18:41:00.286302   83792 server.go:310] "Adding debug handlers to kubelet server"
I0209 18:41:00.286328   83792 kubelet.go:1593] "Container garbage collection succeeded"
E0209 18:41:00.286332   83792 kubelet.go:1615] "Image garbage collection failed once. Stats initialization may not have completed yet" err="imageFs information is unavailable"
I0209 18:41:00.287536   83792 server.go:249] "Starting to serve the podresources API" endpoint="unix:/tmp/hollow-kubelet.3080354323/pod-resources/kubelet.sock"
I0209 18:41:00.287560   83792 logging.go:31] "[core] [Server #7 ListenSocket #8]ListenSocket created\n"
I0209 18:41:00.287694   83792 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0209 18:41:00.287937   83792 status_manager.go:244] "Starting to sync pod status with apiserver"
E0209 18:41:00.287990   83792 kubelet_node_status.go:404] "Error getting the current node from lister" err="node \"kellys-mbp-2\" not found"
I0209 18:41:00.287940   83792 volume_manager.go:311] "The desired_state_of_world populator starts"
I0209 18:41:00.288001   83792 volume_manager.go:313] "Starting Kubelet Volume Manager"
I0209 18:41:00.288036   83792 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
E0209 18:41:00.287971   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:00.288073   83792 kubelet.go:2428] "Starting kubelet main sync loop"
I0209 18:41:00.288131   83792 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.288140   83792 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.288157   83792 reconstruct.go:97] "Volume reconstruction finished"
I0209 18:41:00.288161   83792 reconciler.go:29] "Reconciler: start to sync state"
I0209 18:41:00.288264   83792 reflector.go:358] "Starting reflector" type="*v1.RuntimeClass" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.288273   83792 reflector.go:404] "Listing and watching" type="*v1.RuntimeClass" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:00.288315   83792 kubelet.go:3005] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:; Handlers: , Features: nil"
I0209 18:41:00.288347   83792 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0209 18:41:00.288363   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0209 18:41:00.288374   83792 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
E0209 18:41:00.288515   83792 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="200ms"
    generic.go:240: I0209 18:41:00.288487] GenericPLEG: Relisting
E0209 18:41:00.288554   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
E0209 18:41:00.288582   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0209 18:41:00.288884   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
E0209 18:41:00.289129   83792 kubelet.go:2452] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0209 18:41:00.386721   83792 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0209 18:41:00.386754   83792 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0209 18:41:00.387019   83792 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0209 18:41:00.387023   83792 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0209 18:41:00.387026   83792 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0209 18:41:00.387028   83792 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0209 18:41:00.387032   83792 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0209 18:41:00.387036   83792 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0209 18:41:00.387040   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0209 18:41:00.387045   83792 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0209 18:41:00.387048   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0209 18:41:00.387052   83792 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0209 18:41:00.387064   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0209 18:41:00.387067   83792 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0209 18:41:00.387074   83792 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0209 18:41:00.387078   83792 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0209 18:41:00.387082   83792 container_manager_stub.go:52] "Starting stub container manager"
I0209 18:41:00.387089   83792 state_mem.go:36] "Initializing new in-memory state store" logger="memory-mgr.fake"
I0209 18:41:00.387104   83792 eviction_manager.go:189] "Eviction manager: starting control loop"
I0209 18:41:00.387136   83792 eviction_manager.go:251] "Eviction manager: synchronize housekeeping"
E0209 18:41:00.387151   83792 eviction_manager.go:259] "Eviction manager: failed to get HasDedicatedImageFs" err="imagefs device is not found"
E0209 18:41:00.387157   83792 eviction_manager.go:212] "Eviction manager: failed to synchronize" err="eviction manager: failed to get HasDedicatedImageFs: imagefs device is not found"
I0209 18:41:00.387141   83792 container_log_manager.go:146] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
I0209 18:41:00.387164   83792 kubelet.go:1722] "Starting plugin manager"
I0209 18:41:00.387201   83792 container_log_manager.go:195] "Starting container log rotation sequence"
I0209 18:41:00.387212   83792 container_log_manager.go:186] "Starting container log rotation worker" workerID=1
I0209 18:41:00.387221   83792 plugin_watcher.go:51] "Plugin Watcher Start" path="/tmp/hollow-kubelet.3080354323/plugins_registry"
I0209 18:41:00.387224   83792 plugin_watcher.go:100] "Ensuring Plugin directory" path="/tmp/hollow-kubelet.3080354323/plugins_registry"
I0209 18:41:00.387312   83792 plugin_manager.go:116] "The desired_state_of_world populator (plugin watcher) starts"
I0209 18:41:00.387317   83792 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
I0209 18:41:00.387379   83792 kubelet.go:3005] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:; Handlers: , Features: nil"
E0209 18:41:00.387844   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:00.388074   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={}
I0209 18:41:00.389290   83792 kubelet.go:2515] "SyncLoop ADD" source="file" pods=[]
E0209 18:41:00.488046   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:00.488231   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:00.488305   83792 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0209 18:41:00.488336   83792 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
E0209 18:41:00.490048   83792 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="400ms"
I0209 18:41:00.491969   83792 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0209 18:41:00.492008   83792 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0209 18:41:00.492545   83792 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0209 18:41:00.492551   83792 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0209 18:41:00.492554   83792 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0209 18:41:00.492571   83792 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0209 18:41:00.492575   83792 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0209 18:41:00.492578   83792 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0209 18:41:00.492582   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0209 18:41:00.492585   83792 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0209 18:41:00.492588   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0209 18:41:00.492591   83792 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0209 18:41:00.492611   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0209 18:41:00.492617   83792 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0209 18:41:00.492639   83792 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0209 18:41:00.492643   83792 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0209 18:41:00.492647   83792 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0209 18:41:00.493663   83792 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0209 18:41:00.588322   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:00.588347   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:00.688070   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:00.688319   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:00.694523   83792 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0209 18:41:00.694581   83792 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0209 18:41:00.696729   83792 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0209 18:41:00.696777   83792 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0209 18:41:00.697221   83792 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0209 18:41:00.697232   83792 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0209 18:41:00.697248   83792 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0209 18:41:00.697257   83792 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0209 18:41:00.697264   83792 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0209 18:41:00.697271   83792 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0209 18:41:00.697281   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0209 18:41:00.697289   83792 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0209 18:41:00.697296   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0209 18:41:00.697303   83792 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0209 18:41:00.697310   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0209 18:41:00.697315   83792 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0209 18:41:00.697327   83792 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0209 18:41:00.697336   83792 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0209 18:41:00.697342   83792 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0209 18:41:00.698450   83792 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0209 18:41:00.789897   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:00.789980   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:00.888646   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:00.888725   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:00.890854   83792 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="800ms"
I0209 18:41:00.988883   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:00.988893   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.088623   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.088691   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.099286   83792 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0209 18:41:01.099312   83792 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0209 18:41:01.101413   83792 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0209 18:41:01.101432   83792 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0209 18:41:01.101618   83792 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0209 18:41:01.101623   83792 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0209 18:41:01.101626   83792 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0209 18:41:01.101629   83792 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0209 18:41:01.101646   83792 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0209 18:41:01.101651   83792 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0209 18:41:01.101660   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0209 18:41:01.101665   83792 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0209 18:41:01.101671   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0209 18:41:01.101673   83792 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0209 18:41:01.101676   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0209 18:41:01.101679   83792 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0209 18:41:01.101684   83792 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0209 18:41:01.101688   83792 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0209 18:41:01.101694   83792 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0209 18:41:01.101974   83792 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0209 18:41:01.125022   83792 reflector.go:404] "Listing and watching" type="*v1.RuntimeClass" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:01.125293   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
I0209 18:41:01.188872   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.188886   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.274162   83792 apiserver.go:50] "node sync has not completed yet"
I0209 18:41:01.278772   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0209 18:41:01.288838   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.288855   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
    generic.go:240: I0209 18:41:01.288926] GenericPLEG: Relisting
I0209 18:41:01.289152   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
E0209 18:41:01.387946   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.388201   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:01.388242   83792 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:01.388922   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0209 18:41:01.488914   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.488948   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.589234   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.589335   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:01.688412   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.688420   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.691831   83792 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="1.6s"
I0209 18:41:01.745527   83792 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:01.746020   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0209 18:41:01.787949   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.788128   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:01.856455   83792 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:01.856799   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0209 18:41:01.888844   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.888851   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:01.903021   83792 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0209 18:41:01.903049   83792 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0209 18:41:01.905215   83792 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0209 18:41:01.905229   83792 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0209 18:41:01.905454   83792 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0209 18:41:01.905462   83792 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0209 18:41:01.905465   83792 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0209 18:41:01.905468   83792 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0209 18:41:01.905471   83792 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0209 18:41:01.905474   83792 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0209 18:41:01.905481   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0209 18:41:01.905484   83792 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0209 18:41:01.905487   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0209 18:41:01.905490   83792 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0209 18:41:01.905492   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0209 18:41:01.905494   83792 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0209 18:41:01.905502   83792 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0209 18:41:01.905506   83792 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0209 18:41:01.905510   83792 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0209 18:41:01.905749   83792 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0209 18:41:01.988913   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:01.988938   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.088952   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.088955   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.188915   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.188933   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.276578   83792 apiserver.go:50] "node sync has not completed yet"
I0209 18:41:02.288389   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
E0209 18:41:02.288313   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.288484   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:02.288502   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:02.288509   83792 kubelet.go:2609] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
    generic.go:240: I0209 18:41:02.289323] GenericPLEG: Relisting
I0209 18:41:02.289864   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
E0209 18:41:02.387868   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.388099   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:02.488778   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.488811   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.592386   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.592417   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.688932   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.688956   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.788872   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.788872   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.888866   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:02.888917   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:02.989374   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:02.989416   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.088733   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.088773   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:03.188151   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.188203   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
    ctest_hollow_node_test.go:74: Morph "kubelet" hasn't crashed for 3s. Calling success.
=== RUN   TestCtestHollowNode/proxy
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0209 18:41:03.267842   83792 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
I0209 18:41:03.269218   83792 server.go:527] "Version info" version="v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79"
I0209 18:41:03.269231   83792 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0209 18:41:03.269969   83792 server.go:538] "Failed to apply OOMScore" err="setting OOM scores is unsupported in this build"
I0209 18:41:03.271639   83792 config.go:200] "Starting service config controller"
I0209 18:41:03.271655   83792 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0209 18:41:03.272158   83792 config.go:106] "Starting endpoint slice config controller"
I0209 18:41:03.272163   83792 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0209 18:41:03.273336   83792 config.go:403] "Starting serviceCIDR config controller"
I0209 18:41:03.273341   83792 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0209 18:41:03.273428   83792 reflector.go:358] "Starting reflector" type="*v1.Service" resyncPeriod="30s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:03.273436   83792 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:03.273443   83792 reflector.go:358] "Starting reflector" type="*v1.ServiceCIDR" resyncPeriod="30s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:03.273448   83792 reflector.go:404] "Listing and watching" type="*v1.ServiceCIDR" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:03.273431   83792 reflector.go:358] "Starting reflector" type="*v1.EndpointSlice" resyncPeriod="30s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:03.273494   83792 reflector.go:404] "Listing and watching" type="*v1.EndpointSlice" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:03.273896   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0209 18:41:03.274468   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.EndpointSlice: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0209 18:41:03.276247   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.ServiceCIDR: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
I0209 18:41:03.276691   83792 apiserver.go:50] "node sync has not completed yet"
E0209 18:41:03.276821   83792 event_broadcaster.go:279] "Unable to write event (may retry after sleeping)" err="serializer for text/plain; charset=utf-8 doesn't exist"
I0209 18:41:03.278364   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0209 18:41:03.288864   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.288947   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.290052   83792 generic.go:240] "GenericPLEG: Relisting" logger="TestCtestHollowNode/kubelet leaked goroutine"
I0209 18:41:03.290534   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
E0209 18:41:03.292306   83792 controller.go:145] "Failed to ensure lease exists, will retry" err="serializer for text/plain; charset=utf-8 doesn't exist" interval="3.2s"
I0209 18:41:03.388832   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.388823   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.488139   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.488160   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.506158   83792 kubelet_node_status.go:358] "Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes"
I0209 18:41:03.506192   83792 kubelet_node_status.go:675] "Setting node status condition code" position=0 node="kellys-mbp-2"
I0209 18:41:03.508844   83792 kubelet_node_status.go:675] "Setting node status condition code" position=1 node="kellys-mbp-2"
I0209 18:41:03.508862   83792 kubelet_node_status.go:675] "Setting node status condition code" position=2 node="kellys-mbp-2"
I0209 18:41:03.509102   83792 kubelet_node_status.go:675] "Setting node status condition code" position=3 node="kellys-mbp-2"
I0209 18:41:03.509107   83792 kubelet_node_status.go:675] "Setting node status condition code" position=4 node="kellys-mbp-2"
I0209 18:41:03.509111   83792 kubelet_node_status.go:675] "Setting node status condition code" position=5 node="kellys-mbp-2"
I0209 18:41:03.509113   83792 kubelet_node_status.go:675] "Setting node status condition code" position=6 node="kellys-mbp-2"
I0209 18:41:03.509117   83792 kubelet_node_status.go:675] "Setting node status condition code" position=7 node="kellys-mbp-2"
I0209 18:41:03.509132   83792 kubelet_node_status.go:675] "Setting node status condition code" position=8 node="kellys-mbp-2"
I0209 18:41:03.509136   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientMemory"
I0209 18:41:03.509141   83792 kubelet_node_status.go:675] "Setting node status condition code" position=9 node="kellys-mbp-2"
I0209 18:41:03.509144   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasNoDiskPressure"
I0209 18:41:03.509147   83792 kubelet_node_status.go:675] "Setting node status condition code" position=10 node="kellys-mbp-2"
I0209 18:41:03.509150   83792 kubelet_node_status.go:645] "Recording event message for node" node="kellys-mbp-2" event="NodeHasSufficientPID"
I0209 18:41:03.509153   83792 kubelet_node_status.go:675] "Setting node status condition code" position=11 node="kellys-mbp-2"
I0209 18:41:03.509159   83792 kubelet_node_status.go:675] "Setting node status condition code" position=12 node="kellys-mbp-2"
I0209 18:41:03.509166   83792 kubelet_node_status.go:675] "Setting node status condition code" position=13 node="kellys-mbp-2"
I0209 18:41:03.509170   83792 kubelet_node_status.go:75] "Attempting to register node" node="kellys-mbp-2"
E0209 18:41:03.509466   83792 kubelet_node_status.go:107] "Unable to register node with API server" err="serializer for text/plain; charset=utf-8 doesn't exist" node="kellys-mbp-2"
I0209 18:41:03.520938   83792 reflector.go:404] "Listing and watching" type="*v1.RuntimeClass" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:03.521185   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.RuntimeClass: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
I0209 18:41:03.588856   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.588867   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.688755   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.688760   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:03.788297   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.788296   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:03.823868   83792 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:03.824177   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
I0209 18:41:03.888912   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.888923   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:03.924557   83792 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:41:03.924570   83792 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:03.924861   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0209 18:41:03.924959   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
I0209 18:41:03.988859   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:03.988869   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.088870   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.088877   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.188303   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.188309   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.277084   83792 apiserver.go:50] "node sync has not completed yet"
I0209 18:41:04.278288   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0209 18:41:04.288812   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.288826   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.288845   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:04.288853   83792 kubelet.go:2609] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
I0209 18:41:04.290845   83792 generic.go:240] "GenericPLEG: Relisting" logger="TestCtestHollowNode/kubelet leaked goroutine"
I0209 18:41:04.291209   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
I0209 18:41:04.335337   83792 reflector.go:404] "Listing and watching" type="*v1.Service" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:04.335925   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
I0209 18:41:04.337308   83792 reflector.go:404] "Listing and watching" type="*v1.ServiceCIDR" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:04.337675   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.ServiceCIDR: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
I0209 18:41:04.388905   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.388904   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.488893   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.488894   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.584242   83792 reflector.go:404] "Listing and watching" type="*v1.EndpointSlice" reflector="k8s.io/client-go/informers/factory.go:160"
E0209 18:41:04.584707   83792 reflector.go:205] "Failed to watch" err="failed to list *v1.EndpointSlice: serializer for text/plain; charset=utf-8 doesn't exist" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
I0209 18:41:04.588221   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.588241   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.688692   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.688698   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.788837   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.788845   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.888922   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:04.888972   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:04.987929   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:04.989477   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.088867   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.088871   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:05.188137   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.188178   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.278016   83792 apiserver.go:50] "node sync has not completed yet"
I0209 18:41:05.281447   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
I0209 18:41:05.288734   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.288774   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.291603   83792 generic.go:240] "GenericPLEG: Relisting" logger="TestCtestHollowNode/kubelet leaked goroutine"
I0209 18:41:05.292595   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
E0209 18:41:05.387877   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.388059   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:05.388379   83792 kubelet.go:3005] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:; Handlers: , Features: nil"
I0209 18:41:05.488149   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.488197   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.590271   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.590379   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:05.688899   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.688911   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:05.788408   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.788458   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.888558   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:05.888941   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
E0209 18:41:05.989279   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:05.989844   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:06.087954   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:06.088082   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
E0209 18:41:06.188477   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:06.188485   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:06.278502   83792 apiserver.go:50] "node sync has not completed yet"
    ctest_hollow_node_test.go:74: Morph "proxy" hasn't crashed for 3s. Calling success.
=== RUN   TestCtestHollowNode/#00
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
E0209 18:41:06.287990   83792 kubelet.go:3236] "Unable to register mirror pod because node is not registered yet" err="node \"kellys-mbp-2\" not found" node="kellys-mbp-2"
I0209 18:41:06.288085   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:06.288240   83792 config.go:104] "Looking for sources, have seen" sources=["api","file"] seenSources={"file":{}}
I0209 18:41:06.288275   83792 kubelet.go:2609] "SyncLoop (housekeeping, skipped): sources aren't ready yet"
I0209 18:41:06.296598   83792 generic.go:240] "GenericPLEG: Relisting" logger="TestCtestHollowNode/kubelet leaked goroutine"
I0209 18:41:06.297733   83792 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
    ctest_hollow_node_test.go:71: Run finished unexpectedly with error for morph "": Unknown morph: . allowed values: [kubelet proxy]
I0209 18:41:06.299442   83792 csi_plugin.go:990] Failed to contact API server when waiting for CSINode publishing: serializer for text/plain; charset=utf-8 doesn't exist
=== RUN   TestCtestHollowNode/invalid-morph
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0209 18:41:06.302355   83792 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
    ctest_hollow_node_test.go:71: Run finished unexpectedly with error for morph "invalid-morph": Unknown morph: invalid-morph. allowed values: [kubelet proxy]
=== RUN   TestCtestHollowNode/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
    ctest_hollow_node_test.go:62: read 290 bytes from kubeconfig
I0209 18:41:06.302515   83792 hollow_node.go:186] Version: v0.0.0-master+8cc511e399b929453cd98ae65b419c3cc227ec79
    ctest_hollow_node_test.go:71: Run finished unexpectedly with error for morph "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa": Unknown morph: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa. allowed values: [kubelet proxy]
I0209 18:41:06.302984   83792 kuberuntime_manager.go:496] "Retrieved pods from runtime" all=true
--- FAIL: TestCtestHollowNode (6.04s)
    --- PASS: TestCtestHollowNode/kubelet (3.00s)
    --- PASS: TestCtestHollowNode/proxy (3.02s)
    --- FAIL: TestCtestHollowNode/#00 (0.01s)
    --- FAIL: TestCtestHollowNode/invalid-morph (0.00s)
    --- FAIL: TestCtestHollowNode/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (0.00s)
FAIL
coverage: 48.6% of statements
FAIL	k8s.io/kubernetes/cmd/kubemark/app	6.759s
	k8s.io/kubernetes/cmd/preferredimports		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/cmd/prune-junit-xml	0.848s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 7.8% of statements
ok  	k8s.io/kubernetes/cmd/prune-junit-xml/logparse	1.063s	coverage: 7.8% of statements [no tests to run]
?   	k8s.io/kubernetes/hack/boilerplate/test	[no test files]
	k8s.io/kubernetes/hack/conformance		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/api/endpoints/testing		coverage: 0.0% of statements
=== RUN   TestCtestWarningsForJobSpec

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/job/ctest_warnings_test.go:43]: get default configs: {test_fixture.json [base job spec for warnings] spec [jobs] {<nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> nil <nil> {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[a:b] map[] [] [] []} {[] [] [{abc image [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] <nil> <nil> <nil> <nil>  File IfNotPresent <nil> false false false}] [] OnFailure <nil> <nil> ClusterFirst map[]  <nil>  <nil> []   <nil> <nil>  [] []  <nil> <nil> <nil> [] <nil> map[] <nil> [] <nil> [] [] <nil> <nil>}} <nil> 0x140002ad380 <nil> <nil> <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[jobs]
[DEBUG-CTEST 2026-02-09 18:41:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[jobs], int=1)[DEBUG-CTEST 2026-02-09 18:41:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec" in requested fixtures
[DEBUG-CTEST 2026-02-09 18:41:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/job/ctest_warnings_test.go:48]: Failed to generate effective config: %v mode-combination failed: no values found for field "spec" in requested fixtures
    ctest_warnings_test.go:49: GenerateEffectiveConfigReturnType failed: mode-combination failed: no values found for field "spec" in requested fixtures
--- FAIL: TestCtestWarningsForJobSpec (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/api/job	1.803s
?   	k8s.io/kubernetes/pkg/api/legacyscheme	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/node	1.348s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/persistentvolume	0.711s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/persistentvolumeclaim	0.950s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/pod	0.970s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/api/pod/testing		coverage: 0.0% of statements
=== RUN   TestCtestGetWarningsForService
=== RUN   TestCtestGetWarningsForService/new_topology_mode_set
Running edge test case #0: new topology mode set
=== RUN   TestCtestGetWarningsForService/deprecated_hints_annotation_set
Running edge test case #1: deprecated hints annotation set
=== RUN   TestCtestGetWarningsForService/externalIPs_set_when_type_is_ExternalName
Running edge test case #2: externalIPs set when type is ExternalName
=== RUN   TestCtestGetWarningsForService/externalName_set_when_type_is_not_ExternalName
Running edge test case #3: externalName set when type is not ExternalName
=== RUN   TestCtestGetWarningsForService/LoadBalancerIP_set_when_headless_service
Running edge test case #4: LoadBalancerIP set when headless service
=== RUN   TestCtestGetWarningsForService/ExternalIPs_set_when_headless_service
Running edge test case #5: ExternalIPs set when headless service
=== RUN   TestCtestGetWarningsForService/SessionAffinity_Client_IP_set_when_headless_service
Running edge test case #6: SessionAffinity Client IP set when headless service
=== RUN   TestCtestGetWarningsForService/SessionAffinity_None_set_when_headless_service
Running edge test case #7: SessionAffinity None set when headless service
=== RUN   TestCtestGetWarningsForService/ExternalIPs,_LoadBalancerIP_and_SessionAffinity_set_when_headless_service
Running edge test case #8: ExternalIPs, LoadBalancerIP and SessionAffinity set when headless service
=== RUN   TestCtestGetWarningsForService/empty_annotations
Running edge test case #9: empty annotations
=== RUN   TestCtestGetWarningsForService/both_new_and_deprecated_topology_annotations_set
Running edge test case #10: both new and deprecated topology annotations set
=== RUN   TestCtestGetWarningsForService/externalIPs_empty_slice_with_ExternalName_type_(no_warning)
Running edge test case #11: externalIPs empty slice with ExternalName type (no warning)
=== RUN   TestCtestGetWarningsForService/invalid_Service_type_(empty_string)
Running edge test case #12: invalid Service type (empty string)
=== RUN   TestCtestGetWarningsForService/nil_Service_pointer_(should_not_panic,_no_warnings)
Running edge test case #13: nil Service pointer (should not panic, no warnings)
--- PASS: TestCtestGetWarningsForService (0.00s)
    --- PASS: TestCtestGetWarningsForService/new_topology_mode_set (0.00s)
    --- PASS: TestCtestGetWarningsForService/deprecated_hints_annotation_set (0.00s)
    --- PASS: TestCtestGetWarningsForService/externalIPs_set_when_type_is_ExternalName (0.00s)
    --- PASS: TestCtestGetWarningsForService/externalName_set_when_type_is_not_ExternalName (0.00s)
    --- PASS: TestCtestGetWarningsForService/LoadBalancerIP_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/ExternalIPs_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/SessionAffinity_Client_IP_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/SessionAffinity_None_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/ExternalIPs,_LoadBalancerIP_and_SessionAffinity_set_when_headless_service (0.00s)
    --- PASS: TestCtestGetWarningsForService/empty_annotations (0.00s)
    --- PASS: TestCtestGetWarningsForService/both_new_and_deprecated_topology_annotations_set (0.00s)
    --- PASS: TestCtestGetWarningsForService/externalIPs_empty_slice_with_ExternalName_type_(no_warning) (0.00s)
    --- PASS: TestCtestGetWarningsForService/invalid_Service_type_(empty_string) (0.00s)
    --- PASS: TestCtestGetWarningsForService/nil_Service_pointer_(should_not_panic,_no_warnings) (0.00s)
=== RUN   TestCtestGetWarningsForServiceClusterIPs
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv4_No_failures
Running test case: IPv4 No failures
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv6_No_failures
Running test case: IPv6 No failures
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv4_with_leading_zeros
Running test case: IPv4 with leading zeros
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros
Running test case: Dual Stack IPv4-IPv6 and IPv4 with leading zeros
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros
Running test case: Dual Stack IPv6-IPv4 and IPv4 with leading zeros
=== RUN   TestCtestGetWarningsForServiceClusterIPs/IPv6_non_canonical_format
Running test case: IPv6 non canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv6_non-canonical_format
Running test case: Dual Stack IPv4-IPv6 and IPv6 non-canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv6_non-canonical_formats
Running test case: Dual Stack IPv6-IPv4 and IPv6 non-canonical formats
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format
Running test case: Dual Stack IPv4-IPv6 and IPv4 with leading zeros and IPv6 non-canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format
Running test case: Dual Stack IPv6-IPv4 and IPv4 with leading zeros and IPv6 non-canonical format
=== RUN   TestCtestGetWarningsForServiceClusterIPs/Service_with_all_IPs_fields_with_errors
Running test case: Service with all IPs fields with errors
=== RUN   TestCtestGetWarningsForServiceClusterIPs/empty_clusterIPs_slice_(no_warnings)
Running test case: empty clusterIPs slice (no warnings)
=== RUN   TestCtestGetWarningsForServiceClusterIPs/nil_service_pointer_(should_not_panic,_no_warnings)
Running test case: nil service pointer (should not panic, no warnings)
=== RUN   TestCtestGetWarningsForServiceClusterIPs/service_with_duplicate_IPs_(should_produce_duplicate_warnings)
Running test case: service with duplicate IPs (should produce duplicate warnings)
    ctest_warnings_test.go:313: GetWarningsForService() =   []string(
        - 	nil,
        + 	{`spec.clusterIPs[1]: duplicate IP address "192.12.2.2"`},
          )
--- FAIL: TestCtestGetWarningsForServiceClusterIPs (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv4_No_failures (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv6_No_failures (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv4_with_leading_zeros (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/IPv6_non_canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv6_non-canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv6_non-canonical_formats (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv4-IPv6_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Dual_Stack_IPv6-IPv4_and_IPv4_with_leading_zeros_and_IPv6_non-canonical_format (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/Service_with_all_IPs_fields_with_errors (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/empty_clusterIPs_slice_(no_warnings) (0.00s)
    --- PASS: TestCtestGetWarningsForServiceClusterIPs/nil_service_pointer_(should_not_panic,_no_warnings) (0.00s)
    --- FAIL: TestCtestGetWarningsForServiceClusterIPs/service_with_duplicate_IPs_(should_produce_duplicate_warnings) (0.00s)
FAIL
coverage: 50.0% of statements
FAIL	k8s.io/kubernetes/pkg/api/service	1.170s
	k8s.io/kubernetes/pkg/api/service/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/servicecidr	0.979s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestStorageClassWarnings

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/storage/ctest_util_test.go:23]: hardcoded config: [{test_fixture.json [null] allowedTopologies [storageclasses] <nil>} {test_fixture.json [no warning] allowedTopologies [storageclasses] 0x14000200000} {test_fixture.json [warning] allowedTopologies [storageclasses] 0x14000200480} {test_fixture.json [empty allowedTopologies] allowedTopologies [storageclasses] 0x140001ca600} {test_fixture.json [nil matchLabelExpressions] allowedTopologies [storageclasses] 0x140001ca900}]
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/storage/ctest_util_test.go:27]: processing testInfo: [null]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[storageclasses]
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[storageclasses], int=1)[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [storageclasses]
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:09 load all fixtures failed: requested fixture keys not found in test_fixtures.json: storageclasses
FAIL	k8s.io/kubernetes/pkg/api/storage	0.570s
=== RUN   TestCtestCompatibility_v1_PodSecurityContext
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/testing/ctest_backward_compatibility_test.go:95]: Running case: hostNetwork true
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/testing/ctest_backward_compatibility_test.go:102]: Matched config: {test_fixture.json [hostNetwork true] hostNetwork [pods deployments statefulsets daemonsets replicasets] {[] [] [{a my-container-image [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] [] Never <nil> <nil>  map[]   <nil>  true false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:41:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:09 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/api/testing	0.933s
	k8s.io/kubernetes/pkg/api/testing/compat		coverage: 0.0% of statements
=== RUN   TestCtestPackSubsets

==================== CTEST OVERRIDE ONLY START ====================
    ctest_util_test.go:520: case "negative port number (invalid, should be ignored)": expected ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=1) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.2",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) (len=1) {
              (v1.EndpointPort) {
                Name: (string) "",
                Port: (int32) -1,
                Protocol: (v1.Protocol) "",
                AppProtocol: (*string)(<nil>)
              }
            }
          }
        }
        , got ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=1) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.2",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) <nil>
          }
        }
    ctest_util_test.go:520: case "duplicate IPs with mixed nil and non-nil TargetRef": expected ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=1) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.4",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) (len=1) {
              (v1.EndpointPort) {
                Name: (string) "",
                Port: (int32) 8080,
                Protocol: (v1.Protocol) "",
                AppProtocol: (*string)(<nil>)
              }
            }
          }
        }
        , got ([]v1.EndpointSubset) (len=1) {
          (v1.EndpointSubset) {
            Addresses: ([]v1.EndpointAddress) (len=2) {
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.4",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)(<nil>)
              },
              (v1.EndpointAddress) {
                IP: (string) (len=8) "10.0.0.4",
                Hostname: (string) "",
                NodeName: (*string)(<nil>),
                TargetRef: (*v1.ObjectReference)({
                  Kind: (string) "",
                  Namespace: (string) "",
                  Name: (string) "",
                  UID: (types.UID) (len=7) "uid-dup",
                  APIVersion: (string) "",
                  ResourceVersion: (string) "",
                  FieldPath: (string) ""
                })
              }
            },
            NotReadyAddresses: ([]v1.EndpointAddress) <nil>,
            Ports: ([]v1.EndpointPort) (len=1) {
              (v1.EndpointPort) {
                Name: (string) "",
                Port: (int32) 8080,
                Protocol: (v1.Protocol) "",
                AppProtocol: (*string)(<nil>)
              }
            }
          }
        }
--- FAIL: TestCtestPackSubsets (0.00s)
FAIL
coverage: 98.8% of statements
FAIL	k8s.io/kubernetes/pkg/api/v1/endpoints	2.838s
=== RUN   TestCtestPVSecrets

==================== CTEST UNION MODE START ====================
[DEBUG-CTEST 2026-02-09 18:41:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/persistentvolume/ctest_util_test.go:30]: get default configs: {test_fixture.json [pv secret extraction] spec [persistentvolumes] [{{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil &AzureFilePersistentVolumeSource{SecretName:Spec.PersistentVolumeSource.AzureFile.SecretName,ShareName:,ReadOnly:false,SecretNamespace:nil,} nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil &AzureFilePersistentVolumeSource{SecretName:Spec.PersistentVolumeSource.AzureFile.SecretName,ShareName:,ReadOnly:false,SecretNamespace:*Spec.PersistentVolumeSource.AzureFile.SecretNamespace,} nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil &CephFSPersistentVolumeSource{Monitors:[],Path:,User:,SecretFile:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CephFS.SecretRef,Namespace:cephfs,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil &CephFSPersistentVolumeSource{Monitors:[],Path:,User:,SecretFile:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CephFS.SecretRef,Namespace:,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil &CinderPersistentVolumeSource{VolumeID:,FSType:,ReadOnly:false,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.Cinder.SecretRef,Namespace:cinder,},} nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] nil   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil &FlexPersistentVolumeSource{Driver:,FSType:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.FlexVolume.SecretRef,Namespace:flexns,},ReadOnly:false,Options:map[string]string{},} nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil &FlexPersistentVolumeSource{Driver:,FSType:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.FlexVolume.SecretRef,Namespace:,},ReadOnly:false,Options:map[string]string{},} nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil &RBDPersistentVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.RBD.SecretRef,Namespace:,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil &RBDPersistentVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.RBD.SecretRef,Namespace:rbdns,},ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ScaleIOPersistentVolumeSource{Gateway:,System:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ScaleIO.SecretRef,Namespace:,},SSLEnabled:false,ProtectionDomain:,StoragePool:,StorageMode:,VolumeName:,FSType:,ReadOnly:false,} nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ScaleIOPersistentVolumeSource{Gateway:,System:,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ScaleIO.SecretRef,Namespace:scaleions,},SSLEnabled:false,ProtectionDomain:,StoragePool:,StorageMode:,VolumeName:,FSType:,ReadOnly:false,} nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil &ISCSIPersistentVolumeSource{TargetPortal:,IQN:,Lun:0,ISCSIInterface:,FSType:,ReadOnly:false,Portals:[],DiscoveryCHAPAuth:false,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ISCSI.SecretRef,Namespace:iscsi,},SessionCHAPAuth:false,InitiatorName:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil &ISCSIPersistentVolumeSource{TargetPortal:,IQN:,Lun:0,ISCSIInterface:,FSType:,ReadOnly:false,Portals:[],DiscoveryCHAPAuth:false,SecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.ISCSI.SecretRef,Namespace:,},SessionCHAPAuth:false,InitiatorName:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &StorageOSPersistentVolumeSource{VolumeName:,VolumeNamespace:,FSType:,ReadOnly:false,SecretRef:&ObjectReference{Kind:,Namespace:storageosns,Name:Spec.PersistentVolumeSource.StorageOS.SecretRef,UID:,APIVersion:,ResourceVersion:,FieldPath:,},} nil} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.ControllerPublishSecretRef,Namespace:csi,},NodeStageSecretRef:nil,NodePublishSecretRef:nil,ControllerExpandSecretRef:nil,NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.NodePublishSecretRef,Namespace:csi,},ControllerExpandSecretRef:nil,NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.NodeStageSecretRef,Namespace:csi,},NodePublishSecretRef:nil,ControllerExpandSecretRef:nil,NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:nil,ControllerExpandSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.ControllerExpandSecretRef,Namespace:csi,},NodeExpandSecretRef:nil,}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}} {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIPersistentVolumeSource{Driver:,VolumeHandle:,ReadOnly:false,FSType:,VolumeAttributes:map[string]string{},ControllerPublishSecretRef:nil,NodeStageSecretRef:nil,NodePublishSecretRef:nil,ControllerExpandSecretRef:nil,NodeExpandSecretRef:&SecretReference{Name:Spec.PersistentVolumeSource.CSI.NodeExpandSecretRef,Namespace:csi,},}} [] &ObjectReference{Kind:,Namespace:claimrefns,Name:claimrefname,UID:,APIVersion:,ResourceVersion:,FieldPath:,}   [] <nil> nil <nil>} {   <nil>}}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:41:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-09 18:41:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:41:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:11 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/api/v1/persistentvolume	2.379s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/v1/pod	1.200s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGetResourceRequest

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/resource/ctest_helpers_test.go:138]: Number of test cases: 17

==================== CTEST END ======================
--- PASS: TestCtestGetResourceRequest (0.00s)
=== RUN   TestCtestExtractResourceValue

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/resource/ctest_helpers_test.go:310]: Number of test cases: 18
    ctest_helpers_test.go:316: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/api/v1/resource/ctest_helpers_test.go:316
        	Error:      	Error message not equal:
        	            	expected: "invalid resource name"
        	            	actual  : "unsupported container resource : invalid.resource"
        	Test:       	TestCtestExtractResourceValue
        	Messages:   	expected test case [16] to fail with error invalid resource name; got unsupported container resource : invalid.resource
--- FAIL: TestCtestExtractResourceValue (0.00s)
FAIL
coverage: 60.0% of statements
FAIL	k8s.io/kubernetes/pkg/api/v1/resource	1.695s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/api/v1/service	1.880s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/abac		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/abac/fuzzer		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/apis/abac/latest	[no test files]
=== RUN   TestCtestV0Conversion
    ctest_conversion_test.go:158: empty namespace with resource: expected
        	&abac.Policy{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, Spec:abac.PolicySpec{User:"", Group:"system:authenticated", Readonly:false, APIGroup:"*", Resource:"myresource", Namespace:"", NonResourcePath:""}},
        got
        	&abac.Policy{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, Spec:abac.PolicySpec{User:"", Group:"system:authenticated", Readonly:false, APIGroup:"*", Resource:"myresource", Namespace:"*", NonResourcePath:""}}
--- FAIL: TestCtestV0Conversion (0.00s)
FAIL
coverage: 69.2% of statements
FAIL	k8s.io/kubernetes/pkg/apis/abac/v0	2.540s
=== RUN   TestCtestV1Beta1Conversion

==================== CTEST START ====================
=== RUN   TestCtestV1Beta1Conversion/user
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case user passed
=== RUN   TestCtestV1Beta1Conversion/*_user
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case * user passed
=== RUN   TestCtestV1Beta1Conversion/empty_user
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case empty user passed
=== RUN   TestCtestV1Beta1Conversion/user_and_group_both_set
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case user and group both set passed
=== RUN   TestCtestV1Beta1Conversion/star_group_and_explicit_user
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case star group and explicit user passed
=== RUN   TestCtestV1Beta1Conversion/both_star
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case both star passed
=== RUN   TestCtestV1Beta1Conversion/group
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case group passed
=== RUN   TestCtestV1Beta1Conversion/*_group
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case * group passed
=== RUN   TestCtestV1Beta1Conversion/empty_group
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case empty group passed
=== RUN   TestCtestV1Beta1Conversion/star_user_and_explicit_group
[DEBUG-CTEST 2026-02-09 18:41:14 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/abac/v1beta1/ctest_conversion_test.go:92]: case star user and explicit group passed

==================== CTEST END ======================
--- PASS: TestCtestV1Beta1Conversion (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/*_user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/empty_user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/user_and_group_both_set (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/star_group_and_explicit_user (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/both_star (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/group (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/*_group (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/empty_group (0.00s)
    --- PASS: TestCtestV1Beta1Conversion/star_user_and_explicit_group (0.00s)
PASS
coverage: 40.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/abac/v1beta1	0.544s	coverage: 40.0% of statements
	k8s.io/kubernetes/pkg/apis/admission		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admission/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admissionregistration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admissionregistration/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/admissionregistration/install		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/apis/admissionregistration/v1beta1_test
# [k8s.io/kubernetes/pkg/apis/admissionregistration/v1beta1_test]
pkg/apis/admissionregistration/v1beta1/ctest_defaults_test.go:345:11: non-constant format string in call to (*testing.common).Fatalf
=== RUN   TestCtestDefaultAdmissionWebhook

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:39]: Number of hardcoded config entries: 4
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [ValidatingWebhookConfiguration]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{},"name":"","sideEffects":null}]})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:51]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:53]: Skipping nil config objects
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [MutatingWebhookConfiguration]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{},"name":"","sideEffects":null}]})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:81]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:83]: Skipping nil config objects
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [scope=*]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{},"name":"","rules":[{}],"sideEffects":null}]})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:81]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:83]: Skipping nil config objects
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:42]: Processing config for: [port=443]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "webhooks" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"webhooks":[{"admissionReviewVersions":null,"clientConfig":{"service":{"name":"","namespace":""}},"name":"","sideEffects":null}]})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:81]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:83]: Skipping nil config objects

==================== CTEST END ======================
--- PASS: TestCtestDefaultAdmissionWebhook (0.01s)
=== RUN   TestCtestDefaultAdmissionPolicy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:146]: Number of hardcoded config entries: 3
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:149]: Processing config for: [ValidatingAdmissionPolicy]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec.matchConstraints" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"matchConstraints":{}},"status":{}})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:158]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:160]: Skipping nil config objects
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:149]: Processing config for: [ValidatingAdmissionPolicyBinding]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec.matchResources" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"matchResources":{}}})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:187]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:189]: Skipping nil config objects
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:149]: Processing config for: [scope=*]

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "spec.matchConstraints" in requested fixtures
2026/02/09 18:41:23 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:41:23 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"matchConstraints":{"resourceRules":[{}]}},"status":{}})[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:215]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:41:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1/ctest_defaults_test.go:217]: Skipping nil config objects

==================== CTEST END ======================
--- PASS: TestCtestDefaultAdmissionPolicy (0.01s)
PASS
coverage: 9.1% of statements
ok  	k8s.io/kubernetes/pkg/apis/admissionregistration/v1	0.753s	coverage: 9.1% of statements
=== RUN   TestCtestDefaultAdmissionPolicy
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:26]: Start TestCtestDefaultAdmissionPolicy
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicy
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicy] spec [] &ValidatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Validations:[]Validation{},FailurePolicy:nil,AuditAnnotations:[]AuditAnnotation{},MatchConditions:[]MatchCondition{},Variables:[]Variable{},},Status:ValidatingAdmissionPolicyStatus{ObservedGeneration:0,TypeChecking:nil,Conditions:[]Condition{},},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 58 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[]] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicyBinding
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicyBinding] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:nil,MatchResources:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 44 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/scope=*
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: scope=*
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [scope=*] spec [] &ValidatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{NamedRuleWithOperations{ResourceNames:[],RuleWithOperations:{[] {[] [] [] <nil>}},},},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Validations:[]Validation{},FailurePolicy:nil,AuditAnnotations:[]AuditAnnotation{},MatchConditions:[]MatchCondition{},Variables:[]Variable{},},Status:ValidatingAdmissionPolicyStatus{ObservedGeneration:0,TypeChecking:nil,Conditions:[]Condition{},},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 78 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[resourceRules:[map[]]]] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: MutatingAdmissionPolicy
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [MutatingAdmissionPolicy] spec [] &MutatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:MutatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Variables:[]Variable{},Mutations:[]Mutation{Mutation{PatchType:ApplyConfiguration,ApplyConfiguration:&ApplyConfiguration{Expression:fake string,},JSONPatch:nil,},},FailurePolicy:nil,MatchConditions:[]MatchCondition{},ReinvocationPolicy:Never,},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 174 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] mutations:[map[applyConfiguration:map[expression:fake string] patchType:ApplyConfiguration]] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy.emptySpec
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicy.emptySpec
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicy.emptySpec] spec [] &ValidatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:nil,Validations:[]Validation{},FailurePolicy:nil,AuditAnnotations:[]AuditAnnotation{},MatchConditions:[]MatchCondition{},Variables:[]Variable{},},Status:ValidatingAdmissionPolicyStatus{ObservedGeneration:0,TypeChecking:nil,Conditions:[]Condition{},},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 37 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding.emptySpec
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: ValidatingAdmissionPolicyBinding.emptySpec
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [ValidatingAdmissionPolicyBinding.emptySpec] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:nil,MatchResources:nil,ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 25 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy.emptyMutations
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:151]: Running test case: MutatingAdmissionPolicy.emptyMutations
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:161]: Matched hard‑coded config: {test_fixture.json [MutatingAdmissionPolicy.emptyMutations] spec [] &MutatingAdmissionPolicy{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:MutatingAdmissionPolicySpec{ParamKind:nil,MatchConstraints:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},Variables:[]Variable{},Mutations:[]Mutation{},FailurePolicy:nil,MatchConditions:[]MatchCondition{},ReinvocationPolicy:Never,},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 75 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchConstraints:map[] reinvocationPolicy:Never] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object

==================== CTEST END ======================
--- FAIL: TestCtestDefaultAdmissionPolicy (0.03s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/scope=* (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicy.emptySpec (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/ValidatingAdmissionPolicyBinding.emptySpec (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicy/MutatingAdmissionPolicy.emptyMutations (0.00s)
=== RUN   TestCtestDefaultAdmissionPolicyBinding
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:316]: Start TestCtestDefaultAdmissionPolicyBinding
=== RUN   TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:361]: Running test case: ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:368]: Matched config: {test_fixture.json [ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:&ParamRef{Name:,Namespace:,Selector:nil,ParameterNotFoundAction:nil,},MatchResources:nil,ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 38 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.MatchResources
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:361]: Running test case: ValidatingAdmissionPolicyBinding.MatchResources
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:368]: Matched config: {test_fixture.json [ValidatingAdmissionPolicyBinding.MatchResources] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:nil,MatchResources:&MatchResources{NamespaceSelector:nil,ObjectSelector:nil,ResourceRules:[]NamedRuleWithOperations{},ExcludeResourceRules:[]NamedRuleWithOperations{},MatchPolicy:nil,},ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 44 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[matchResources:map[]] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object
=== RUN   TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.empty
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:361]: Running test case: ValidatingAdmissionPolicyBinding.ParamRef.empty
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/v1alpha1/ctest_defaults_test.go:368]: Matched config: {test_fixture.json [ValidatingAdmissionPolicyBinding.ParamRef.empty] spec [] &ValidatingAdmissionPolicyBinding{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:ValidatingAdmissionPolicyBindingSpec{PolicyName:,ParamRef:&ParamRef{Name:,Namespace:,Selector:nil,ParameterNotFoundAction:nil,},MatchResources:nil,ValidationActions:[],},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:24 Mode: 1
2026/02/09 18:41:24 Base JSON size: 38 bytes
2026/02/09 18:41:24 Number of external values: 89
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:24   [KEEP] spec: map[paramRef:map[]] (missing in external)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:24 [DEBUG-CTEST 2026-02-09 18:41:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
    ctest_defaults_test.go:303: GenerateEffectiveConfigReturnType failed: failed to unmarshal original config to type <nil>: json: cannot unmarshal object into Go value of type runtime.Object

==================== CTEST END ======================
--- FAIL: TestCtestDefaultAdmissionPolicyBinding (0.01s)
    --- FAIL: TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.ParameterNotFoundAction (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.MatchResources (0.00s)
    --- FAIL: TestCtestDefaultAdmissionPolicyBinding/ValidatingAdmissionPolicyBinding.ParamRef.empty (0.00s)
FAIL
coverage: 10.5% of statements
FAIL	k8s.io/kubernetes/pkg/apis/admissionregistration/v1alpha1	0.559s
FAIL	k8s.io/kubernetes/pkg/apis/admissionregistration/v1beta1 [build failed]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/admissionregistration/validation	0.927s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/apidiscovery		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apidiscovery/v2		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apidiscovery/v2beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apiserverinternal		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apiserverinternal/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/apiserverinternal/install	0.251s	coverage: 100.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/apiserverinternal/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/apiserverinternal/validation	0.305s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/apps		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apps/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/apps/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 4.3% of statements
ok  	k8s.io/kubernetes/pkg/apis/apps/v1	0.275s	coverage: 4.3% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 5.7% of statements
ok  	k8s.io/kubernetes/pkg/apis/apps/v1beta1	0.271s	coverage: 5.7% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 4.7% of statements
ok  	k8s.io/kubernetes/pkg/apis/apps/v1beta2	0.465s	coverage: 4.7% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/apps/validation	0.317s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/authentication		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authentication/validation		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/authorization/v1beta1		coverage: 0.0% of statements
# k8s.io/kubernetes/pkg/apis/autoscaling/v2_test
# [k8s.io/kubernetes/pkg/apis/autoscaling/v2_test]
pkg/apis/autoscaling/v2/ctest_defaults_test.go:590:4: (*testing.common).Errorf format %#v reads arg #2, but call has 1 arg
=== RUN   TestCtestValidateSARSpec
=== RUN   TestCtestValidateSARSpec/neither_request
=== RUN   TestCtestValidateSARSpec/both_requests
=== RUN   TestCtestValidateSARSpec/no_subject
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_both
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_neither
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_no_key
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_not_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_exists
=== RUN   TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_not_exists
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_both
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_neither
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_no_key
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_invalid_label_name
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_not_in
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_exists
=== RUN   TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_not_exists
=== RUN   TestCtestValidateSARSpec/empty_user_string
=== RUN   TestCtestValidateSARSpec/group_with_empty_string
    ctest_validation_test.go:358: group with empty string: expected failure for "spec.groups[0]: Invalid value: \"\": at least one of user or group must be specified"
    ctest_validation_test.go:365: group with empty string: expected failure for "spec.groups[0]: Invalid value: \"\": at least one of user or group must be specified"
    ctest_validation_test.go:371: group with empty string: expected failure for "spec.groups[0]: Invalid value: \"\": at least one of user or group must be specified"
--- FAIL: TestCtestValidateSARSpec (0.00s)
    --- PASS: TestCtestValidateSARSpec/neither_request (0.00s)
    --- PASS: TestCtestValidateSARSpec/both_requests (0.00s)
    --- PASS: TestCtestValidateSARSpec/no_subject (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_both (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_specify_neither (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_no_key (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_no_value_for_not_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_field_selector_values_for_not_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_both (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_specify_neither (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_no_key (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_invalid_label_name (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_no_value_for_not_in (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/resource_attributes:_label_selector_values_for_not_exists (0.00s)
    --- PASS: TestCtestValidateSARSpec/empty_user_string (0.00s)
    --- FAIL: TestCtestValidateSARSpec/group_with_empty_string (0.00s)
=== RUN   TestCtestValidateSelfSAR
    ctest_validation_test.go:434: empty resource attributes: expected failure for "spec.resourceAttributes: Required value: at least one selector must be specified"
    ctest_validation_test.go:441: empty resource attributes: expected failure for "spec.resourceAttributes: Required value: at least one selector must be specified"
--- FAIL: TestCtestValidateSelfSAR (0.00s)
=== RUN   TestCtestValidateLocalSAR
    ctest_validation_test.go:535: empty object name with namespace set: unexpected error: "spec.resourceAttributes.namespace: Invalid value: \"\": must match metadata.namespace", expected: "must be empty except for namespace"
--- FAIL: TestCtestValidateLocalSAR (0.00s)
FAIL
coverage: 97.0% of statements
FAIL	k8s.io/kubernetes/pkg/apis/authorization/validation	0.237s
	k8s.io/kubernetes/pkg/apis/autoscaling		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/autoscaling/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/autoscaling/install		coverage: 0.0% of statements
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth1
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth2
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithoutMetrics
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUUtilizationOnly
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_NilMinReplicas_ZeroMaxReplicas_NilMetrics
=== RUN   TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_UnknownMetricType
--- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth1 (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUAverageValueAndUtilizationBoth2 (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithoutMetrics (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/TestConversionWithCPUUtilizationOnly (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_NilMinReplicas_ZeroMaxReplicas_NilMetrics (0.00s)
    --- PASS: TestCtestConvert_autoscaling_HorizontalPodAutoscalerSpec_To_v1_HorizontalPodAutoscalerSpec/EdgeCase_UnknownMetricType (0.00s)
PASS
coverage: 9.9% of statements
ok  	k8s.io/kubernetes/pkg/apis/autoscaling/v1	0.329s	coverage: 9.9% of statements
FAIL	k8s.io/kubernetes/pkg/apis/autoscaling/v2 [build failed]
=== RUN   TestCtestNilOrEmptyConversion
--- FAIL: TestCtestNilOrEmptyConversion (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x8 pc=0x1028a5340]

goroutine 49 [running]:
testing.tRunner.func1.2({0x104220340, 0x105c86400})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x104220340?, 0x105c86400?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1.Convert_autoscaling_ExternalMetricSource_To_v2beta1_ExternalMetricSource(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta1/conversion.go:107
k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1.RegisterConversions.func16({0x104249420?, 0x0?}, {0x104515500?, 0x0?}, {0x1045154e8?, 0x103fcd145?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta1/zz_generated.conversion.go:119 +0xf0
k8s.io/apimachinery/pkg/conversion.(*Converter).Convert(0x14000a16558, {0x104249420, 0x0}, {0x104515500, 0x0}, 0x14000552630)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apimachinery/pkg/conversion/converter.go:210 +0x2d0
k8s.io/apimachinery/pkg/runtime.(*Scheme).Convert(0x140004476c0, {0x104249420, 0x0}, {0x104515500, 0x0}, {0x0, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apimachinery/pkg/runtime/scheme.go:458 +0x134
k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1.TestCtestNilOrEmptyConversion(0x14000103180)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta1/ctest_conversion_test.go:81 +0x3b8
testing.tRunner(0x14000103180, 0x104633ef0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/apis/autoscaling/v2beta1	1.225s
=== RUN   TestCtestGenerateScaleDownRules
[DEBUG-CTEST 2026-02-09 18:41:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta2/ctest_defaults_test.go:27]: Running TestCtestGenerateScaleDownRules
Running test case 0: Default values
=== RUN   TestCtestGenerateScaleDownRules/Default_values
Running test case 1: All parameters are specified
=== RUN   TestCtestGenerateScaleDownRules/All_parameters_are_specified
Running test case 2: Percent policy is specified
=== RUN   TestCtestGenerateScaleDownRules/Percent_policy_is_specified
Running test case 3: Pods policy is specified
=== RUN   TestCtestGenerateScaleDownRules/Pods_policy_is_specified
Running test case 4: Nil selectPolicy and nil stabilization (should use defaults)
=== RUN   TestCtestGenerateScaleDownRules/Nil_selectPolicy_and_nil_stabilization_(should_use_defaults)
Running test case 5: Negative Pods rate
=== RUN   TestCtestGenerateScaleDownRules/Negative_Pods_rate
Running test case 6: Large Percent value
=== RUN   TestCtestGenerateScaleDownRules/Large_Percent_value
--- PASS: TestCtestGenerateScaleDownRules (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Default_values (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/All_parameters_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Percent_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Pods_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Nil_selectPolicy_and_nil_stabilization_(should_use_defaults) (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Negative_Pods_rate (0.00s)
    --- PASS: TestCtestGenerateScaleDownRules/Large_Percent_value (0.00s)
=== RUN   TestCtestGenerateScaleUpRules
[DEBUG-CTEST 2026-02-09 18:41:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta2/ctest_defaults_test.go:151]: Running TestCtestGenerateScaleUpRules
Running test case 0: Default values
=== RUN   TestCtestGenerateScaleUpRules/Default_values
Running test case 1: All parameters are specified
=== RUN   TestCtestGenerateScaleUpRules/All_parameters_are_specified
Running test case 2: Pod policy is specified
=== RUN   TestCtestGenerateScaleUpRules/Pod_policy_is_specified
Running test case 3: Percent policy is specified
=== RUN   TestCtestGenerateScaleUpRules/Percent_policy_is_specified
Running test case 4: Pod policy and stabilization window are specified
=== RUN   TestCtestGenerateScaleUpRules/Pod_policy_and_stabilization_window_are_specified
Running test case 5: Percent policy and stabilization window are specified
=== RUN   TestCtestGenerateScaleUpRules/Percent_policy_and_stabilization_window_are_specified
Running test case 6: Nil selectPolicy and zero stabilization (defaults)
=== RUN   TestCtestGenerateScaleUpRules/Nil_selectPolicy_and_zero_stabilization_(defaults)
Running test case 7: Negative Percent value
=== RUN   TestCtestGenerateScaleUpRules/Negative_Percent_value
Running test case 8: Huge Pods period
=== RUN   TestCtestGenerateScaleUpRules/Huge_Pods_period
--- PASS: TestCtestGenerateScaleUpRules (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Default_values (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/All_parameters_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Pod_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Percent_policy_is_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Pod_policy_and_stabilization_window_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Percent_policy_and_stabilization_window_are_specified (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Nil_selectPolicy_and_zero_stabilization_(defaults) (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Negative_Percent_value (0.00s)
    --- PASS: TestCtestGenerateScaleUpRules/Huge_Pods_period (0.00s)
=== RUN   TestCtestHorizontalPodAutoscalerAnnotations
[DEBUG-CTEST 2026-02-09 18:41:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/v2beta2/ctest_defaults_test.go:298]: Running TestCtestHorizontalPodAutoscalerAnnotations
--- PASS: TestCtestHorizontalPodAutoscalerAnnotations (0.00s)
PASS
coverage: 28.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/autoscaling/v2beta2	0.761s	coverage: 28.0% of statements
=== RUN   TestCtestValidateScaleForDeclarative

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/autoscaling/validation/ctest_declarative_validation_test.go:32]: get default configs: {test_fixture.json [default scale spec] replicas [] {{ } {abc  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {1} {0 }}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:36 [DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:36 Mode: 1
2026/02/09 18:41:36 Base JSON size: 96 bytes
2026/02/09 18:41:36 Number of external values: 22
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36   [REPLACE ALL] : entire structure replaced
2026/02/09 18:41:36 [DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:36 [DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=22)
[DEBUG-CTEST 2026-02-09 18:41:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"Replicas":1},"Status":{"Replicas":0,"Selector":""},"name":"abc","namespace":"default"})    ctest_declarative_validation_test.go:37: failed to generate effective config: k8s json unmarshal into autoscaling.Scale failed: json: cannot unmarshal number into Go value of type autoscaling.Scale
--- FAIL: TestCtestValidateScaleForDeclarative (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/apis/autoscaling/validation	1.640s
	k8s.io/kubernetes/pkg/apis/batch		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/batch/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/batch/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 5.2% of statements
ok  	k8s.io/kubernetes/pkg/apis/batch/v1	1.933s	coverage: 5.2% of statements [no tests to run]
=== RUN   TestCtestSetDefaultCronJob

==================== CTEST OVERRIDE ONLY START ====================
=== RUN   TestCtestSetDefaultCronJob/set_fields_should_not_be_defaulted
[DEBUG-CTEST 2026-02-09 18:41:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/batch/v1beta1/ctest_defaults_test.go:70]: get default configs: {test_fixture.json [set fields should not be defaulted] spec [cronjobs] { <nil> <nil> Forbid 0x140002d4edc {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {<nil> <nil> <nil> nil nil <nil> <nil> <nil> nil <nil> {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}} <nil> <nil> <nil> <nil> <nil>}} 0x140002d4ee0 0x140002d4ee4}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[cronjobs]
[DEBUG-CTEST 2026-02-09 18:41:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[cronjobs], int=1)[DEBUG-CTEST 2026-02-09 18:41:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [cronjobs]
[DEBUG-CTEST 2026-02-09 18:41:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:40 load all fixtures failed: requested fixture keys not found in test_fixtures.json: cronjobs
FAIL	k8s.io/kubernetes/pkg/apis/batch/v1beta1	0.886s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/batch/validation	0.403s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/certificates		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/certificates/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestIsKubeletServingCSR

==================== CTEST START ====================
=== RUN   TestCtestIsKubeletServingCSR/defaults_without_key_encipherment_for_kubelet-serving
=== RUN   TestCtestIsKubeletServingCSR/does_not_default_to_kube-apiserver-client-kubelet_if_org_is_not_'system:nodes'
=== RUN   TestCtestIsKubeletServingCSR/nil_request
--- FAIL: TestCtestIsKubeletServingCSR (0.00s)
    --- PASS: TestCtestIsKubeletServingCSR/defaults_without_key_encipherment_for_kubelet-serving (0.00s)
    --- PASS: TestCtestIsKubeletServingCSR/does_not_default_to_kube-apiserver-client-kubelet_if_org_is_not_'system:nodes' (0.00s)
    --- FAIL: TestCtestIsKubeletServingCSR/nil_request (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0xb8 pc=0x1033b31cc]

goroutine 85 [running]:
testing.tRunner.func1.2({0x104cbc4e0, 0x1066b1670})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x104cbc4e0?, 0x1066b1670?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/certificates.ValidateKubeletServingCSR(0x0, 0x140000a7ea8)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/helpers.go:69 +0xcc
k8s.io/kubernetes/pkg/apis/certificates.IsKubeletServingCSR(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/helpers.go:66
k8s.io/kubernetes/pkg/apis/certificates/v1beta1.IsKubeletServingCSR(0x0, {0x1066cf160, 0x3, 0x102c92ebc?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/v1beta1/defaults.go:69 +0x294
k8s.io/kubernetes/pkg/apis/certificates/v1beta1.TestCtestIsKubeletServingCSR.func2(0x140006d6a80)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/certificates/v1beta1/ctest_defaults_test.go:76 +0x34
testing.tRunner(0x140006d6a80, 0x1400086b0e0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 82
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/apis/certificates/v1beta1	1.319s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/certificates/validation	0.395s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/coordination		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/v1alpha2		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/coordination/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateLeaseSpec

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:114]: Number of test cases: 9
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 4
Running 5 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 5
Running 6 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 6
Running 7 th test case.
    ctest_validation_test.go:119: Expected err, got no err for case 7
Running 8 th test case.

==================== CTEST END ======================
--- FAIL: TestCtestValidateLeaseSpec (0.00s)
=== RUN   TestCtestValidateLeaseCandidateSpec

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:249]: Number of test cases: 12
Running 0 th test case: valid
Running 1 th test case: valid custom strategy should not require emulationVersion
Running 2 th test case: binaryVersion is required
Running 3 th test case: no lease name
Running 4 th test case: bad binaryVersion
Running 5 th test case: emulation should be greater than or equal to binary version
Running 6 th test case: strategy bad
Running 7 th test case: strategy missing
Running 8 th test case: strategy good but emulationVersion missing
Running 9 th test case: empty binaryVersion string
Running 10 th test case: extremely long binaryVersion
Running 11 th test case: missing strategy with only binary version

==================== CTEST END ======================
--- PASS: TestCtestValidateLeaseCandidateSpec (0.00s)
=== RUN   TestCtestValidateLeaseCandidateUpdate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:337]: Number of test cases: 4
Running 0 th test case: valid update
Running 1 th test case: update LeaseName should fail
Running 2 th test case: changing Strategy should fail
    ctest_validation_test.go:347: Expected err, got no err for tc: changing Strategy should fail
Running 3 th test case: missing ResourceVersion on update (should be ignored)

==================== CTEST END ======================
--- FAIL: TestCtestValidateLeaseCandidateUpdate (0.00s)
=== RUN   TestCtestValidateCoordinatedLeaseStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/coordination/validation/ctest_validation_test.go:397]: Number of test cases: 8
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
Running 5 th test case.
Running 6 th test case.
Running 7 th test case.

==================== CTEST END ======================
--- PASS: TestCtestValidateCoordinatedLeaseStrategy (0.00s)
FAIL
coverage: 85.3% of statements
FAIL	k8s.io/kubernetes/pkg/apis/coordination/validation	1.012s
=== RUN   TestCtestTaintToString
    ctest_taint_test.go:67: [4] expected taint &{empty   <nil>} converted to empty=:, got empty
--- FAIL: TestCtestTaintToString (0.00s)
=== RUN   TestCtestMatchTaint
--- PASS: TestCtestMatchTaint (0.00s)
=== RUN   TestCtestMatchToleration
--- PASS: TestCtestMatchToleration (0.00s)
FAIL
coverage: 0.2% of statements
FAIL	k8s.io/kubernetes/pkg/apis/core	1.197s
	k8s.io/kubernetes/pkg/apis/core/fuzzer		coverage: 0.0% of statements
=== RUN   TestCtestSemantic
--- PASS: TestCtestSemantic (0.00s)
=== RUN   TestCtestIsStandardResource
--- PASS: TestCtestIsStandardResource (0.00s)
=== RUN   TestCtestIsStandardContainerResource
--- PASS: TestCtestIsStandardContainerResource (0.00s)
=== RUN   TestCtestGetAccessModesFromString
--- PASS: TestCtestGetAccessModesFromString (0.00s)
=== RUN   TestCtestRemoveDuplicateAccessModes
--- PASS: TestCtestRemoveDuplicateAccessModes (0.00s)
=== RUN   TestCtestIsHugePageResourceName
    ctest_helpers_test.go:177: resource: hugepages- expected result: false
--- FAIL: TestCtestIsHugePageResourceName (0.00s)
=== RUN   TestCtestIsHugePageResourceValueDivisible
--- PASS: TestCtestIsHugePageResourceValueDivisible (0.00s)
=== RUN   TestCtestHugePageResourceName
    ctest_helpers_test.go:267: pageSize: 0, expected: , but got: hugepages-0
--- FAIL: TestCtestHugePageResourceName (0.00s)
=== RUN   TestCtestHugePageSizeFromResourceName
--- PASS: TestCtestHugePageSizeFromResourceName (0.00s)
=== RUN   TestCtestIsOvercommitAllowed
    ctest_helpers_test.go:335: Unexpected result for unknown
--- FAIL: TestCtestIsOvercommitAllowed (0.00s)
=== RUN   TestCtestIsServiceIPSet
=== RUN   TestCtestIsServiceIPSet/nil_cluster_ip
=== RUN   TestCtestIsServiceIPSet/headless_service
=== RUN   TestCtestIsServiceIPSet/one_ipv4
=== RUN   TestCtestIsServiceIPSet/one_ipv6
=== RUN   TestCtestIsServiceIPSet/v4,_v6
=== RUN   TestCtestIsServiceIPSet/v6,_v4
=== RUN   TestCtestIsServiceIPSet/empty_spec
--- PASS: TestCtestIsServiceIPSet (0.00s)
    --- PASS: TestCtestIsServiceIPSet/nil_cluster_ip (0.00s)
    --- PASS: TestCtestIsServiceIPSet/headless_service (0.00s)
    --- PASS: TestCtestIsServiceIPSet/one_ipv4 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/one_ipv6 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/v4,_v6 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/v6,_v4 (0.00s)
    --- PASS: TestCtestIsServiceIPSet/empty_spec (0.00s)
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/valid_values
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/empty_terms
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/invalid_label_value
=== RUN   TestCtestHasInvalidLabelValueInNodeSelectorTerms/nil_slice
--- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/valid_values (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/empty_terms (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/invalid_label_value (0.00s)
    --- PASS: TestCtestHasInvalidLabelValueInNodeSelectorTerms/nil_slice (0.00s)
FAIL
coverage: 30.5% of statements
FAIL	k8s.io/kubernetes/pkg/apis/core/helper	1.395s
	k8s.io/kubernetes/pkg/apis/core/helper/qos		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/core/install	0.271s	coverage: 100.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/core/pods	0.487s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 9.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/core/v1	0.624s	coverage: 9.0% of statements [no tests to run]
=== RUN   TestCtestIsNativeResource
=== RUN   TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=,_expected_value=true
=== RUN   TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false
=== PAUSE TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false
=== RUN   TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true
=== PAUSE TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false
=== CONT  TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false
=== CONT  TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false
=== CONT  TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true
=== CONT  TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false
--- PASS: TestCtestIsNativeResource (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=foo/bar/baz,_expected_value=false (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=trailingSlash/,_expected_value=false (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=a/b,_expected_value=false (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=kubernetes.io/resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=verylongresourcenamethatisdefinitelyvalidandhasnospacesorslashes,_expected_value=true (0.00s)
    --- PASS: TestCtestIsNativeResource/resourceName_input=/leadingSlash,_expected_value=false (0.00s)
=== RUN   TestCtestHugePageSizeFromResourceName
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI}
=== RUN   TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI}
=== PAUSE TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__}
=== CONT  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI}
=== NAME  TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__}
    ctest_helpers_test.go:125: [4]expected error but got none.
    ctest_helpers_test.go:131: Got {{0 -3} {<nil>}  DecimalSI} but expected {{0 0} {<nil>}  }
--- FAIL: TestCtestHugePageSizeFromResourceName (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1Gi,_expected_value={{1073741824_0}_{<nil>}_1Gi_BinarySI} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-abc,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-100m,_expected_value={{100_-3}_{<nil>}_100m_DecimalSI} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-,_expected_value={{0_0}_{<nil>}__} (0.00s)
    --- PASS: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-1024Mi,_expected_value={{1073741824_0}_{<nil>}__BinarySI} (0.00s)
    --- FAIL: TestCtestHugePageSizeFromResourceName/resourceName_input=hugepages-0m,_expected_value={{0_0}_{<nil>}__} (0.00s)
=== RUN   TestCtestHugePageSizeFromMedium
=== RUN   TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium
=== RUN   TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate)
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate)
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate)
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate)
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
=== RUN   TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize
=== PAUSE TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize
=== RUN   TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional)
=== PAUSE TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional)
=== CONT  TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional)
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi
=== CONT  TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate)
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size
=== CONT  TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate)
=== CONT  TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize
=== NAME  TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi
    ctest_helpers_test.go:213: [7]expected error but got none.
    ctest_helpers_test.go:219: Got {{0 0} {<nil>}  BinarySI} but expected {{0 0} {<nil>}  }
--- FAIL: TestCtestHugePageSizeFromMedium (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-64Ki (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-128Mi_(additional) (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-2Mi (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Valid:_HugePages-1Gi (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size_(duplicate) (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid:_HugePages_without_size (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid_hugepages_medium_(duplicate) (0.00s)
    --- PASS: TestCtestHugePageSizeFromMedium/Invalid:_HugePages-InvalidSize (0.00s)
    --- FAIL: TestCtestHugePageSizeFromMedium/Invalid:_HugePages-0Gi (0.00s)
=== RUN   TestCtestIsOvercommitAllowed
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false
=== RUN   TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true
=== PAUSE TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true
    ctest_helpers_test.go:266: Got false but expected true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true
=== CONT  TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false
--- FAIL: TestCtestIsOvercommitAllowed (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=pod.alpha.kubernetes.io/opaque-int-resource-foo,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=hugepages-0m,_expected_value=false (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=kubernetes.io/resource-foo,_expected_value=true (0.00s)
    --- FAIL: TestCtestIsOvercommitAllowed/resourceName_input=invalid/resource,_expected_value=true (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=hugepages-1Gi,_expected_value=false (0.00s)
    --- PASS: TestCtestIsOvercommitAllowed/resourceName_input=hugepages-100m,_expected_value=false (0.00s)
=== RUN   TestCtestGetAccessModesFromString
--- PASS: TestCtestGetAccessModesFromString (0.00s)
=== RUN   TestCtestRemoveDuplicateAccessModes
--- PASS: TestCtestRemoveDuplicateAccessModes (0.00s)
=== RUN   TestCtestTopologySelectorRequirementsAsSelector
    ctest_helpers_test.go:418: [6]expected error but got none.
    ctest_helpers_test.go:424: [6]expected:
        	<nil>
        but got:
        	dup in (a),dup in (b)
--- FAIL: TestCtestTopologySelectorRequirementsAsSelector (0.00s)
=== RUN   TestCtestMatchTopologySelectorTerms
=== RUN   TestCtestMatchTopologySelectorTerms/nil_term_list
=== RUN   TestCtestMatchTopologySelectorTerms/nil_term
=== RUN   TestCtestMatchTopologySelectorTerms/label_matches_MatchLabelExpressions_terms
=== RUN   TestCtestMatchTopologySelectorTerms/label_does_not_match_MatchLabelExpressions_terms
=== RUN   TestCtestMatchTopologySelectorTerms/multi-values_in_one_requirement,_one_matched
=== RUN   TestCtestMatchTopologySelectorTerms/multi-terms_was_set,_one_matched
=== RUN   TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_fully_matched
=== RUN   TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_partial_matched
=== RUN   TestCtestMatchTopologySelectorTerms/empty_labels_map_with_terms
--- PASS: TestCtestMatchTopologySelectorTerms (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/nil_term_list (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/nil_term (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/label_matches_MatchLabelExpressions_terms (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/label_does_not_match_MatchLabelExpressions_terms (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-values_in_one_requirement,_one_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-terms_was_set,_one_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_fully_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/multi-requirement_in_one_term,_partial_matched (0.00s)
    --- PASS: TestCtestMatchTopologySelectorTerms/empty_labels_map_with_terms (0.00s)
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/empty_set_of_keys_in_empty_set_of_terms
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_all_keys_specified
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_one_of_the_keys_specified
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_without_any_of_the_keys_specified
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_empty_set_of_terms
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/requirement_with_empty_key
=== RUN   TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/terms_with_empty_MatchExpressions
--- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/empty_set_of_keys_in_empty_set_of_terms (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_all_keys_specified (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_with_one_of_the_keys_specified (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_terms_without_any_of_the_keys_specified (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/key_existence_in_empty_set_of_terms (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/requirement_with_empty_key (0.00s)
    --- PASS: TestCtestNodeSelectorRequirementKeyExistsInNodeSelectorTerms/terms_with_empty_MatchExpressions (0.00s)
=== RUN   TestCtestHugePageUnitSizeFromByteSize
    ctest_helpers_test.go:881: HugePageUnitSizeFromByteSize() expected error = size: 3069MB must be guaranteed to divisible into the largest units
    ctest_helpers_test.go:888: HugePageUnitSizeFromByteSize() expected 0B but got 0PB
    ctest_helpers_test.go:888: HugePageUnitSizeFromByteSize() expected  but got -1KB
    ctest_helpers_test.go:888: HugePageUnitSizeFromByteSize() expected  but got 1PB
--- FAIL: TestCtestHugePageUnitSizeFromByteSize (0.00s)
FAIL
coverage: 52.6% of statements
FAIL	k8s.io/kubernetes/pkg/apis/core/v1/helper	0.315s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/core/v1/helper/qos	0.845s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestValidateResourceRequirements
=== RUN   TestCtestValidateResourceRequirements/Resources_with_Requests_equal_to_Limits
=== RUN   TestCtestValidateResourceRequirements/Resources_with_only_Limits
=== RUN   TestCtestValidateResourceRequirements/Resources_with_only_Requests
=== RUN   TestCtestValidateResourceRequirements/Resources_with_Requests_Less_Than_Limits
=== RUN   TestCtestValidateResourceRequirements/Empty_ResourceRequirements_(no_requests,_no_limits)
=== RUN   TestCtestValidateResourceRequirements/Resources_with_Requests_Larger_Than_Limits
=== RUN   TestCtestValidateResourceRequirements/Invalid_Resources_with_Requests
=== RUN   TestCtestValidateResourceRequirements/Invalid_Resources_with_Limits
=== RUN   TestCtestValidateResourceRequirements/Empty_Requests_map_with_non‑empty_Limits_(should_error_on_missing_request)
    ctest_validation_test.go:133: expected error
    validation_test.go:152: error must contain limit name
    validation_test.go:155: error must contain limit value
=== RUN   TestCtestValidateResourceRequirements/Zero_quantity_in_Limits_(invalid)
    ctest_validation_test.go:133: expected error
    validation_test.go:152: error must contain limit name
--- FAIL: TestCtestValidateResourceRequirements (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_Requests_equal_to_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_only_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_only_Requests (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_Requests_Less_Than_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Empty_ResourceRequirements_(no_requests,_no_limits) (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Resources_with_Requests_Larger_Than_Limits (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Invalid_Resources_with_Requests (0.00s)
    --- PASS: TestCtestValidateResourceRequirements/Invalid_Resources_with_Limits (0.00s)
    --- FAIL: TestCtestValidateResourceRequirements/Empty_Requests_map_with_non‑empty_Limits_(should_error_on_missing_request) (0.00s)
    --- FAIL: TestCtestValidateResourceRequirements/Zero_quantity_in_Limits_(invalid) (0.00s)
=== RUN   TestCtestValidateContainerResourceName
=== RUN   TestCtestValidateContainerResourceName/CPU_resource
=== RUN   TestCtestValidateContainerResourceName/Memory_resource
=== RUN   TestCtestValidateContainerResourceName/Hugepages_resource
=== RUN   TestCtestValidateContainerResourceName/Namespaced_resource
=== RUN   TestCtestValidateContainerResourceName/Extended_Resource
=== RUN   TestCtestValidateContainerResourceName/Empty_string_(treated_as_valid_in_this_context)
    ctest_validation_test.go:167: unexpected error: [[]: Invalid value: "": name part must be non-empty []: Invalid value: "": name part must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]') []: Invalid value: "": must be a standard resource for containers]
=== RUN   TestCtestValidateContainerResourceName/Invalid_standard_resource
=== RUN   TestCtestValidateContainerResourceName/Invalid_namespaced_resource
=== RUN   TestCtestValidateContainerResourceName/Invalid_extended_resource
=== RUN   TestCtestValidateContainerResourceName/Resource_name_with_spaces
=== RUN   TestCtestValidateContainerResourceName/Resource_name_with_leading_dash
--- FAIL: TestCtestValidateContainerResourceName (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/CPU_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Memory_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Hugepages_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Namespaced_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Extended_Resource (0.00s)
    --- FAIL: TestCtestValidateContainerResourceName/Empty_string_(treated_as_valid_in_this_context) (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Invalid_standard_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Invalid_namespaced_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Invalid_extended_resource (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Resource_name_with_spaces (0.00s)
    --- PASS: TestCtestValidateContainerResourceName/Resource_name_with_leading_dash (0.00s)
=== RUN   TestCtestValidatePodLogOptions
=== RUN   TestCtestValidatePodLogOptions/Empty_PodLogOptions
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_TailLines
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_only_sinceSeconds
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines_with_SinceSeconds
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_stdout_Stream
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_stderr_Stream_and_Follow
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_All_Stream,_TailLines_and_LimitBytes
=== RUN   TestCtestValidatePodLogOptions/PodLogOptions_with_SinceTime_only_(valid)
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_Negative_TailLines
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_LimitBytes
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_SinceSeconds
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_both_SinceSeconds_and_SinceTime_set
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_invalid_Stream
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stdout_Stream_and_TailLines_set
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stderr_Stream_and_TailLines_set
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_nil_Stream_but_TailLines_set
    ctest_validation_test.go:345: expected error
=== RUN   TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_empty_string_Stream_(treated_as_invalid)
--- FAIL: TestCtestValidatePodLogOptions (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Empty_PodLogOptions (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_TailLines (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_only_sinceSeconds (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_LimitBytes_with_TailLines_with_SinceSeconds (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_stdout_Stream (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_stderr_Stream_and_Follow (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_All_Stream,_TailLines_and_LimitBytes (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/PodLogOptions_with_SinceTime_only_(valid) (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_Negative_TailLines (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_LimitBytes (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_zero_or_negative_SinceSeconds (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_both_SinceSeconds_and_SinceTime_set (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_invalid_Stream (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stdout_Stream_and_TailLines_set (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_stderr_Stream_and_TailLines_set (0.00s)
    --- FAIL: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_nil_Stream_but_TailLines_set (0.00s)
    --- PASS: TestCtestValidatePodLogOptions/Invalid_podLogOptions_with_empty_string_Stream_(treated_as_invalid) (0.00s)
=== RUN   TestCtestAccumulateUniqueHostPorts
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_the_same_port_with_different_protocol
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_different_ports
=== RUN   TestCtestAccumulateUniqueHostPorts/No_containers_(empty_slice)_should_succeed
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated_while_containers_use_the_same_port_with_UDP
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated
=== RUN   TestCtestAccumulateUniqueHostPorts/HostPort_zero_(invalid)_with_UDP
    ctest_validation_test.go:449: expected error, but got nil
--- FAIL: TestCtestAccumulateUniqueHostPorts (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_the_same_port_with_different_protocol (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_not_allocated_while_containers_use_different_ports (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/No_containers_(empty_slice)_should_succeed (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated_while_containers_use_the_same_port_with_UDP (0.00s)
    --- PASS: TestCtestAccumulateUniqueHostPorts/HostPort_is_already_allocated (0.00s)
    --- FAIL: TestCtestAccumulateUniqueHostPorts/HostPort_zero_(invalid)_with_UDP (0.00s)
FAIL
coverage: 98.6% of statements
FAIL	k8s.io/kubernetes/pkg/apis/core/v1/validation	0.896s
=== RUN   TestCtestIsKubernetesSignerName
Starting TestCtestIsKubernetesSignerName
Number of test cases: 11
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io
Running test case #0: kubernetes.io
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/a
Running test case #1: kubernetes.io/a
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/a/b.c/d.e
Running test case #2: kubernetes.io/a/b.c/d.e
=== RUN   TestCtestIsKubernetesSignerName/foo.kubernetes.io
Running test case #3: foo.kubernetes.io
=== RUN   TestCtestIsKubernetesSignerName/fookubernetes.io
Running test case #4: fookubernetes.io
=== RUN   TestCtestIsKubernetesSignerName/foo.com/a
Running test case #5: foo.com/a
=== RUN   TestCtestIsKubernetesSignerName/#00
Running test case #6: 
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/
Running test case #7: kubernetes.io/
    ctest_names_test.go:66: IsKubernetesSignerName("kubernetes.io/"); got true, want false
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io//a
Running test case #8: kubernetes.io//a
    ctest_names_test.go:66: IsKubernetesSignerName("kubernetes.io//a"); got true, want false
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/@@@@
Running test case #9: kubernetes.io/@@@@
    ctest_names_test.go:66: IsKubernetesSignerName("kubernetes.io/@@@@"); got true, want false
=== RUN   TestCtestIsKubernetesSignerName/kubernetes.io/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y/z
Running test case #10: kubernetes.io/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y/z
--- FAIL: TestCtestIsKubernetesSignerName (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io/a (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io/a/b.c/d.e (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/foo.kubernetes.io (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/fookubernetes.io (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/foo.com/a (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/#00 (0.00s)
    --- FAIL: TestCtestIsKubernetesSignerName/kubernetes.io/ (0.00s)
    --- FAIL: TestCtestIsKubernetesSignerName/kubernetes.io//a (0.00s)
    --- FAIL: TestCtestIsKubernetesSignerName/kubernetes.io/@@@@ (0.00s)
    --- PASS: TestCtestIsKubernetesSignerName/kubernetes.io/a/b/c/d/e/f/g/h/i/j/k/l/m/n/o/p/q/r/s/t/u/v/w/x/y/z (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/apis/core/validation	0.340s
	k8s.io/kubernetes/pkg/apis/discovery		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/discovery/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/discovery/install		coverage: 0.0% of statements
=== RUN   TestCtestSetDefaultEndpointPort

==================== CTEST START ====================
=== RUN   TestCtestSetDefaultEndpointPort/should_set_appropriate_defaults
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints]
[DEBUG-CTEST 2026-02-09 18:41:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints], int=1)[DEBUG-CTEST 2026-02-09 18:41:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints]
[DEBUG-CTEST 2026-02-09 18:41:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:50 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints
FAIL	k8s.io/kubernetes/pkg/apis/discovery/v1	0.754s
=== RUN   TestCtestEndpointZoneConversion
=== RUN   TestCtestEndpointZoneConversion/no_topology_field
=== RUN   TestCtestEndpointZoneConversion/non_empty_topology_map,_but_no_zone
=== RUN   TestCtestEndpointZoneConversion/non_empty_topology_map,_with_zone
=== RUN   TestCtestEndpointZoneConversion/only_zone_in_topology_map
=== RUN   TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_different_values
=== RUN   TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_same_values
=== RUN   TestCtestEndpointZoneConversion/only_topology[hostname]_is_populated
=== RUN   TestCtestEndpointZoneConversion/only_nodeName_is_populated_(with_matching_hostname_in_topology)
=== RUN   TestCtestEndpointZoneConversion/empty_topology_map
    ctest_conversion_test.go:180: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:180
        	Error:      	Not equal: 
        	            	expected: v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string{}, NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	actual  : v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string(nil), NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -9,4 +9,3 @@
        	            	  TargetRef: (*v1.ObjectReference)(<nil>),
        	            	- Topology: (map[string]string) {
        	            	- },
        	            	+ Topology: (map[string]string) <nil>,
        	            	  NodeName: (*string)(<nil>),
        	Test:       	TestCtestEndpointZoneConversion/empty_topology_map
        	Messages:   	discovery.Endpoint -> v1beta1.Endpoint
=== RUN   TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value
    ctest_conversion_test.go:176: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:176
        	Error:      	Not equal: 
        	            	expected: discovery.Endpoint{Addresses:[]string(nil), Conditions:discovery.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*core.ObjectReference)(nil), DeprecatedTopology:map[string]string(nil), NodeName:(*string)(nil), Zone:(*string)(nil), Hints:(*discovery.EndpointHints)(nil)}
        	            	actual  : discovery.Endpoint{Addresses:[]string(nil), Conditions:discovery.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*core.ObjectReference)(nil), DeprecatedTopology:map[string]string(nil), NodeName:(*string)(nil), Zone:(*string)(0x14000113980), Hints:(*discovery.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -11,3 +11,3 @@
        	            	  NodeName: (*string)(<nil>),
        	            	- Zone: (*string)(<nil>),
        	            	+ Zone: (*string)(""),
        	            	  Hints: (*discovery.EndpointHints)(<nil>)
        	Test:       	TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value
        	Messages:   	v1beta1.Endpoint -> discovery.Endpoint
    ctest_conversion_test.go:180: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:180
        	Error:      	Not equal: 
        	            	expected: v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string{"topology.kubernetes.io/zone":""}, NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	actual  : v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string(nil), NodeName:(*string)(nil), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -9,5 +9,3 @@
        	            	  TargetRef: (*v1.ObjectReference)(<nil>),
        	            	- Topology: (map[string]string) (len=1) {
        	            	-  (string) (len=27) "topology.kubernetes.io/zone": (string) ""
        	            	- },
        	            	+ Topology: (map[string]string) <nil>,
        	            	  NodeName: (*string)(<nil>),
        	Test:       	TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value
        	Messages:   	discovery.Endpoint -> v1beta1.Endpoint
=== RUN   TestCtestEndpointZoneConversion/nodeName_empty_string
    ctest_conversion_test.go:180: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/discovery/v1beta1/ctest_conversion_test.go:180
        	Error:      	Not equal: 
        	            	expected: v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string(nil), NodeName:(*string)(0x14000113200), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	actual  : v1beta1.Endpoint{Addresses:[]string(nil), Conditions:v1beta1.EndpointConditions{Ready:(*bool)(nil), Serving:(*bool)(nil), Terminating:(*bool)(nil)}, Hostname:(*string)(nil), TargetRef:(*v1.ObjectReference)(nil), Topology:map[string]string{"kubernetes.io/hostname":""}, NodeName:(*string)(0x14000113210), Hints:(*v1beta1.EndpointHints)(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -9,3 +9,5 @@
        	            	  TargetRef: (*v1.ObjectReference)(<nil>),
        	            	- Topology: (map[string]string) <nil>,
        	            	+ Topology: (map[string]string) (len=1) {
        	            	+  (string) (len=22) "kubernetes.io/hostname": (string) ""
        	            	+ },
        	            	  NodeName: (*string)(""),
        	Test:       	TestCtestEndpointZoneConversion/nodeName_empty_string
        	Messages:   	discovery.Endpoint -> v1beta1.Endpoint
=== RUN   TestCtestEndpointZoneConversion/nil_nodeName_pointer_with_non-empty_topology
=== RUN   TestCtestEndpointZoneConversion/topology_with_multiple_unrelated_keys
--- FAIL: TestCtestEndpointZoneConversion (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/no_topology_field (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/non_empty_topology_map,_but_no_zone (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/non_empty_topology_map,_with_zone (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/only_zone_in_topology_map (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_different_values (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/nodeName_and_topology[hostname]_are_populated_with_same_values (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/only_topology[hostname]_is_populated (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/only_nodeName_is_populated_(with_matching_hostname_in_topology) (0.00s)
    --- FAIL: TestCtestEndpointZoneConversion/empty_topology_map (0.00s)
    --- FAIL: TestCtestEndpointZoneConversion/topology_with_zone_key_but_empty_value (0.00s)
    --- FAIL: TestCtestEndpointZoneConversion/nodeName_empty_string (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/nil_nodeName_pointer_with_non-empty_topology (0.00s)
    --- PASS: TestCtestEndpointZoneConversion/topology_with_multiple_unrelated_keys (0.00s)
FAIL
coverage: 36.5% of statements
FAIL	k8s.io/kubernetes/pkg/apis/discovery/v1beta1	0.991s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/discovery/validation	0.274s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/events		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/events/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/events/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/events/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/extensions		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/extensions/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/extensions/install		coverage: 0.0% of statements
=== RUN   TestCtestIngressBackendConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/extensions/v1beta1/ctest_conversion_test.go:134]: get default configs: {test_fixture.json [service-port-number] backend [ingressws] {{<nil> &IngressBackend{ServiceName:test-backend,ServicePort:{0 8080 },Resource:nil,} [] []} {<nil> 0x14000624c40 [] []}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[ingressws]
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[ingressws], int=1)[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [ingressws]
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:54 load all fixtures failed: requested fixture keys not found in test_fixtures.json: ingressws
FAIL	k8s.io/kubernetes/pkg/apis/extensions/v1beta1	1.515s
	k8s.io/kubernetes/pkg/apis/flowcontrol		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/flowcontrol/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/flowcontrol/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 89.5% of statements
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/internalbootstrap	0.675s	coverage: 89.5% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/flowcontrol/util		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.5% of statements
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/v1	0.334s	coverage: 0.5% of statements [no tests to run]
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response
=== RUN   TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value)
--- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response (0.00s)
    --- PASS: TestCtest_Convert_v1beta1_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value) (0.00s)
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response
=== RUN   TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value)
--- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_empty_limit_response (0.00s)
    --- PASS: TestCtest_Convert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta1_LimitedPriorityLevelConfiguration/missing_limit_response_(zero_value) (0.00s)
=== RUN   TestCtestDefaultWithPriorityLevelConfiguration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:58]: get default configs: {test_fixture.json [Defaulting for Exempt] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {Exempt nil &ExemptPriorityLevelConfiguration{NominalConcurrencyShares:nil,LendablePercent:nil,}} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:54 Mode: 1
2026/02/09 18:41:54 Base JSON size: 64 bytes
2026/02/09 18:41:54 Number of external values: 1
2026/02/09 18:41:54   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:54   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:54   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"exempt":{},"type":"Exempt"},"status":{}})[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:68]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:58]: get default configs: {test_fixture.json [LendablePercent is not specified, should default to zero] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {Limited &LimitedPriorityLevelConfiguration{AssuredConcurrencyShares:5,LimitResponse:LimitResponse{Type:Reject,Queuing:nil,},LendablePercent:nil,BorrowingLimitPercent:nil,} nil} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:54 Mode: 1
2026/02/09 18:41:54 Base JSON size: 128 bytes
2026/02/09 18:41:54 Number of external values: 1
2026/02/09 18:41:54   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:54   [KEEP] spec: map[limited:map[assuredConcurrencyShares:5 limitResponse:map[type:Reject]] type:Limited] (missing in external)
2026/02/09 18:41:54   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"limited":{"assuredConcurrencyShares":5,"limitResponse":{"type":"Reject"}},"type":"Limited"},"status":{}})[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:68]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:58]: get default configs: {test_fixture.json [Edge: No Spec fields] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} { nil nil} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:54 Mode: 1
2026/02/09 18:41:54 Base JSON size: 46 bytes
2026/02/09 18:41:54 Number of external values: 1
2026/02/09 18:41:54   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:54   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:54   [KEEP] spec: map[type:] (missing in external)
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:54 [DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"type":""},"status":{}})[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta1/ctest_defaults_test.go:68]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestDefaultWithPriorityLevelConfiguration (0.00s)
PASS
coverage: 17.6% of statements
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/v1beta1	0.578s	coverage: 17.6% of statements
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject
=== RUN   TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_treated_as_zero_value
--- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nominal_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject (0.00s)
    --- PASS: TestCtestConvert_v1beta2_LimitedPriorityLevelConfiguration_To_flowcontrol_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_treated_as_zero_value (0.00s)
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject
=== RUN   TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_preserved_through_conversion
--- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/assured_concurrency_shares_is_set_as_expected (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/zero_concurrency_shares_with_reject_response (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/nil_limit_response_defaults_to_reject (0.00s)
    --- PASS: TestCtestConvert_flowcontrol_LimitedPriorityLevelConfiguration_To_v1beta2_LimitedPriorityLevelConfiguration/unsupported_limit_response_type_(empty)_preserved_through_conversion (0.00s)
=== RUN   TestCtestDefaultWithPriorityLevelConfiguration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta2/ctest_defaults_test.go:155]: get default configs: {test_fixture.json [prioritylevelconfiguration exempt] spec [] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {Exempt nil &ExemptPriorityLevelConfiguration{NominalConcurrencyShares:nil,LendablePercent:nil,}} {[]}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:56 [DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:56 Mode: 1
2026/02/09 18:41:56 Base JSON size: 64 bytes
2026/02/09 18:41:56 Number of external values: 89
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] status: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] metadata: map[] (missing in external)
2026/02/09 18:41:56   [KEEP] spec: map[exempt:map[] type:Exempt] (missing in external)
2026/02/09 18:41:56 [DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:56 [DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=89)
[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"exempt":{},"type":"Exempt"},"status":{}})[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/flowcontrol/v1beta2/ctest_defaults_test.go:31]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestDefaultWithPriorityLevelConfiguration (0.01s)
PASS
coverage: 5.5% of statements
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/v1beta2	0.570s	coverage: 5.5% of statements
=== RUN   TestCtestConvert_v1beta3_PriorityLevelConfiguration_To_flowcontrol_PriorityLevelConfiguration

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:41:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:41:57 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/apis/flowcontrol/v1beta3	1.301s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/flowcontrol/validation	0.796s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/imagepolicy		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/imagepolicy/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/imagepolicy/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/imagepolicy/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/networking		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/networking/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/networking/install		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 13.8% of statements
ok  	k8s.io/kubernetes/pkg/apis/networking/v1	0.246s	coverage: 13.8% of statements [no tests to run]
=== RUN   TestCtestIngressBackendConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:28]: Base config: {test_fixture.json [base external spec] backend [ingresses] {<nil> &IngressBackend{ServiceName:test-backend,ServicePort:{0 8080 },Resource:nil,} [] []}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[ingresses]
[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[ingresses], int=1)[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:41:59 [DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:41:59 Mode: 1
2026/02/09 18:41:59 Base JSON size: 61 bytes
2026/02/09 18:41:59 Number of external values: 1
2026/02/09 18:41:59   [KEEP] backend: map[serviceName:test-backend servicePort:8080] (missing in external)
2026/02/09 18:41:59 [DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:41:59 [DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"backend":{"serviceName":"test-backend","servicePort":8080}})[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:37]: Base JSON: 
[DEBUG-CTEST 2026-02-09 18:41:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:61]: Total external test cases: 6
=== RUN   TestCtestIngressBackendConversion/external-0
=== RUN   TestCtestIngressBackendConversion/external-1
=== RUN   TestCtestIngressBackendConversion/external-2
=== RUN   TestCtestIngressBackendConversion/external-3
    ctest_conversion_test.go:78: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/networking/v1beta1/ctest_conversion_test.go:78
        	Error:      	Not equal: 
        	            	expected: v1beta1.IngressSpec{IngressClassName:(*string)(nil), Backend:(*v1beta1.IngressBackend)(0x14000516680), TLS:[]v1beta1.IngressTLS(nil), Rules:[]v1beta1.IngressRule(nil)}
        	            	actual  : v1beta1.IngressSpec{IngressClassName:(*string)(nil), Backend:(*v1beta1.IngressBackend)(0x14000517780), TLS:[]v1beta1.IngressTLS(nil), Rules:[]v1beta1.IngressRule(nil)}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -5,3 +5,3 @@
        	            	   ServicePort: (intstr.IntOrString) {
        	            	-   Type: (intstr.Type) 1,
        	            	+   Type: (intstr.Type) 0,
        	            	    IntVal: (int32) 0,
        	Test:       	TestCtestIngressBackendConversion/external-3
        	Messages:   	networking.IngressSpec -> v1beta1.IngressSpec
=== RUN   TestCtestIngressBackendConversion/external-4
=== RUN   TestCtestIngressBackendConversion/external-5

==================== CTEST END ======================
--- FAIL: TestCtestIngressBackendConversion (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-0 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-1 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-2 (0.00s)
    --- FAIL: TestCtestIngressBackendConversion/external-3 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-4 (0.00s)
    --- PASS: TestCtestIngressBackendConversion/external-5 (0.00s)
FAIL
coverage: 20.3% of statements
FAIL	k8s.io/kubernetes/pkg/apis/networking/v1beta1	0.783s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/networking/validation	0.271s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/node		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/node/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/node/v1		coverage: 0.0% of statements
=== RUN   TestCtestRuntimeClassConversion
=== RUN   TestCtestRuntimeClassConversion/empty
=== RUN   TestCtestRuntimeClassConversion/nil-overhead
=== RUN   TestCtestRuntimeClassConversion/nil-scheduling
=== RUN   TestCtestRuntimeClassConversion/empty-name
=== RUN   TestCtestRuntimeClassConversion/missing-handler
=== RUN   TestCtestRuntimeClassConversion/fully-specified
=== RUN   TestCtestRuntimeClassConversion/empty-overhead
=== RUN   TestCtestRuntimeClassConversion/empty-scheduling
--- PASS: TestCtestRuntimeClassConversion (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/nil-overhead (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/nil-scheduling (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty-name (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/missing-handler (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/fully-specified (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty-overhead (0.00s)
    --- PASS: TestCtestRuntimeClassConversion/empty-scheduling (0.00s)
PASS
coverage: 38.6% of statements
ok  	k8s.io/kubernetes/pkg/apis/node/v1alpha1	0.992s	coverage: 38.6% of statements
	k8s.io/kubernetes/pkg/apis/node/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateRuntimeClass

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:29]: get default configs: {test_fixture.json [baseline runtimeclass] runtimeClass [] {{ } {foo      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} bar <nil> <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "runtimeClass" in requested fixtures
2026/02/09 18:42:00 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:42:00 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Handler":"bar","Overhead":null,"Scheduling":null,"name":"foo"})[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:37]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateRuntimeClass (0.00s)
=== RUN   TestCtestValidateRuntimeUpdate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "runtimeClass" in requested fixtures
2026/02/09 18:42:00 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:42:00 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Handler":"bar","Overhead":null,"Scheduling":null,"name":"foo"})[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:139]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateRuntimeUpdate (0.00s)
=== RUN   TestCtestValidateOverhead

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "overhead" in requested fixtures
2026/02/09 18:42:00 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:42:00 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"PodFixed":{"cpu":"10","memory":"10G"}})[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/node/validation/ctest_validation_test.go:232]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateOverhead (0.00s)
=== RUN   TestCtestValidateScheduling

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:42:00 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:00 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/apis/node/validation	0.565s
	k8s.io/kubernetes/pkg/apis/policy		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/policy/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/policy/install		coverage: 0.0% of statements
=== RUN   TestCtestConversion
=== RUN   TestCtestConversion/v1_to_internal_with_match_none_selector
=== RUN   TestCtestConversion/v1_to_internal_with_nil_selector
=== RUN   TestCtestConversion/v1_to_internal_with_match_all_selector
=== RUN   TestCtestConversion/v1_to_internal_with_selector_having_empty_key
=== RUN   TestCtestConversion/v1_to_internal_with_selector_having_both_MatchLabels_and_MatchExpressions
=== RUN   TestCtestConversion/v1_to_internal_with_selector_having_empty_MatchExpressions_slice
--- PASS: TestCtestConversion (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_match_none_selector (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_nil_selector (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_match_all_selector (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_selector_having_empty_key (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_selector_having_both_MatchLabels_and_MatchExpressions (0.00s)
    --- PASS: TestCtestConversion/v1_to_internal_with_selector_having_empty_MatchExpressions_slice (0.00s)
PASS
coverage: 34.2% of statements
ok  	k8s.io/kubernetes/pkg/apis/policy/v1	0.727s	coverage: 34.2% of statements
testing: warning: no tests to run
PASS
coverage: 1.7% of statements
ok  	k8s.io/kubernetes/pkg/apis/policy/v1beta1	0.949s	coverage: 1.7% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/policy/validation	0.633s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestResourceMatches
[DEBUG-CTEST 2026-02-09 18:42:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:21]: Start TestCtestResourceMatches
[DEBUG-CTEST 2026-02-09 18:42:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:147]: Number of test cases: 16
Running 0 th test case.
{all matches 01 [*] foo  true}
=== RUN   TestCtestResourceMatches/all_matches_01
Running 1 th test case.
{checks all rules [doesn't match *] foo  true}
=== RUN   TestCtestResourceMatches/checks_all_rules
Running 2 th test case.
{matches exact rule [foo/bar] foo/bar bar true}
=== RUN   TestCtestResourceMatches/matches_exact_rule
Running 3 th test case.
{matches exact rule 02 [foo/bar] foo  false}
=== RUN   TestCtestResourceMatches/matches_exact_rule_02
Running 4 th test case.
{matches subresource [*/scale] foo/scale scale true}
=== RUN   TestCtestResourceMatches/matches_subresource
Running 5 th test case.
{doesn't match partial subresource hit [foo/bar */other] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_match_partial_subresource_hit
Running 6 th test case.
{matches subresource with multiple slashes [*/other/segment] foo/other/segment other/segment true}
=== RUN   TestCtestResourceMatches/matches_subresource_with_multiple_slashes
Running 7 th test case.
{doesn't fail on empty [] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_fail_on_empty
Running 8 th test case.
{doesn't fail on slash [/] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_fail_on_slash
Running 9 th test case.
{doesn't fail on missing subresource [*/] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_fail_on_missing_subresource
Running 10 th test case.
{doesn't match on not star [*something/other/segment] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_match_on_not_star
Running 11 th test case.
{doesn't match on something else [something/other/segment] foo/other/segment other/segment false}
=== RUN   TestCtestResourceMatches/doesn't_match_on_something_else
Running 12 th test case.
{nil resources slice [] foo  false}
=== RUN   TestCtestResourceMatches/nil_resources_slice
Running 13 th test case.
{empty combined resource with wildcard [*]   false}
=== RUN   TestCtestResourceMatches/empty_combined_resource_with_wildcard
    ctest_helpers_test.go:158: expected false, got true
Running 14 th test case.
{wildcard with subpath not allowed [foo/*] foo/bar  false}
=== RUN   TestCtestResourceMatches/wildcard_with_subpath_not_allowed
Running 15 th test case.
{resource with trailing slash only [foo/] foo  false}
=== RUN   TestCtestResourceMatches/resource_with_trailing_slash_only
--- FAIL: TestCtestResourceMatches (0.00s)
    --- PASS: TestCtestResourceMatches/all_matches_01 (0.00s)
    --- PASS: TestCtestResourceMatches/checks_all_rules (0.00s)
    --- PASS: TestCtestResourceMatches/matches_exact_rule (0.00s)
    --- PASS: TestCtestResourceMatches/matches_exact_rule_02 (0.00s)
    --- PASS: TestCtestResourceMatches/matches_subresource (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_match_partial_subresource_hit (0.00s)
    --- PASS: TestCtestResourceMatches/matches_subresource_with_multiple_slashes (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_fail_on_empty (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_fail_on_slash (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_fail_on_missing_subresource (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_match_on_not_star (0.00s)
    --- PASS: TestCtestResourceMatches/doesn't_match_on_something_else (0.00s)
    --- PASS: TestCtestResourceMatches/nil_resources_slice (0.00s)
    --- FAIL: TestCtestResourceMatches/empty_combined_resource_with_wildcard (0.00s)
    --- PASS: TestCtestResourceMatches/wildcard_with_subpath_not_allowed (0.00s)
    --- PASS: TestCtestResourceMatches/resource_with_trailing_slash_only (0.00s)
=== RUN   TestCtestPolicyRuleBuilder
[DEBUG-CTEST 2026-02-09 18:42:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:165]: Start TestCtestPolicyRuleBuilder
[DEBUG-CTEST 2026-02-09 18:42:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/ctest_helpers_test.go:330]: Number of test cases: 12
Running 0 th test case.
{all empty [] [] [] [] [] false {[] [] [] [] []}}
Running 1 th test case.
{normal resource case [get] [] [pod] [gakki] [] true {[get] [] [pod] [gakki] []}}
Running 2 th test case.
{normal noResourceURLs case [get] [] [] [] [/api/registry/healthz] true {[get] [] [] [] [/api/registry/healthz]}}
Running 3 th test case.
{nonResourceURLs with no-empty groups [get] [] [] [] [/api/registry/healthz] false {[] [] [] [] []}}
Running 4 th test case.
{nonResourceURLs with no-empty resources [get] [] [deployments secrets] [] [/api/registry/healthz] false {[] [] [] [] []}}
Running 5 th test case.
{nonResourceURLs with no-empty resourceNames [get] [] [] [gakki] [/api/registry/healthz] false {[] [] [] [] []}}
Running 6 th test case.
{resource without apiGroups [get] [] [pod] [] [] false {[] [] [] [] []}}
Running 7 th test case.
{resourceNames with illegal verb [list watch create deletecollection] [] [pod] [gakki] [] false {[] [] [] [] []}}
Running 8 th test case.
{no nonResourceURLs nor resources [get] [rbac.authorization.k8s.io] [] [gakki] [] false {[] [] [] [] []}}
Running 9 th test case.
{verbs nil but urls present [] [] [] [] [/healthz] false {[] [] [] [] []}}
Running 10 th test case.
{empty verb string [] [] [pod] [] [] false {[] [] [] [] []}}
    ctest_helpers_test.go:343: Expected PolicyRule{} got PolicyRule{APIGroups:[""], Resources:["pod"], Verbs:[""]}.
Running 11 th test case.
{duplicate verbs with valid resources [get get] [] [pod] [] [] true {[get get] [] [pod] [] []}}
    ctest_helpers_test.go:343: Expected PolicyRule{APIGroups:[""], Resources:["pod"], Verbs:["get" "get"]} got PolicyRule{APIGroups:[""], Resources:["pod"], Verbs:["get"]}.
--- FAIL: TestCtestPolicyRuleBuilder (0.00s)
FAIL
coverage: 17.6% of statements
FAIL	k8s.io/kubernetes/pkg/apis/rbac	0.603s
	k8s.io/kubernetes/pkg/apis/rbac/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/rbac/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/rbac/v1		coverage: 0.0% of statements
=== RUN   TestCtestConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/v1alpha1/ctest_conversion_test.go:32]: get default configs: {test_fixture.json [specific user] rolebinding [rolebindings] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{User rbac.authorization.k8s.io/v1alpha1 bob }] {rbac.authorization.k8s.io  foo}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[rolebindings]
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[rolebindings], int=1)[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [rolebindings]
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:04 load all fixtures failed: requested fixture keys not found in test_fixtures.json: rolebindings
FAIL	k8s.io/kubernetes/pkg/apis/rbac/v1alpha1	0.572s
	k8s.io/kubernetes/pkg/apis/rbac/v1beta1		coverage: 0.0% of statements
=== RUN   TestCtestValidateClusterRoleBinding

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/rbac/validation/ctest_validation_test.go:26]: get default configs: {test_fixture.json [valid clusterrolebinding] roleRef [clusterrolebindings] {{ } {master      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{ServiceAccount  validsaname foo} {User rbac.authorization.k8s.io valid@username } {Group rbac.authorization.k8s.io valid@groupname }] {rbac.authorization.k8s.io ClusterRole valid}}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[clusterrolebindings]
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[clusterrolebindings], int=1)[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [clusterrolebindings]
[DEBUG-CTEST 2026-02-09 18:42:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:04 load all fixtures failed: requested fixture keys not found in test_fixtures.json: clusterrolebindings
FAIL	k8s.io/kubernetes/pkg/apis/rbac/validation	0.826s
	k8s.io/kubernetes/pkg/apis/resource		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/resource/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/resource/install	1.244s	coverage: 100.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 12.1% of statements
ok  	k8s.io/kubernetes/pkg/apis/resource/v1	1.027s	coverage: 12.1% of statements [no tests to run]
=== RUN   TestCtestSetDefaultDeviceTaint

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/v1alpha3/ctest_defaults_test.go:34]: get default configs: {test_fixture.json [device taint default and preset] spec [customresourcedefinitions] [&DeviceTaintRule{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:DeviceTaintRuleSpec{DeviceSelector:nil,Taint:,},} &DeviceTaintRule{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:DeviceTaintRuleSpec{DeviceSelector:nil,Taint:,},}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:42:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:42:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:42:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:10 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/apis/resource/v1alpha3	0.921s
=== RUN   TestCtestConversion
=== RUN   TestCtestConversion/v1beta1_to_internal_without_alternatives
=== RUN   TestCtestConversion/internal_to_v1beta1_without_alternatives
=== RUN   TestCtestConversion/v1beta1_to_internal_with_alternatives
=== RUN   TestCtestConversion/internal_to_v1beta1_with_alternatives
=== RUN   TestCtestConversion/v1beta1_to_internal_missing_selectors_and_capacity
    ctest_conversion_test.go:511: unexpected result:
           &resource.ResourceClaim{
          	TypeMeta:   {},
          	ObjectMeta: {},
          	Spec: resource.ResourceClaimSpec{
          		Devices: resource.DeviceClaim{
          			Requests: []resource.DeviceRequest{
          				{
          					Name: "foo",
          					Exactly: &resource.ExactDeviceRequest{
          						DeviceClassName: "class-a",
        - 						Selectors:       []resource.DeviceSelector{},
        + 						Selectors:       nil,
          						AllocationMode:  "ExactCount",
          						Count:           1,
          						... // 3 identical fields
          					},
          					FirstAvailable: nil,
          				},
          			},
          			Constraints: nil,
          			Config:      nil,
          		},
          	},
          	Status: {},
          }
=== RUN   TestCtestConversion/internal_to_v1beta1_missing_selectors_and_capacity
=== RUN   TestCtestConversion/v1beta1_to_internal_zero_count
=== RUN   TestCtestConversion/v1beta1_to_internal_large_count
--- FAIL: TestCtestConversion (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_without_alternatives (0.00s)
    --- PASS: TestCtestConversion/internal_to_v1beta1_without_alternatives (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_with_alternatives (0.00s)
    --- PASS: TestCtestConversion/internal_to_v1beta1_with_alternatives (0.00s)
    --- FAIL: TestCtestConversion/v1beta1_to_internal_missing_selectors_and_capacity (0.00s)
    --- PASS: TestCtestConversion/internal_to_v1beta1_missing_selectors_and_capacity (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_zero_count (0.00s)
    --- PASS: TestCtestConversion/v1beta1_to_internal_large_count (0.00s)
FAIL
coverage: 20.8% of statements
FAIL	k8s.io/kubernetes/pkg/apis/resource/v1beta1	0.373s
=== RUN   TestCtestSetDefaultAllocationMode

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/v1beta2/ctest_defaults_test.go:35]: get default configs: {test_fixture.json [empty exactly] exactly [resourceclaims] { []  0 <nil> [] nil}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceclaims]
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceclaims], int=1)[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceclaims]
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:11 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceclaims
FAIL	k8s.io/kubernetes/pkg/apis/resource/v1beta2	1.331s
=== RUN   TestCtestTruncateIfTooLong

==================== CTEST START ====================
=== RUN   TestCtestTruncateIfTooLong/truncate-odd-string-odd-limit
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-odd-string-odd-limit
Input: "abcdefghijklmnopqrs", maxLen: 17, expected: "abcdefg...mnopqrs"
=== RUN   TestCtestTruncateIfTooLong/nop
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: nop
Input: "hello", maxLen: 10, expected: "hello"
=== RUN   TestCtestTruncateIfTooLong/truncate-to-builtin-limit
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-to-builtin-limit
Input: "hello world how are you", maxLen: 1, expected: "hello w...re you"
=== RUN   TestCtestTruncateIfTooLong/truncate-even-string-even-limit
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-even-string-even-limit
Input: "abcdefghijklmnopqrst", maxLen: 16, expected: "abcdefg...opqrst"
=== RUN   TestCtestTruncateIfTooLong/truncate-even-string-odd-limit
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-even-string-odd-limit
Input: "abcdefghijklmnopqrst", maxLen: 17, expected: "abcdefg...nopqrst"
=== RUN   TestCtestTruncateIfTooLong/empty-string
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: empty-string
Input: "", maxLen: 5, expected: ""
=== RUN   TestCtestTruncateIfTooLong/very-large-maxlen
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: very-large-maxlen
Input: "short", maxLen: 1000, expected: "short"
=== RUN   TestCtestTruncateIfTooLong/truncate-to-limit
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-to-limit
Input: "hello world how are you", maxLen: 18, expected: "hello wo...are you"
=== RUN   TestCtestTruncateIfTooLong/truncate-odd-string-even-limit
[DEBUG-CTEST 2026-02-09 18:42:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_common_test.go:75]: Running case: truncate-odd-string-even-limit
Input: "abcdefghijklmnopqrs", maxLen: 16, expected: "abcdefg...nopqrs"

==================== CTEST END ======================
--- PASS: TestCtestTruncateIfTooLong (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-odd-string-odd-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/nop (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-to-builtin-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-even-string-even-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-even-string-odd-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/empty-string (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/very-large-maxlen (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-to-limit (0.00s)
    --- PASS: TestCtestTruncateIfTooLong/truncate-odd-string-even-limit (0.00s)
=== RUN   TestCtestValidateDeviceCapacity
=== RUN   TestCtestValidateDeviceCapacity/no-policy
=== RUN   TestCtestValidateDeviceCapacity/policy-without-default
=== RUN   TestCtestValidateDeviceCapacity/valid-range-with-max
=== RUN   TestCtestValidateDeviceCapacity/valid-single-option
=== RUN   TestCtestValidateDeviceCapacity/valid-two-options
=== RUN   TestCtestValidateDeviceCapacity/invalid-options-duplicate
=== RUN   TestCtestValidateDeviceCapacity/zero-capacity-no-policy
=== RUN   TestCtestValidateDeviceCapacity/negative-capacity-with-policy
    ctest_validation_device_capacity_test.go:178: unexpected field errors (-want, +got):
          field.ErrorList(
        - 	{e`spec.devices.capacity: Invalid value: "-1Gi": capacity must be non-negative`},
        + 	nil,
          )
=== RUN   TestCtestValidateDeviceCapacity/policy-with-valid-values-without-default
=== RUN   TestCtestValidateDeviceCapacity/valid-simple-range
=== RUN   TestCtestValidateDeviceCapacity/valid-range-with-max-and-step
=== RUN   TestCtestValidateDeviceCapacity/more-than-one-policy
=== RUN   TestCtestValidateDeviceCapacity/invalid-range-large-min-small-max
=== RUN   TestCtestValidateDeviceCapacity/invalid-range-large-step
=== RUN   TestCtestValidateDeviceCapacity/policy-with-valid-range-step-zero
--- FAIL: TestCtestValidateDeviceCapacity (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/no-policy (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/policy-without-default (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/valid-range-with-max (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/valid-single-option (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/valid-two-options (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/invalid-options-duplicate (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/zero-capacity-no-policy (0.00s)
    --- FAIL: TestCtestValidateDeviceCapacity/negative-capacity-with-policy (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/policy-with-valid-values-without-default (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/valid-simple-range (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/valid-range-with-max-and-step (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/more-than-one-policy (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/invalid-range-large-min-small-max (0.00s)
    --- PASS: TestCtestValidateDeviceCapacity/invalid-range-large-step (0.00s)
    --- FAIL: TestCtestValidateDeviceCapacity/policy-with-valid-range-step-zero (0.00s)
panic: runtime error: integer divide by zero [recovered]
	panic: runtime error: integer divide by zero

goroutine 111 [running]:
testing.tRunner.func1.2({0x10726bbe0, 0x108d6e510})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10726bbe0?, 0x108d6e510?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/resource/validation.validateRequestPolicyRangeStep({{0x40000000, 0x0}, {0x0}, {0x106b1dcd5, 0x3}, {0x106b24019, 0x8}}, {{0x40000000, 0x0}, {0x0}, ...}, ...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:1042 +0x34c
k8s.io/kubernetes/pkg/apis/resource/validation.validateRequestPolicyRange({{0x40000000, 0x0}, {0x0}, {0x106b1dcd5, 0x3}, {0x106b24019, 0x8}}, {{0x280000000, 0x0}, {0x0}, ...}, ...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:1028 +0x9a0
k8s.io/kubernetes/pkg/apis/resource/validation.validateValidRequestValues({{0x280000000, 0x0}, {0x0}, {0x106b1e3a9, 0x4}, {0x106b24019, 0x8}}, 0x140005e7a70, 0x140007bfad0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:962 +0x534
k8s.io/kubernetes/pkg/apis/resource/validation.validateRequestPolicy({{0x280000000, 0x0}, {0x0}, {0x106b1e3a9, 0x4}, {0x106b24019, 0x8}}, 0x140000a1ef8?, 0x12d8ec524?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:944 +0x230
k8s.io/kubernetes/pkg/apis/resource/validation.validateMultiAllocatableDeviceCapacity({{{0x280000000, 0x0}, {0x0}, {0x106b1e3a9, 0x4}, {0x106b24019, 0x8}}, 0x140005e7a70}, 0x140005c69c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/validation.go:922 +0x1e0
k8s.io/kubernetes/pkg/apis/resource/validation.TestCtestValidateDeviceCapacity.func1(0x14000430fc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/resource/validation/ctest_validation_device_capacity_test.go:177 +0x48
testing.tRunner(0x14000430fc0, 0x14000348ee0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 48
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/apis/resource/validation	1.768s
	k8s.io/kubernetes/pkg/apis/scheduling		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/scheduling/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/scheduling/install		coverage: 0.0% of statements
=== RUN   TestCtestIsKnownSystemPriorityClass

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/scheduling/v1/ctest_helpers_test.go:27]: get default configs: {test_fixture.json [priorityclass basic config] priorityClass [] {{ } {example-priorityclass      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} 1000 false example description <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "priorityClass" in requested fixtures
2026/02/09 18:42:12 [DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:42:12 Mode: 1
2026/02/09 18:42:12 Base JSON size: 94 bytes
2026/02/09 18:42:12 Number of external values: 0
2026/02/09 18:42:12 [DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:42:12 [DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"description":"example description","metadata":{"name":"example-priorityclass"},"value":1000})[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:42:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/scheduling/v1/ctest_helpers_test.go:37]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestIsKnownSystemPriorityClass (0.00s)
PASS
coverage: 18.3% of statements
ok  	k8s.io/kubernetes/pkg/apis/scheduling/v1	2.236s	coverage: 18.3% of statements
testing: warning: no tests to run
PASS
coverage: 21.6% of statements
ok  	k8s.io/kubernetes/pkg/apis/scheduling/v1alpha1	1.566s	coverage: 21.6% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 21.6% of statements
ok  	k8s.io/kubernetes/pkg/apis/scheduling/v1beta1	1.776s	coverage: 21.6% of statements [no tests to run]
=== RUN   TestCtestValidatePriorityClass
    ctest_validation_test.go:84: Expected error for negative value, but it succeeded
--- FAIL: TestCtestValidatePriorityClass (0.00s)
=== RUN   TestCtestValidatePriorityClassUpdate
--- PASS: TestCtestValidatePriorityClassUpdate (0.00s)
FAIL
coverage: 86.7% of statements
FAIL	k8s.io/kubernetes/pkg/apis/scheduling/validation	1.728s
	k8s.io/kubernetes/pkg/apis/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storage/fuzzer		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storage/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storage/util		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 11.3% of statements
ok  	k8s.io/kubernetes/pkg/apis/storage/v1	0.852s	coverage: 11.3% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/storage/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 11.3% of statements
ok  	k8s.io/kubernetes/pkg/apis/storage/v1beta1	0.911s	coverage: 11.3% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/apis/storage/validation	0.942s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/apis/storagemigration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storagemigration/install		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/apis/storagemigration/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestValidateStorageVersionMigration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/storagemigration/validation/ctest_validation_test.go:24]: get default configs: {test_fixture.json [default svm resource] resource [customresourcedefinitions] {non-empty non-empty non-empty}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:20 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/apis/storagemigration/validation	1.083s
=== RUN   TestCtestAuthorizeV0EdgeCases
W0209 18:42:21.357536   85479 abac.go:112] Policy file /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/abac_test3408849883 contained unversioned rules. See docs/admin/authorization.md#abac-mode for ABAC file format details.
--- PASS: TestCtestAuthorizeV0EdgeCases (0.00s)
=== RUN   TestCtestAuthorizeV1beta1EdgeCases
    ctest_abac_test.go:118: expected Allow for /api path, got 2
--- FAIL: TestCtestAuthorizeV1beta1EdgeCases (0.00s)
=== RUN   TestCtestSubjectMatchesEdgeCases
--- FAIL: TestCtestSubjectMatchesEdgeCases (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x102cd1e88]

goroutine 6 [running]:
testing.tRunner.func1.2({0x102e60480, 0x1032da700})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x102e60480?, 0x1032da700?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/auth/authorizer/abac.TestCtestSubjectMatchesEdgeCases(0x14000003c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/auth/authorizer/abac/ctest_abac_test.go:140 +0x38
testing.tRunner(0x14000003c00, 0x102f1cb10)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/auth/authorizer/abac	1.771s
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/auth/nodeidentifier/ctest_default_test.go:12]: Starting TestCtestDefaultNodeIdentifier_NodeIdentity
Running test case #0: nil user
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/nil_user
Running test case #1: node username without group
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/node_username_without_group
Running test case #2: node group without username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_without_username
Running test case #3: node group and username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_and_username
Running test case #4: empty username with node group
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/empty_username_with_node_group
Running test case #5: malformed node username (missing prefix)
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/malformed_node_username_(missing_prefix)
Running test case #6: multiple node groups, valid username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/multiple_node_groups,_valid_username
Running test case #7: username with extra colon segments
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/username_with_extra_colon_segments
    ctest_default_test.go:80: DefaultNodeIdentifier.NodeIdentity() got nodeName = foo:extra, want 
    ctest_default_test.go:83: DefaultNodeIdentifier.NodeIdentity() got isNode = true, want false
Running test case #8: nil groups slice with node username
=== RUN   TestCtestDefaultNodeIdentifier_NodeIdentity/nil_groups_slice_with_node_username
[DEBUG-CTEST 2026-02-09 18:42:20 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/auth/nodeidentifier/ctest_default_test.go:87]: Completed TestCtestDefaultNodeIdentifier_NodeIdentity
--- FAIL: TestCtestDefaultNodeIdentifier_NodeIdentity (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/nil_user (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/node_username_without_group (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_without_username (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/node_group_and_username (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/empty_username_with_node_group (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/malformed_node_username_(missing_prefix) (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/multiple_node_groups,_valid_username (0.00s)
    --- FAIL: TestCtestDefaultNodeIdentifier_NodeIdentity/username_with_extra_colon_segments (0.00s)
    --- PASS: TestCtestDefaultNodeIdentifier_NodeIdentity/nil_groups_slice_with_node_username (0.00s)
FAIL
coverage: 100.0% of statements
FAIL	k8s.io/kubernetes/pkg/auth/nodeidentifier	0.611s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/capabilities	1.960s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/certauthorization		coverage: 0.0% of statements
=== RUN   TestCtestPodRunning
[DEBUG-CTEST 2026-02-09 18:42:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:14]: Start TestCtestPodRunning
[DEBUG-CTEST 2026-02-09 18:42:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:121]: Total test cases: 8
Running 0 th test case.
=== RUN   TestCtestPodRunning/Watch_type_is_deleted
Running 1 th test case.
=== RUN   TestCtestPodRunning/Pod_Status_type_is_PodRunning
Running 2 th test case.
=== RUN   TestCtestPodRunning/Pod_Status_is_PodFailed
Running 3 th test case.
=== RUN   TestCtestPodRunning/Object_type_is_not_pod
Running 4 th test case.
=== RUN   TestCtestPodRunning/Watch_type_is_Modified_with_PodRunning
    ctest_conditions_test.go:131: PodRunning() = true, want false
Running 5 th test case.
=== RUN   TestCtestPodRunning/Object_is_nil
    ctest_conditions_test.go:127: PodRunning() error = <nil>, wantErr true
Running 6 th test case.
=== RUN   TestCtestPodRunning/Pod_Phase_empty_string
Running 7 th test case.
=== RUN   TestCtestPodRunning/Pod_Phase_unknown_value
--- FAIL: TestCtestPodRunning (0.00s)
    --- PASS: TestCtestPodRunning/Watch_type_is_deleted (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Status_type_is_PodRunning (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Status_is_PodFailed (0.00s)
    --- PASS: TestCtestPodRunning/Object_type_is_not_pod (0.00s)
    --- FAIL: TestCtestPodRunning/Watch_type_is_Modified_with_PodRunning (0.00s)
    --- FAIL: TestCtestPodRunning/Object_is_nil (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Phase_empty_string (0.00s)
    --- PASS: TestCtestPodRunning/Pod_Phase_unknown_value (0.00s)
=== RUN   TestCtestPodCompleted
[DEBUG-CTEST 2026-02-09 18:42:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:138]: Start TestCtestPodCompleted
[DEBUG-CTEST 2026-02-09 18:42:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/conditions/ctest_conditions_test.go:245]: Total test cases: 8
Running 0 th test case.
=== RUN   TestCtestPodCompleted/Watch_type_is_deleted
Running 1 th test case.
=== RUN   TestCtestPodCompleted/Pod_Status_is_PodSucceeded
Running 2 th test case.
=== RUN   TestCtestPodCompleted/Pod_Status_is_PodRunning
Running 3 th test case.
=== RUN   TestCtestPodCompleted/Object_type_is_not_pod
Running 4 th test case.
=== RUN   TestCtestPodCompleted/Watch_type_is_Modified_with_PodSucceeded
    ctest_conditions_test.go:255: PodCompleted() = true, want false
Running 5 th test case.
=== RUN   TestCtestPodCompleted/Object_is_nil
    ctest_conditions_test.go:251: PodCompleted() error = <nil>, wantErr true
Running 6 th test case.
=== RUN   TestCtestPodCompleted/Pod_Phase_empty_string
Running 7 th test case.
=== RUN   TestCtestPodCompleted/Pod_Phase_unknown_value
--- FAIL: TestCtestPodCompleted (0.00s)
    --- PASS: TestCtestPodCompleted/Watch_type_is_deleted (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Status_is_PodSucceeded (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Status_is_PodRunning (0.00s)
    --- PASS: TestCtestPodCompleted/Object_type_is_not_pod (0.00s)
    --- FAIL: TestCtestPodCompleted/Watch_type_is_Modified_with_PodSucceeded (0.00s)
    --- FAIL: TestCtestPodCompleted/Object_is_nil (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Phase_empty_string (0.00s)
    --- PASS: TestCtestPodCompleted/Pod_Phase_unknown_value (0.00s)
FAIL
coverage: 100.0% of statements
FAIL	k8s.io/kubernetes/pkg/client/conditions	1.548s
=== RUN   TestCtestForwardPorts
=== RUN   TestCtestForwardPorts/forward_2_ports_with_bidirectional_data
Forwarding from 127.0.0.1:49614 -> 5001
Forwarding from [::1]:49614 -> 5001
Forwarding from 127.0.0.1:49617 -> 6000
Forwarding from [::1]:49617 -> 6000
Handling connection for 49614
Handling connection for 49617
=== RUN   TestCtestForwardPorts/empty_ports_list_should_error
=== RUN   TestCtestForwardPorts/malformed_port_specification_should_error
=== RUN   TestCtestForwardPorts/forward_1_port_with_no_data_either_direction
Forwarding from 127.0.0.1:49624 -> 5000
Forwarding from [::1]:49624 -> 5000
--- PASS: TestCtestForwardPorts (0.01s)
    --- PASS: TestCtestForwardPorts/forward_2_ports_with_bidirectional_data (0.01s)
    --- PASS: TestCtestForwardPorts/empty_ports_list_should_error (0.00s)
    --- PASS: TestCtestForwardPorts/malformed_port_specification_should_error (0.00s)
    --- PASS: TestCtestForwardPorts/forward_1_port_with_no_data_either_direction (0.00s)
=== RUN   TestCtestForwardPortsReturnsErrorWhenAllBindsFailed
Forwarding from 127.0.0.1:49627 -> 5555
Forwarding from [::1]:49627 -> 5555
Unable to listen on port 49627: Listeners failed to create with the following errors: [unable to create listener: Error listen tcp4 127.0.0.1:49627: bind: address already in use unable to create listener: Error listen tcp6 [::1]:49627: bind: address already in use]
--- PASS: TestCtestForwardPortsReturnsErrorWhenAllBindsFailed (0.00s)
=== RUN   TestCtestStream

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:22 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/tests/ctest_remotecommand_test.go:130]: Number of test cases: 8
=== RUN   TestCtestStream/error_(exec)
  E0209 18:42:22.075967   85490 exec.go:72] "Unhandled Error" err="error executing command in container: bail" logger="UnhandledError"
=== RUN   TestCtestStream/error_(attach)
  E0209 18:42:22.076849   85490 attach.go:53] "Unhandled Error" err="error attaching to container: bail" logger="UnhandledError"
=== RUN   TestCtestStream/in/out/err_(exec)
=== RUN   TestCtestStream/in/out/err_(attach)
=== RUN   TestCtestStream/oversized_stdin_(exec)
=== RUN   TestCtestStream/oversized_stdin_(attach)
=== RUN   TestCtestStream/in/out/tty_(exec)
=== RUN   TestCtestStream/in/out/tty_(attach)
=== RUN   TestCtestStream/empty_all_fields_(exec)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: empty all fields (exec): unexpected error: 
=== RUN   TestCtestStream/empty_all_fields_(attach)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: empty all fields (attach): unexpected error: 
=== RUN   TestCtestStream/zero_message_count_with_stdout_(exec)
=== RUN   TestCtestStream/zero_message_count_with_stdout_(attach)
=== RUN   TestCtestStream/invalid_protocol_names_(exec)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: invalid protocol names (exec): unexpected error: 
=== RUN   TestCtestStream/invalid_protocol_names_(attach)
    remotecommand_test.go:126: unexpected error you must specify at least 1 of stdin, stdout, stderr
    ctest_remotecommand_test.go:215: invalid protocol names (attach): unexpected error: 
=== RUN   TestCtestStream/negative_message_count_(treated_as_zero)_(exec)
2026/02/09 18:42:22 http: panic serving 127.0.0.1:49658: strings: negative Repeat count
goroutine 415 [running]:
net/http.(*conn).serve.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/http/server.go:1947 +0xb0
panic({0x1020ed540?, 0x1026aa430?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
strings.Repeat({0x0?, 0x140003c6110?}, 0x1?)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/strings/strings.go:624 +0x588
k8s.io/kubernetes/pkg/client/tests.TestCtestStream.func1.fakeServer.1({0x1026c5ac8, 0x140039640e0}, 0x140004392c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/tests/remotecommand_test.go:136 +0x2f8
net/http.HandlerFunc.ServeHTTP(0x0?, {0x1026c5ac8?, 0x140039640e0?}, 0x140000a7b60?)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/http/server.go:2294 +0x38
net/http.serverHandler.ServeHTTP({0x140006f4ab0?}, {0x1026c5ac8?, 0x140039640e0?}, 0x1?)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/http/server.go:3301 +0xbc
net/http.(*conn).serve(0x140002450e0, {0x1026dbf58, 0x140006f4990})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/http/server.go:2102 +0x52c
created by net/http.(*Server).Serve in goroutine 717
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/http/server.go:3454 +0x3d8
--- FAIL: TestCtestStream (0.08s)
    --- PASS: TestCtestStream/error_(exec) (0.00s)
    --- PASS: TestCtestStream/error_(attach) (0.00s)
    --- PASS: TestCtestStream/in/out/err_(exec) (0.00s)
    --- PASS: TestCtestStream/in/out/err_(attach) (0.00s)
    --- PASS: TestCtestStream/oversized_stdin_(exec) (0.03s)
    --- PASS: TestCtestStream/oversized_stdin_(attach) (0.03s)
    --- PASS: TestCtestStream/in/out/tty_(exec) (0.00s)
    --- PASS: TestCtestStream/in/out/tty_(attach) (0.00s)
    --- FAIL: TestCtestStream/empty_all_fields_(exec) (0.00s)
    --- FAIL: TestCtestStream/empty_all_fields_(attach) (0.00s)
    --- PASS: TestCtestStream/zero_message_count_with_stdout_(exec) (0.00s)
    --- PASS: TestCtestStream/zero_message_count_with_stdout_(attach) (0.00s)
    --- FAIL: TestCtestStream/invalid_protocol_names_(exec) (0.00s)
    --- FAIL: TestCtestStream/invalid_protocol_names_(attach) (0.00s)
    --- FAIL: TestCtestStream/negative_message_count_(treated_as_zero)_(exec) (0.00s)
panic: strings: negative Repeat count [recovered]
	panic: strings: negative Repeat count

goroutine 716 [running]:
testing.tRunner.func1.2({0x1020ed540, 0x1026aa430})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1020ed540?, 0x1026aa430?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
strings.Repeat({0x101b81131?, 0x1026dbe78?}, 0x103e49fc0?)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/strings/strings.go:624 +0x588
k8s.io/kubernetes/pkg/client/tests.TestCtestStream.func1(0x14003ad0000)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/client/tests/ctest_remotecommand_test.go:219 +0xf78
testing.tRunner(0x14003ad0000, 0x140006da5d0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 193
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/client/tests	2.351s
?   	k8s.io/kubernetes/pkg/cluster/ports	[no test files]
=== RUN   TestCtestClaimPods
=== RUN   TestCtestClaimPods/Claim_pods_with_correct_label
=== RUN   TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_pods
=== RUN   TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_new_pods
=== RUN   TestCtestClaimPods/Controller_can_not_claim_pods_owned_by_another_controller
=== RUN   TestCtestClaimPods/Controller_releases_claimed_pods_when_selector_doesn't_match
I0209 18:42:27.690509   85570 controller_ref_manager.go:240] "Patching pod to remove its controllerRef" pod="default/pod2" gvk="/, Kind=" controller=""
=== RUN   TestCtestClaimPods/Controller_does_not_claim_orphaned_pods_marked_for_deletion
=== RUN   TestCtestClaimPods/Controller_claims_or_release_pods_according_to_selector_with_finalizers
I0209 18:42:27.690890   85570 controller_ref_manager.go:240] "Patching pod to remove its controllerRef" pod="default/pod2" gvk="/, Kind=" controller=""
=== RUN   TestCtestClaimPods/Controller_does_not_claim_pods_of_different_namespace
=== RUN   TestCtestClaimPods/Cluster_scoped_controller_claims_pods_of_specified_namespace
=== RUN   TestCtestClaimPods/Pod_with_nil_labels_map_should_not_be_claimed
=== RUN   TestCtestClaimPods/Pod_with_empty_labels_map_should_not_be_claimed
=== RUN   TestCtestClaimPods/Pod_with_matching_label_but_controller_UID_empty_should_not_be_claimed_as_controller
--- PASS: TestCtestClaimPods (0.00s)
    --- PASS: TestCtestClaimPods/Claim_pods_with_correct_label (0.00s)
    --- PASS: TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_pods (0.00s)
    --- PASS: TestCtestClaimPods/Controller_marked_for_deletion_can_not_claim_new_pods (0.00s)
    --- PASS: TestCtestClaimPods/Controller_can_not_claim_pods_owned_by_another_controller (0.00s)
    --- PASS: TestCtestClaimPods/Controller_releases_claimed_pods_when_selector_doesn't_match (0.00s)
    --- PASS: TestCtestClaimPods/Controller_does_not_claim_orphaned_pods_marked_for_deletion (0.00s)
    --- PASS: TestCtestClaimPods/Controller_claims_or_release_pods_according_to_selector_with_finalizers (0.00s)
    --- PASS: TestCtestClaimPods/Controller_does_not_claim_pods_of_different_namespace (0.00s)
    --- PASS: TestCtestClaimPods/Cluster_scoped_controller_claims_pods_of_specified_namespace (0.00s)
    --- PASS: TestCtestClaimPods/Pod_with_nil_labels_map_should_not_be_claimed (0.00s)
    --- PASS: TestCtestClaimPods/Pod_with_empty_labels_map_should_not_be_claimed (0.00s)
    --- PASS: TestCtestClaimPods/Pod_with_matching_label_but_controller_UID_empty_should_not_be_claimed_as_controller (0.00s)
=== RUN   TestCtestGeneratePatchBytesForDelete
=== RUN   TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes
=== RUN   TestCtestGeneratePatchBytesForDelete/check_if_parent_uid_is_escaped
=== RUN   TestCtestGeneratePatchBytesForDelete/check_if_revision_uid_uid_is_escaped
=== RUN   TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_multiple_owners
=== RUN   TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_a_finalizer_and_multiple_owners
=== RUN   TestCtestGeneratePatchBytesForDelete/empty_ownerUID_slice_should_produce_uid_only
    ctest_controller_ref_manager_test.go:323: generatePatchBytesForDelete() got = {"metadata":{"uid":"dep","ownerReferences":null}}, want {"metadata":{"uid":"dep","ownerReferences":[]}}
--- FAIL: TestCtestGeneratePatchBytesForDelete (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_if_parent_uid_is_escaped (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_if_revision_uid_uid_is_escaped (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_multiple_owners (0.00s)
    --- PASS: TestCtestGeneratePatchBytesForDelete/check_the_structure_of_patch_bytes_with_a_finalizer_and_multiple_owners (0.00s)
    --- FAIL: TestCtestGeneratePatchBytesForDelete/empty_ownerUID_slice_should_produce_uid_only (0.00s)
FAIL
coverage: 10.1% of statements
FAIL	k8s.io/kubernetes/pkg/controller	0.934s
	k8s.io/kubernetes/pkg/controller/apis/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/apis/config/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/apis/config/scheme	0.410s	coverage: 100.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 1.1% of statements
ok  	k8s.io/kubernetes/pkg/controller/apis/config/v1alpha1	1.155s	coverage: 1.1% of statements [no tests to run]
=== RUN   TestCtestSimpleSign

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/bootstrap/ctest_bootstrapsigner_test.go:210]: get default configs: {test_fixture.json [default configmap data] data [configmaps] map[kubeconfig:payload]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:42:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:28 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/controller/bootstrap	1.259s
# k8s.io/kubernetes/pkg/controller/certificates/signer
# [k8s.io/kubernetes/pkg/controller/certificates/signer]
pkg/controller/certificates/signer/ctest_signer_test.go:294:32: conversion from untyped int to KeyUsage (string) yields a string of one rune, not a string of digits
=== RUN   TestCtestCertificateController

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/ctest_certificate_controller_test.go:189]: get default configs: {test_fixture.json [default csr] metadata.name [certificatesigningrequests] {{ } {test-csr      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[]  <nil> []   [] map[]} {[] []}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[certificatesigningrequests]
[DEBUG-CTEST 2026-02-09 18:42:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[certificatesigningrequests], int=1)[DEBUG-CTEST 2026-02-09 18:42:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [certificatesigningrequests]
[DEBUG-CTEST 2026-02-09 18:42:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:29 load all fixtures failed: requested fixture keys not found in test_fixtures.json: certificatesigningrequests
FAIL	k8s.io/kubernetes/pkg/controller/certificates	2.423s
=== RUN   TestCtestHandle
=== RUN   TestCtestHandle/recognized:false,allowed:false,err:false,clientError:false
=== RUN   TestCtestHandle/recognized:false,allowed:true,err:false,clientError:false
=== RUN   TestCtestHandle/recognized:true,allowed:false,err:true,clientError:false
=== RUN   TestCtestHandle/recognized:true,allowed:true,err:false,clientError:false
=== RUN   TestCtestHandle/recognized:true,allowed:true,err:true,clientError:true
    ctest_sarapprove_test.go:101: expected no client actions due to client error but got: []testing.Action{testing.CreateActionImpl{ActionImpl:testing.ActionImpl{Namespace:"", Verb:"create", Resource:schema.GroupVersionResource{Group:"authorization.k8s.io", Version:"v1", Resource:"subjectaccessreviews"}, Subresource:""}, Name:"", Object:(*v1.SubjectAccessReview)(0x140002a1500), CreateOptions:v1.CreateOptions{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, DryRun:[]string(nil), FieldManager:"", FieldValidation:""}}}
--- FAIL: TestCtestHandle (0.00s)
    --- PASS: TestCtestHandle/recognized:false,allowed:false,err:false,clientError:false (0.00s)
    --- PASS: TestCtestHandle/recognized:false,allowed:true,err:false,clientError:false (0.00s)
    --- PASS: TestCtestHandle/recognized:true,allowed:false,err:true,clientError:false (0.00s)
    --- PASS: TestCtestHandle/recognized:true,allowed:true,err:false,clientError:false (0.00s)
    --- FAIL: TestCtestHandle/recognized:true,allowed:true,err:true,clientError:true (0.00s)
FAIL
coverage: 59.6% of statements
FAIL	k8s.io/kubernetes/pkg/controller/certificates/approver	1.592s
=== RUN   TestCtestCertificateAuthority
=== RUN   TestCtestCertificateAuthority/ca_info
=== RUN   TestCtestCertificateAuthority/key_usage
=== RUN   TestCtestCertificateAuthority/ext_key_usage
=== RUN   TestCtestCertificateAuthority/backdate_without_short
=== RUN   TestCtestCertificateAuthority/backdate_without_short_and_super_small_ttl
=== RUN   TestCtestCertificateAuthority/backdate_with_short
=== RUN   TestCtestCertificateAuthority/backdate_with_short_and_super_small_ttl
=== RUN   TestCtestCertificateAuthority/backdate_with_short_but_longer_ttl
=== RUN   TestCtestCertificateAuthority/truncate_expiration
=== RUN   TestCtestCertificateAuthority/uri_sans
=== RUN   TestCtestCertificateAuthority/expired_ca
=== RUN   TestCtestCertificateAuthority/expired_ca_with_backdate
=== RUN   TestCtestCertificateAuthority/zero_ttl
=== RUN   TestCtestCertificateAuthority/very_large_ttl_(expect_truncation_to_24h)
=== RUN   TestCtestCertificateAuthority/negative_ttl_(treated_as_zero)
    ctest_authority_test.go:275: unexpected diff:   x509.Certificate{
          	... // 13 ignored fields
          	NotBefore: Inverse(RoundTime, s"2026-02-09 18:42:30 -0600 CST"),
        - 	NotAfter:  time.Time(Inverse(RoundTime, s"2026-02-09 23:42:30 +0000 UTC")),
        + 	NotAfter:  time.Time(Inverse(RoundTime, s"2026-02-09 18:42:30 -0600 CST")),
          	KeyUsage:  0,
          	... // 26 ignored and 10 identical fields
          }
--- FAIL: TestCtestCertificateAuthority (0.02s)
    --- PASS: TestCtestCertificateAuthority/ca_info (0.00s)
    --- PASS: TestCtestCertificateAuthority/key_usage (0.00s)
    --- PASS: TestCtestCertificateAuthority/ext_key_usage (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_without_short (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_without_short_and_super_small_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_with_short (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_with_short_and_super_small_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/backdate_with_short_but_longer_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/truncate_expiration (0.00s)
    --- PASS: TestCtestCertificateAuthority/uri_sans (0.00s)
    --- PASS: TestCtestCertificateAuthority/expired_ca (0.00s)
    --- PASS: TestCtestCertificateAuthority/expired_ca_with_backdate (0.00s)
    --- PASS: TestCtestCertificateAuthority/zero_ttl (0.00s)
    --- PASS: TestCtestCertificateAuthority/very_large_ttl_(expect_truncation_to_24h) (0.00s)
    --- FAIL: TestCtestCertificateAuthority/negative_ttl_(treated_as_zero) (0.00s)
FAIL
coverage: 83.1% of statements
FAIL	k8s.io/kubernetes/pkg/controller/certificates/authority	2.112s
=== RUN   TestCtestCleanerWithApprovedExpiredCSR

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/cleaner/ctest_cleaner_test.go:249]: Total test cases: 18
Running 0 th test case: no delete approved not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline
Running 1 th test case: no delete approved passed deadline not issued
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_passed_deadline_not_issued
Running 2 th test case: delete approved passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline
  I0209 18:42:36.761711   85688 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="fake-csr" approvedExpiration="1h0m0s"
Running 3 th test case: no delete denied not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_denied_not_passed_deadline
Running 4 th test case: delete denied passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_denied_passed_deadline
  I0209 18:42:36.761897   85688 cleaner.go:149] "Cleaning CSR as it is more than deniedExpiration duration old and denied." csr="fake-csr" deniedExpiration="1h0m0s"
Running 5 th test case: no delete failed not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_failed_not_passed_deadline
Running 6 th test case: delete failed passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_failed_passed_deadline
  I0209 18:42:36.761927   85688 cleaner.go:162] "Cleaning CSR as it is more than deniedExpiration duration old and failed." csr="fake-csr" deniedExpiration="1h0m0s"
Running 7 th test case: no delete pending not passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_pending_not_passed_deadline
Running 8 th test case: delete pending passed deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_pending_passed_deadline
  I0209 18:42:36.761964   85688 cleaner.go:137] "Cleaning CSR as it is more than pendingExpiration duration old and unhandled." csr="fake-csr" pendingExpiration="24h0m0s"
Running 9 th test case: no delete approved not passed deadline unexpired
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline_unexpired
Running 10 th test case: delete approved not passed deadline expired
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_approved_not_passed_deadline_expired
  I0209 18:42:36.762052   85688 cleaner.go:123] "Cleaning CSR as the associated certificate is expired." csr="fake-csr"
Running 11 th test case: delete approved passed deadline unparseable
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline_unparseable
  I0209 18:42:36.762074   85688 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="fake-csr" approvedExpiration="1h0m0s"
Running 12 th test case: zero creation timestamp
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/zero_creation_timestamp
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions
Running 13 th test case: future creation timestamp
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/future_creation_timestamp
Running 14 th test case: unknown condition type
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/unknown_condition_type
Running 15 th test case: nil conditions slice
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/nil_conditions_slice
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions
Running 16 th test case: empty certificate data
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/empty_certificate_data
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions
Running 17 th test case: certificate nil with approved condition past deadline
=== RUN   TestCtestCleanerWithApprovedExpiredCSR/certificate_nil_with_approved_condition_past_deadline
    ctest_cleaner_test.go:277: got 0 actions, wanted 1 actions

==================== CTEST END ======================
--- FAIL: TestCtestCleanerWithApprovedExpiredCSR (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_passed_deadline_not_issued (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_denied_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_denied_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_failed_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_failed_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_pending_not_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_pending_passed_deadline (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/no_delete_approved_not_passed_deadline_unexpired (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_approved_not_passed_deadline_expired (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/delete_approved_passed_deadline_unparseable (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/zero_creation_timestamp (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/future_creation_timestamp (0.00s)
    --- PASS: TestCtestCleanerWithApprovedExpiredCSR/unknown_condition_type (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/nil_conditions_slice (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/empty_certificate_data (0.00s)
    --- FAIL: TestCtestCleanerWithApprovedExpiredCSR/certificate_nil_with_approved_condition_past_deadline (0.00s)
=== RUN   TestCtestPCRCleaner
=== RUN   TestCtestPCRCleaner/Pending_request_within_the_threshold_should_be_left_alone
=== RUN   TestCtestPCRCleaner/Pending_request_outside_the_threshold_should_be_deleted
=== RUN   TestCtestPCRCleaner/Terminal_request_within_the_threshold_should_be_left_alone
=== RUN   TestCtestPCRCleaner/Terminal_request_outside_the_threshold_should_be_deleted
=== RUN   TestCtestPCRCleaner/Pending_request_with_zero_MaxExpirationSeconds_within_threshold
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_nil_PKIXPublicKey
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_nil_ProofOfPossession
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_negative_MaxExpirationSeconds
    ctest_pcrcleaner_test.go:314: Bad error output: <nil>
=== RUN   TestCtestPCRCleaner/Pending_request_with_huge_MaxExpirationSeconds_within_threshold
=== RUN   TestCtestPCRCleaner/Pending_request_without_ServiceAccountUID_within_threshold
--- FAIL: TestCtestPCRCleaner (0.05s)
    --- PASS: TestCtestPCRCleaner/Pending_request_within_the_threshold_should_be_left_alone (0.05s)
    --- PASS: TestCtestPCRCleaner/Pending_request_outside_the_threshold_should_be_deleted (0.00s)
    --- PASS: TestCtestPCRCleaner/Terminal_request_within_the_threshold_should_be_left_alone (0.00s)
    --- PASS: TestCtestPCRCleaner/Terminal_request_outside_the_threshold_should_be_deleted (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_zero_MaxExpirationSeconds_within_threshold (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_nil_PKIXPublicKey (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_nil_ProofOfPossession (0.00s)
    --- FAIL: TestCtestPCRCleaner/Pending_request_with_negative_MaxExpirationSeconds (0.00s)
    --- PASS: TestCtestPCRCleaner/Pending_request_with_huge_MaxExpirationSeconds_within_threshold (0.00s)
    --- PASS: TestCtestPCRCleaner/Pending_request_without_ServiceAccountUID_within_threshold (0.00s)
FAIL
coverage: 54.3% of statements
FAIL	k8s.io/kubernetes/pkg/controller/certificates/cleaner	1.230s
=== RUN   TestCtestSyncCounter

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/clustertrustbundlepublisher/ctest_metrics_test.go:112]: Number of test cases: 7
Running #0 test case: nil error
=== RUN   TestCtestSyncCounter/nil_error
Running #1 test case: kube api error
=== RUN   TestCtestSyncCounter/kube_api_error
Running #2 test case: nested kube api error
=== RUN   TestCtestSyncCounter/nested_kube_api_error
Running #3 test case: kube api error without code
=== RUN   TestCtestSyncCounter/kube_api_error_without_code
Running #4 test case: general error
=== RUN   TestCtestSyncCounter/general_error
Running #5 test case: custom wrapped error without kube code
=== RUN   TestCtestSyncCounter/custom_wrapped_error_without_kube_code
Running #6 test case: nil error (duplicate edge)
=== RUN   TestCtestSyncCounter/nil_error_(duplicate_edge)

==================== CTEST END ======================
--- PASS: TestCtestSyncCounter (0.00s)
    --- PASS: TestCtestSyncCounter/nil_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_error (0.00s)
    --- PASS: TestCtestSyncCounter/nested_kube_api_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_error_without_code (0.00s)
    --- PASS: TestCtestSyncCounter/general_error (0.00s)
    --- PASS: TestCtestSyncCounter/custom_wrapped_error_without_kube_code (0.00s)
    --- PASS: TestCtestSyncCounter/nil_error_(duplicate_edge) (0.00s)
PASS
coverage: 12.5% of statements
ok  	k8s.io/kubernetes/pkg/controller/certificates/clustertrustbundlepublisher	2.090s	coverage: 12.5% of statements
=== RUN   TestCtestSyncCounter

==================== CTEST START ====================
Running 0 th test case: nil error
=== RUN   TestCtestSyncCounter/nil_error
Running 1 th test case: kube api notfound error
=== RUN   TestCtestSyncCounter/kube_api_notfound_error
Running 2 th test case: kube api generic status error without code
=== RUN   TestCtestSyncCounter/kube_api_generic_status_error_without_code
Running 3 th test case: general error
=== RUN   TestCtestSyncCounter/general_error
Running 4 th test case: empty error string
=== RUN   TestCtestSyncCounter/empty_error_string
Running 5 th test case: api conflict error (code 409)
=== RUN   TestCtestSyncCounter/api_conflict_error_(code_409)
Running 6 th test case: api error with empty resource
=== RUN   TestCtestSyncCounter/api_error_with_empty_resource
Running 7 th test case: nil pointer error (treated as nil)
=== RUN   TestCtestSyncCounter/nil_pointer_error_(treated_as_nil)

==================== CTEST END ======================
--- PASS: TestCtestSyncCounter (0.00s)
    --- PASS: TestCtestSyncCounter/nil_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_notfound_error (0.00s)
    --- PASS: TestCtestSyncCounter/kube_api_generic_status_error_without_code (0.00s)
    --- PASS: TestCtestSyncCounter/general_error (0.00s)
    --- PASS: TestCtestSyncCounter/empty_error_string (0.00s)
    --- PASS: TestCtestSyncCounter/api_conflict_error_(code_409) (0.00s)
    --- PASS: TestCtestSyncCounter/api_error_with_empty_resource (0.00s)
    --- PASS: TestCtestSyncCounter/nil_pointer_error_(treated_as_nil) (0.00s)
=== RUN   TestCtestConfigMapCreation

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/certificates/rootcacertpublisher/ctest_publisher_test.go:44]: get default configs: {test_fixture.json [default-ca-configmap] data [configmaps] map[ca.crt:placeholder-root-ca]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:42:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:36 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/controller/certificates/rootcacertpublisher	0.647s
FAIL	k8s.io/kubernetes/pkg/controller/certificates/signer [build failed]
	k8s.io/kubernetes/pkg/controller/certificates/signer/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/certificates/signer/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/clusterroleaggregation	1.521s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/cronjob	2.364s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/cronjob/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/cronjob/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/cronjob/metrics		coverage: 0.0% of statements
=== RUN   TestCtestStoreDaemonSetStatus
=== RUN   TestCtestStoreDaemonSetStatus/succeed_immediately
=== RUN   TestCtestStoreDaemonSetStatus/succeed_after_one_update_failure
=== RUN   TestCtestStoreDaemonSetStatus/fail_after_two_update_failures
=== RUN   TestCtestStoreDaemonSetStatus/fail_after_one_update_failure_and_one_get_failure
=== RUN   TestCtestStoreDaemonSetStatus/fail_when_get_always_errors_(edge_case)
    ctest_daemon_controller_test.go:129: storeDaemonSetStatus() got <nil>, expected fake get error
    ctest_daemon_controller_test.go:132: Get() was called 0 times, expected 3 times
    ctest_daemon_controller_test.go:135: UpdateStatus() was called 1 times, expected 0 times
=== RUN   TestCtestStoreDaemonSetStatus/fail_when_update_errors_exceed_max_retries_(edge_case)
    ctest_daemon_controller_test.go:132: Get() was called 1 times, expected 2 times
    ctest_daemon_controller_test.go:135: UpdateStatus() was called 2 times, expected 3 times
--- FAIL: TestCtestStoreDaemonSetStatus (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/succeed_immediately (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/succeed_after_one_update_failure (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/fail_after_two_update_failures (0.00s)
    --- PASS: TestCtestStoreDaemonSetStatus/fail_after_one_update_failure_and_one_get_failure (0.00s)
    --- FAIL: TestCtestStoreDaemonSetStatus/fail_when_get_always_errors_(edge_case) (0.00s)
    --- FAIL: TestCtestStoreDaemonSetStatus/fail_when_update_errors_exceed_max_retries_(edge_case) (0.00s)
FAIL
coverage: 2.2% of statements
FAIL	k8s.io/kubernetes/pkg/controller/daemon	0.378s
	k8s.io/kubernetes/pkg/controller/daemon/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/daemon/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/daemon/util	0.608s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestRequeueStuckDeployment

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestRequeueStuckDeployment/nil_progressDeadlineSeconds_specified
Running test case 0: nil progressDeadlineSeconds specified
=== RUN   TestCtestRequeueStuckDeployment/infinite_progressDeadlineSeconds_specified
Running test case 1: infinite progressDeadlineSeconds specified
=== RUN   TestCtestRequeueStuckDeployment/no_progressing_condition_found
Running test case 2: no progressing condition found
=== RUN   TestCtestRequeueStuckDeployment/complete_deployment_does_not_need_to_be_requeued
Running test case 3: complete deployment does not need to be requeued
=== RUN   TestCtestRequeueStuckDeployment/already_failed_deployment_does_not_need_to_be_requeued
Running test case 4: already failed deployment does not need to be requeued
=== RUN   TestCtestRequeueStuckDeployment/stuck_deployment_-_30s
Running test case 5: stuck deployment - 30s
    progress.go:195: I0209 18:42:44.076330] Queueing up deployment for a progress check deployment="progress-test" queueAfter=30
=== RUN   TestCtestRequeueStuckDeployment/stuck_deployment_-_1s
Running test case 6: stuck deployment - 1s
    progress.go:195: I0209 18:42:44.077157] Queueing up deployment for a progress check deployment="progress-test" queueAfter=1
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_less_than_a_second_=>_now
Running test case 7: failed deployment - less than a second => now
    progress.go:191: I0209 18:42:44.077232] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_now
Running test case 8: failed deployment - now
    progress.go:191: I0209 18:42:44.077343] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_1s_after_deadline
Running test case 9: failed deployment - 1s after deadline
    progress.go:191: I0209 18:42:44.077389] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/failed_deployment_-_60s_after_deadline
Running test case 10: failed deployment - 60s after deadline
    progress.go:191: I0209 18:42:44.077445] Queueing up deployment for a progress check now deployment="progress-test"
=== RUN   TestCtestRequeueStuckDeployment/edge:_zero_progressDeadlineSeconds_with_stuck_condition
Running test case 11: edge: zero progressDeadlineSeconds with stuck condition
    progress.go:191: I0209 18:42:44.077540] Queueing up deployment for a progress check now deployment="progress-test"

==================== CTEST END ======================
--- PASS: TestCtestRequeueStuckDeployment (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/nil_progressDeadlineSeconds_specified (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/infinite_progressDeadlineSeconds_specified (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/no_progressing_condition_found (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/complete_deployment_does_not_need_to_be_requeued (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/already_failed_deployment_does_not_need_to_be_requeued (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/stuck_deployment_-_30s (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/stuck_deployment_-_1s (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_less_than_a_second_=>_now (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_now (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_1s_after_deadline (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/failed_deployment_-_60s_after_deadline (0.00s)
    --- PASS: TestCtestRequeueStuckDeployment/edge:_zero_progressDeadlineSeconds_with_stuck_condition (0.00s)
=== RUN   TestCtestSyncRolloutStatus

==================== CTEST OVERRIDE ONLY START ====================
=== RUN   TestCtestSyncRolloutStatus/General:_remove_Progressing_condition_and_do_not_estimate_progress_if_deployment_has_no_Progress_Deadline
Running test case 0: General: remove Progressing condition and do not estimate progress if deployment has no Progress Deadline
=== RUN   TestCtestSyncRolloutStatus/General:_do_not_estimate_progress_of_deployment_with_only_one_active_ReplicaSet
Running test case 1: General: do not estimate progress of deployment with only one active ReplicaSet
=== RUN   TestCtestSyncRolloutStatus/DeploymentProgressing:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True
Running test case 2: DeploymentProgressing: dont update lastTransitionTime if deployment already has Progressing=True
=== RUN   TestCtestSyncRolloutStatus/DeploymentProgressing:_update_everything_if_deployment_has_Progressing=False
Running test case 3: DeploymentProgressing: update everything if deployment has Progressing=False
=== RUN   TestCtestSyncRolloutStatus/DeploymentProgressing:_create_Progressing_condition_if_it_does_not_exist
Running test case 4: DeploymentProgressing: create Progressing condition if it does not exist
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True
Running test case 5: DeploymentComplete: dont update lastTransitionTime if deployment already has Progressing=True
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_update_everything_if_deployment_has_Progressing=False
Running test case 6: DeploymentComplete: update everything if deployment has Progressing=False
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_create_Progressing_condition_if_it_does_not_exist
Running test case 7: DeploymentComplete: create Progressing condition if it does not exist
=== RUN   TestCtestSyncRolloutStatus/DeploymentComplete:_defend_against_NPE_when_newRS=nil
Running test case 8: DeploymentComplete: defend against NPE when newRS=nil
=== RUN   TestCtestSyncRolloutStatus/DeploymentTimedOut:_update_status_if_rollout_exceeds_Progress_Deadline
Running test case 9: DeploymentTimedOut: update status if rollout exceeds Progress Deadline
    deployment_util.go:812: I0209 18:42:44.077998] Deployment timed out from last progress check deployment="progress-test" timeout=true from="2017-02-15 18:49:00 +0000 UTC" now="2026-02-09 18:42:44.077957 -0600 CST m=+0.054143126"
=== RUN   TestCtestSyncRolloutStatus/DeploymentTimedOut:_do_not_update_status_if_deployment_has_existing_timedOut_condition
Running test case 10: DeploymentTimedOut: do not update status if deployment has existing timedOut condition
=== RUN   TestCtestSyncRolloutStatus/Edge:_nil_ProgressDeadlineSeconds_and_newRS=nil
Running test case 11: Edge: nil ProgressDeadlineSeconds and newRS=nil

==================== CTEST END ======================
--- PASS: TestCtestSyncRolloutStatus (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/General:_remove_Progressing_condition_and_do_not_estimate_progress_if_deployment_has_no_Progress_Deadline (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/General:_do_not_estimate_progress_of_deployment_with_only_one_active_ReplicaSet (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentProgressing:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentProgressing:_update_everything_if_deployment_has_Progressing=False (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentProgressing:_create_Progressing_condition_if_it_does_not_exist (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_dont_update_lastTransitionTime_if_deployment_already_has_Progressing=True (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_update_everything_if_deployment_has_Progressing=False (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_create_Progressing_condition_if_it_does_not_exist (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentComplete:_defend_against_NPE_when_newRS=nil (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentTimedOut:_update_status_if_rollout_exceeds_Progress_Deadline (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/DeploymentTimedOut:_do_not_update_status_if_deployment_has_existing_timedOut_condition (0.00s)
    --- PASS: TestCtestSyncRolloutStatus/Edge:_nil_ProgressDeadlineSeconds_and_newRS=nil (0.00s)
=== RUN   TestCtestScaleDownOldReplicaSets

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/deployment/ctest_recreate_test.go:31]: get default configs: {test_fixture.json [scale down old replica sets deployment] spec [deployments replicasets] &Deployment{ObjectMeta:{foo      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:DeploymentSpec{Replicas:*3,Selector:&v1.LabelSelector{MatchLabels:map[string]string{foo: bar,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[foo:bar] map[] [] [] []} {[] [] [{default-container busybox [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}},Strategy:DeploymentStrategy{Type:,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:nil,Paused:false,ProgressDeadlineSeconds:nil,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,TerminatingReplicas:nil,},}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[deployments replicasets]
[DEBUG-CTEST 2026-02-09 18:42:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[deployments replicasets], int=2)[DEBUG-CTEST 2026-02-09 18:42:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [replicasets]
[DEBUG-CTEST 2026-02-09 18:42:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:44 load all fixtures failed: requested fixture keys not found in test_fixtures.json: replicasets
FAIL	k8s.io/kubernetes/pkg/controller/deployment	0.791s
	k8s.io/kubernetes/pkg/controller/deployment/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/deployment/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/deployment/util	1.111s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/devicetainteviction	1.319s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/devicetainteviction/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/disruption	1.834s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestStaleEndpointsTracker

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:42:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/endpoint/ctest_endpoints_tracker_test.go:27]: get default configs: {test_fixture.json [default endpoints subset] subsets [endpoints] [{[{6.7.8.9  0x109709e30 nil}] [] [{ 1000  <nil>}]}]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints]
[DEBUG-CTEST 2026-02-09 18:42:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints], int=1)[DEBUG-CTEST 2026-02-09 18:42:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints]
[DEBUG-CTEST 2026-02-09 18:42:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:48 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints
FAIL	k8s.io/kubernetes/pkg/controller/endpoint	0.673s
	k8s.io/kubernetes/pkg/controller/endpoint/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/endpoint/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/endpointslice	0.939s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/endpointslice/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/endpointslice/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestShouldMirror

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:42:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints services endpointSlices pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:42:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints services endpointSlices pods deployments statefulsets daemonsets replicasets], int=8)[DEBUG-CTEST 2026-02-09 18:42:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:42:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints endpointSlices statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:42:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:42:51 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints, endpointSlices, statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/controller/endpointslicemirroring	0.839s
	k8s.io/kubernetes/pkg/controller/endpointslicemirroring/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/endpointslicemirroring/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/endpointslicemirroring/metrics	0.249s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/garbagecollector	1.001s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/garbagecollector/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/garbagecollector/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/garbagecollector/metaonly		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/garbagecollector/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/history	0.461s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestNewBackoffRecord
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_with_one_success
=== RUN   TestCtestNewBackoffRecord/Empty_store_with_zero_(epoch)_failure_time
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_and_one_new_failure
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures,_one_success_and_two_more_failures
=== RUN   TestCtestNewBackoffRecord/Nil_store_initializer_(should_behave_like_empty)
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_and_two_new_failures
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures_followed_by_success
=== RUN   TestCtestNewBackoffRecord/Backoff_store_having_failure_count_2_and_one_new_failure
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_with_success_and_failure_at_same_timestamp
=== RUN   TestCtestNewBackoffRecord/Empty_backoff_store_with_no_success/failure
--- PASS: TestCtestNewBackoffRecord (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_with_one_success (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_store_with_zero_(epoch)_failure_time (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_and_one_new_failure (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures,_one_success_and_two_more_failures (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Nil_store_initializer_(should_behave_like_empty) (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_and_two_new_failures (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store,_two_failures_followed_by_success (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Backoff_store_having_failure_count_2_and_one_new_failure (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_with_success_and_failure_at_same_timestamp (0.00s)
    --- PASS: TestCtestNewBackoffRecord/Empty_backoff_store_with_no_success/failure (0.00s)
=== RUN   TestCtestGetFinishedTime
=== RUN   TestCtestGetFinishedTime/Pod_with_terminated_container_but_nil_FinishedAt
=== RUN   TestCtestGetFinishedTime/Pod_with_multiple_containers_and_all_containers_terminated
=== RUN   TestCtestGetFinishedTime/fallback_to_deletionTimestamp
=== RUN   TestCtestGetFinishedTime/Pod_with_sidecar_container_and_all_containers_terminated
=== RUN   TestCtestGetFinishedTime/Pod_with_multiple_containers;_two_containers_in_terminated_state_and_one_in_running_state;_fallback_to_deletionTimestamp
=== RUN   TestCtestGetFinishedTime/fallback_to_deletionTimestamp,_decremented_by_grace_period
=== RUN   TestCtestGetFinishedTime/fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_False
=== RUN   TestCtestGetFinishedTime/skip_fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_True
=== RUN   TestCtestGetFinishedTime/fallback_to_creationTimestamp
=== RUN   TestCtestGetFinishedTime/Pod_with_no_container_statuses_and_no_timestamps
=== RUN   TestCtestGetFinishedTime/Pod_with_nil_DeletionTimestamp_and_zero_CreationTimestamp
--- PASS: TestCtestGetFinishedTime (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_terminated_container_but_nil_FinishedAt (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_multiple_containers_and_all_containers_terminated (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_deletionTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_sidecar_container_and_all_containers_terminated (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_multiple_containers;_two_containers_in_terminated_state_and_one_in_running_state;_fallback_to_deletionTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_deletionTimestamp,_decremented_by_grace_period (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_False (0.00s)
    --- PASS: TestCtestGetFinishedTime/skip_fallback_to_PodReady.LastTransitionTime_when_status_of_the_condition_is_True (0.00s)
    --- PASS: TestCtestGetFinishedTime/fallback_to_creationTimestamp (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_no_container_statuses_and_no_timestamps (0.00s)
    --- PASS: TestCtestGetFinishedTime/Pod_with_nil_DeletionTimestamp_and_zero_CreationTimestamp (0.00s)
=== RUN   TestCtestGetRemainingBackoffTime
=== RUN   TestCtestGetRemainingBackoffTime/nil_lastFailureTime_should_behave_like_no_failures
--- FAIL: TestCtestGetRemainingBackoffTime (0.00s)
    --- FAIL: TestCtestGetRemainingBackoffTime/nil_lastFailureTime_should_behave_like_no_failures (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x102733334]

goroutine 108 [running]:
testing.tRunner.func1.2({0x1030bdde0, 0x10547a6c0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1030bdde0?, 0x10547a6c0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/controller/job.getRemainingTimeForFailuresCount({0x10365f240?, 0x1400036eb90?}, 0x0?, 0x1054e2240?, 0xa1f08?, 0x100692180?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/job/backoff_utils.go:272 +0x1f4
k8s.io/kubernetes/pkg/controller/job.backoffRecord.getRemainingTime(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/job/backoff_utils.go:240
k8s.io/kubernetes/pkg/controller/job.TestCtestGetRemainingBackoffTime.func1(0x14000103340)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/job/ctest_backoff_utils_test.go:585 +0x130
testing.tRunner(0x14000103340, 0x1400002d6e0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 107
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controller/job	1.358s
	k8s.io/kubernetes/pkg/controller/job/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/job/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/job/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/job/util	1.341s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/namespace		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/namespace/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/namespace/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestUpdateConditions
[DEBUG-CTEST 2026-02-09 18:43:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:21]: Starting TestCtestUpdateConditions
[DEBUG-CTEST 2026-02-09 18:43:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:221]: Total test cases: 10
Running 0 th test case: leave unknown
=== RUN   TestCtestUpdateConditions/leave_unknown
Running 1 th test case: replace with success
=== RUN   TestCtestUpdateConditions/replace_with_success
Running 2 th test case: leave different order
=== RUN   TestCtestUpdateConditions/leave_different_order
Running 3 th test case: overwrite with failure
=== RUN   TestCtestUpdateConditions/overwrite_with_failure
Running 4 th test case: write new failure
=== RUN   TestCtestUpdateConditions/write_new_failure
Running 5 th test case: empty starting status and no new conditions
=== RUN   TestCtestUpdateConditions/empty_starting_status_and_no_new_conditions
Running 6 th test case: starting status nil (should not panic)
=== RUN   TestCtestUpdateConditions/starting_status_nil_(should_not_panic)
    ctest_status_condition_utils_test.go:230: panic occurred with nil startingStatus: runtime error: invalid memory address or nil pointer dereference
Running 7 th test case: new condition with unknown type and true status
=== RUN   TestCtestUpdateConditions/new_condition_with_unknown_type_and_true_status
    ctest_status_condition_utils_test.go:242: expected 6 conditions, got 5: [{NamespaceDeletionDiscoveryFailure False 2026-02-09 18:43:02.297177 -0600 CST m=+0.027432251 ResourcesDiscovered All resources successfully discovered} {NamespaceDeletionGroupVersionParsingFailure False 2026-02-09 18:43:02.297177 -0600 CST m=+0.027432418 ParsedGroupVersions All legacy kube types successfully parsed} {NamespaceDeletionContentFailure False 2026-02-09 18:43:02.297177 -0600 CST m=+0.027432543 ContentDeleted All content successfully deleted, may be waiting on finalization} {NamespaceContentRemaining False 2026-02-09 18:43:02.297177 -0600 CST m=+0.027432710 ContentRemoved All content successfully removed} {NamespaceFinalizersRemaining False 2026-02-09 18:43:02.297178 -0600 CST m=+0.027432835 ContentHasNoFinalizers All content-preserving finalizers finished}]
Running 8 th test case: duplicate condition types with differing statuses
=== RUN   TestCtestUpdateConditions/duplicate_condition_types_with_differing_statuses
    ctest_status_condition_utils_test.go:249: condition mismatch at index 0: expected v1.NamespaceCondition{Type:"NamespaceDeletionDiscoveryFailure", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}, got v1.NamespaceCondition{Type:"NamespaceDeletionDiscoveryFailure", Status:"False", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}
Running 9 th test case: very large number of existing unknown conditions
=== RUN   TestCtestUpdateConditions/very_large_number_of_existing_unknown_conditions
--- FAIL: TestCtestUpdateConditions (0.00s)
    --- PASS: TestCtestUpdateConditions/leave_unknown (0.00s)
    --- PASS: TestCtestUpdateConditions/replace_with_success (0.00s)
    --- PASS: TestCtestUpdateConditions/leave_different_order (0.00s)
    --- PASS: TestCtestUpdateConditions/overwrite_with_failure (0.00s)
    --- PASS: TestCtestUpdateConditions/write_new_failure (0.00s)
    --- PASS: TestCtestUpdateConditions/empty_starting_status_and_no_new_conditions (0.00s)
    --- FAIL: TestCtestUpdateConditions/starting_status_nil_(should_not_panic) (0.00s)
    --- FAIL: TestCtestUpdateConditions/new_condition_with_unknown_type_and_true_status (0.00s)
    --- FAIL: TestCtestUpdateConditions/duplicate_condition_types_with_differing_statuses (0.00s)
    --- PASS: TestCtestUpdateConditions/very_large_number_of_existing_unknown_conditions (0.00s)
=== RUN   TestCtestProcessContentTotals
[DEBUG-CTEST 2026-02-09 18:43:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:260]: Starting TestCtestProcessContentTotals
[DEBUG-CTEST 2026-02-09 18:43:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/namespace/deletion/ctest_status_condition_utils_test.go:381]: Total ProcessContentTotals test cases: 8
Running 0 th ProcessContentTotals test case: nothing
=== RUN   TestCtestProcessContentTotals/nothing
Running 1 th ProcessContentTotals test case: just remaining
=== RUN   TestCtestProcessContentTotals/just_remaining
Running 2 th ProcessContentTotals test case: just finalizers
=== RUN   TestCtestProcessContentTotals/just_finalizers
Running 3 th ProcessContentTotals test case: both
=== RUN   TestCtestProcessContentTotals/both
Running 4 th ProcessContentTotals test case: nil maps (should behave like empty)
=== RUN   TestCtestProcessContentTotals/nil_maps_(should_behave_like_empty)
Running 5 th ProcessContentTotals test case: very large remaining counts
=== RUN   TestCtestProcessContentTotals/very_large_remaining_counts
Running 6 th ProcessContentTotals test case: empty GroupVersionResource key
=== RUN   TestCtestProcessContentTotals/empty_GroupVersionResource_key
Running 7 th ProcessContentTotals test case: negative counts (invalid, should be ignored)
=== RUN   TestCtestProcessContentTotals/negative_counts_(invalid,_should_be_ignored)
    ctest_status_condition_utils_test.go:396: expected 0 conditions, got 2
--- FAIL: TestCtestProcessContentTotals (0.00s)
    --- PASS: TestCtestProcessContentTotals/nothing (0.00s)
    --- PASS: TestCtestProcessContentTotals/just_remaining (0.00s)
    --- PASS: TestCtestProcessContentTotals/just_finalizers (0.00s)
    --- PASS: TestCtestProcessContentTotals/both (0.00s)
    --- PASS: TestCtestProcessContentTotals/nil_maps_(should_behave_like_empty) (0.00s)
    --- PASS: TestCtestProcessContentTotals/very_large_remaining_counts (0.00s)
    --- PASS: TestCtestProcessContentTotals/empty_GroupVersionResource_key (0.00s)
    --- FAIL: TestCtestProcessContentTotals/negative_counts_(invalid,_should_be_ignored) (0.00s)
FAIL
coverage: 11.4% of statements
FAIL	k8s.io/kubernetes/pkg/controller/namespace/deletion	0.584s
	k8s.io/kubernetes/pkg/controller/nodeipam		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/nodeipam/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/nodeipam/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestOccupyServiceCIDR
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/nodeipam/ipam/ctest_controller_test.go:17]: Running TestCtestOccupyServiceCIDR with extended test cases
Running test case #0: serviceCIDR=10.0.255.0/24
Running test case #1: serviceCIDR=10.1.0.0/24
Running test case #2: serviceCIDR=10.1.255.0/24
Running test case #3: serviceCIDR=10.2.0.0/24
Running test case #4: serviceCIDR=10.1.0.0/32
Running test case #5: serviceCIDR=10.1.0.0/16
Running test case #6: serviceCIDR=10.0.0.0/24
Running test case #7: serviceCIDR=10.1.255.255/32
--- PASS: TestCtestOccupyServiceCIDR (0.00s)
=== RUN   TestCtestTimeout

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/nodeipam/ipam/ctest_timeout_test.go:23]: get default configs: {test_fixture.json [default timeout config] Resync [] {10s 5s 1s 0 0}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "Resync" in requested fixtures
2026/02/09 18:43:03 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:03 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"InitialRetry":1000000000,"MaxBackoff":5000000000,"Resync":10000000000})[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:43:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/nodeipam/ipam/ctest_timeout_test.go:33]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestTimeout (0.00s)
PASS
coverage: 1.4% of statements
ok  	k8s.io/kubernetes/pkg/controller/nodeipam/ipam	0.856s	coverage: 1.4% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/nodeipam/ipam/cidrset	0.232s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/nodeipam/ipam/sync	1.177s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/nodeipam/ipam/test		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 1.6% of statements
ok  	k8s.io/kubernetes/pkg/controller/nodelifecycle	0.528s	coverage: 1.6% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/nodelifecycle/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/nodelifecycle/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler	0.989s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/podautoscaler	1.331s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/podautoscaler/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/podautoscaler/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/podautoscaler/metrics	0.812s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/podautoscaler/monitor		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/podgc	0.689s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/podgc/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/podgc/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/podgc/metrics		coverage: 0.0% of statements
=== RUN   TestCtestCalculateStatus

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replicaset/ctest_replica_set_utils_test.go:39]: get default configs: [{test_fixture.json [default minReadySeconds] minReadySeconds [replicasets deployments statefulsets daemonsets] 3600}]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[replicasets deployments statefulsets daemonsets]
[DEBUG-CTEST 2026-02-09 18:43:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[replicasets deployments statefulsets daemonsets], int=4)[DEBUG-CTEST 2026-02-09 18:43:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [replicasets statefulsets daemonsets]
[DEBUG-CTEST 2026-02-09 18:43:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:43:13 load all fixtures failed: requested fixture keys not found in test_fixtures.json: replicasets, statefulsets, daemonsets
FAIL	k8s.io/kubernetes/pkg/controller/replicaset	0.857s
	k8s.io/kubernetes/pkg/controller/replicaset/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replicaset/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replicaset/metrics		coverage: 0.0% of statements
=== RUN   TestCtestGetCondition

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:25]: get default configs: {test_fixture.json [replicationcontroller get condition status] conditions [] {0 0 0 0 0 [{ReplicaFailure True 0001-01-01 00:00:00 +0000 UTC OtherFailure }]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "conditions" in requested fixtures
2026/02/09 18:43:17 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:17 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"conditions":[{"lastTransitionTime":null,"reason":"OtherFailure","status":"True","type":"ReplicaFailure"}],"replicas":0})[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:33]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestGetCondition (0.01s)
=== RUN   TestCtestSetCondition

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:99]: get default configs: {test_fixture.json [replicationcontroller set condition status] conditions [] {0 0 0 0 0 []}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "conditions" in requested fixtures
2026/02/09 18:43:17 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:17 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"replicas":0})[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:107]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestSetCondition (0.00s)
=== RUN   TestCtestRemoveCondition

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:191]: get default configs: {test_fixture.json [replicationcontroller remove condition status] conditions [] {0 0 0 0 0 [{ReplicaFailure True 0001-01-01 00:00:00 +0000 UTC OtherFailure }]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "conditions" in requested fixtures
2026/02/09 18:43:17 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:17 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"conditions":[{"lastTransitionTime":null,"reason":"OtherFailure","status":"True","type":"ReplicaFailure"}],"replicas":0})[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:43:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/replication/ctest_replication_controller_utils_test.go:199]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestRemoveCondition (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/replication	1.539s	coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replication/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/replication/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/resourceclaim	1.050s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/resourceclaim/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/resourcequota	0.785s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/resourcequota/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/resourcequota/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/serviceaccount	0.440s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/serviceaccount/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/serviceaccount/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/servicecidrs	0.539s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestStatefulSetCompatibility

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[statefulsets]
[DEBUG-CTEST 2026-02-09 18:43:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[statefulsets], int=1)[DEBUG-CTEST 2026-02-09 18:43:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets]
[DEBUG-CTEST 2026-02-09 18:43:26 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:43:26 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets
FAIL	k8s.io/kubernetes/pkg/controller/statefulset	0.638s
	k8s.io/kubernetes/pkg/controller/statefulset/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/statefulset/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/storageversiongc	0.676s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/storageversionmigrator		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/tainteviction	1.100s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/tainteviction/metrics		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/testutil		coverage: 0.0% of statements
=== RUN   TestCtestPatchNode

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttl/ctest_ttl_controller_test.go:33]: matched config: {test_fixture.json [default node config patch] annotations [nodes] &Node{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{},Allocatable:ResourceList{},Phase:,Conditions:[]NodeCondition{},Addresses:[]NodeAddress{},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:0,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:,KubeProxyVersion:,OperatingSystem:,Architecture:,Swap:nil,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,RuntimeHandlers:[]NodeRuntimeHandler{},Features:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:43:33 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/pkg/controller/ttl	1.649s
=== RUN   TestCtestTimeLeft

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttlafterfinished/ctest_ttlafterfinished_controller_test.go:32]: get default configs: {test_fixture.json [default job spec] ttlSecondsAfterFinished [jobs] {<nil> <nil> <nil> nil nil <nil> <nil> <nil> nil <nil> {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}} 0x140003cb6c8 <nil> <nil> <nil> <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[jobs]
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[jobs], int=1)[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "ttlSecondsAfterFinished" in requested fixtures
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttlafterfinished/ctest_ttlafterfinished_controller_test.go:37]: Failed to get matched fixtures: mode-combination failed: no values found for field "ttlSecondsAfterFinished" in requested fixtures
[DEBUG-CTEST 2026-02-09 18:43:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/ttlafterfinished/ctest_ttlafterfinished_controller_test.go:38]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestTimeLeft (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/ttlafterfinished	2.143s	coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/ttlafterfinished/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/ttlafterfinished/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/ttlafterfinished/metrics		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/util/endpointslice		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/util/node		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/util/selectors	1.761s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestTypeChecking

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestTypeChecking/deployment_with_correct_expression
  I0209 18:43:39.114044   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:39.114283   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_with_type_confusion
  I0209 18:43:40.226622   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:40.226646   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/two_expressions_different_type_checking_errors
  I0209 18:43:41.336160   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:41.336224   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/one_expression,_two_warnings
  I0209 18:43:42.445293   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:42.445312   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_with_empty_validations
  I0209 18:43:43.557328   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:43.557358   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_with_nil_validations
  I0209 18:43:44.668181   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:44.668204   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
=== RUN   TestCtestTypeChecking/deployment_without_match_constraints
  I0209 18:43:45.774777   86469 shared_informer.go:349] "Waiting for caches to sync" controller="validatingadmissionpolicy-status"
  I0209 18:43:45.774790   86469 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"

==================== CTEST END ======================
--- PASS: TestCtestTypeChecking (7.77s)
    --- PASS: TestCtestTypeChecking/deployment_with_correct_expression (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_with_type_confusion (1.11s)
    --- PASS: TestCtestTypeChecking/two_expressions_different_type_checking_errors (1.11s)
    --- PASS: TestCtestTypeChecking/one_expression,_two_warnings (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_with_empty_validations (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_with_nil_validations (1.11s)
    --- PASS: TestCtestTypeChecking/deployment_without_match_constraints (1.11s)
PASS
coverage: 79.6% of statements
ok  	k8s.io/kubernetes/pkg/controller/validatingadmissionpolicystatus	8.685s	coverage: 79.6% of statements
	k8s.io/kubernetes/pkg/controller/validatingadmissionpolicystatus/config		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 3.3% of statements
ok  	k8s.io/kubernetes/pkg/controller/validatingadmissionpolicystatus/config/v1alpha1	0.295s	coverage: 3.3% of statements [no tests to run]
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_attached
I0209 18:43:39.503076   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:39.503151   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:39.503156   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:39.503839   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:39.503876   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.503882   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.503910   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.503919   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.503957   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.503971   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504022   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504029   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504087   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504089   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504113   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504133   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504138   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504154   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504174   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.503873   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504219   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504222   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504287   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504301   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:39.504328   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:39.504399] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:39.504490] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:39.504518] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:39.504524] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:39.504531] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:39.504536] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:39.504542] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:39.504549] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:39.504554] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:39.504579] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:39.603978   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:39.604048] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:39.604127] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:39.604147] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:39.604160] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:39.604243] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:39.604275] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:39.604297] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:39.604307] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:39.604314] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:39.604326] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:39.604335] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:39.604341] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:39.604347] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:39.604353] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:39.604359] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:39.604369] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:39.604375] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:39.604380] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:39.604386] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:39.604395] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:39.604404] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:736: I0209 18:43:39.604428] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 attachState="Detached"
    attach_detach_controller.go:424: I0209 18:43:39.604438] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0209 18:43:39.604588] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:39.605222] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:39.605246] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:39.605275] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:39.605291] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:39.605374   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0209 18:43:39.605375] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:39.605409] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:39.605416   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    reconciler.go:279: I0209 18:43:39.605420] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:39.605431] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:39.605516] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:39.605524] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:39.605536] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:39.605546] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:39.605595] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
I0209 18:43:39.605605   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    reconciler.go:271: I0209 18:43:39.605605] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:39.605610   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:43:39.605619   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    reconciler.go:279: I0209 18:43:39.605617] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:39.605622   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:43:39.605627   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    actual_state_of_world.go:434: I0209 18:43:39.605625] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:39.605630   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    node_status_updater.go:129: I0209 18:43:39.605689] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0209 18:43:39.605697] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:39.605706] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:39.605714] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:39.605720] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0209 18:43:39.605729] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0209 18:43:39.605735   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    reconciler.go:241: I0209 18:43:39.605735] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0209 18:43:39.605739   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    reconciler.go:361: I0209 18:43:39.609092] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0209 18:43:39.609158] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3","mynamespace/mypod-4"]
I0209 18:43:39.609446   86473 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0209 18:43:39.609510   86473 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0209 18:43:39.709409] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:39.709505] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:39.809870] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:39.809905] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:39.913685] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:39.913739] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.014032] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.014063] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.115148] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.115176] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.215837] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.215863] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.317080] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.317127] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.417463] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.417511] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0209 18:43:40.505203   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.505228   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.505203   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:40.505242] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:40.505278] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:40.505299] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:40.505306] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:40.505313] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:40.505318] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:40.505324] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:40.505329] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:40.505335] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:40.505340] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:40.518227] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:251: I0209 18:43:40.518269] RemoveVolumeFromReportAsAttached failed while removing volume from node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 err="volume \"kubernetes.io/testPlugin/vol1\" does not exist in volumesToReportAsAttached list or node \"mynode-1\" does not exist in nodesToUpdateStatusFor list"
    reconciler.go:271: I0209 18:43:40.518281] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:279: I0209 18:43:40.518319] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:127: I0209 18:43:40.518330] Starting reconciling attached volumes still attached
E0209 18:43:40.518375   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
I0209 18:43:40.518499   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0209 18:43:40.518508   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0209 18:43:40.604915   86473 watch.go:142] "Stopping fake watcher"
I0209 18:43:40.604960   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.604990   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.605042   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.605049   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.605049   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.605054   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.605061   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_unattached
I0209 18:43:40.606196   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:40.606208   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:40.606230   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:40.606346   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:40.606382   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606384   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606387   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606388   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606422   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606455   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606457   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606468   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606476   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606481   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606485   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606459   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606495   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606500   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606499   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606494   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606531   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606587   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606612   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606625   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:40.606634   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:40.606623] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:40.606670] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:40.606686] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:40.606696] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:40.606704] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:40.606710] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:40.606716] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:40.606721] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:40.606728] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:40.606735] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:40.707410   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:40.707430] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:40.707480] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:40.707509] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:40.707521] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:40.707539] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0209 18:43:40.707550] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:40.707560] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:40.707568] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:40.707574] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:40.707579] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:40.707585] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:40.707600] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:40.707614] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:40.707624] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:40.707631] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:40.707638] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:40.707644] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:40.707650] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:40.707657] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:40.707663] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:40.707672] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0209 18:43:40.707692] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 attachState="Detached"
    attach_detach_controller.go:424: I0209 18:43:40.707703] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0209 18:43:40.707788] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:40.711110] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0209 18:43:40.711127] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:40.711143] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:40.711152] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:40.711163   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0209 18:43:40.711170   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0209 18:43:40.711198] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:40.711207] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:40.711216] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:40.711225   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    actual_state_of_world.go:434: I0209 18:43:40.711224] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:40.711229   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0209 18:43:40.711268] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:40.711276] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:40.711285] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:40.711295   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:43:40.711298   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    actual_state_of_world.go:434: I0209 18:43:40.711296] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:40.711329] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:40.711336] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:40.711350] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:40.711357] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:40.711361   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:43:40.711365   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0209 18:43:40.711389] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:40.711396] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:40.711405] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:40.711414   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    actual_state_of_world.go:434: I0209 18:43:40.711412] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:40.711418   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    reconciler.go:241: I0209 18:43:40.711418] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0209 18:43:40.711425] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.711431] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:361: I0209 18:43:40.713594] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0209 18:43:40.713623] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3","mynamespace/mypod-4"]
I0209 18:43:40.713661   86473 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0209 18:43:40.713673   86473 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0209 18:43:40.814463] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.814490] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:40.915784] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:40.915822] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:41.015955] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:41.016050] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:41.117314] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:41.117409] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:41.218505] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:41.218532] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:41.318831] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:41.318884] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:41.420076] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:41.420179] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:241: I0209 18:43:41.520781] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:41.520821] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
I0209 18:43:41.606664   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.606672   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.606731   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:41.606847] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:41.606953] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.607001] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:41.607032] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.607040] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:41.607048] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.607057] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:41.607105] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:41.607141] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:41.607150] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:41.621333] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:251: I0209 18:43:41.621558] RemoveVolumeFromReportAsAttached failed while removing volume from node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1 err="volume \"kubernetes.io/testPlugin/vol1\" does not exist in volumesToReportAsAttached list or node \"mynode-1\" does not exist in nodesToUpdateStatusFor list"
    reconciler.go:271: I0209 18:43:41.621583] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:279: I0209 18:43:41.621625] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol1
    reconciler.go:127: I0209 18:43:41.621661] Starting reconciling attached volumes still attached
E0209 18:43:41.621744   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
I0209 18:43:41.622023   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0209 18:43:41.622035   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "pv1" (UniqueName: "kubernetes.io/testPlugin/vol1") on node "mynode-1" 
I0209 18:43:41.709282   86473 watch.go:142] "Stopping fake watcher"
I0209 18:43:41.710291   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.710308   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.710324   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.710330   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.710338   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.710344   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.710350   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Scheduled_Pod_with_migrated_PV
I0209 18:43:41.711025   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:41.711047   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:41.711052   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:41.711505   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:41.711611   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711626   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711630   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711650   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711746   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711754   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711755   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711766   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711766   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711844   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711866   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711873   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711876   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711900   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711884   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711976   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.711905   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.712010   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.712057   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.712112   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.712124   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0209 18:43:41.711951] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-0"
    attach_detach_controller.go:669: I0209 18:43:41.712264] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:41.712288] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:41.712304] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:41.712310] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.712323] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:41.712330] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.712351] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:41.712360] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.712368] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:41.712374] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:41.813892   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:41.813909] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:41.813936] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:41.813958] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.813969] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:41.813976] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.813993] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:41.813998] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:41.814005] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:41.814015] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:506: I0209 18:43:41.814024] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:41.814031] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/vol1
    actual_state_of_world.go:400: I0209 18:43:41.814042] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:41.814048] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.814055] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:41.814060] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.814066] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:41.814071] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.814076] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:41.814082] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.814087] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:424: I0209 18:43:41.814106] Populating DesiredStateOfworld
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Deleted_Pod_with_migrated_PV
I0209 18:43:41.814336   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:41.814344   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:41.814347   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:41.814464   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:41.814475   86473 watch.go:142] "Stopping fake watcher"
I0209 18:43:41.814490   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814496   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814505   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814510   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814515   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814521   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814525   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814588   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814592   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814643   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814664   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814667   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814683   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814745   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814748   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814761   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814830   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814845   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814886   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814959   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.814965   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815023   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815068   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815075   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815099   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815143   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815160   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.815200   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0209 18:43:41.815291] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-0"
    util.go:211: I0209 18:43:41.815317] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-1"
    util.go:211: I0209 18:43:41.815331] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-2"
    util.go:211: I0209 18:43:41.815343] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-3"
    util.go:211: I0209 18:43:41.815355] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-4"
    attach_detach_controller.go:669: I0209 18:43:41.815376] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:41.815403] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:41.815418] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:41.815430] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.815443] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:41.815455] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.815468] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:41.815480] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.815491] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:41.815508] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:41.915511   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:41.915534] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:41.915564] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:41.915582] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:41.915598] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:41.915615] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0209 18:43:41.915623] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.915631] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:41.915638] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.915644] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:41.915651] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.915657] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:41.915664] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.915671] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:41.915677] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.915683] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:41.915690] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.915697] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:41.915703] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:41.915708] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:41.915716] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:41.915724] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0209 18:43:41.915743] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/vol1 attachState="Detached"
    attach_detach_controller.go:424: I0209 18:43:41.915751] Populating DesiredStateOfworld
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Empty_volume_name
I0209 18:43:41.915854   86473 watch.go:142] "Stopping fake watcher"
I0209 18:43:41.915892   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.915926   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.915928   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.915955   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.915964   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.915950   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.915975   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.916383   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:41.916394   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:41.916397   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:41.917456   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:41.917487   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917487   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917499   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917508   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917510   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917514   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917523   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917527   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917528   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917546   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917551   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917556   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917559   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917511   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917553   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917491   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:41.917574] processVolumesInUse for node node="mynode"
I0209 18:43:41.917938   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.917577   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.918041   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:41.918079   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:41.918107] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:41.918224] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:41.918268] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.918277] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:41.918285] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.918293] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:41.918300] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:41.918307] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:41.918314] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:41.918530   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:42.017869   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:42.017893] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:42.017934] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:42.017949] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:42.017959] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:42.017970] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0209 18:43:42.017980] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:42.017988] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:42.017995] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:42.018001] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:42.018009] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:42.018016] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:42.018023] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:42.018029] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:42.018035] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:42.018040] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:42.018047] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:42.018053] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:42.018059] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:42.018065] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:42.018072] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:42.018078] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:731: E0209 18:43:42.018095] Failed to find unique name for volume err="failed to GetVolumeName from volumePlugin for volumeSpec \"pv-empty\" err=<nil>" node="mynode-1" vaName="va-empty" PV="pv-empty"
    attach_detach_controller.go:424: I0209 18:43:42.018108] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0209 18:43:42.018187] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:42.018359] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0209 18:43:42.018370] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:42.018397] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:42.018410] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:42.018418   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0209 18:43:42.018425   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0209 18:43:42.018463] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:42.018474] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:42.018485] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:42.018494] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:42.018507   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0209 18:43:42.018511   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    node_status_updater.go:129: I0209 18:43:42.018531] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:42.018540] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:42.018551] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:42.018558] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:42.018589   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    node_status_updater.go:129: I0209 18:43:42.018593] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
I0209 18:43:42.018604   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    reconciler.go:271: I0209 18:43:42.018602] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:42.018609] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:42.018617] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:42.018627   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:43:42.018630   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0209 18:43:42.018653] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:42.018662] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:42.018672] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:42.018679] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.018687] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:42.018716   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0209 18:43:42.018720   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    reconciler.go:361: I0209 18:43:42.019158] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0209 18:43:42.019183] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3","mynamespace/mypod-4"]
I0209 18:43:42.019533   86473 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0209 18:43:42.019541   86473 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0209 18:43:42.119326] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.220414] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.321602] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.422004] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.523149] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.641834] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.745087] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:42.845999] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:42.919519   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:42.919521   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:42.919591   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:42.920512] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:42.921276] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:42.922023] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:42.922053] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:42.922067] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:42.922073] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:42.922079] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:42.922085] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:42.922091] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:42.922097] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:42.947978] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:42.948225] Starting reconciling attached volumes still attached
E0209 18:43:42.949602   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.051447] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.151731] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.252824] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.353929] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.454779] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.555859] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.656616] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.757074] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:43.858176] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:43.919667   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:43.919722   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:43.919711] processVolumesInUse for node node="mynode-3"
I0209 18:43:43.919734   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:43.919747] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:43.919762] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:43.919771] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:43.919779] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:43.919804] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:43.919813] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:43.919831] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:43.919836] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:43.919842] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:43.958666] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:43.958729] Starting reconciling attached volumes still attached
E0209 18:43:43.958749   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.059800] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.160879] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.261958] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.362247] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.463015] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.563591] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.663927] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.765024] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:44.866088] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:44.919699   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:44.919743   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:44.919730] processVolumesInUse for node node="mynode"
I0209 18:43:44.919761   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:44.919759] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:44.919784] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:44.919791] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:44.919801] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:44.919807] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:44.919813] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:44.919818] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:44.919824] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:44.919829] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:44.967196] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:44.967224] Starting reconciling attached volumes still attached
E0209 18:43:44.967234   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.067440] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.168520] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.269614] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.370697] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.471754] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.572877] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.673043] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.773270] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:45.874340] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:45.920385   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:45.920394   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:45.920477] processVolumesInUse for node node="mynode-4"
I0209 18:43:45.920509   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:45.920518] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:45.920535] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:45.920546] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:45.920554] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:45.920559] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:45.920565] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:45.920571] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:45.920585] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:45.920590] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:45.974607] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:45.974639] Starting reconciling attached volumes still attached
E0209 18:43:45.974656   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.075696] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.176767] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.277824] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.378914] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.479992] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.580738] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.681818] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.782141] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:46.883228] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:46.922078   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:46.922395   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:46.922382] processVolumesInUse for node node="mynode-1"
I0209 18:43:46.922414   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:46.922409] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:46.922419] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:46.922435] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:46.922442] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:46.922449] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:46.922454] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:46.922459] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:46.922469] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:46.922481] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0209 18:43:46.983644] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:46.983704] Starting reconciling attached volumes still attached
E0209 18:43:46.983727   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.084413] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.186209] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.287301] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.388402] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.489555] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.589909] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.691014] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.791277] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:47.891749] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:47.923400   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:47.923408   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:47.923345   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:47.923656] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:47.923695] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:47.923710] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:47.923720] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:47.923982] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:47.924015] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:47.924034] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:47.924056] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:47.924080] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:47.924095] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:47.992911] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:47.993101] Starting reconciling attached volumes still attached
E0209 18:43:47.993143   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.094240] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.195349] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.296427] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.397502] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.498569] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.599648] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.700922] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.802061] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:48.902736] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:48.923568   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:48.923570   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:48.923679   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:48.923823] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:48.923861] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:48.923874] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:48.923881] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:48.923907] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:48.923919] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:48.923928] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:48.923936] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:48.923986] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:48.923995] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:49.003860] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:49.003934] Starting reconciling attached volumes still attached
E0209 18:43:49.003956   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.105060] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.205643] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.306721] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.407793] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.508354] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.608639] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.709746] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.810624] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:49.911497] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:49.923652   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:49.923689   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:49.923806   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:49.923837] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:49.923862] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:49.923877] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:49.923884] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:49.923892] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:49.923898] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:49.923906] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:49.923921] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:49.923945] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:49.923952] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:50.012636] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:50.012896] Starting reconciling attached volumes still attached
E0209 18:43:50.012917   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.114102] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.217232] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.317646] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.420462] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.521503] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.625416] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.729529] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:50.830864] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:50.924680   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:50.924926   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:50.924967   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:50.925025] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:50.925086] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:50.925099] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:50.925117] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:50.925125] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:50.925143] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:50.925149] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:50.925154] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:50.925160] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:50.925165] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:50.931743] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.033134] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:51.033201] Starting reconciling attached volumes still attached
E0209 18:43:51.033224   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.133767] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.234913] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.336053] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.437693] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.540702] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.641868] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.743270] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:51.844894] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:43:51.926032   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:51.926046   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:51.926032   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:51.926431] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:51.926469] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:51.926480] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:51.926487] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:51.926504] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:51.926577] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:51.926614] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:51.926657] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:51.926674] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:51.926683] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:51.946092] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    attach_detach_controller_test.go:704: Expected detach operation not found, node:mynode-1, volume: , tries: 10
I0209 18:43:52.031059   86473 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Nil_pod_name_(should_be_ignored)
I0209 18:43:52.031965   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.032055   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.032102   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.032106   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.032161   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.032171   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.032163   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.041067   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:52.041940   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:52.042390   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:52.049568   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:52.053163   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053171   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053265   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053270   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053299   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053309   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053345   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053350   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053369   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053372   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053373   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053376   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053381   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.053384   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.057134   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.057117   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.057399   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.057468   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.057473   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.058137   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:52.058148   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:52.057802] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:52.058400] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:52.058439] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:52.058450] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:52.058470] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:52.058566] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:52.058579] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:52.058586] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:52.058594] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:52.058600] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:52.152594   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:52.152621] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:52.152655] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:52.152669] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:52.152799] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:52.152821] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0209 18:43:52.152832] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:52.153007] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:52.153017] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:52.153026] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:52.153035] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:52.153041] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:52.153050] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:52.153057] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:52.153067] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:52.153073] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:52.153089] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:52.153121] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:52.153157] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:52.153164] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:52.153171] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:52.153178] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0209 18:43:52.153730] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod attachState="Detached"
    attach_detach_controller.go:424: I0209 18:43:52.153746] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0209 18:43:52.154684] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:52.163083] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0209 18:43:52.163107] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:52.163155] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:52.163253] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:52.163330] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:52.163341] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:52.163377] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:52.163425] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:52.163486] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:52.163505] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:52.163522] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:52.163531] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:52.163575] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:52.163583] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:52.163594] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:52.163600] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:52.163641] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:52.163649] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:52.163661] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:52.163668] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.163674] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0209 18:43:52.163681] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.163686] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
I0209 18:43:52.163768   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0209 18:43:52.163764   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:43:52.163784   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0209 18:43:52.163808   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:43:52.163769   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0209 18:43:52.163932   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:43:52.163941   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0209 18:43:52.163940   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:43:52.163949   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0209 18:43:52.163959   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    reconciler.go:361: I0209 18:43:52.168277] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0209 18:43:52.168423] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3","mynamespace/mypod-4"]
I0209 18:43:52.169005   86473 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0209 18:43:52.169117   86473 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0209 18:43:52.269157] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.269229] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.370349] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.370384] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.471497] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.471527] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.572577] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.572599] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.673656] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.673684] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.774784] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.774813] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.875875] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.875906] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:241: I0209 18:43:52.978052] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:52.978146] Cannot detach volume because it is still mounted node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
I0209 18:43:53.059644   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.059715   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.060570   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:53.060667] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:53.060774] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:53.060786] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:53.060798] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:53.060805] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:53.060813] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:53.060819] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:53.060824] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:53.060830] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:53.060836] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:53.080024] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:251: I0209 18:43:53.081021] RemoveVolumeFromReportAsAttached failed while removing volume from node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod err="volume \"kubernetes.io/testPlugin/vol-nil-pod\" does not exist in volumesToReportAsAttached list or node \"mynode-1\" does not exist in nodesToUpdateStatusFor list"
    reconciler.go:271: I0209 18:43:53.081837] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:279: I0209 18:43:53.081921] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-nil-pod
    reconciler.go:127: I0209 18:43:53.081960] Starting reconciling attached volumes still attached
E0209 18:43:53.081998   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
I0209 18:43:53.082191   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "pv-nil-pod" (UniqueName: "kubernetes.io/testPlugin/vol-nil-pod") on node "mynode-1" 
I0209 18:43:53.082199   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "pv-nil-pod" (UniqueName: "kubernetes.io/testPlugin/vol-nil-pod") on node "mynode-1" 
I0209 18:43:53.155536   86473 watch.go:142] "Stopping fake watcher"
I0209 18:43:53.155628   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Invalid_node_name_characters
I0209 18:43:53.155640   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.155663   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.155655   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.155671   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.155678   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.155747   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.155952   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:43:53.155969   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:43:53.155974   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:43:53.156119   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:43:53.156177   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156181   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156183   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156186   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156206   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156217   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156222   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156222   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156230   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156235   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156239   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156236   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156259   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156263   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156276   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156279   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156283   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156287   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156401   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156423   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:53.156404   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    util.go:211: I0209 18:43:53.156453] Skipping processing of pod, it is scheduled to node which is not managed by the controller node="mynode" pod="mynamespace/mypod-0"
    attach_detach_controller.go:669: I0209 18:43:53.157036] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:53.157073] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:53.157092] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:53.157100] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:53.157110] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:53.157118] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:53.157145] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:53.157152] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:53.157159] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:53.157164] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:53.256202   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:43:53.256223] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:43:53.256260] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:43:53.256275] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:43:53.256285] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:43:53.256304] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0209 18:43:53.256314] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:53.256325] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:43:53.256331] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:53.256338] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:53.256346] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:53.256354] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:43:53.256361] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:53.256368] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:53.256374] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:53.256380] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:43:53.256394] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:53.256402] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:43:53.256414] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:43:53.256421] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:43:53.256428] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:43:53.256435] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:736: I0209 18:43:53.256453] Marking volume attachment as uncertain as volume is not attached node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node attachState="Detached"
    attach_detach_controller.go:424: I0209 18:43:53.256463] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0209 18:43:53.256597] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:53.256957] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:53.256975] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:53.256993] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:53.257003] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:53.257037   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:43:53.257046   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0209 18:43:53.257056] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:53.257065] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:53.257076] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:53.257087] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:53.257144   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0209 18:43:53.257151   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
    node_status_updater.go:129: I0209 18:43:53.257191] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0209 18:43:53.257204] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:53.257221] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:53.257230] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:53.257257   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0209 18:43:53.257261   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0209 18:43:53.257272] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:53.257281] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:53.257290] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:43:53.257299] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:43:53.257330] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:43:53.257339] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:43:53.257346] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:43:53.257350   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0209 18:43:53.257353   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    actual_state_of_world.go:434: I0209 18:43:53.257353] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:53.257359] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:434: I0209 18:43:53.257366] Set detach request time to current time for volume on node node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:53.257401   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:43:53.257406   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
    reconciler.go:218: I0209 18:43:53.257897] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.257911] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.257921] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:361: I0209 18:43:53.258710] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-2
                            name: mypod-2
                            namespace: mynamespace
                            uid: mypod-2
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0209 18:43:53.258758] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-1","mynamespace/mypod-2","mynamespace/mypod-3","mynamespace/mypod-4","mynamespace/mypod-0"]
I0209 18:43:53.261990   86473 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0209 18:43:53.262014   86473 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0209 18:43:53.359228] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.359262] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.359270] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.359278] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:53.460367] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.460400] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.460408] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.460416] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:53.561615] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.561683] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.561699] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.561772] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:53.662112] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.662209] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.662236] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.662263] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:53.763541] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.763579] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.763587] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.763594] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:53.864723] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.864769] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.864777] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.864811] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:53.965243] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:53.965270] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:53.965278] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:53.965285] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.066376] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.066485] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.067314] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.067345] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:54.158735   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:54.158856] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:54.158925] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:54.158950] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:54.158965] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:54.158995   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:54.159017] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:54.159054] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:54.159068] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:54.159081] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:54.159095] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:54.159108] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:54.159222   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0209 18:43:54.174072] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.174135] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.174152] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.174168] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0209 18:43:54.174189] Starting reconciling attached volumes still attached
E0209 18:43:54.174219   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.274506] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.274630] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.274662] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.274675] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:54.375169] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.375246] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.375255] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.375278] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.475544] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.475624] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.475639] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.475650] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.576730] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.576775] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.576793] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.576801] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.677219] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.677248] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.677256] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.677263] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:43:54.778355] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.778415] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.778438] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.778454] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:54.879547] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.879580] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.879596] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.879643] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:54.979868] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:54.979924] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:54.979935] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:54.979976] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.081049] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.081107] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.081119] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.081153] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:55.158865   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:55.158950] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:55.158998] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:55.159016] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:55.159029] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:55.159036] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:55.159042] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:55.159053] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:55.159058] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:55.159064] processVolumesInUse for node node="mynode-4"
I0209 18:43:55.159060   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:55.159070] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:55.159251   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:218: I0209 18:43:55.182229] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.182252] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.182264] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.182274] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:55.182285] Starting reconciling attached volumes still attached
E0209 18:43:55.182780   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.284085] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.284127] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.284142] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.284166] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:55.385259] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.385310] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.385320] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.385329] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.487408] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.487442] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.487459] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.487467] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.589019] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.589047] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.589055] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.589069] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.690128] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.690192] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.690246] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.690289] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.792360] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.792395] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.792403] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.792411] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.894458] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.894482] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.894489] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.894508] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:55.996582] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:55.996608] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:55.996616] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:55.996634] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:56.098698] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.098737] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.098744] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.098753] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:56.159004   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:56.159040] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:56.159063] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:56.159074] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:56.159097] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:56.159104] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:56.159110] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:56.159125] processVolumesInUse for node node="mynode-3"
I0209 18:43:56.159133   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:56.159136] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:56.159149] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:56.159155] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:43:56.159269   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0209 18:43:56.199807] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.199860] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.199869] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.199879] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0209 18:43:56.199890] Starting reconciling attached volumes still attached
E0209 18:43:56.199900   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:56.301986] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.302091] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.302150] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.302161] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:56.403575] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.403616] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.403626] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.403651] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:56.504822] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.504850] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.504861] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.504870] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:43:56.606944] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.606971] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.606986] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:56.607000] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:56.709104] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.709169] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.709185] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.709202] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:56.811291] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:56.811323] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.811331] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.811339] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:43:56.913226] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:56.913260] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:56.913274] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:56.913285] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.015340] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.015366] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.015382] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.015392] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:57.117436] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.117468] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.117477] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.117484] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:57.159129   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:57.159167] processVolumesInUse for node node="mynode"
I0209 18:43:57.159186   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:57.159197] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:57.159221] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:57.159234] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:57.159241] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:57.159247] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:57.159255] processVolumesInUse for node node="mynode-3"
I0209 18:43:57.159291   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    actual_state_of_world.go:400: I0209 18:43:57.159315] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:57.159353] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:57.159363] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:218: I0209 18:43:57.219777] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.219802] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.219824] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.219834] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:43:57.219843] Starting reconciling attached volumes still attached
E0209 18:43:57.219872   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:57.321758] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.321814] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.321832] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.321850] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:43:57.423101] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.423136] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.423214] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.423240] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:57.525320] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.525383] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.525399] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.525441] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.637449] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.637480] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.637488] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.637495] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.741850] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.741917] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.741952] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.741970] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.842927] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:57.842999] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.843020] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.843056] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:43:57.943858] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:57.943903] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:57.943928] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:57.943947] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:58.044971] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.045003] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.045011] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.045036] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.145161] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.145188] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.145196] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.145206] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:58.160720   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:58.160724   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:58.160737   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:58.160763] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:58.160788] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:58.160802] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:58.160814] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:58.160823] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:58.160832] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:58.160843] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:58.160853] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:58.160861] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:58.160867] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:58.245674] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.245801] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.245837] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.245857] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0209 18:43:58.245879] Starting reconciling attached volumes still attached
E0209 18:43:58.245901   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:58.346009] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.346089] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.346107] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.346124] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.447165] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.447248] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.447281] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.447318] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.548459] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.548514] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.548531] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.548546] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.649692] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.649722] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.649731] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.649747] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.751469] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.751498] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.751506] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.751512] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.853176] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.853270] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.853296] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.853327] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:58.955841] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:58.956043] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:58.956065] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:58.956086] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.056740] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.056798] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.056825] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.056844] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.158561] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.158668] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.158744] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.158800] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:43:59.161333   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:59.161359   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:43:59.161377   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:43:59.161388] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:43:59.161422] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:43:59.161438] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:43:59.161452] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:59.161466] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:43:59.161481] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:59.161510] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:43:59.161542] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:43:59.161562] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:43:59.161579] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:43:59.259594] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.259668] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.259715] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.259735] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0209 18:43:59.259758] Starting reconciling attached volumes still attached
E0209 18:43:59.259777   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:43:59.361639] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.361724] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.361735] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.361747] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.463195] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.463295] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.463322] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.463343] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.566513] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.566573] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.566589] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.566606] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.667459] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.667491] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.667499] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.667506] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.767956] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.767984] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.767992] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.767999] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.870196] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.870228] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.870236] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.870243] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:43:59.971478] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:43:59.971547] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:43:59.971574] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:43:59.971629] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.071953] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.072020] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.072034] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.072052] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:44:00.164348   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:00.164871] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:00.164905] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:00.164914] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:00.164947] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:00.164958] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:00.164964] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:00.164972] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:00.164980] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:00.164987] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:00.165001] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0209 18:44:00.165361   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:00.166633   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0209 18:44:00.174157] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.174212] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.174245] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.174262] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.279794] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.279832] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.279841] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.280196] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0209 18:44:00.280219] Starting reconciling attached volumes still attached
E0209 18:44:00.280239   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:00.382149] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.382198] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.382207] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.382217] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.490169] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.490201] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.490208] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.490216] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.591075] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.591143] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.591152] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.591161] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.691464] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.691490] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.691498] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.691510] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.792479] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.792510] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.792518] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.792526] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.894088] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.894188] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.894198] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.894219] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:00.996040] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:00.996108] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:00.996117] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:00.996127] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.098204] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.098233] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.098245] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.098253] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:44:01.164866   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:01.164934] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:01.164995] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:01.165012] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:01.165021] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:01.165032] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:01.165041] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:01.165047] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:01.165053] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:01.165072] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:01.165111] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
I0209 18:44:01.165514   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:01.167020   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    reconciler.go:241: I0209 18:44:01.199492] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.199525] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.199534] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.199542] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:44:01.301626] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.301657] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.301681] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.301713] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:01.301729] Starting reconciling attached volumes still attached
E0209 18:44:01.301743   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:01.403873] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.403943] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.404031] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.404052] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.504193] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.504221] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.504229] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.504236] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.606356] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.606411] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.606430] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.606444] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.707626] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.707714] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.707747] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.707760] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.808589] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.808624] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.808639] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.808662] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:01.909890] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:01.909921] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:01.909929] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:01.909936] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.012107] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.012163] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.012171] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.012226] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:44:02.113329] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.113358] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.113371] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.113380] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:02.166935   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:02.167001   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:02.167086   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:02.167254] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:02.167292] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:02.167311] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:02.167318] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:02.167363] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:02.167382] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:02.167406] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:02.167431] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:02.167461] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:02.167500] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:02.215466] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.215505] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.215514] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.215525] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.316932] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.316973] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.316988] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.317060] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:127: I0209 18:44:02.317078] Starting reconciling attached volumes still attached
E0209 18:44:02.317106   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:02.417410] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.417437] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.417448] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.417456] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.519551] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.519580] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.519588] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.519596] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.620289] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.620363] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.620377] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.620387] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:218: I0209 18:44:02.722434] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.722457] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.722467] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.722475] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:02.823372] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.823426] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.823482] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.823491] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:02.931282] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:02.932045] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:02.932621] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:02.932633] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:03.033426] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:03.033494] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:03.033845] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:03.033862] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    reconciler.go:241: I0209 18:44:03.135935] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:03.135965] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:03.135972] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:03.135979] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
I0209 18:44:03.169186   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.169185   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.169233   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:03.169763] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:03.169794] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.169912] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:03.169927] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.169938] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:03.169947] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.169955] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:03.169961] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.169968] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:03.170251] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0209 18:44:03.236969] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:218: I0209 18:44:03.237070] Failed to get health of node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:233: I0209 18:44:03.237129] Failed to get taint specs for node node="node!@#" err="node \"node!@#\" not found"
    reconciler.go:241: I0209 18:44:03.237160] Cannot detach volume because it is still mounted node="node!@#" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/vol-bad-node
    attach_detach_controller_test.go:692: Expected node not found, node:node!@#, op: detach, tries: 10
I0209 18:44:03.277875   86473 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/Zero-length_VA_name
I0209 18:44:03.279114   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.279220   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.279223   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.279239   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.279253   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.279252   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.279252   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.286896   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:44:03.287661   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:44:03.287670   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:44:03.291757   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:44:03.297527   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297534   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297578   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297620   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297718   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297856   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297854   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297884   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297887   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297893   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297897   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297900   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297928   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.297960   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.302842   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.303529   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.303562   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.303641   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.303653   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.303668   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:03.303899   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:03.304540] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:03.304776] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:03.304890] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:03.304913] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.304935] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:03.304952] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.304967] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:03.304982] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:03.304999] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:03.305014] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:44:03.395944   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:44:03.395977] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:44:03.396003] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:44:03.396021] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:03.396030] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:03.396041] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:03.396055] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:44:03.396064] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:44:03.396071] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:44:03.396080] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    actual_state_of_world.go:358: I0209 18:44:03.396087] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-1" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:03.396097] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:44:03.396104] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:03.396110] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:03.396125] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:03.396131] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:44:03.396141] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:03.396150] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:03.396157] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:03.396172] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:44:03.396179] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:03.396185] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:424: I0209 18:44:03.396191] Populating DesiredStateOfworld
    actual_state_of_world.go:434: I0209 18:44:03.397118] Set detach request time to current time for volume on node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:44:03.409841] Updating status for node succeeded node="mynode-2" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:44:03.409863] Starting attacherDetacher.DetachVolume node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:44:03.410046] attacherDetacher.DetachVolume started node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:44:03.410060] Set detach request time to current time for volume on node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:44:03.410143] Updating status for node succeeded node="mynode-3" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:44:03.410182] Starting attacherDetacher.DetachVolume node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:44:03.410198] attacherDetacher.DetachVolume started node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:44:03.410207] Set detach request time to current time for volume on node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    node_status_updater.go:129: I0209 18:44:03.410260] Updating status for node succeeded node="mynode-4" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:44:03.410270] Starting attacherDetacher.DetachVolume node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:44:03.410280] attacherDetacher.DetachVolume started node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:44:03.410289] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:44:03.410455   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:44:03.410461   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0209 18:44:03.410463   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-2" 
I0209 18:44:03.410465   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-4" 
I0209 18:44:03.410455   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
I0209 18:44:03.410474   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-3" 
    node_status_updater.go:129: I0209 18:44:03.411084] Updating status for node succeeded node="mynode" patchBytes="{\"status\":{\"volumesAttached\":[{\"devicePath\":\"fake/path\",\"name\":\"kubernetes.io/testPlugin/inUseVolume\"}]}}" attachedVolumes=<[]v1.AttachedVolume | len:1, cap:1>: 
                - devicePath: fake/path
                  name: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:271: I0209 18:44:03.411101] Starting attacherDetacher.DetachVolume node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:44:03.411111] attacherDetacher.DetachVolume started node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:44:03.411118] Set detach request time to current time for volume on node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
I0209 18:44:03.411127   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
I0209 18:44:03.411147   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode" 
    node_status_updater.go:129: I0209 18:44:03.411170] Updating status for node succeeded node="mynode-1" patchBytes="{\"status\":{\"volumesAttached\":null}}" attachedVolumes=<[]v1.AttachedVolume | len:0, cap:0>: 
                []
    reconciler.go:271: I0209 18:44:03.411178] Starting attacherDetacher.DetachVolume node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    reconciler.go:279: I0209 18:44:03.411190] attacherDetacher.DetachVolume started node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:434: I0209 18:44:03.411200] Set detach request time to current time for volume on node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:03.411209   86473 operation_generator.go:1517] Verified volume is safe to detach for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
I0209 18:44:03.411212   86473 operation_generator.go:414] DetachVolume.Detach succeeded for volume "nil" (UniqueName: "kubernetes.io/testPlugin/lostVolumeName") on node "mynode-1" 
    reconciler.go:241: I0209 18:44:03.411211] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:361: I0209 18:44:03.417419] Starting attacherDetacher.AttachVolume volume=<cache.VolumeToAttach>: {
                VolumeToAttach: {
                    MultiAttachErrorReported: false,
                    VolumeName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName",
                    VolumeSpec: {
                        Volume: nil,
                        PersistentVolume: 
                            metadata:
                              name: pd.csi.storage.gke.io-pdName
                            spec:
                              accessModes:
                              - ReadOnlyMany
                              csi:
                                driver: pd.csi.storage.gke.io
                                fsType: ext4
                                readOnly: true
                                volumeAttributes:
                                  partition: ""
                                volumeHandle: projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName
                              volumeMode: Filesystem
                            status: {},
                        ReadOnly: false,
                        InlineVolumeSpecForCSIMigration: true,
                        Migrated: true,
                    },
                    NodeName: "mynode",
                    ScheduledPods: 
                        - metadata:
                            labels:
                              name: mypod-3
                            name: mypod-3
                            namespace: mynamespace
                            uid: mypod-3
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-4
                            name: mypod-4
                            namespace: mynamespace
                            uid: mypod-4
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-0
                            name: mypod-0
                            namespace: mynamespace
                            uid: mypod-0
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName
                              resources: {}
                              volumeMounts:
                              - mountPath: /mnt
                                name: volumeMountName
                                readOnly: true
                            nodeName: mynode
                            volumes:
                            - gcePersistentDisk:
                                fsType: ext4
                                pdName: pdName
                                readOnly: true
                              name: volumeName
                          status:
                            phase: Running
                        - metadata:
                            labels:
                              name: mypod-1
                            name: mypod-1
                            namespace: mynamespace
                            uid: mypod-1
                          spec:
                            containers:
                            - image: containerImage
                              name: containerName...
        
        Gomega truncated this representation as it exceeds 'format.MaxLength'.
        Consider having the object provide a custom 'GomegaStringer' representation
        or adjust the parameters in Gomega's 'format' package.
        
        Learn more here: https://onsi.github.io/gomega/#adjusting-output
    reconciler.go:364: I0209 18:44:03.417976] attacherDetacher.AttachVolume started volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName nodeName=<types.NodeName>: mynode scheduledPods=["mynamespace/mypod-3","mynamespace/mypod-4","mynamespace/mypod-0","mynamespace/mypod-1","mynamespace/mypod-2"]
I0209 18:44:03.419751   86473 csi_attacher.go:125] kubernetes.io/csi: attachment [csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66] for volume [projects/UNSPECIFIED/zones/UNSPECIFIED/disks/pdName] created successfully
I0209 18:44:03.419875   86473 csi_attacher.go:173] kubernetes.io/csi: probing VolumeAttachment [id=csi-4ff6fd4b10c02f804e7435dffeb62054338b590749cdaacf18e198ee0ab6dd66]
    reconciler.go:241: I0209 18:44:03.520033] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:03.621863] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:03.723526] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:03.825212] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:03.925381] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.025908] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.126019] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.228127] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:04.305219   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:04.305257   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:04.305251   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:04.305383] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:04.305460] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:04.305480] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:04.305486] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:04.305496] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:04.305502] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:04.305509] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:04.305516] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:04.305536] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:04.305545] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:04.330187] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:04.330210] Starting reconciling attached volumes still attached
E0209 18:44:04.330227   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.432277] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.534000] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.636094] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.737868] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.839998] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:04.940270] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.041956] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.143650] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.244575] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:05.305607   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:05.305670   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:05.305684   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:05.306561] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:05.306627] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:05.306692] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:05.306712] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:05.306739] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:05.306760] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:05.306779] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:05.306810] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:05.306859] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:05.306882] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:05.346033] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:05.346083] Starting reconciling attached volumes still attached
E0209 18:44:05.346361   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.447893] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.550409] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.651795] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.754099] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.855495] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:05.957632] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.059647] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.161246] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.263353] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:06.307466   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:06.307524   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:06.307645   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:06.307746] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:06.307776] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:06.307798] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:06.307822] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:06.307838] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:06.307846] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:06.307855] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:06.307875] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:06.307892] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:06.307905] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0209 18:44:06.365659] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:06.365708] Starting reconciling attached volumes still attached
E0209 18:44:06.365735   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.467832] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.569098] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.670137] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.770540] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.871305] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:06.973532] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.075622] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.176337] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.278379] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:07.316113   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:07.316118   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:07.316147   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:07.317674] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:07.321952] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:07.322961] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:07.322977] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:07.322987] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:07.322994] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:07.323006] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:07.323013] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:07.323020] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:07.323039] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    reconciler.go:241: I0209 18:44:07.380516] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:07.380592] Starting reconciling attached volumes still attached
E0209 18:44:07.382912   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.483334] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.583881] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.684125] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.785358] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.886673] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:07.987499] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.089582] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.189752] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.291898] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:08.317967   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:08.317966   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:08.317982   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:08.318186] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:08.318308] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:08.318328] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:08.318335] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:08.318373] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:08.318399] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:08.318408] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:08.318415] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:08.318422] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:08.318527] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:08.392824] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:08.392859] Starting reconciling attached volumes still attached
E0209 18:44:08.392891   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.494496] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.596585] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.696829] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.796994] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.897579] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:08.999657] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.099805] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.201902] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.304384] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:09.320628   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:09.320648   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:09.320633   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:09.322031] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:09.322546] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:09.322561] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:09.322719] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:09.322738] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:09.322759] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:09.322768] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:09.322785] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:09.322793] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:09.322798] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:09.404729] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:09.404771] Starting reconciling attached volumes still attached
E0209 18:44:09.404909   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.505749] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.607703] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.709732] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.810263] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:09.912373] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.014464] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.116546] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.218212] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.319396] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:10.321951   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:10.321958   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:10.321951   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:10.321992] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:10.322020] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:10.322032] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:10.322037] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:10.322046] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:10.322056] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:10.322065] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:10.322071] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:10.322078] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:10.322084] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:10.421502] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:10.421546] Starting reconciling attached volumes still attached
E0209 18:44:10.421566   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.523583] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.623721] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.726632] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.828735] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:10.930808] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.033283] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.135353] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.236499] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:11.322060   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:11.322060   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:11.322068   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:11.322149] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:11.322180] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:11.322195] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:11.322202] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:11.322218] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:11.322244] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:11.322254] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:11.322262] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:11.322272] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:11.322280] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:11.336690] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.438615] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:11.438661] Starting reconciling attached volumes still attached
E0209 18:44:11.438696   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.540754] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.641868] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.743943] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.846020] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:11.948090] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.049835] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.150725] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.252832] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:12.323155   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:12.323152   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:12.323161   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:12.323202] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:12.323233] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:12.323252] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:12.323263] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:12.323271] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:12.323278] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:12.323303] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:12.323310] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:12.323324] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:12.323331] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:12.354887] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.456940] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:127: I0209 18:44:12.456965] Starting reconciling attached volumes still attached
E0209 18:44:12.456977   86473 operation_generator.go:175] VerifyVolumesAreAttached.GenerateVolumesAreAttachedFunc: nil spec for volume kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.559037] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.659279] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.760616] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.862684] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:12.964758] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:13.066829] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:13.168891] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    reconciler.go:241: I0209 18:44:13.270411] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
I0209 18:44:13.324000   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.324014   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.324002   86473 reflector.go:456] "Forcing resync" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:13.324049] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:13.324083] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:13.324096] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:13.324103] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:13.324119] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:13.324125] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:13.324135] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:13.324141] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:13.324146] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:13.324152] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    reconciler.go:241: I0209 18:44:13.372476] Cannot detach volume because it is still mounted node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    attach_detach_controller_test.go:704: Expected detach operation not found, node:mynode-1, volume: vol-zero-va, tries: 10
I0209 18:44:13.415849   86473 watch.go:142] "Stopping fake watcher"
=== RUN   TestCtest_ADC_VolumeAttachmentRecovery/CSI_migration_with_empty_volume_name
I0209 18:44:13.417226   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.417253   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.417273   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.417294   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.417294   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.417292   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.417299   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.424063   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/testPlugin"
I0209 18:44:13.425056   86473 csi_plugin.go:364] Cast from VolumeHost to KubeletVolumeHost failed. Skipping CSINode initialization, not running on kubelet
I0209 18:44:13.425062   86473 plugins.go:610] "Loaded volume plugin" pluginName="kubernetes.io/csi"
I0209 18:44:13.427206   86473 shared_informer.go:349] "Waiting for caches to sync" controller="attach detach"
I0209 18:44:13.429757   86473 reflector.go:358] "Starting reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429765   86473 reflector.go:404] "Listing and watching" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429766   86473 reflector.go:358] "Starting reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429773   86473 reflector.go:404] "Listing and watching" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429779   86473 reflector.go:358] "Starting reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429758   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429784   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429785   86473 reflector.go:404] "Listing and watching" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429803   86473 reflector.go:358] "Starting reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429780   86473 reflector.go:358] "Starting reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429809   86473 reflector.go:404] "Listing and watching" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429813   86473 reflector.go:404] "Listing and watching" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429766   86473 reflector.go:358] "Starting reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.429826   86473 reflector.go:404] "Listing and watching" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.431128   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolumeClaim" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.431225   86473 reflector.go:436] "Caches populated" type="*v1.CSIDriver" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.431696   86473 reflector.go:436] "Caches populated" type="*v1.CSINode" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.431697   86473 reflector.go:436] "Caches populated" type="*v1.PersistentVolume" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.431711   86473 reflector.go:436] "Caches populated" type="*v1.VolumeAttachment" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.431697   86473 reflector.go:436] "Caches populated" type="*v1.Node" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.432268   86473 reflector.go:436] "Caches populated" type="*v1.Pod" reflector="k8s.io/client-go/informers/factory.go:160"
    attach_detach_controller.go:669: I0209 18:44:13.432052] processVolumesInUse for node node="mynode"
    actual_state_of_world.go:400: I0209 18:44:13.432432] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:669: I0209 18:44:13.432443] processVolumesInUse for node node="mynode-1"
    actual_state_of_world.go:400: I0209 18:44:13.432450] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:13.432456] processVolumesInUse for node node="mynode-2"
    actual_state_of_world.go:400: I0209 18:44:13.432462] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:13.432472] processVolumesInUse for node node="mynode-3"
    actual_state_of_world.go:400: I0209 18:44:13.432478] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    attach_detach_controller.go:669: I0209 18:44:13.432486] processVolumesInUse for node node="mynode-4"
    actual_state_of_world.go:400: I0209 18:44:13.432492] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
I0209 18:44:13.530202   86473 shared_informer.go:356] "Caches are synced" controller="attach detach"
    attach_detach_controller.go:369: I0209 18:44:13.530222] Populating ActualStateOfworld
    actual_state_of_world.go:506: I0209 18:44:13.530278] Add new node to nodesToUpdateStatusFor node="mynode-1"
    actual_state_of_world.go:514: I0209 18:44:13.530294] Report volume as attached to node node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:13.530315] SetVolumesMountedByNode volume to the node node="mynode-1" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:13.530336] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-2" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:13.530354] Add new node to nodesToUpdateStatusFor node="mynode-2"
    actual_state_of_world.go:514: I0209 18:44:13.530360] Report volume as attached to node node="mynode-2" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:13.530366] SetVolumesMountedByNode volume to the node node="mynode-2" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:13.530374] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-3" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:13.530380] Add new node to nodesToUpdateStatusFor node="mynode-3"
    actual_state_of_world.go:514: I0209 18:44:13.530386] Report volume as attached to node node="mynode-3" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:13.530393] SetVolumesMountedByNode volume to the node node="mynode-3" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:13.530401] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode-4" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:13.530410] Add new node to nodesToUpdateStatusFor node="mynode-4"
    actual_state_of_world.go:514: I0209 18:44:13.530415] Report volume as attached to node node="mynode-4" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:400: I0209 18:44:13.530421] SetVolumesMountedByNode volume to the node node="mynode-4" volumeNames=<[]v1.UniqueVolumeName | len:0, cap:0>: nil
    actual_state_of_world.go:358: I0209 18:44:13.530429] Volume is already added to attachedVolume list to node, update device path volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName node="mynode" devicePath="fake/path"
    actual_state_of_world.go:506: I0209 18:44:13.530435] Add new node to nodesToUpdateStatusFor node="mynode"
    actual_state_of_world.go:514: I0209 18:44:13.530452] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/lostVolumeName
    actual_state_of_world.go:514: I0209 18:44:13.530459] Report volume as attached to node node="mynode" volumeName=<v1.UniqueVolumeName>: kubernetes.io/testPlugin/inUseVolume
    actual_state_of_world.go:400: I0209 18:44:13.530497] SetVolumesMountedByNode volume to the node node="mynode" volumeNames=<[]v1.UniqueVolumeName | len:1, cap:1>: [
                "kubernetes.io/testPlugin/inUseVolume",
            ]
    attach_detach_controller.go:736: I0209 18:44:13.531661] Marking volume attachment as uncertain as volume is not attached node="mynode-1" volumeName=<v1.UniqueVolumeName>: kubernetes.io/csi/pd.csi.storage.gke.io^projects/UNSPECIFIED/zones/UNSPECIFIED/disks/ attachState="Detached"
    attach_detach_controller.go:424: I0209 18:44:13.531675] Populating DesiredStateOfworld
--- FAIL: TestCtest_ADC_VolumeAttachmentRecovery (34.03s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_attached (1.10s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/VA_status_is_unattached (1.10s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/Scheduled_Pod_with_migrated_PV (0.10s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/Deleted_Pod_with_migrated_PV (0.10s)
    --- FAIL: TestCtest_ADC_VolumeAttachmentRecovery/Empty_volume_name (10.11s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/Nil_pod_name_(should_be_ignored) (1.12s)
    --- FAIL: TestCtest_ADC_VolumeAttachmentRecovery/Invalid_node_name_characters (10.12s)
    --- FAIL: TestCtest_ADC_VolumeAttachmentRecovery/Zero-length_VA_name (10.14s)
    --- PASS: TestCtest_ADC_VolumeAttachmentRecovery/CSI_migration_with_empty_volume_name (0.11s)
I0209 18:44:13.532185   86473 watch.go:142] "Stopping fake watcher"
I0209 18:44:13.532218   86473 reflector.go:364] "Stopping reflector" type="*v1.Pod" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.532224   86473 reflector.go:364] "Stopping reflector" type="*v1.VolumeAttachment" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.532232   86473 reflector.go:364] "Stopping reflector" type="*v1.CSIDriver" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.532235   86473 reflector.go:364] "Stopping reflector" type="*v1.CSINode" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.532254   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolume" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:44:13.532268   86473 reflector.go:364] "Stopping reflector" type="*v1.Node" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
FAIL
I0209 18:44:13.532300   86473 reflector.go:364] "Stopping reflector" type="*v1.PersistentVolumeClaim" resyncPeriod="1s" reflector="k8s.io/client-go/informers/factory.go:160"
coverage: 40.3% of statements
FAIL	k8s.io/kubernetes/pkg/controller/volume/attachdetach	35.443s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/cache	1.752s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/attachdetach/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/attachdetach/config/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestVolumesInUseMetricCollection

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:38]: Matched PodSpec config: {test_fixture.json [volumes-in-use podspec] spec [pods] {[{metric-test-volume-name {nil nil nil nil nil nil nil nil nil &PersistentVolumeClaimVolumeSource{ClaimName:metric-test-pvc,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> metric-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:43:40 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:40 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"nodeName":"metric-test-host","volumes":[{"name":"metric-test-volume-name","persistentVolumeClaim":{"claimName":"metric-test-pvc"}}]})[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:43:40 [DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{metric-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil &PersistentVolumeClaimVolumeSource{ClaimName:metric-test-pvc,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> metric-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{metric-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil &PersistentVolumeClaimVolumeSource{ClaimName:metric-test-pvc,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> metric-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:44]: PodSpec JSON: [{"volumes":[{"name":"metric-test-volume-name","hostPath":{"path":"/etc/kubernetes"},"persistentVolumeClaim":{"claimName":"metric-test-pvc"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"nodeName":"metric-test-host"}]
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:49]: Number of PodSpec configs: 1
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/metrics/ctest_metrics_test.go:57]: Matched PVC config: {test_fixture.json [volumes-in-use pvc] spec [persistentvolumeclaims] {[ReadOnlyMany ReadWriteOnce] nil {map[] map[storage:{{2 9} {<nil>} 2G DecimalSI}]} test-metric-pv-1 <nil> <nil> nil nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumeclaims], int=1)[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:43:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:43:40 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumeclaims
FAIL	k8s.io/kubernetes/pkg/controller/volume/attachdetach/metrics	2.495s
=== RUN   TestCtestFindAndAddActivePods_FindAndRemoveDeletedPods

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:42]: get default configs: {test_fixture.json [default pod spec for RBD volume] spec [pods] {[{dswp-test-volume-name {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:43:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:45 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"nodeName":"dswp-test-host","volumes":[{"name":"dswp-test-volume-name","rbd":{"image":"dswp-test-fake-device","monitors":null}}]})[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:43:45 [DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:53]: New Json Test Configs: [{"volumes":[{"name":"dswp-test-volume-name","hostPath":{"path":"/etc/kubernetes"},"rbd":{"monitors":null,"image":"dswp-test-fake-device"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"nodeName":"dswp-test-host"}]
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:54]: Num of Test Cases: 1
Running 0 th test case.
{[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:dswp-test-fake-device,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    desired_state_of_world_populator.go:158: I0209 18:43:45.610581] Removing pod from dsw because it does not exist in pod informer podName="dswp-test/dswp-test-pod-e69b2995-ce60-4c5f-bda3-08342634bb60" podUID="dswp-test-pod-uid-570397d4-79b2-45f4-b9b4-3091719a88ca"
    ctest_desired_state_of_world_populator_test.go:179: VolumeExists("fake-plugin/dswp-test-fake-device") failed. Expected: <false> Actual: <true>
--- FAIL: TestCtestFindAndAddActivePods_FindAndRemoveDeletedPods (0.00s)
=== RUN   TestCtestFindAndRemoveNonattachableVolumes

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:195]: get default configs: {test_fixture.json [default pod spec for CSI volume] spec [pods] {[{dswp-test-volume-name {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:43:45 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:43:45 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"nodeName":"dswp-test-host","volumes":[{"csi":{"driver":"dswp-test-fake-csi-driver"},"name":"dswp-test-volume-name"}]})[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:43:45 [DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:206]: New Json Test Configs: [{"volumes":[{"name":"dswp-test-volume-name","hostPath":{"path":"/etc/kubernetes"},"csi":{"driver":"dswp-test-fake-csi-driver"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null,"nodeName":"dswp-test-host"}]
[DEBUG-CTEST 2026-02-09 18:43:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/attachdetach/populator/ctest_desired_state_of_world_populator_test.go:207]: Num of Test Cases: 1
Running 0 th test case.
{[{dswp-test-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &CSIVolumeSource{Driver:dswp-test-fake-csi-driver,ReadOnly:nil,FSType:nil,VolumeAttributes:map[string]string{},NodePublishSecretRef:nil,} nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil> dswp-test-host false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    desired_state_of_world_populator.go:168: I0209 18:43:45.611863] Volume changes from attachable to non-attachable volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0209 18:43:45.611889] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:168: I0209 18:43:45.611899] Volume changes from attachable to non-attachable volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:172: I0209 18:43:45.611906] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:168: I0209 18:43:45.612041] Volume changes from attachable to non-attachable volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0209 18:43:45.612059] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:168: I0209 18:43:45.612065] Volume changes from attachable to non-attachable volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:172: I0209 18:43:45.612071] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:168: I0209 18:43:45.612210] Volume changes from attachable to non-attachable volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0209 18:43:45.612222] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/dswp-test-fake-csi-driver"
    desired_state_of_world_populator.go:168: I0209 18:43:45.612237] Volume changes from attachable to non-attachable volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:172: I0209 18:43:45.612244] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/socket-dir"
    desired_state_of_world_populator.go:168: I0209 18:43:45.612252] Volume changes from attachable to non-attachable volumeName="fake-plugin/extra-fake-csi-driver"
    desired_state_of_world_populator.go:172: I0209 18:43:45.612259] Removing podUID and volume on node from desired state of world because of the change of volume attachability node="dswp-test-host" podUID="dswp-test-pod-uid-d793f384-5112-4d75-8644-f3e06d8a332c" volumeName="fake-plugin/extra-fake-csi-driver"

==================== CTEST END ======================
--- PASS: TestCtestFindAndRemoveNonattachableVolumes (0.00s)
FAIL
coverage: 61.7% of statements
FAIL	k8s.io/kubernetes/pkg/controller/volume/attachdetach/populator	1.385s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/reconciler	0.799s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/statusupdater	0.448s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/attachdetach/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/attachdetach/util	1.854s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/common		coverage: 0.0% of statements
/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build547637365/b3789/ephemeral.test flag redefined: v
panic: /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build547637365/b3789/ephemeral.test flag redefined: v

goroutine 1 [running]:
flag.(*FlagSet).Var(0x14000150070, {0x105b2d2c8, 0x1079165b0}, {0x10529ac68, 0x1}, {0x104ecaa89, 0x22})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/flag/flag.go:1028 +0x2d4
k8s.io/klog/v2.InitFlags.func1(0x140001e7a70?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/k8s.io/klog/v2/klog.go:447 +0x3c
flag.(*FlagSet).VisitAll(0x0?, 0x14000807dd8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/flag/flag.go:458 +0x48
k8s.io/klog/v2.InitFlags(0x0?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/k8s.io/klog/v2/klog.go:446 +0x44
k8s.io/kubernetes/pkg/controller/volume/ephemeral.init.0()
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/ephemeral/controller_test.go:56 +0x20
FAIL	k8s.io/kubernetes/pkg/controller/volume/ephemeral	1.037s
	k8s.io/kubernetes/pkg/controller/volume/ephemeral/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/ephemeral/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/ephemeral/metrics		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/controller/volume/events	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/expand	0.433s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/persistentvolume	0.815s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/config		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/config/v1alpha1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/metrics		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/options		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/controller/volume/persistentvolume/testing		coverage: 0.0% of statements
=== RUN   TestCtestIsDeletionCandidateLackDeleteTimeAndFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:54]: Number of test cases: 5
Running 0 th test case.
{pv lacks delete time and finalizer 0x1400076d188 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc lacks delete time and finalizer 0x140003954a0 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv empty finalizer 0x1400076d408  false}
Running 3 th test case.
{pvc empty finalizer 0x14000395680  false}
Running 4 th test case.
{service object with pv finalizer 0x1400076d688 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateLackDeleteTimeAndFinalizer (0.00s)
=== RUN   TestCtestIsDeletionCandidateLackDeleteTime

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:108]: Number of test cases: 5
Running 0 th test case.
{pv lacks delete time 0x1400076d908 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc lacks delete time 0x14000395860 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv empty finalizer after setting delete time 0x1400076db88  false}
Running 3 th test case.
{pvc empty finalizer after setting delete time 0x14000395a40  false}
Running 4 th test case.
{service object with pvc finalizer 0x140000d4008 kubernetes.io/pvc-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateLackDeleteTime (0.00s)
=== RUN   TestCtestIsDeletionCandidateLackFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:162]: Number of test cases: 5
Running 0 th test case.
{pv lacks finalizer 0x140000d4288 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc lacks finalizer 0x14000395c20 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv with empty finalizer string 0x140000d4508  false}
Running 3 th test case.
{pvc with empty finalizer string 0x1400014a780  false}
Running 4 th test case.
{service object with pv finalizer 0x140000d4788 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateLackFinalizer (0.00s)
=== RUN   TestCtestIsDeletionCandidateSuccess

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:222]: Number of test cases: 5
Running 0 th test case.
{pv is to delete 0x140000d4a08 kubernetes.io/pv-protection true}
Running 1 th test case.
{pvc is to delete 0x1400014a960 kubernetes.io/pvc-protection true}
Running 2 th test case.
{pv missing finalizer but has delete time 0x140000d4c88 kubernetes.io/pv-protection false}
Running 3 th test case.
{pvc missing delete time but has finalizer 0x1400014ab40 kubernetes.io/pvc-protection false}
Running 4 th test case.
{service object with correct finalizer 0x140000d4f08 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestIsDeletionCandidateSuccess (0.00s)
=== RUN   TestCtestNeedToAddFinalizerHasDeleteTimeAndFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:278]: Number of test cases: 5
Running 0 th test case.
{pv has delete time and finalizer 0x140000d5188 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc has delete time and finalizer 0x1400014ad20 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv has delete time but empty finalizer list 0x140000d5408 kubernetes.io/pv-protection true}
    ctest_utils_test.go:283: pv has delete time but empty finalizer list
Running 3 th test case.
{pvc has delete time but empty finalizer list 0x1400014af00 kubernetes.io/pvc-protection true}
    ctest_utils_test.go:283: pvc has delete time but empty finalizer list
Running 4 th test case.
{service object with delete time and finalizer 0x140000d5688 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- FAIL: TestCtestNeedToAddFinalizerHasDeleteTimeAndFinalizer (0.00s)
=== RUN   TestCtestNeedToAddFinalizerHasDeleteTime

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:332]: Number of test cases: 5
Running 0 th test case.
{pv has delete 0x140000d5908 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc has delete 0x1400014b0e0 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv has delete but empty finalizer string 0x140000d5b88  true}
Running 3 th test case.
{pvc has delete but empty finalizer string 0x1400014b2c0  true}
Running 4 th test case.
{service with delete time 0x140000d6008 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestNeedToAddFinalizerHasDeleteTime (0.00s)
=== RUN   TestCtestNeedToAddFinalizerHasFinalizer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:390]: Number of test cases: 5
Running 0 th test case.
{pv has finalizer 0x140000d6288 kubernetes.io/pv-protection false}
Running 1 th test case.
{pvc has finalizer 0x1400014b4a0 kubernetes.io/pvc-protection false}
Running 2 th test case.
{pv has unrelated finalizer 0x140000d6508 kubernetes.io/pv-protection true}
Running 3 th test case.
{pvc has unrelated finalizer 0x1400014b680 kubernetes.io/pvc-protection true}
Running 4 th test case.
{service object with finalizer 0x140000d6788 kubernetes.io/pv-protection false}

==================== CTEST END ======================
--- PASS: TestCtestNeedToAddFinalizerHasFinalizer (0.00s)
=== RUN   TestCtestNeedToAddFinalizerSuccess

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/protectionutil/ctest_utils_test.go:439]: Number of test cases: 5
Running 0 th test case.
{pv needs add finalizer 0x140000d6a08 kubernetes.io/pv-protection true}
Running 1 th test case.
{pvc needs add finalizer 0x1400014b860 kubernetes.io/pvc-protection true}
Running 2 th test case.
{pv with empty finalizer string 0x140000d6c88  true}
Running 3 th test case.
{pvc with empty finalizer string 0x1400014ba40  true}
Running 4 th test case.
{service object without delete time or finalizer 0x140000d6f08 kubernetes.io/pv-protection true}

==================== CTEST END ======================
--- PASS: TestCtestNeedToAddFinalizerSuccess (0.00s)
FAIL
coverage: 3.7% of statements
FAIL	k8s.io/kubernetes/pkg/controller/volume/protectionutil	1.130s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/pvcprotection	0.648s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPVProtectionController

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/pvprotection/ctest_pv_protection_controller_test.go:41]: Base config item: {test_fixture.json [basic pv] finalizers [persistentvolumes] &PersistentVolume{ObjectMeta:{default-pv      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PersistentVolumeSpec{Capacity:ResourceList{},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:nil,Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[],ClaimRef:nil,PersistentVolumeReclaimPolicy:,StorageClassName:,MountOptions:[],VolumeMode:nil,NodeAffinity:nil,VolumeAttributesClassName:nil,},Status:PersistentVolumeStatus{Phase:,Message:,Reason:,LastPhaseTransitionTime:<nil>,},}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:43:57 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/controller/volume/pvprotection	1.314s
=== RUN   TestCtestSELinuxWarningController_Sync
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:166]: Running SELinuxWarningController sync tests, total: 8
=== RUN   TestCtestSELinuxWarningController_Sync/existing_pod_with_no_volumes
[DEBUG-CTEST 2026-02-09 18:43:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 0: existing pod with no volumes
    selinux_warning_controller.go:418: I0209 18:43:58.028241] Syncing pod pod="ns1/pod1"
    selinux_warning_controller.go:441: I0209 18:43:58.028413] skipping not found volume pod="ns1/pod1" volume="vol1"
    selinux_warning_controller.go:348: I0209 18:43:58.028547] Starting SELinux warning controller
  I0209 18:43:58.028717   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.028754   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/existing_pod_with_unbound_PVC
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 1: existing pod with unbound PVC
    selinux_warning_controller.go:348: I0209 18:43:58.130001] Starting SELinux warning controller
    selinux_warning_controller.go:418: I0209 18:43:58.130178] Syncing pod pod="ns1/pod1"
  I0209 18:43:58.130203   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.130215   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
    selinux_warning_controller.go:441: I0209 18:43:58.130248] skipping not found volume pod="ns1/pod1" volume="vol1"
=== RUN   TestCtestSELinuxWarningController_Sync/existing_pod_with_fully_bound_PVC
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 2: existing pod with fully bound PVC
    selinux_warning_controller.go:348: I0209 18:43:58.233479] Starting SELinux warning controller
  I0209 18:43:58.233551   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.233560   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
    selinux_warning_controller.go:418: I0209 18:43:58.233543] Syncing pod pod="ns1/pod1"
    selinux_warning_controller.go:494: I0209 18:43:58.233678] Syncing pod volume pod="ns1/pod1" volume="pv1" label=":::s0:c1,c2" uniqueVolumeName="fake-plugin/pv1" changePolicy="MountOption" csiDriver="ebs.csi.aws.com"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_nil_security_context
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 3: pod with nil security context
    selinux_warning_controller.go:418: I0209 18:43:58.338517] Syncing pod pod="ns1/pod-nil-sc"
    selinux_warning_controller.go:441: I0209 18:43:58.338583] skipping not found volume pod="ns1/pod-nil-sc" volume="vol1"
    ctest_selinux_warning_controller_test.go:242: got unexpected events: map[]
    ctest_selinux_warning_controller_test.go:243: missing events: map[Normal SELinuxLabelMissing SELinuxLabel missing on pod pod-nil-sc:{}]
    selinux_warning_controller.go:348: I0209 18:43:58.338667] Starting SELinux warning controller
  I0209 18:43:58.338761   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.338777   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_empty_SELinux_level
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 4: pod with empty SELinux level
    selinux_warning_controller.go:418: I0209 18:43:58.439730] Syncing pod pod="ns1/pod-empty-level"
    selinux_warning_controller.go:441: I0209 18:43:58.439925] skipping not found volume pod="ns1/pod-empty-level" volume="vol1"
    selinux_warning_controller.go:348: I0209 18:43:58.440115] Starting SELinux warning controller
  I0209 18:43:58.440258   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.440276   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_unknown_SELinuxChangePolicy_(invalid_pointer)
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 5: pod with unknown SELinuxChangePolicy (invalid pointer)
    selinux_warning_controller.go:418: I0209 18:43:58.541963] Syncing pod pod="ns1/pod-unknown-policy"
    selinux_warning_controller.go:441: I0209 18:43:58.542057] skipping not found volume pod="ns1/pod-unknown-policy" volume="vol1"
    ctest_selinux_warning_controller_test.go:242: got unexpected events: map[]
    ctest_selinux_warning_controller_test.go:243: missing events: map[Warning SELinuxChangePolicyInvalid SELinuxChangePolicy "UnknownPolicy" is not supported:{}]
    selinux_warning_controller.go:348: I0209 18:43:58.542167] Starting SELinux warning controller
  I0209 18:43:58.542473   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.542487   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/PVC_missing_volume_source_(nil_PV)
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 6: PVC missing volume source (nil PV)
    selinux_warning_controller.go:418: I0209 18:43:58.643185] Syncing pod pod="ns1/pod-missing-pv"
    selinux_warning_controller.go:441: I0209 18:43:58.643273] skipping not found volume pod="ns1/pod-missing-pv" volume="vol1"
    selinux_warning_controller.go:348: I0209 18:43:58.643544] Starting SELinux warning controller
  I0209 18:43:58.643631   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.643644   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
=== RUN   TestCtestSELinuxWarningController_Sync/pod_with_duplicate_volumeMount_names
[DEBUG-CTEST 2026-02-09 18:43:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controller/volume/selinuxwarning/ctest_selinux_warning_controller_test.go:169]: Test case 7: pod with duplicate volumeMount names
    selinux_warning_controller.go:418: I0209 18:43:58.747816] Syncing pod pod="ns1/pod-dup"
    selinux_warning_controller.go:441: I0209 18:43:58.747866] skipping not found volume pod="ns1/pod-dup" volume="vol1"
    selinux_warning_controller.go:494: I0209 18:43:58.747895] Syncing pod volume pod="ns1/pod-dup" volume="pvdup" label=":::s0:c1,c2" uniqueVolumeName="fake-plugin/pvdup" changePolicy="MountOption" csiDriver="ebs.csi.aws.com"
    ctest_selinux_warning_controller_test.go:224: expected error, got nil
    selinux_warning_controller.go:348: I0209 18:43:58.748014] Starting SELinux warning controller
  I0209 18:43:58.748084   86754 shared_informer.go:349] "Waiting for caches to sync" controller="selinux_warning"
  I0209 18:43:58.748092   86754 shared_informer.go:356] "Caches are synced" controller="selinux_warning"
--- FAIL: TestCtestSELinuxWarningController_Sync (0.88s)
    --- PASS: TestCtestSELinuxWarningController_Sync/existing_pod_with_no_volumes (0.16s)
    --- PASS: TestCtestSELinuxWarningController_Sync/existing_pod_with_unbound_PVC (0.10s)
    --- PASS: TestCtestSELinuxWarningController_Sync/existing_pod_with_fully_bound_PVC (0.10s)
    --- FAIL: TestCtestSELinuxWarningController_Sync/pod_with_nil_security_context (0.11s)
    --- PASS: TestCtestSELinuxWarningController_Sync/pod_with_empty_SELinux_level (0.10s)
    --- FAIL: TestCtestSELinuxWarningController_Sync/pod_with_unknown_SELinuxChangePolicy_(invalid_pointer) (0.10s)
    --- PASS: TestCtestSELinuxWarningController_Sync/PVC_missing_volume_source_(nil_PV) (0.10s)
    --- FAIL: TestCtestSELinuxWarningController_Sync/pod_with_duplicate_volumeMount_names (0.10s)
FAIL
coverage: 34.0% of statements
FAIL	k8s.io/kubernetes/pkg/controller/volume/selinuxwarning	2.420s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/selinuxwarning/cache	2.120s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/selinuxwarning/translator	3.561s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controller/volume/vacprotection	3.153s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controlplane	0.794s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controlplane/apiserver	1.387s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controlplane/apiserver/admission	1.850s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCompleteForServiceAccount
=== RUN   TestCtestCompleteForServiceAccount/endpoint_and_key_file
I0209 18:44:18.125882   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-61515000.sock
I0209 18:44:18.126926   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-61515000.sock
=== RUN   TestCtestCompleteForServiceAccount/max_token_expiration_breaching_acceptable_values
I0209 18:44:18.757729   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-136433000.sock
I0209 18:44:18.757797   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-136433000.sock
=== RUN   TestCtestCompleteForServiceAccount/path_to_a_signing_key_provided
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_endpoint_expiration
I0209 18:44:19.506177   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-786607000.sock
I0209 18:44:19.506266   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-786607000.sock
I0209 18:44:19.532550   87065 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_local_smaller_expirations
I0209 18:44:20.282981   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-533054000.sock
I0209 18:44:20.283016   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-533054000.sock
I0209 18:44:20.283740   87065 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_want_larger_than_signer_can_provide
I0209 18:44:20.869018   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-284884000.sock
I0209 18:44:20.869037   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-284884000.sock
I0209 18:44:20.870337   87065 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_but_return_smaller_than_acceptable_max_token_exp
I0209 18:44:21.194090   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-870543000.sock
I0209 18:44:21.194107   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-870543000.sock
I0209 18:44:21.194523   87065 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_getting_metadata
I0209 18:44:21.694618   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-194707000.sock
I0209 18:44:21.694634   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-194707000.sock
I0209 18:44:21.696429   87065 keycache.go:72] "Key cache shutting down"
=== RUN   TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_creating_plugin_(during_initial_fetch)
I0209 18:44:22.433647   87065 externalsigner_mock.go:230] Starting Mock Signer at socketPath mock-external-jwt-signer-696560000.sock
I0209 18:44:22.433676   87065 externalsigner_mock.go:236] Mock Signer listening at socketPath mock-external-jwt-signer-696560000.sock
=== RUN   TestCtestCompleteForServiceAccount/max_token_expiration_at_upper_limit
--- PASS: TestCtestCompleteForServiceAccount (5.67s)
    --- PASS: TestCtestCompleteForServiceAccount/endpoint_and_key_file (1.07s)
    --- PASS: TestCtestCompleteForServiceAccount/max_token_expiration_breaching_acceptable_values (0.62s)
    --- PASS: TestCtestCompleteForServiceAccount/path_to_a_signing_key_provided (0.03s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_endpoint_expiration (0.75s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided,_use_local_smaller_expirations (0.75s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_want_larger_than_signer_can_provide (0.59s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_but_return_smaller_than_acceptable_max_token_exp (0.32s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_getting_metadata (0.50s)
    --- PASS: TestCtestCompleteForServiceAccount/signing_endpoint_provided_and_error_when_creating_plugin_(during_initial_fetch) (0.74s)
    --- PASS: TestCtestCompleteForServiceAccount/max_token_expiration_at_upper_limit (0.00s)
=== RUN   TestCtestValidateAPIPriorityAndFairness
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/all=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/beta=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/ga=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/ga=true
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta1=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta2=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1=false
    ctest_validation_test.go:103: Expected no error, but got: "--runtime-config=flowcontrol.apiserver.k8s.io/v1=false conflicts with --enable-priority-and-fairness=true"
=== RUN   TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true,flowcontrol.apiserver.k8s.io/v1=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/#00
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/unknown=false
=== RUN   TestCtestValidateAPIPriorityAndFairness/api/beta=maybe
=== RUN   TestCtestValidateAPIPriorityAndFairness/invalid-format
--- FAIL: TestCtestValidateAPIPriorityAndFairness (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/all=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/beta=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/ga=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/ga=true (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta1=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta2=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true (0.00s)
    --- FAIL: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/flowcontrol.apiserver.k8s.io/v1beta3=true,flowcontrol.apiserver.k8s.io/v1=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/#00 (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/unknown=false (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/api/beta=maybe (0.00s)
    --- PASS: TestCtestValidateAPIPriorityAndFairness/invalid-format (0.00s)
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerCAFile_set
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertiseIP_set
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertisePort_set
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_empty_peerCAFile
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_both_peerCAFile_and_peerAdvertiseIP
=== RUN   TestCtestValidateUnknownVersionInteroperabilityProxy/invalid_peerAdvertisePort_(non-numeric)
    ctest_validation_test.go:175: Expected error message to contain: "--peer-advertise-port requires a numeric value", but got: ""
--- FAIL: TestCtestValidateUnknownVersionInteroperabilityProxy (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerCAFile_set (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertiseIP_set (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_disabled_but_peerAdvertisePort_set (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_empty_peerCAFile (0.00s)
    --- PASS: TestCtestValidateUnknownVersionInteroperabilityProxy/feature_enabled_with_both_peerCAFile_and_peerAdvertiseIP (0.00s)
    --- FAIL: TestCtestValidateUnknownVersionInteroperabilityProxy/invalid_peerAdvertisePort_(non-numeric) (0.00s)
=== RUN   TestCtestValidateOptions
=== RUN   TestCtestValidateOptions/validate_master_count_equal_0
=== RUN   TestCtestValidateOptions/validate_token_request_enable_not_attempted
=== RUN   TestCtestValidateOptions/nil_options
=== RUN   TestCtestValidateOptions/empty_Options_struct
--- FAIL: TestCtestValidateOptions (0.00s)
    --- PASS: TestCtestValidateOptions/validate_master_count_equal_0 (0.00s)
    --- PASS: TestCtestValidateOptions/validate_token_request_enable_not_attempted (0.00s)
    --- PASS: TestCtestValidateOptions/nil_options (0.00s)
    --- FAIL: TestCtestValidateOptions/empty_Options_struct (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x78 pc=0x105da1944]

goroutine 293 [running]:
testing.tRunner.func1.2({0x1069ac780, 0x108739390})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1069ac780?, 0x108739390?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/apiserver/pkg/server/options.(*ServerRunOptions).Validate(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/staging/src/k8s.io/apiserver/pkg/server/options/server_run_options.go:193 +0x24
k8s.io/kubernetes/pkg/controlplane/apiserver/options.(*Options).Validate(0x1400089ab00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/options/validation.go:142 +0x94
k8s.io/kubernetes/pkg/controlplane/apiserver/options.TestCtestValidateOptions.func1(0x140006e3c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/options/ctest_validation_test.go:254 +0x34
testing.tRunner(0x140006e3c00, 0x14000a808d0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 289
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controlplane/apiserver/options	9.072s
?   	k8s.io/kubernetes/pkg/controlplane/apiserver/samples	[no test files]
	k8s.io/kubernetes/pkg/controlplane/apiserver/samples/generic		coverage: 0.0% of statements
=== RUN   TestCtestDefaultOffAdmissionPlugins

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/samples/generic/server/ctest_admission_test.go:92]: get default configs: {test_fixture.json [default admission plugins] plugins [] [LimitRanger DefaultStorageClass PersistentVolumeClaimResize StorageObjectInUseProtection Priority TaintNodesByCondition RuntimeClass DefaultIngressClass PodSecurity PodTopologyLabels]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "plugins" in requested fixtures
2026/02/09 18:44:16 [DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:44:16 Mode: 1
2026/02/09 18:44:16 Base JSON size: 204 bytes
2026/02/09 18:44:16 Number of external values: 0
2026/02/09 18:44:16 [DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:44:16 [DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=["LimitRanger","DefaultStorageClass","PersistentVolumeClaimResize","StorageObjectInUseProtection","Priority","TaintNodesByCondition","RuntimeClass","DefaultIngressClass","PodSecurity","PodTopologyLabels"])[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/samples/generic/server/ctest_admission_test.go:40]: New Json Test Configs: [[LimitRanger DefaultStorageClass PersistentVolumeClaimResize StorageObjectInUseProtection Priority TaintNodesByCondition RuntimeClass DefaultIngressClass PodSecurity PodTopologyLabels]]
[DEBUG-CTEST 2026-02-09 18:44:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/apiserver/samples/generic/server/ctest_admission_test.go:41]: Num of Test Cases: 1
Running 0 th test case.
Plugin list: [LimitRanger DefaultStorageClass PersistentVolumeClaimResize StorageObjectInUseProtection Priority TaintNodesByCondition RuntimeClass DefaultIngressClass PodSecurity PodTopologyLabels]

==================== CTEST END ======================
--- PASS: TestCtestDefaultOffAdmissionPlugins (0.00s)
PASS
coverage: 3.0% of statements
ok  	k8s.io/kubernetes/pkg/controlplane/apiserver/samples/generic/server	2.649s	coverage: 3.0% of statements
	k8s.io/kubernetes/pkg/controlplane/apiserver/samples/generic/server/testing		coverage: 0.0% of statements
=== RUN   TestCtest_Controller

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtest_Controller/lease_not_expired
[DEBUG-CTEST 2026-02-09 18:44:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/apiserverleasegc/ctest_gc_controller_test.go:51]: Matched config for lease not expired : {test_fixture.json [lease not expired] spec [leases] &Lease{ObjectMeta:{kube-apiserver-12345  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[apiserver.kubernetes.io/identity:kube-apiserver] map[] [] [] []},Spec:LeaseSpec{HolderIdentity:*kube-apiserver-12345,LeaseDurationSeconds:*10,AcquireTime:<nil>,RenewTime:2026-02-09 18:44:23.354105 -0600 CST m=+0.296354918,LeaseTransitions:nil,Strategy:nil,PreferredHolder:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[leases]
[DEBUG-CTEST 2026-02-09 18:44:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[leases], int=1)[DEBUG-CTEST 2026-02-09 18:44:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:44:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [leases]
[DEBUG-CTEST 2026-02-09 18:44:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:44:23 load all fixtures failed: requested fixture keys not found in test_fixtures.json: leases
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/apiserverleasegc	2.565s
=== RUN   TestCtestWriteClientCAsEdge
=== RUN   TestCtestWriteClientCAsEdge/nil_clusterAuthInfo_(no_data)
=== RUN   TestCtestWriteClientCAsEdge/empty_CA_provider_(zero_length)
    ctest_cluster_authentication_trust_controller_test.go:143: empty CA provider (zero length): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s"&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{client-ca-file: ,},BinaryData:map[string][]byte{},Immutable:nil,}",
          }
=== RUN   TestCtestWriteClientCAsEdge/extremely_long_header_slice_(edge_size)
    ctest_cluster_authentication_trust_controller_test.go:143: extremely long header slice (edge size): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s"&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{requestheader-username-headers: [\"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"...,
          }
=== RUN   TestCtestWriteClientCAsEdge/requestheader_UID_without_feature_gate_(should_be_dropped)
    ctest_cluster_authentication_trust_controller_test.go:143: requestheader UID without feature gate (should be dropped): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s"&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}",
          }
=== RUN   TestCtestWriteClientCAsEdge/requestheader_UID_with_feature_gate_(should_be_added)
    ctest_cluster_authentication_trust_controller_test.go:143: requestheader UID with feature gate (should be added): diff   map[string]*v1.ConfigMap{
        - 	"extension-apiserver-authentication": s`&ConfigMap{ObjectMeta:{extension-apiserver-authentication  kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string]string{requestheader-uid-headers: ["uid-123"],},BinaryData:map[string][]byte{},Immutable:nil,}`,
          }
--- FAIL: TestCtestWriteClientCAsEdge (0.00s)
    --- PASS: TestCtestWriteClientCAsEdge/nil_clusterAuthInfo_(no_data) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/empty_CA_provider_(zero_length) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/extremely_long_header_slice_(edge_size) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/requestheader_UID_without_feature_gate_(should_be_dropped) (0.00s)
    --- FAIL: TestCtestWriteClientCAsEdge/requestheader_UID_with_feature_gate_(should_be_added) (0.00s)
FAIL
coverage: 33.7% of statements
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/clusterauthenticationtrust	1.645s
=== RUN   TestCtestHandleVersionUpdate
[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/crdregistration/ctest_crdregistration_controller_test.go:24]: Start test TestCtestHandleVersionUpdate
[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/crdregistration/ctest_crdregistration_controller_test.go:31]: get default configs: {test_fixture.json [starting crds] spec.versions [customresourcedefinitions] [&CustomResourceDefinition{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:CustomResourceDefinitionSpec{Group:group.com,Names:CustomResourceDefinitionNames{Plural:,Singular:,ShortNames:[],Kind:,ListKind:,Categories:[],},Scope:,Versions:[]CustomResourceDefinitionVersion{CustomResourceDefinitionVersion{Name:v1,Served:true,Storage:true,Schema:nil,Subresources:nil,AdditionalPrinterColumns:[]CustomResourceColumnDefinition{},Deprecated:false,DeprecationWarning:nil,SelectableFields:[]SelectableField{},},},Conversion:nil,PreserveUnknownFields:false,},Status:CustomResourceDefinitionStatus{Conditions:[]CustomResourceDefinitionCondition{},AcceptedNames:CustomResourceDefinitionNames{Plural:,Singular:,ShortNames:[],Kind:,ListKind:,Categories:[],},StoredVersions:[],},}]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:44:21 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:44:21 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/crdregistration	1.199s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/controlplane/controller/defaultservicecidr	0.701s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreateOrUpdateMasterService
=== RUN   TestCtestCreateOrUpdateMasterService/service_does_not_exist
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:72]: Running create test: service does not exist
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:44:24 Mode: 1
2026/02/09 18:44:24 Base JSON size: 170 bytes
2026/02/09 18:44:24 Number of external values: 23
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → tcp
2026/02/09 18:44:24   [OVERRIDE] ports[0].protocol: TCP → TCP
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 5432
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 5432
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 6379
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 6379
2026/02/09 18:44:24   [KEEP] ports[0].name: foo (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [KEEP] type: ClusterIP (missing in external)
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 3306
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 3306
2026/02/09 18:44:24   [KEEP] ports[0].name: foo (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [KEEP] type: ClusterIP (missing in external)
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [KEEP] type: ClusterIP (missing in external)
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 3100
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 3100
2026/02/09 18:44:24   [KEEP] ports[0].name: foo (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].protocol: TCP → TCP
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → db-service
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 5432
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 5432
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → LoadBalancer
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 80
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → LoadBalancer
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → tcp
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 5432
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 5432
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8848
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8848
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].name: foo (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 3306
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 3306
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [KEEP] type: ClusterIP (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].name: foo (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 9090
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → NodePort
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → redis-service
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 6379
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 6379
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8081
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 80
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → result-service
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → NodePort
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → NodePort
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → web
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 4444
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 4444
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 4444
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 4444
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → web
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → NodePort
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → NodePort
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 4444
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 4444
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → web
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → http
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 8080
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → ClusterIP
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ipFamilyPolicy: SingleStack (missing in external)
2026/02/09 18:44:24   [KEEP] ports[0].protocol: TCP (missing in external)
2026/02/09 18:44:24   [OVERRIDE] ports[0].port: 8080 → 8080
2026/02/09 18:44:24   [OVERRIDE] ports[0].targetPort: 8080 → 80
2026/02/09 18:44:24   [OVERRIDE] ports[0].name: foo → vote-service
2026/02/09 18:44:24   [KEEP] clusterIP: 1.2.3.4 (missing in external)
2026/02/09 18:44:24   [OVERRIDE] type: ClusterIP → NodePort
2026/02/09 18:44:24   [KEEP] sessionAffinity: None (missing in external)
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=23)
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"clusterIP":"1.2.3.4","ipFamilyPolicy":"SingleStack","ports":[{"name":"foo","port":8080,"protocol":"TCP","targetPort":8080}],"sessionAffinity":"None","type":"ClusterIP"})[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000491760 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000491760 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 2
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000491a50 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000491a50 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 3
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140001123a0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140001123a0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 4
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000113820 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000113820 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 5
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000113ac0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000113ac0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 6
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 3100 {0 3100 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000113d60 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 3100 {0 3100 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000113d60 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 7
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{db-service TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000680280 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{db-service TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000680280 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 8
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006806a0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006806a0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 9
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x140006809c0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x140006809c0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 10
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 80 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x14000680c90 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 80 {0 8080 } 0}] map[] 1.2.3.4 [] LoadBalancer [] None  []   0 false nil [] 0x14000680c90 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 11
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000680f60 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{tcp TCP <nil> 5432 {0 5432 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000680f60 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 12
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000681270 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000681270 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 13
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8848 {0 8848 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000681550 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8848 {0 8848 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x14000681550 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 14
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006817f0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 3306 {0 3306 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x140006817f0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 15
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{foo TCP <nil> 8080 {0 9090 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x14000681a90 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{foo TCP <nil> 8080 {0 9090 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x14000681a90 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 16
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{redis-service TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400064e160 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{redis-service TCP <nil> 6379 {0 6379 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400064e160 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 17
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{result-service TCP <nil> 8081 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064e410 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{result-service TCP <nil> 8081 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064e410 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 18
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064e6d0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064e6d0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 19
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064e970 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064e970 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 20
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064ec10 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{web TCP <nil> 4444 {0 4444 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064ec10 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 21
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400064eeb0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400064eeb0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 22
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400064f150 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{http TCP <nil> 8080 {0 8080 } 0}] map[] 1.2.3.4 [] ClusterIP [] None  []   0 false nil [] 0x1400064f150 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 23
2026/02/09 18:44:24 [DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.ServiceSpec={[{vote-service TCP <nil> 8080 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064f3f0 <nil> <nil> <nil> <nil>})
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{vote-service TCP <nil> 8080 {0 80 } 0}] map[] 1.2.3.4 [] NodePort [] None  []   0 false nil [] 0x1400064f3f0 <nil> <nil> <nil> <nil>}
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 23
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:88]: New Json Test Configs: [{"ports":[{"name":"tcp","protocol":"TCP","port":5432,"targetPort":5432}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":6379,"targetPort":6379}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":3306,"targetPort":3306}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":3100,"targetPort":3100}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"db-service","protocol":"TCP","port":5432,"targetPort":5432}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"LoadBalancer","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":80,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"LoadBalancer","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"tcp","protocol":"TCP","port":5432,"targetPort":5432}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8848,"targetPort":8848}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":3306,"targetPort":3306}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"foo","protocol":"TCP","port":8080,"targetPort":9090}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"redis-service","protocol":"TCP","port":6379,"targetPort":6379}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"result-service","protocol":"TCP","port":8081,"targetPort":80}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"web","protocol":"TCP","port":4444,"targetPort":4444}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"web","protocol":"TCP","port":4444,"targetPort":4444}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"web","protocol":"TCP","port":4444,"targetPort":4444}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"http","protocol":"TCP","port":8080,"targetPort":8080}],"clusterIP":"1.2.3.4","type":"ClusterIP","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"},{"ports":[{"name":"vote-service","protocol":"TCP","port":8080,"targetPort":80}],"clusterIP":"1.2.3.4","type":"NodePort","sessionAffinity":"None","ipFamilyPolicy":"SingleStack"}]
    ctest_controller_test.go:112: case "service does not exist": expected create spec:
        v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"tcp", Protocol:"TCP", AppProtocol:(*string)(nil), Port:5432, TargetPort:intstr.IntOrString{Type:0, IntVal:5432, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(0x14000491760), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}
        got:
        v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"foo", Protocol:"TCP", AppProtocol:(*string)(nil), Port:8080, TargetPort:intstr.IntOrString{Type:0, IntVal:8080, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(0x1400064f7b0), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}
=== RUN   TestCtestCreateOrUpdateMasterService/empty_service_ports_(edge_case)
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:72]: Running create test: empty service ports (edge case)
    ctest_controller_test.go:116: case "empty service ports (edge case)": no create expected, yet saw: [{{default create /v1, Resource=services }  &Service{ObjectMeta:{empty-ports  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[component:apiserver provider:kubernetes] map[] [] [] []},Spec:ServiceSpec{Ports:[]ServicePort{},Selector:map[string]string{},ClusterIP:1.2.3.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[],IPFamilies:[],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,TrafficDistribution:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},} {{ } []  }}]
=== RUN   TestCtestCreateOrUpdateMasterService/service_definition_wrong_port
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:199]: Running reconcile test: service definition wrong port
  W0209 18:44:24.611938   87159 controller.go:218] Resetting master service "foo" to &v1.Service{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"foo", Protocol:"TCP", AppProtocol:(*string)(nil), Port:8080, TargetPort:intstr.IntOrString{Type:0, IntVal:8080, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(nil), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}, Status:v1.ServiceStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}, Conditions:[]v1.Condition(nil)}}
=== RUN   TestCtestCreateOrUpdateMasterService/service_definition_missing_port_(edge_case)
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:199]: Running reconcile test: service definition missing port (edge case)
  W0209 18:44:24.612201   87159 controller.go:218] Resetting master service "foo" to &v1.Service{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.ServiceSpec{Ports:[]v1.ServicePort{v1.ServicePort{Name:"foo", Protocol:"TCP", AppProtocol:(*string)(nil), Port:8080, TargetPort:intstr.IntOrString{Type:0, IntVal:8080, StrVal:""}, NodePort:0}, v1.ServicePort{Name:"baz", Protocol:"TCP", AppProtocol:(*string)(nil), Port:1000, TargetPort:intstr.IntOrString{Type:0, IntVal:1000, StrVal:""}, NodePort:0}}, Selector:map[string]string(nil), ClusterIP:"1.2.3.4", ClusterIPs:[]string(nil), Type:"ClusterIP", ExternalIPs:[]string(nil), SessionAffinity:"None", LoadBalancerIP:"", LoadBalancerSourceRanges:[]string(nil), ExternalName:"", ExternalTrafficPolicy:"", HealthCheckNodePort:0, PublishNotReadyAddresses:false, SessionAffinityConfig:(*v1.SessionAffinityConfig)(nil), IPFamilies:[]v1.IPFamily(nil), IPFamilyPolicy:(*v1.IPFamilyPolicy)(nil), AllocateLoadBalancerNodePorts:(*bool)(nil), LoadBalancerClass:(*string)(nil), InternalTrafficPolicy:(*v1.ServiceInternalTrafficPolicy)(nil), TrafficDistribution:(*string)(nil)}, Status:v1.ServiceStatus{LoadBalancer:v1.LoadBalancerStatus{Ingress:[]v1.LoadBalancerIngress(nil)}, Conditions:[]v1.Condition(nil)}}
=== RUN   TestCtestCreateOrUpdateMasterService/service_definition_wrong_port,_no_expected_update
[DEBUG-CTEST 2026-02-09 18:44:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/kubernetesservice/ctest_controller_test.go:267]: Running non‑reconcile test: service definition wrong port, no expected update
--- FAIL: TestCtestCreateOrUpdateMasterService (0.01s)
    --- FAIL: TestCtestCreateOrUpdateMasterService/service_does_not_exist (0.01s)
    --- FAIL: TestCtestCreateOrUpdateMasterService/empty_service_ports_(edge_case) (0.00s)
    --- PASS: TestCtestCreateOrUpdateMasterService/service_definition_wrong_port (0.00s)
    --- PASS: TestCtestCreateOrUpdateMasterService/service_definition_missing_port_(edge_case) (0.00s)
    --- PASS: TestCtestCreateOrUpdateMasterService/service_definition_wrong_port,_no_expected_update (0.00s)
FAIL
coverage: 28.4% of statements
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/kubernetesservice	3.685s
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/empty
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/single_candidate
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_different_emulation_versions
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_different_binary_versions
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_same_binary_versions,_different_creation_timestamps
=== RUN   TestCtestPickBestLeaderOldestEmulationVersion/nil_candidate_in_slice
--- FAIL: TestCtestPickBestLeaderOldestEmulationVersion (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/empty (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/single_candidate (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_different_emulation_versions (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_different_binary_versions (0.00s)
    --- PASS: TestCtestPickBestLeaderOldestEmulationVersion/multiple_candidates,_same_emulation_versions,_same_binary_versions,_different_creation_timestamps (0.00s)
    --- FAIL: TestCtestPickBestLeaderOldestEmulationVersion/nil_candidate_in_slice (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x138 pc=0x10382c7a0]

goroutine 13 [running]:
testing.tRunner.func1.2({0x103b97620, 0x10464d690})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x103b97620?, 0x10464d690?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/controlplane/controller/leaderelection.validLeaseCandidateForOldestEmulationVersion(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/leaderelection/election.go:66 +0x90
k8s.io/kubernetes/pkg/controlplane/controller/leaderelection.pickBestLeaderOldestEmulationVersion({0x140002c6ec0, 0x2, 0x140000abf38?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/leaderelection/election.go:32 +0x110
k8s.io/kubernetes/pkg/controlplane/controller/leaderelection.TestCtestPickBestLeaderOldestEmulationVersion.func1(0x140003b9c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/leaderelection/ctest_election_test.go:208 +0x30
testing.tRunner(0x140003b9c00, 0x1400043a500)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 7
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/leaderelection	2.077s
=== RUN   TestCtestSyncConfigMap

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:44:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:44:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:44:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:44:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:44:27 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/legacytokentracking	0.906s
=== RUN   TestCtest_Controller

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:44:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/systemnamespaces/ctest_system_namespaces_controller_test.go:31]: get default configs: {test_fixture.json [system namespaces list] systemNamespaces [namespaces] [kube-system kube-public kube-node-lease default]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[namespaces]
[DEBUG-CTEST 2026-02-09 18:44:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[namespaces], int=1)[DEBUG-CTEST 2026-02-09 18:44:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:44:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "systemNamespaces" in requested fixtures
[DEBUG-CTEST 2026-02-09 18:44:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/controller/systemnamespaces/ctest_system_namespaces_controller_test.go:36]: Failed to generate config: %v mode-combination failed: no values found for field "systemNamespaces" in requested fixtures
    ctest_system_namespaces_controller_test.go:37: config generation error: mode-combination failed: no values found for field "systemNamespaces" in requested fixtures
--- FAIL: TestCtest_Controller (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/controlplane/controller/systemnamespaces	1.145s
=== RUN   TestCtestEndpointsAdapterGet
=== RUN   TestCtestEndpointsAdapterGet/wrong-name
=== RUN   TestCtestEndpointsAdapterGet/empty-namespace
=== RUN   TestCtestEndpointsAdapterGet/empty-name
=== RUN   TestCtestEndpointsAdapterGet/nil-initial-state
=== RUN   TestCtestEndpointsAdapterGet/single-existing-endpoints
=== RUN   TestCtestEndpointsAdapterGet/endpoints_exists,_endpointslice_does_not
=== RUN   TestCtestEndpointsAdapterGet/endpointslice_exists,_endpoints_does_not
=== RUN   TestCtestEndpointsAdapterGet/wrong-namespace
--- PASS: TestCtestEndpointsAdapterGet (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/wrong-name (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/empty-namespace (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/empty-name (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/nil-initial-state (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/single-existing-endpoints (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/endpoints_exists,_endpointslice_does_not (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/endpointslice_exists,_endpoints_does_not (0.00s)
    --- PASS: TestCtestEndpointsAdapterGet/wrong-namespace (0.00s)
=== RUN   TestCtestEndpointsAdapterCreate
=== RUN   TestCtestEndpointsAdapterCreate/single-endpoint-full-ipv6
=== RUN   TestCtestEndpointsAdapterCreate/existing-endpoints
=== RUN   TestCtestEndpointsAdapterCreate/existing-endpointslice-incorrect
=== RUN   TestCtestEndpointsAdapterCreate/single-endpoint
=== RUN   TestCtestEndpointsAdapterCreate/existing-endpointslice-correct
=== RUN   TestCtestEndpointsAdapterCreate/nil-endpoints-param
    ctest_endpointsadapter_test.go:205: Expected error: endpoints parameter is nil, got: object does not implement the Object interfaces
    ctest_endpointsadapter_test.go:211: unexpected error in side effects: expected 0 creates got 1
=== RUN   TestCtestEndpointsAdapterCreate/empty-namespace-param
    ctest_endpointsadapter_test.go:205: Expected error: namespace parameter is empty, got: request namespace does not match object namespace, request: "" object: "testing"
    ctest_endpointsadapter_test.go:211: unexpected error in side effects: [expected 0 creates got 1, expected create 0 to be:
        <nil>
        got:
        &v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"testing", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"endpointslice.kubernetes.io/skip-mirror":"true"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Subsets:[]v1.EndpointSubset{v1.EndpointSubset{Addresses:[]v1.EndpointAddress{v1.EndpointAddress{IP:"10.1.2.3", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x1400036ca80)}, v1.EndpointAddress{IP:"10.1.2.4", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x1400036caf0)}}, NotReadyAddresses:[]v1.EndpointAddress(nil), Ports:[]v1.EndpointPort{v1.EndpointPort{Name:"port-0", Port:80, Protocol:"TCP", AppProtocol:(*string)(nil)}}}}}
        ]
=== RUN   TestCtestEndpointsAdapterCreate/empty-endpoints-subsets
    ctest_endpointsadapter_test.go:211: unexpected error in side effects: [expected 1 creates got 2, expected create 1 to be:
        <nil>
        got:
        &v1.EndpointSlice{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"empty", GenerateName:"", Namespace:"testing", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"kubernetes.io/service-name":"empty"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, AddressType:"IPv4", Endpoints:[]v1.Endpoint(nil), Ports:[]v1.EndpointPort(nil)}
        ]
=== RUN   TestCtestEndpointsAdapterCreate/single-endpoint-partial-ipv6
--- FAIL: TestCtestEndpointsAdapterCreate (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/single-endpoint-full-ipv6 (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/existing-endpoints (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/existing-endpointslice-incorrect (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/single-endpoint (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/existing-endpointslice-correct (0.00s)
    --- FAIL: TestCtestEndpointsAdapterCreate/nil-endpoints-param (0.00s)
    --- FAIL: TestCtestEndpointsAdapterCreate/empty-namespace-param (0.00s)
    --- FAIL: TestCtestEndpointsAdapterCreate/empty-endpoints-subsets (0.00s)
    --- PASS: TestCtestEndpointsAdapterCreate/single-endpoint-partial-ipv6 (0.00s)
=== RUN   TestCtestEndpointsAdapterUpdate
=== RUN   TestCtestEndpointsAdapterUpdate/missing-endpointslice
=== RUN   TestCtestEndpointsAdapterUpdate/nil-endpoints-param
    ctest_endpointsadapter_test.go:335: Expected error: endpoints parameter is nil, got: object does not implement the Object interfaces
    ctest_endpointsadapter_test.go:341: unexpected error in side effects: expected 0 updates got 1
=== RUN   TestCtestEndpointsAdapterUpdate/single-existing-endpoints-no-change
=== RUN   TestCtestEndpointsAdapterUpdate/endpoints-correct-endpointslice-wrong
=== RUN   TestCtestEndpointsAdapterUpdate/endpointslice-correct-endpoints-wrong
=== RUN   TestCtestEndpointsAdapterUpdate/missing-endpoints
=== RUN   TestCtestEndpointsAdapterUpdate/empty-namespace
    ctest_endpointsadapter_test.go:335: Expected error: namespace parameter is empty, got: request namespace does not match object namespace, request: "" object: "testing"
    ctest_endpointsadapter_test.go:341: unexpected error in side effects: [expected 0 updates got 1, expected update 0 to be:
        <nil>
        got:
        &v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"foo", GenerateName:"", Namespace:"testing", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"endpointslice.kubernetes.io/skip-mirror":"true"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Subsets:[]v1.EndpointSubset{v1.EndpointSubset{Addresses:[]v1.EndpointAddress{v1.EndpointAddress{IP:"10.1.2.3", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x14000545500)}, v1.EndpointAddress{IP:"10.1.2.4", Hostname:"", NodeName:(*string)(nil), TargetRef:(*v1.ObjectReference)(0x14000545570)}}, NotReadyAddresses:[]v1.EndpointAddress(nil), Ports:[]v1.EndpointPort{v1.EndpointPort{Name:"port-0", Port:80, Protocol:"TCP", AppProtocol:(*string)(nil)}}}}}
        ]
=== RUN   TestCtestEndpointsAdapterUpdate/empty-endpoints-subsets
    ctest_endpointsadapter_test.go:335: Expected error: <nil>, got: endpoints "empty" not found
    ctest_endpointsadapter_test.go:338: Expected endpoints: &Endpoints{ObjectMeta:{empty  testing    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Subsets:[]EndpointSubset{},}, got: &Endpoints{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Subsets:[]EndpointSubset{},}
=== RUN   TestCtestEndpointsAdapterUpdate/existing-endpointslice-replaced-with-updated-ipv4-address-type
=== RUN   TestCtestEndpointsAdapterUpdate/add-ports-and-ips
=== RUN   TestCtestEndpointsAdapterUpdate/wrong-endpoints
--- FAIL: TestCtestEndpointsAdapterUpdate (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/missing-endpointslice (0.00s)
    --- FAIL: TestCtestEndpointsAdapterUpdate/nil-endpoints-param (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/single-existing-endpoints-no-change (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/endpoints-correct-endpointslice-wrong (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/endpointslice-correct-endpoints-wrong (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/missing-endpoints (0.00s)
    --- FAIL: TestCtestEndpointsAdapterUpdate/empty-namespace (0.00s)
    --- FAIL: TestCtestEndpointsAdapterUpdate/empty-endpoints-subsets (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/existing-endpointslice-replaced-with-updated-ipv4-address-type (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/add-ports-and-ips (0.00s)
    --- PASS: TestCtestEndpointsAdapterUpdate/wrong-endpoints (0.00s)
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/empty-namespace
    ctest_endpointsadapter_test.go:415: Expected error: namespace parameter is empty, got: request namespace does not match object namespace, request: "" object: "testing"
    ctest_endpointsadapter_test.go:423: Expected Endpoint Slice: nil, got: &EndpointSlice{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Endpoints:[]Endpoint{},Ports:[]EndpointPort{},AddressType:,}
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/empty-endpoints-subsets
    ctest_endpointsadapter_test.go:423: Expected Endpoint Slice: &EndpointSlice{ObjectMeta:{empty  testing    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Endpoints:[]Endpoint{},Ports:[]EndpointPort{},AddressType:IPv4,}, got: &EndpointSlice{ObjectMeta:{empty  testing    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[kubernetes.io/service-name:empty] map[] [] [] []},Endpoints:[]Endpoint{},Ports:[]EndpointPort{},AddressType:IPv4,}
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-no-change
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-change
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/missing-endpointslice
=== RUN   TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/nil-endpoints-param
--- FAIL: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints (0.00s)
    --- FAIL: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/empty-namespace (0.00s)
    --- FAIL: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/empty-endpoints-subsets (0.00s)
    --- PASS: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-no-change (0.00s)
    --- PASS: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/existing-endpointslice-change (0.00s)
    --- PASS: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/missing-endpointslice (0.00s)
    --- FAIL: TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints/nil-endpoints-param (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x28 pc=0x102841514]

goroutine 247 [running]:
testing.tRunner.func1.2({0x1030859c0, 0x104f98890})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1030859c0?, 0x104f98890?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/controlplane/reconcilers.endpointSliceFromEndpoints(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/reconcilers/endpointsadapter.go:120 +0xa4
k8s.io/kubernetes/pkg/controlplane/reconcilers.(*EndpointsAdapter).EnsureEndpointSliceFromEndpoints(0x14000a5ded8, {0x1028b2c43, 0x7}, 0x103e994a6?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/reconcilers/endpointsadapter.go:84 +0xa4
k8s.io/kubernetes/pkg/controlplane/reconcilers.TestCtestEndpointManagerEnsureEndpointSliceFromEndpoints.func1(0x140002b8540)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/controlplane/reconcilers/ctest_endpointsadapter_test.go:413 +0x200
testing.tRunner(0x140002b8540, 0x1400041f090)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 241
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/controlplane/reconcilers	1.657s
?   	k8s.io/kubernetes/pkg/controlplane/storageversionhashdata	[no test files]
=== RUN   TestCtestURLsMatch
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:18]: Start TestCtestURLsMatch
    ctest_keyring_test.go:52: Expected match result of  and  to be false, but was true
    ctest_keyring_test.go:52: Expected match result of * and  to be false, but was true
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:56]: End TestCtestURLsMatch
--- FAIL: TestCtestURLsMatch (0.00s)
=== RUN   TestCtestDockerKeyringForGlob
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:60]: Start TestCtestDockerKeyringForGlob
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:122]: End TestCtestDockerKeyringForGlob
--- PASS: TestCtestDockerKeyringForGlob (0.00s)
=== RUN   TestCtestKeyringMiss
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:126]: Start TestCtestKeyringMiss
    ctest_keyring_test.go:162: Expected not to find URL , but found
    ctest_keyring_test.go:162: Expected not to find URL invalid, but found
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:165]: End TestCtestKeyringMiss
--- FAIL: TestCtestKeyringMiss (0.00s)
=== RUN   TestCtestKeyringMissWithDockerHubCredentials
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:169]: Start TestCtestKeyringMissWithDockerHubCredentials
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:193]: End TestCtestKeyringMissWithDockerHubCredentials
--- PASS: TestCtestKeyringMissWithDockerHubCredentials (0.00s)
=== RUN   TestCtestKeyringHitWithUnqualifiedDockerHub
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:197]: Start TestCtestKeyringHitWithUnqualifiedDockerHub
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:236]: End TestCtestKeyringHitWithUnqualifiedDockerHub
--- PASS: TestCtestKeyringHitWithUnqualifiedDockerHub (0.00s)
=== RUN   TestCtestKeyringHitWithUnqualifiedLibraryDockerHub
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:240]: Start TestCtestKeyringHitWithUnqualifiedLibraryDockerHub
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:279]: End TestCtestKeyringHitWithUnqualifiedLibraryDockerHub
--- PASS: TestCtestKeyringHitWithUnqualifiedLibraryDockerHub (0.00s)
=== RUN   TestCtestKeyringHitWithQualifiedDockerHub
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:283]: Start TestCtestKeyringHitWithQualifiedDockerHub
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:322]: End TestCtestKeyringHitWithQualifiedDockerHub
--- PASS: TestCtestKeyringHitWithQualifiedDockerHub (0.00s)
=== RUN   TestCtestIsDefaultRegistryMatch
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:326]: Start TestCtestIsDefaultRegistryMatch
    ctest_keyring_test.go:343: Expected 'registry.k8s.io:8080' to be false, got true
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:347]: End TestCtestIsDefaultRegistryMatch
--- FAIL: TestCtestIsDefaultRegistryMatch (0.00s)
=== RUN   TestCtestProvidersDockerKeyring
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:351]: Start TestCtestProvidersDockerKeyring
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:376]: End TestCtestProvidersDockerKeyring
--- PASS: TestCtestProvidersDockerKeyring (0.00s)
=== RUN   TestCtestDockerKeyringLookup
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:380]: Start TestCtestDockerKeyringLookup
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:435]: End TestCtestDockerKeyringLookup
--- PASS: TestCtestDockerKeyringLookup (0.00s)
=== RUN   TestCtestIssue3797
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:442]: Start TestCtestIssue3797
[DEBUG-CTEST 2026-02-09 18:44:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/ctest_keyring_test.go:481]: End TestCtestIssue3797
--- PASS: TestCtestIssue3797 (0.00s)
FAIL
coverage: 45.8% of statements
FAIL	k8s.io/kubernetes/pkg/credentialprovider	0.564s
=== RUN   TestCtestReadCredentialProviderConfig
=== RUN   TestCtestReadCredentialProviderConfig/empty_directory_with_no_JSON_or_YAML_files
=== RUN   TestCtestReadCredentialProviderConfig/directory_with_unsupported_file_extensions
=== RUN   TestCtestReadCredentialProviderConfig/config_with_1_plugin_and_1_image_matcher
=== RUN   TestCtestReadCredentialProviderConfig/directory_with_a_valid_config_and_an_empty_file
    ctest_config_test.go:173: error decoding config "/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/config-dir2136835551/config-001.yaml": Object 'Kind' is missing in ''
--- FAIL: TestCtestReadCredentialProviderConfig (0.00s)
    --- PASS: TestCtestReadCredentialProviderConfig/empty_directory_with_no_JSON_or_YAML_files (0.00s)
    --- PASS: TestCtestReadCredentialProviderConfig/directory_with_unsupported_file_extensions (0.00s)
    --- PASS: TestCtestReadCredentialProviderConfig/config_with_1_plugin_and_1_image_matcher (0.00s)
    --- FAIL: TestCtestReadCredentialProviderConfig/directory_with_a_valid_config_and_an_empty_file (0.00s)
=== RUN   TestCtestValidateCredentialProviderConfig
=== RUN   TestCtestValidateCredentialProviderConfig/nil_config
--- FAIL: TestCtestValidateCredentialProviderConfig (0.00s)
    --- FAIL: TestCtestValidateCredentialProviderConfig/nil_config (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x28 pc=0x105febc0c]

goroutine 166 [running]:
testing.tRunner.func1.2({0x1067834c0, 0x1083aa850})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1067834c0?, 0x1083aa850?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/credentialprovider/plugin.validateCredentialProviderConfig(0x0, 0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/plugin/config.go:149 +0x9c
k8s.io/kubernetes/pkg/credentialprovider/plugin.TestCtestValidateCredentialProviderConfig.func1(0x14000495180)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/plugin/ctest_config_test.go:390 +0x2c
testing.tRunner(0x14000495180, 0x140003f32c0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 165
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/credentialprovider/plugin	2.948s
=== RUN   TestCtestMakeDockerKeyring
[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/secrets/ctest_secrets_test.go:281]: Start TestCtestMakeDockerKeyring
Running 0 th test case: with .dockerconfigjson and auth field
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[secrets]
[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[secrets], int=1)[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "type" in requested fixtures
2026/02/09 18:44:34 [DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:44:34 Mode: 1
2026/02/09 18:44:34 Base JSON size: 213 bytes
2026/02/09 18:44:34 Number of external values: 0
2026/02/09 18:44:34 [DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:44:34 [DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"data":{".dockerconfigjson":"eyJhdXRocyI6eyJ0ZXN0LnJlZ2lzdHJ5LmlvIjp7ImF1dGgiOiJkWE5sY2pwd1lYTnpkMjl5WkE9PSJ9fX0="},"metadata":{"name":"s1","namespace":"ns1","uid":"uid1"},"type":"kubernetes.io/dockerconfigjson"})[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:44:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/secrets/ctest_secrets_test.go:302]: No config objects generated for with .dockerconfigjson and auth field
--- FAIL: TestCtestMakeDockerKeyring (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x18 pc=0x103f1b61c]

goroutine 15 [running]:
testing.tRunner.func1.2({0x104646f40, 0x106076310})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x104646f40?, 0x106076310?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/credentialprovider/secrets.TestCtestMakeDockerKeyring(0x140004a3dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/credentialprovider/secrets/ctest_secrets_test.go:311 +0xfcc
testing.tRunner(0x140004a3dc0, 0x104a57ab0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/credentialprovider/secrets	2.392s
testing: warning: no tests to run
PASS
coverage: 72.2% of statements
ok  	k8s.io/kubernetes/pkg/features	1.631s	coverage: 72.2% of statements [no tests to run]
=== RUN   TestCtestExtractFieldPathAsString
--- FAIL: TestCtestExtractFieldPathAsString (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x20 pc=0x1012b6a00]

goroutine 5 [running]:
testing.tRunner.func1.2({0x10142d3c0, 0x1017b9550})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10142d3c0?, 0x1017b9550?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/api/core/v1.(*Pod).GetName(0x1013021a0?)
	<autogenerated>:1
k8s.io/kubernetes/pkg/fieldpath.ExtractFieldPathAsString({0x1014cf7c0?, 0x0?}, {0x1013021a0, 0xd})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/fieldpath/fieldpath.go:85 +0x760
k8s.io/kubernetes/pkg/fieldpath.TestCtestExtractFieldPathAsString(0x14000003dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/fieldpath/ctest_fieldpath_test.go:167 +0x644
testing.tRunner(0x14000003dc0, 0x1014d8a70)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/fieldpath	1.869s
?   	k8s.io/kubernetes/pkg/generated	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/generated/openapi	0.718s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/generated/openapi/cmd/models-schema		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/admission		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/admission/exclusion		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/authenticator		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubeapiserver/authorizer		coverage: 0.0% of statements
=== RUN   TestCtestIsValidAuthorizationMode

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:44:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubeapiserver/authorizer/modes/ctest_modes_test.go:34]: Number of test cases: 14
Running 0 th test case: authzMode="" expected=false
Running 1 th test case: authzMode="rBAC" expected=false
Running 2 th test case: authzMode="falsy value" expected=false
Running 3 th test case: authzMode="RBAC" expected=true
Running 4 th test case: authzMode="ABAC" expected=true
Running 5 th test case: authzMode="Webhook" expected=true
Running 6 th test case: authzMode="AlwaysAllow" expected=true
Running 7 th test case: authzMode="AlwaysDeny" expected=true
Running 8 th test case: authzMode=" " expected=false
Running 9 th test case: authzMode="rbac" expected=false
Running 10 th test case: authzMode="RBAC " expected=false
Running 11 th test case: authzMode="RBAC\n" expected=false
Running 12 th test case: authzMode="\x00" expected=false
Running 13 th test case: authzMode="\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" expected=false

==================== CTEST END ======================
--- PASS: TestCtestIsValidAuthorizationMode (0.00s)
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/kubeapiserver/authorizer/modes	0.736s	coverage: 100.0% of statements
=== RUN   TestCtestComputeEnabledAdmission
=== RUN   TestCtestComputeEnabledAdmission/matches
=== RUN   TestCtestComputeEnabledAdmission/choose_one
=== RUN   TestCtestComputeEnabledAdmission/enabled_not_in_all
    ctest_admission_test.go:69: expected enabled [], got [three]
=== RUN   TestCtestComputeEnabledAdmission/empty_all_slice
    ctest_admission_test.go:69: expected enabled [], got [one]
=== RUN   TestCtestComputeEnabledAdmission/nil_enabled_slice
    ctest_admission_test.go:69: expected enabled [], got []
=== RUN   TestCtestComputeEnabledAdmission/duplicate_entries_in_all
--- FAIL: TestCtestComputeEnabledAdmission (0.00s)
    --- PASS: TestCtestComputeEnabledAdmission/matches (0.00s)
    --- PASS: TestCtestComputeEnabledAdmission/choose_one (0.00s)
    --- FAIL: TestCtestComputeEnabledAdmission/enabled_not_in_all (0.00s)
    --- FAIL: TestCtestComputeEnabledAdmission/empty_all_slice (0.00s)
    --- FAIL: TestCtestComputeEnabledAdmission/nil_enabled_slice (0.00s)
    --- PASS: TestCtestComputeEnabledAdmission/duplicate_entries_in_all (0.00s)
=== RUN   TestCtestAdmissionOptionsAddFlags
--- PASS: TestCtestAdmissionOptionsAddFlags (0.00s)
=== RUN   TestCtestAuthzValidate
=== RUN   TestCtestAuthzValidate/Unknown_modes_should_return_errors
=== RUN   TestCtestAuthzValidate/At_least_one_authorizationMode_is_necessary
=== RUN   TestCtestAuthzValidate/ModeAlwaysAllow_specified_more_than_once
=== RUN   TestCtestAuthzValidate/ModeAlwaysAllow_and_ModeAlwaysDeny_should_return_without_authorizationPolicyFile
=== RUN   TestCtestAuthzValidate/ModeABAC_requires_a_policy_file
=== RUN   TestCtestAuthzValidate/Authorization_Policy_file_cannot_be_used_without_ModeABAC
=== RUN   TestCtestAuthzValidate/ModeABAC_should_not_error_if_a_valid_policy_path_is_provided
=== RUN   TestCtestAuthzValidate/ModeWebhook_requires_a_config_file
=== RUN   TestCtestAuthzValidate/Cannot_provide_webhook_config_file_without_ModeWebhook
=== RUN   TestCtestAuthzValidate/ModeWebhook_should_not_error_if_a_valid_config_file_is_provided
=== RUN   TestCtestAuthzValidate/ModeWebhook_should_error_if_an_invalid_number_of_webhook_retry_attempts_is_provided
=== RUN   TestCtestAuthzValidate/Empty_string_mode_should_be_rejected
=== RUN   TestCtestAuthzValidate/Duplicate_mode_with_extra_whitespace
    ctest_authorization_test.go:186: expected to find error: has mode specified more than once, but got: authorization-mode " AlwaysAllow " is not a valid mode
=== RUN   TestCtestAuthzValidate/Nil_webhook_retry_backoff_with_ModeWebhook_should_error
    ctest_authorization_test.go:182: should return an error
=== RUN   TestCtestAuthzValidate/Negative_steps_in_webhook_retry_backoff
=== RUN   TestCtestAuthzValidate/Zero_duration_but_positive_steps_in_webhook_retry_backoff
    ctest_authorization_test.go:182: should return an error
=== RUN   TestCtestAuthzValidate/PolicyFile_path_does_not_exist_(simulated)
    ctest_authorization_test.go:182: should return an error
=== RUN   TestCtestAuthzValidate/WebhookConfigFile_empty_string_with_ModeWebhook
--- FAIL: TestCtestAuthzValidate (0.00s)
    --- PASS: TestCtestAuthzValidate/Unknown_modes_should_return_errors (0.00s)
    --- PASS: TestCtestAuthzValidate/At_least_one_authorizationMode_is_necessary (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeAlwaysAllow_specified_more_than_once (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeAlwaysAllow_and_ModeAlwaysDeny_should_return_without_authorizationPolicyFile (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeABAC_requires_a_policy_file (0.00s)
    --- PASS: TestCtestAuthzValidate/Authorization_Policy_file_cannot_be_used_without_ModeABAC (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeABAC_should_not_error_if_a_valid_policy_path_is_provided (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeWebhook_requires_a_config_file (0.00s)
    --- PASS: TestCtestAuthzValidate/Cannot_provide_webhook_config_file_without_ModeWebhook (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeWebhook_should_not_error_if_a_valid_config_file_is_provided (0.00s)
    --- PASS: TestCtestAuthzValidate/ModeWebhook_should_error_if_an_invalid_number_of_webhook_retry_attempts_is_provided (0.00s)
    --- PASS: TestCtestAuthzValidate/Empty_string_mode_should_be_rejected (0.00s)
    --- FAIL: TestCtestAuthzValidate/Duplicate_mode_with_extra_whitespace (0.00s)
    --- FAIL: TestCtestAuthzValidate/Nil_webhook_retry_backoff_with_ModeWebhook_should_error (0.00s)
    --- PASS: TestCtestAuthzValidate/Negative_steps_in_webhook_retry_backoff (0.00s)
    --- FAIL: TestCtestAuthzValidate/Zero_duration_but_positive_steps_in_webhook_retry_backoff (0.00s)
    --- FAIL: TestCtestAuthzValidate/PolicyFile_path_does_not_exist_(simulated) (0.00s)
    --- PASS: TestCtestAuthzValidate/WebhookConfigFile_empty_string_with_ModeWebhook (0.00s)
FAIL
coverage: 15.8% of statements
FAIL	k8s.io/kubernetes/pkg/kubeapiserver/options	0.623s
?   	k8s.io/kubernetes/pkg/kubectl	[no test files]
=== RUN   TestCtestConvertObject

==================== CTEST START ====================
=== RUN   TestCtestConvertObject/apps_deployment_to_extensions_deployment_apiVersion:_extensions/v1beta1_(field_#0)
=== RUN   TestCtestConvertObject/extensions_deployment_to_apps_deployment_apiVersion:_apps/v1beta2_(field_#0)
=== RUN   TestCtestConvertObject/v1_HPA_to_v2beta1_HPA_apiVersion:_autoscaling/v2beta1_(field_#0)
=== RUN   TestCtestConvertObject/v1_HPA_to_v2beta1_HPA_name:_cpu_(field_#1)
=== RUN   TestCtestConvertObject/v1_HPA_to_v2beta1_HPA_targetAverageUtilization:_50_(field_#2)
=== RUN   TestCtestConvertObject/v2beta1_HPA_to_v1_HPA_apiVersion:_autoscaling/v1_(field_#0)
=== RUN   TestCtestConvertObject/v2beta1_HPA_to_v1_HPA_targetCPUUtilizationPercentage:_50_(field_#1)
=== RUN   TestCtestConvertObject/v1beta1_Ingress_to_extensions_Ingress_apiVersion:_extensions/v1beta1_(field_#0)
=== RUN   TestCtestConvertObject/converting_multiple_including_service_to_neworking.k8s.io/v1_apiVersion:_networking.k8s.io/v1_(field_#0)
=== RUN   TestCtestConvertObject/empty_output_version_(should_fallback_to_original)__(field_#0)
=== RUN   TestCtestConvertObject/non‑existent_file_path__(field_#0)
error: the path "nonexistent/path.yaml" does not exist
FAIL	k8s.io/kubernetes/pkg/kubectl/cmd/convert	0.711s
=== RUN   TestCtestNewActiveDeadlineHandler

==================== CTEST START ====================
    ctest_active_deadline_test.go:60: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/ctest_active_deadline_test.go:60
        	Error:      	An error is expected but got nil.
        	Test:       	TestCtestNewActiveDeadlineHandler

==================== CTEST END ======================
--- FAIL: TestCtestNewActiveDeadlineHandler (0.00s)
=== RUN   TestCtestActiveDeadlineHandler

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/ctest_active_deadline_test.go:76]: get default configs: {test_fixture.json [default pod spec] activeDeadlineSeconds [pods deployments statefulsets daemonsets replicasets] {[] [] [] []  <nil> 0x14000988840  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:45:03 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/kubelet	1.688s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/allocation	0.728s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtest_stateCheckpoint_storeState
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.1
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_0.03
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_100
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_512
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1000
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_1024
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700Ei
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700E
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_700
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Ki
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Mi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Gi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Ti
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000Pi
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000n
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000u
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000m
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000k
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000M
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000G
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000T
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000P
=== RUN   TestCtest_stateCheckpoint_storeState/resource_-_10000
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0Ki
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0.0
=== RUN   TestCtest_stateCheckpoint_storeState/edge_-_0.0Ki
--- PASS: TestCtest_stateCheckpoint_storeState (1.29s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Ki (0.03s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Mi (0.02s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1G (0.03s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1u (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1G (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1E (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.1 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Ti (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03P (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_0.03 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_100 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_512 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1000 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_1024 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700Ei (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700n (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700E (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_700 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Mi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Gi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Ti (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000Pi (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000n (0.00s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000u (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000m (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000k (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000M (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000G (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000T (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000P (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/resource_-_10000 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0Ki (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0.0 (0.01s)
    --- PASS: TestCtest_stateCheckpoint_storeState/edge_-_0.0Ki (0.01s)
PASS
coverage: 48.8% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/allocation/state	2.246s	coverage: 48.8% of statements
=== RUN   TestCtestComponentConfigSetup

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/config/ctest_register_test.go:28]: get default configs: {test_fixture.json [component config package verification] AllowedTags [] {kubelet kubelet.config.k8s.io kubelet.config.k8s.io/__internal 0x104fc5870 map[] map[v1.NodeConfigSource:true v1.TracingConfiguration:true v1.TypeMeta:true v1.Taint:true v1.LoggingConfiguration:true v1.Duration:true v1.Time:true] map[]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:312]: failed to marshal HardcodedConfig to JSON
    ctest_register_test.go:33: failed to generate config objects: failed to marshal HardcodedConfig to JSON: json: unsupported type: func(*runtime.Scheme) error
--- FAIL: TestCtestComponentConfigSetup (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/apis/config	2.400s
	k8s.io/kubernetes/pkg/kubelet/apis/config/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/apis/config/scheme	2.971s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/apis/config/v1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/apis/config/v1alpha1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.3% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/apis/config/v1beta1	0.579s	coverage: 0.3% of statements [no tests to run]
=== RUN   TestCtestValidateReservedMemoryConfiguration
--- FAIL: TestCtestValidateReservedMemoryConfiguration (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x560 pc=0x103c50bc4]

goroutine 70 [running]:
testing.tRunner.func1.2({0x1047dd200, 0x106282ba0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1047dd200?, 0x106282ba0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/apis/config/validation.validateReservedMemoryConfiguration(0x14000951208?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/config/validation/validation_reserved_memory.go:29 +0x94
k8s.io/kubernetes/pkg/kubelet/apis/config/validation.TestCtestValidateReservedMemoryConfiguration(0x14000386fc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/config/validation/ctest_validation_reserved_memory_test.go:172 +0xe38
testing.tRunner(0x14000386fc0, 0x104c09f70)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/apis/config/validation	0.710s
	k8s.io/kubernetes/pkg/kubelet/apis/grpc		coverage: 0.0% of statements
=== RUN   TestCtestListPodResourcesV1alpha1
[DEBUG-CTEST 2026-02-09 18:45:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/ctest_server_v1alpha1_test.go:20]: Start TestCtestListPodResourcesV1alpha1
Running test case #0: no pods
=== RUN   TestCtestListPodResourcesV1alpha1/no_pods
Running test case #1: pod without devices
=== RUN   TestCtestListPodResourcesV1alpha1/pod_without_devices
Running test case #2: pod with devices
=== RUN   TestCtestListPodResourcesV1alpha1/pod_with_devices
Running test case #3: pod with nil containers slice
=== RUN   TestCtestListPodResourcesV1alpha1/pod_with_nil_containers_slice
Running test case #4: pod with empty container name
=== RUN   TestCtestListPodResourcesV1alpha1/pod_with_empty_container_name
    mock.go:351: 
        
        mock: Unexpected Method Call
        -----------------------------
        
        GetDevices(string,string)
        		0: "pod-uid"
        		1: ""
        
        The closest call I have is: 
        
        GetDevices(string,string)
        		0: "pod-uid"
        		1: "container-name"
        
        
        Diff: 0: PASS:  (string=pod-uid) == (string=pod-uid)
        	1: FAIL:  (string=) != (string=container-name)
        at: [/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/testing/devices_provider.go:89 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/server_v1alpha1.go:74 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/ctest_server_v1alpha1_test.go:248]
Running test case #5: devices with empty resource name
=== RUN   TestCtestListPodResourcesV1alpha1/devices_with_empty_resource_name
Running test case #6: nil pods slice with devices
=== RUN   TestCtestListPodResourcesV1alpha1/nil_pods_slice_with_devices
[DEBUG-CTEST 2026-02-09 18:45:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/apis/podresources/ctest_server_v1alpha1_test.go:257]: End TestCtestListPodResourcesV1alpha1
--- FAIL: TestCtestListPodResourcesV1alpha1 (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/no_pods (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/pod_without_devices (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/pod_with_devices (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/pod_with_nil_containers_slice (0.00s)
    --- FAIL: TestCtestListPodResourcesV1alpha1/pod_with_empty_container_name (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/devices_with_empty_resource_name (0.00s)
    --- PASS: TestCtestListPodResourcesV1alpha1/nil_pods_slice_with_devices (0.00s)
FAIL
coverage: 18.8% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/apis/podresources	1.478s
	k8s.io/kubernetes/pkg/kubelet/apis/podresources/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/cadvisor		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/cadvisor/testing		coverage: 0.0% of statements
=== RUN   TestCtestAddressesToHostnamesAndIPs
=== RUN   TestCtestAddressesToHostnamesAndIPs/empty
=== RUN   TestCtestAddressesToHostnamesAndIPs/ignore_empty_values
=== RUN   TestCtestAddressesToHostnamesAndIPs/ignore_invalid_IPs
=== RUN   TestCtestAddressesToHostnamesAndIPs/dedupe_values
=== RUN   TestCtestAddressesToHostnamesAndIPs/order_values
=== RUN   TestCtestAddressesToHostnamesAndIPs/handle_IP_and_DNS_hostnames
=== RUN   TestCtestAddressesToHostnamesAndIPs/unknown_address_type
=== RUN   TestCtestAddressesToHostnamesAndIPs/whitespace_address
    ctest_kubelet_test.go:127: addressesToHostnamesAndIPs() gotDNSNames = [   ], want []
=== RUN   TestCtestAddressesToHostnamesAndIPs/duplicate_many_entries
--- FAIL: TestCtestAddressesToHostnamesAndIPs (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/empty (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/ignore_empty_values (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/ignore_invalid_IPs (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/dedupe_values (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/order_values (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/handle_IP_and_DNS_hostnames (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/unknown_address_type (0.00s)
    --- FAIL: TestCtestAddressesToHostnamesAndIPs/whitespace_address (0.00s)
    --- PASS: TestCtestAddressesToHostnamesAndIPs/duplicate_many_entries (0.00s)
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_enabled
W0209 18:45:08.782035   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_disabled
W0209 18:45:08.782308   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_enabled
W0209 18:45:08.782338   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_disabled
W0209 18:45:08.782377   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_enabled
W0209 18:45:08.782406   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_disabled
W0209 18:45:08.782439   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_enabled
W0209 18:45:08.782467   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_disabled
W0209 18:45:08.782496   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=false. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/unknown_address_type
W0209 18:45:08.782521   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/empty_address_with_valid_type
W0209 18:45:08.782546   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
=== RUN   TestCtestNewCertificateManagerConfigGetTemplate/malformed_IP_address
W0209 18:45:08.782573   87571 feature_gate.go:350] Setting deprecated feature gate AllowDNSOnlyNodeCSR=true. It will be removed in a future release.
--- PASS: TestCtestNewCertificateManagerConfigGetTemplate (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/node_addresses_or_hostnames_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_hostnames_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/only_IP_addresses_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_enabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/IP_addresses_and_hostnames_and_gate_disabled (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/unknown_address_type (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/empty_address_with_valid_type (0.00s)
    --- PASS: TestCtestNewCertificateManagerConfigGetTemplate/malformed_IP_address (0.00s)
FAIL
coverage: 19.7% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/certificate	0.977s
=== RUN   TestCtestLoadClientConfig
=== RUN   TestCtestLoadClientConfig/non‑existent_kubeconfig_path
=== RUN   TestCtestLoadClientConfig/bootstrap_path_is_non‑yaml_file
E0209 18:45:10.536837   87615 bootstrap.go:256] "Unhandled Error" err="unable to load TLS certificates from existing bootstrap client config read from /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestLoadClientConfig1762460210/001/valid-kubeconfig864899214: data does not contain any valid RSA or ECDSA certificates" logger="UnhandledError"
=== RUN   TestCtestLoadClientConfig/certDir_does_not_exist
    ctest_bootstrap_test.go:99: expected error but got nil
--- FAIL: TestCtestLoadClientConfig (0.00s)
    --- PASS: TestCtestLoadClientConfig/non‑existent_kubeconfig_path (0.00s)
    --- PASS: TestCtestLoadClientConfig/bootstrap_path_is_non‑yaml_file (0.00s)
    --- FAIL: TestCtestLoadClientConfig/certDir_does_not_exist (0.00s)
=== RUN   TestCtestLoadRESTClientConfig
=== RUN   TestCtestLoadRESTClientConfig/valid_config
=== RUN   TestCtestLoadRESTClientConfig/malformed_config
=== RUN   TestCtestLoadRESTClientConfig/non‑existent_file
--- PASS: TestCtestLoadRESTClientConfig (0.00s)
    --- PASS: TestCtestLoadRESTClientConfig/valid_config (0.00s)
    --- PASS: TestCtestLoadRESTClientConfig/malformed_config (0.00s)
    --- PASS: TestCtestLoadRESTClientConfig/non‑existent_file (0.00s)
=== RUN   TestCtestRequestNodeCertificate
    ctest_bootstrap_test.go:201: expected error when node name is empty
--- FAIL: TestCtestRequestNodeCertificate (0.00s)
FAIL
coverage: 38.6% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/certificate/bootstrap	0.518s
=== RUN   TestCtestCheckpointManager
    ctest_checkpoint_manager_test.go:80: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/checkpointmanager/ctest_checkpoint_manager_test.go:80
        	Error:      	Received unexpected error:
        	            	checkpoint is corrupted
        	Test:       	TestCtestCheckpointManager
    ctest_checkpoint_manager_test.go:83: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/checkpointmanager/ctest_checkpoint_manager_test.go:83
        	Error:      	Not equal: 
        	            	expected: []*checkpointmanager.PortMapping{}
        	            	actual  : []*checkpointmanager.PortMapping(nil)
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1,3 +1,2 @@
        	            	-([]*checkpointmanager.PortMapping) {
        	            	-}
        	            	+([]*checkpointmanager.PortMapping) <nil>
        	            	 
        	Test:       	TestCtestCheckpointManager
    ctest_checkpoint_manager_test.go:114: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/checkpointmanager/ctest_checkpoint_manager_test.go:114
        	Error:      	Error message not equal:
        	            	expected: "checkpoint is corrupted"
        	            	actual  : "invalid character '\\u0084' looking for beginning of value"
        	Test:       	TestCtestCheckpointManager
--- FAIL: TestCtestCheckpointManager (0.00s)
FAIL
coverage: 76.7% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/checkpointmanager	0.716s
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/checksum		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/errors		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/checkpointmanager/testing/example_checkpoint_formats/v1		coverage: 0.0% of statements
=== RUN   TestCtestMakeTransportInvalid
=== RUN   TestCtestMakeTransportInvalid/invalid_cert_files
=== RUN   TestCtestMakeTransportInvalid/empty_cert_fields
    ctest_kubelet_client_test.go:47: expected error but got none
    ctest_kubelet_client_test.go:50: expected nil transport on error
--- FAIL: TestCtestMakeTransportInvalid (0.00s)
    --- PASS: TestCtestMakeTransportInvalid/invalid_cert_files (0.00s)
    --- FAIL: TestCtestMakeTransportInvalid/empty_cert_fields (0.00s)
=== RUN   TestCtestMakeTransportValid
=== RUN   TestCtestMakeTransportValid/valid_config_with_explicit_port
=== RUN   TestCtestMakeTransportValid/valid_config_with_zero_port_(should_default)
--- PASS: TestCtestMakeTransportValid (0.00s)
    --- PASS: TestCtestMakeTransportValid/valid_config_with_explicit_port (0.00s)
    --- PASS: TestCtestMakeTransportValid/valid_config_with_zero_port_(should_default) (0.00s)
=== RUN   TestCtestMakeTransportWithLookUp
=== RUN   TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer
=== PAUSE TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer
=== RUN   TestCtestMakeTransportWithLookUp/lookup_returns_error
=== PAUSE TestCtestMakeTransportWithLookUp/lookup_returns_error
=== RUN   TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
=== PAUSE TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
=== RUN   TestCtestMakeTransportWithLookUp/no_lookup_closure_provided
=== PAUSE TestCtestMakeTransportWithLookUp/no_lookup_closure_provided
=== CONT  TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer
=== CONT  TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
=== CONT  TestCtestMakeTransportWithLookUp/no_lookup_closure_provided
=== CONT  TestCtestMakeTransportWithLookUp/lookup_returns_error
=== NAME  TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid)
    ctest_kubelet_client_test.go:161: expected error but got none
--- FAIL: TestCtestMakeTransportWithLookUp (0.00s)
    --- PASS: TestCtestMakeTransportWithLookUp/lookup_returns_error (0.00s)
    --- FAIL: TestCtestMakeTransportWithLookUp/lookup_nil_without_error_(invalid) (0.00s)
    --- PASS: TestCtestMakeTransportWithLookUp/lookup_returns_valid_dialer (0.01s)
    --- PASS: TestCtestMakeTransportWithLookUp/no_lookup_closure_provided (0.01s)
=== RUN   TestCtestMakeInsecureTransport
=== RUN   TestCtestMakeInsecureTransport/valid_insecure_transport
=== RUN   TestCtestMakeInsecureTransport/zero_port_(should_default)
--- PASS: TestCtestMakeInsecureTransport (0.00s)
    --- PASS: TestCtestMakeInsecureTransport/valid_insecure_transport (0.00s)
    --- PASS: TestCtestMakeInsecureTransport/zero_port_(should_default) (0.00s)
=== RUN   TestCtestNewNodeConnectionInfoGetter
=== RUN   TestCtestNewNodeConnectionInfoGetter/valid_config
=== PAUSE TestCtestNewNodeConnectionInfoGetter/valid_config
=== RUN   TestCtestNewNodeConnectionInfoGetter/invalid_cert_file
=== PAUSE TestCtestNewNodeConnectionInfoGetter/invalid_cert_file
=== RUN   TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types
=== PAUSE TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types
=== RUN   TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
=== PAUSE TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
=== RUN   TestCtestNewNodeConnectionInfoGetter/zero_port
=== PAUSE TestCtestNewNodeConnectionInfoGetter/zero_port
=== CONT  TestCtestNewNodeConnectionInfoGetter/valid_config
=== CONT  TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
=== CONT  TestCtestNewNodeConnectionInfoGetter/zero_port
=== CONT  TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types
=== CONT  TestCtestNewNodeConnectionInfoGetter/invalid_cert_file
=== NAME  TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter
    ctest_kubelet_client_test.go:300: expected error but got none
--- FAIL: TestCtestNewNodeConnectionInfoGetter (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/valid_config (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/zero_port (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/invalid_cert_file (0.00s)
    --- FAIL: TestCtestNewNodeConnectionInfoGetter/nil_nodes_getter (0.00s)
    --- PASS: TestCtestNewNodeConnectionInfoGetter/empty_preferred_address_types (0.00s)
=== RUN   TestCtestGetConnectionInfo
=== RUN   TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred
=== PAUSE TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred
=== RUN   TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal)
=== PAUSE TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal)
=== RUN   TestCtestGetConnectionInfo/node_not_found
=== PAUSE TestCtestGetConnectionInfo/node_not_found
=== RUN   TestCtestGetConnectionInfo/node_with_no_addresses
=== PAUSE TestCtestGetConnectionInfo/node_with_no_addresses
=== RUN   TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address)
=== PAUSE TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address)
=== CONT  TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred
=== CONT  TestCtestGetConnectionInfo/node_with_no_addresses
=== CONT  TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address)
=== CONT  TestCtestGetConnectionInfo/node_not_found
=== CONT  TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal)
--- PASS: TestCtestGetConnectionInfo (0.00s)
    --- PASS: TestCtestGetConnectionInfo/valid_node_with_external_IP_preferred (0.00s)
    --- PASS: TestCtestGetConnectionInfo/node_with_no_addresses (0.00s)
    --- PASS: TestCtestGetConnectionInfo/node_not_found (0.00s)
    --- PASS: TestCtestGetConnectionInfo/empty_preferred_address_types_(should_fallback_to_first_address) (0.00s)
    --- PASS: TestCtestGetConnectionInfo/valid_node_without_external_IP_(uses_internal) (0.00s)
FAIL
coverage: 94.9% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/client	1.020s
=== RUN   TestCtestLazyInformerManager_ensureManagerSet
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_unavailable
    clustertrustbundle_manager.go:422: I0209 18:45:14.376371] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1alpha1
    clustertrustbundle_manager.go:428: I0209 18:45:14.376720] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1alpha1"
    clustertrustbundle_manager.go:435: I0209 18:45:14.376753] Waiting for ClusterTrustBundle informer to sync
I0209 18:45:14.376819   87703 envvar.go:172] "Feature gate default state" feature="InOrderInformers" enabled=true
I0209 18:45:14.376827   87703 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0209 18:45:14.376829   87703 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0209 18:45:14.376831   87703 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0209 18:45:14.376834   87703 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0209 18:45:14.376982   87703 reflector.go:358] "Starting reflector" type="*v1alpha1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.376987   87703 reflector.go:404] "Listing and watching" type="*v1alpha1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.377256   87703 reflector.go:436] "Caches populated" type="*v1alpha1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0209 18:45:14.478910] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_an_unhandled_version
I0209 18:45:14.479003   87703 watch.go:218] "Stopping fake watcher"
    clustertrustbundle_manager.go:422: I0209 18:45:14.479023] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
I0209 18:45:14.479039   87703 reflector.go:364] "Stopping reflector" type="*v1alpha1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1beta1
    clustertrustbundle_manager.go:428: I0209 18:45:14.479136] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0209 18:45:14.479151] Waiting for ClusterTrustBundle informer to sync
I0209 18:45:14.479208   87703 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.479222   87703 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.479310   87703 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0209 18:45:14.579466] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1_-_currently_unhandled
I0209 18:45:14.579538   87703 watch.go:218] "Stopping fake watcher"
I0209 18:45:14.579571   87703 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:422: I0209 18:45:14.579582] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery_but_beta_API_shard_discovered
    clustertrustbundle_manager.go:428: I0209 18:45:14.579654] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0209 18:45:14.579669] Waiting for ClusterTrustBundle informer to sync
I0209 18:45:14.579734   87703 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.579742   87703 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.579777   87703 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0209 18:45:14.680178] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_alpha_and_beta_-_prefer_beta
    clustertrustbundle_manager.go:428: I0209 18:45:14.680294] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0209 18:45:14.680313] Waiting for ClusterTrustBundle informer to sync
I0209 18:45:14.680353   87703 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.680362   87703 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.680393   87703 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.680353   87703 watch.go:218] "Stopping fake watcher"
I0209 18:45:14.680454   87703 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0209 18:45:14.781736] ClusterTrustBundle informer synced
I0209 18:45:14.781785   87703 watch.go:218] "Stopping fake watcher"
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/API_available_in_multiple_handled_and_unhandled_versions_-_prefer_the_most-GA_handled_version
I0209 18:45:14.781803   87703 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:428: I0209 18:45:14.781844] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0209 18:45:14.781861] Waiting for ClusterTrustBundle informer to sync
I0209 18:45:14.781888   87703 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.781899   87703 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.781930   87703 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0209 18:45:14.882377] ClusterTrustBundle informer synced
I0209 18:45:14.882462   87703 watch.go:218] "Stopping fake watcher"
I0209 18:45:14.882484   87703 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_empty_GV_list
    clustertrustbundle_manager.go:422: I0209 18:45:14.882588] No version of the ClusterTrustBundle API was found, the ClusterTrustBundle informer won't be started
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_duplicate_GVs
    clustertrustbundle_manager.go:428: I0209 18:45:14.882642] Started ClusterTrustBundle informer apiGroup="certificates.k8s.io/v1beta1"
    clustertrustbundle_manager.go:435: I0209 18:45:14.882654] Waiting for ClusterTrustBundle informer to sync
I0209 18:45:14.882713   87703 reflector.go:358] "Starting reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.882718   87703 reflector.go:404] "Listing and watching" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
I0209 18:45:14.882756   87703 reflector.go:436] "Caches populated" type="*v1beta1.ClusterTrustBundle" reflector="k8s.io/client-go/informers/factory.go:160"
    clustertrustbundle_manager.go:440: I0209 18:45:14.983820] ClusterTrustBundle informer synced
=== RUN   TestCtestLazyInformerManager_ensureManagerSet/non‑nil_error_with_nil_GV_slice
I0209 18:45:14.983889   87703 watch.go:218] "Stopping fake watcher"
I0209 18:45:14.983907   87703 reflector.go:364] "Stopping reflector" type="*v1beta1.ClusterTrustBundle" resyncPeriod="0s" reflector="k8s.io/client-go/informers/factory.go:160"
--- PASS: TestCtestLazyInformerManager_ensureManagerSet (0.65s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_unavailable (0.04s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1alpha1 (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_an_unhandled_version (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1beta1 (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_v1_-_currently_unhandled (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/err_in_discovery_but_beta_API_shard_discovered (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_alpha_and_beta_-_prefer_beta (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/API_available_in_multiple_handled_and_unhandled_versions_-_prefer_the_most-GA_handled_version (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_empty_GV_list (0.00s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/nil_error_with_duplicate_GVs (0.10s)
    --- PASS: TestCtestLazyInformerManager_ensureManagerSet/non‑nil_error_with_nil_GV_slice (0.00s)
PASS
coverage: 34.4% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/clustertrustbundle	1.300s	coverage: 34.4% of statements
=== RUN   TestCtestParseQOSReserved
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/ctest_cgroup_manager_test.go:13]: Starting TestCtestParseQOSReserved
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/ctest_cgroup_manager_test.go:77]: Number of test cases: 13
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
Running 5 th test case.
Running 6 th test case.
Running 7 th test case.
    ctest_cgroup_manager_test.go:82: Unexpected success, input: map[], expected: <nil>, actual: &map[], err: <nil>
    ctest_cgroup_manager_test.go:90: Unexpected result, input: map[], expected: <nil>, actual: &map[], err: <nil>
Running 8 th test case.
Running 9 th test case.
Running 10 th test case.
Running 11 th test case.
Running 12 th test case.
--- FAIL: TestCtestParseQOSReserved (0.00s)
FAIL
coverage: 5.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/cm	0.564s
=== RUN   TestCtestAdmissionErrors

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:15]: Starting TestCtestAdmissionErrors
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:59]: Number of test cases: 6
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: <nil>
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: Not an AdmissionError error
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: Is an AdmissionError error
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: 
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: 
Running 5 th test case.
[DEBUG-CTEST 2026-02-09 18:45:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/admission/ctest_errors_test.go:63]: Test case error: 

==================== CTEST END ======================
--- PASS: TestCtestAdmissionErrors (0.00s)
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/admission	0.695s	coverage: 100.0% of statements
=== RUN   TestCtestContainerMapCloneUnshared
Running edge case: clone empty ContainerMap
--- PASS: TestCtestContainerMapCloneUnshared (0.00s)
=== RUN   TestCtestContainerMap
Running test case for podUID="fakePodUID", containerNames=[fakeContainerName-1 fakeContainerName-2], containerIDs=[fakeContainerID-1 fakeContainerName-2]
Running test case for podUID="", containerNames=[], containerIDs=[]
Running test case for podUID="dupPodUID", containerNames=[dupContainer-1 dupContainer-2], containerIDs=[dupID dupID]
Running test case for podUID="nilSlicePodUID", containerNames=[], containerIDs=[]
--- PASS: TestCtestContainerMap (0.00s)
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/containermap	1.687s	coverage: 100.0% of statements
=== RUN   TestCtestPolicyDefaultsAvailable
=== RUN   TestCtestPolicyDefaultsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyDefaultsAvailable/full-pcpus-only
=== RUN   TestCtestPolicyDefaultsAvailable/#00
=== RUN   TestCtestPolicyDefaultsAvailable/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
--- PASS: TestCtestPolicyDefaultsAvailable (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/full-pcpus-only (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa (0.00s)
=== RUN   TestCtestPolicyOptionsAvailable
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01
=== RUN   TestCtestPolicyOptionsAvailable/align-by-socket
=== RUN   TestCtestPolicyOptionsAvailable/align-by-socket#01
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa#01
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores
=== RUN   TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores#01
=== RUN   TestCtestPolicyOptionsAvailable/strict-cpu-reservation
=== RUN   TestCtestPolicyOptionsAvailable/strict-cpu-reservation#01
=== RUN   TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache
=== RUN   TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache#01
=== RUN   TestCtestPolicyOptionsAvailable/#00
=== RUN   TestCtestPolicyOptionsAvailable/bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
--- PASS: TestCtestPolicyOptionsAvailable (0.01s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/align-by-socket (0.01s)
    --- PASS: TestCtestPolicyOptionsAvailable/align-by-socket#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-numa#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/distribute-cpus-across-cores#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/strict-cpu-reservation (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/strict-cpu-reservation#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-align-cpus-by-uncorecache#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb (0.00s)
=== RUN   TestCtestPolicyOptionsAlwaysAvailableOnceGA
=== RUN   TestCtestPolicyOptionsAlwaysAvailableOnceGA/full-pcpus-only
--- PASS: TestCtestPolicyOptionsAlwaysAvailableOnceGA (0.00s)
    --- PASS: TestCtestPolicyOptionsAlwaysAvailableOnceGA/full-pcpus-only (0.00s)
=== RUN   TestCtestValidateStaticPolicyOptions
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_not_enabled
I0209 18:45:16.720408   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="single-numa-node"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_topology_manager_single_numa_node
I0209 18:45:16.720573   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="single-numa-node"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_num_sockets_>_num_numa
I0209 18:45:16.720612   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_None_policy
I0209 18:45:16.720654   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_best-effort_policy
I0209 18:45:16.720691   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_restricted_policy
I0209 18:45:16.720725   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
=== RUN   TestCtestValidateStaticPolicyOptions/Empty_policy_map
I0209 18:45:16.720762   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="single-numa-node"
=== RUN   TestCtestValidateStaticPolicyOptions/Invalid_boolean_value
I0209 18:45:16.720799   87711 fake_topology_manager.go:48] "NewFakeManagerWithPolicy" policy="none"
    ctest_policy_options_test.go:240: testCase "Invalid boolean value" failed, got false expected true
--- FAIL: TestCtestValidateStaticPolicyOptions (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_not_enabled (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_topology_manager_single_numa_node (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled_with_num_sockets_>_num_numa (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_None_policy (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_best-effort_policy (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Align_by_socket_enabled:_with_topology_manager_restricted_policy (0.00s)
    --- PASS: TestCtestValidateStaticPolicyOptions/Empty_policy_map (0.00s)
    --- FAIL: TestCtestValidateStaticPolicyOptions/Invalid_boolean_value (0.00s)
=== RUN   TestCtestPolicyOptionsCompatibility
=== RUN   TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_set_to_true_only
=== RUN   TestCtestPolicyOptionsCompatibility/DistributeCPUsAcrossCores_set_to_true_only
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_StrictCPUReservation_set_to_true
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_FullPCPUsOnly_set_to_true
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_AlignBySocket_set_to_true
=== RUN   TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_and_DistributeCPUsAcrossCores_options_can_not_coexist
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossCores_options_can_not_coexist
=== RUN   TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossNUMA_options_can_not_coexist
=== RUN   TestCtestPolicyOptionsCompatibility/Empty_policy_options_map
=== RUN   TestCtestPolicyOptionsCompatibility/Unknown_option_with_feature_gate_enabled
=== RUN   TestCtestPolicyOptionsCompatibility/Valid_option_but_feature_gate_disabled
    ctest_policy_options_test.go:354: testCase "Valid option but feature gate disabled" failed, got false expected true
--- FAIL: TestCtestPolicyOptionsCompatibility (0.01s)
    --- PASS: TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_set_to_true_only (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/DistributeCPUsAcrossCores_set_to_true_only (0.01s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_StrictCPUReservation_set_to_true (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_FullPCPUsOnly_set_to_true (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_AlignBySocket_set_to_true (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/FullPhysicalCPUsOnly_and_DistributeCPUsAcrossCores_options_can_not_coexist (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossCores_options_can_not_coexist (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/PreferAlignByUnCoreCache_and_DistributeCPUsAcrossNUMA_options_can_not_coexist (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/Empty_policy_options_map (0.00s)
    --- PASS: TestCtestPolicyOptionsCompatibility/Unknown_option_with_feature_gate_enabled (0.00s)
    --- FAIL: TestCtestPolicyOptionsCompatibility/Valid_option_but_feature_gate_disabled (0.00s)
FAIL
coverage: 5.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager	1.569s
=== RUN   TestCtestClone
=== Start TestCtestClone ===
Running test case #0: non‑empty map
Running test case #1: empty map
Running test case #2: nil map
    ctest_state_test.go:46: test case "nil map" failed: expected map[], got map[]
--- FAIL: TestCtestClone (0.00s)
FAIL
coverage: 3.2% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/state	1.070s
	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/state/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/topology	0.866s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/devicemanager	0.581s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/cm/devicemanager/checkpoint		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/cm/devicemanager/plugin/v1beta1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/dra	0.526s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGRPCConnIsReused
--- PASS: TestCtestGRPCConnIsReused (0.00s)
=== RUN   TestCtestGRPCConnUsableAfterIdle
    ctest_dra_plugin_test.go:89: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/plugin/ctest_dra_plugin_test.go:89
        	Error:      	Received unexpected error:
        	            	listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCConnUsableAfterIdle3639607631/001/dra.sock: bind: invalid argument
        	Test:       	TestCtestGRPCConnUsableAfterIdle
--- FAIL: TestCtestGRPCConnUsableAfterIdle (0.00s)
=== RUN   TestCtestGetDRAPlugin
==== Starting TestGetDRAPlugin ====
=== RUN   TestCtestGetDRAPlugin/driver-name_is_empty
=== RUN   TestCtestGetDRAPlugin/driver_name_not_found_in_the_list
=== RUN   TestCtestGetDRAPlugin/plugin_exists
=== RUN   TestCtestGetDRAPlugin/driver_name_whitespace_only
=== RUN   TestCtestGetDRAPlugin/driver_name_with_special_characters
==== Finished TestGetDRAPlugin ====
--- PASS: TestCtestGetDRAPlugin (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver-name_is_empty (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver_name_not_found_in_the_list (0.00s)
    --- PASS: TestCtestGetDRAPlugin/plugin_exists (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver_name_whitespace_only (0.00s)
    --- PASS: TestCtestGetDRAPlugin/driver_name_with_special_characters (0.00s)
=== RUN   TestCtestGRPCMethods
==== Starting TestGRPCMethods ====
=== RUN   TestCtestGRPCMethods/v1beta1
=== RUN   TestCtestGRPCMethods/v1
=== RUN   TestCtestGRPCMethods/mismatch
=== RUN   TestCtestGRPCMethods/internal-error
    ctest_dra_plugin_test.go:230: listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCMethodsinternal-error1597630139/001/dra.sock: bind: invalid argument
=== RUN   TestCtestGRPCMethods/empty-service
    ctest_dra_plugin_test.go:230: listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCMethodsempty-service853430608/001/dra.sock: bind: invalid argument
=== RUN   TestCtestGRPCMethods/empty-chosen-service
    ctest_dra_plugin_test.go:230: listen unix /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/TestCtestGRPCMethodsempty-chosen-service3354734973/001/dra.sock: bind: invalid argument
==== Finished TestGRPCMethods ====
--- FAIL: TestCtestGRPCMethods (0.00s)
    --- PASS: TestCtestGRPCMethods/v1beta1 (0.00s)
    --- PASS: TestCtestGRPCMethods/v1 (0.00s)
    --- PASS: TestCtestGRPCMethods/mismatch (0.00s)
    --- FAIL: TestCtestGRPCMethods/internal-error (0.00s)
    --- FAIL: TestCtestGRPCMethods/empty-service (0.00s)
    --- FAIL: TestCtestGRPCMethods/empty-chosen-service (0.00s)
=== RUN   TestCtestPlugin_WatchResources
--- PASS: TestCtestPlugin_WatchResources (0.00s)
=== RUN   TestCtestRegistrationHandler
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/plugin/ctest_registration_test.go:45]: get default configs: {test_fixture.json [registration handler slice spec] nodeName [resourceslices] { { 0 0} 0x140003ea840 nil <nil> [] <nil> []}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceslices]
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceslices], int=1)[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceslices]
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:45:25 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceslices
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/dra/plugin	1.845s
# k8s.io/kubernetes/pkg/kubelet/container
# [k8s.io/kubernetes/pkg/kubelet/container]
pkg/kubelet/container/ctest_ref_test.go:73:6: (*testing.common).Errorf format %s has arg i of wrong type int
pkg/kubelet/container/ctest_ref_test.go:78:5: (*testing.common).Errorf format %s has arg i of wrong type int
pkg/kubelet/container/ctest_ref_test.go:82:5: (*testing.common).Errorf format %s has arg i of wrong type int
=== RUN   TestCtestCheckpointGetOrCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:199]: Number of test cases: 10
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: new-checkpoint
=== RUN   TestCtestCheckpointGetOrCreate/new-checkpoint
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: single-claim-info-state
=== RUN   TestCtestCheckpointGetOrCreate/single-claim-info-state
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: claim-info-state-with-multiple-devices
=== RUN   TestCtestCheckpointGetOrCreate/claim-info-state-with-multiple-devices
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: two-claim-info-states
=== RUN   TestCtestCheckpointGetOrCreate/two-claim-info-states
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: incorrect-checksum
=== RUN   TestCtestCheckpointGetOrCreate/incorrect-checksum
Running 5 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: invalid-JSON
=== RUN   TestCtestCheckpointGetOrCreate/invalid-JSON
Running 6 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: upgraded-structure
=== RUN   TestCtestCheckpointGetOrCreate/upgraded-structure
Running 7 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: upgraded-structure-and-version
=== RUN   TestCtestCheckpointGetOrCreate/upgraded-structure-and-version
Running 8 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: empty-data-field
=== RUN   TestCtestCheckpointGetOrCreate/empty-data-field
    ctest_checkpointer_test.go:223: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:223
        	Error:      	An error is expected but got nil.
        	Test:       	TestCtestCheckpointGetOrCreate/empty-data-field
Running 9 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:202]: missing-claimuid
=== RUN   TestCtestCheckpointGetOrCreate/missing-claimuid
    ctest_checkpointer_test.go:223: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:223
        	Error:      	Error "failed to get checkpoint dramanager_checkpoint_test: checkpoint is corrupted" does not contain "missing required field"
        	Test:       	TestCtestCheckpointGetOrCreate/missing-claimuid

==================== CTEST END ======================
--- FAIL: TestCtestCheckpointGetOrCreate (0.08s)
    --- PASS: TestCtestCheckpointGetOrCreate/new-checkpoint (0.03s)
    --- PASS: TestCtestCheckpointGetOrCreate/single-claim-info-state (0.01s)
    --- PASS: TestCtestCheckpointGetOrCreate/claim-info-state-with-multiple-devices (0.00s)
    --- PASS: TestCtestCheckpointGetOrCreate/two-claim-info-states (0.02s)
    --- PASS: TestCtestCheckpointGetOrCreate/incorrect-checksum (0.00s)
    --- PASS: TestCtestCheckpointGetOrCreate/invalid-JSON (0.00s)
    --- PASS: TestCtestCheckpointGetOrCreate/upgraded-structure (0.00s)
    --- PASS: TestCtestCheckpointGetOrCreate/upgraded-structure-and-version (0.00s)
    --- FAIL: TestCtestCheckpointGetOrCreate/empty-data-field (0.00s)
    --- FAIL: TestCtestCheckpointGetOrCreate/missing-claimuid (0.00s)
=== RUN   TestCtestCheckpointStateStore

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:346]: Number of test cases: 4
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: single-claim-info-state
=== RUN   TestCtestCheckpointStateStore/single-claim-info-state
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: claim-info-state-with-multiple-devices
=== RUN   TestCtestCheckpointStateStore/claim-info-state-with-multiple-devices
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: two-claim-info-states
=== RUN   TestCtestCheckpointStateStore/two-claim-info-states
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:45:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:349]: empty-claim-info-list
=== RUN   TestCtestCheckpointStateStore/empty-claim-info-list
    ctest_checkpointer_test.go:372: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/dra/state/ctest_checkpointer_test.go:372
        	Error:      	Not equal: 
        	            	expected: "{\"Data\":\"{\\\"kind\\\":\\\"DRACheckpoint\\\",\\\"apiVersion\\\":\\\"checkpoint.dra.kubelet.k8s.io/v1\\\",\\\"ClaimInfoStateList\\\":[]}\", \"Checksum\":0}"
        	            	actual  : "{\"Data\":\"{\\\"kind\\\":\\\"DRACheckpoint\\\",\\\"apiVersion\\\":\\\"checkpoint.dra.kubelet.k8s.io/v1\\\",\\\"ClaimInfoStateList\\\":[]}\",\"Checksum\":2361995144}"
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1 +1 @@
        	            	-{"Data":"{\"kind\":\"DRACheckpoint\",\"apiVersion\":\"checkpoint.dra.kubelet.k8s.io/v1\",\"ClaimInfoStateList\":[]}", "Checksum":0}
        	            	+{"Data":"{\"kind\":\"DRACheckpoint\",\"apiVersion\":\"checkpoint.dra.kubelet.k8s.io/v1\",\"ClaimInfoStateList\":[]}","Checksum":2361995144}
        	Test:       	TestCtestCheckpointStateStore/empty-claim-info-list

==================== CTEST END ======================
--- FAIL: TestCtestCheckpointStateStore (0.02s)
    --- PASS: TestCtestCheckpointStateStore/single-claim-info-state (0.00s)
    --- PASS: TestCtestCheckpointStateStore/claim-info-state-with-multiple-devices (0.00s)
    --- PASS: TestCtestCheckpointStateStore/two-claim-info-states (0.00s)
    --- FAIL: TestCtestCheckpointStateStore/empty-claim-info-list (0.00s)
FAIL
coverage: 44.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/dra/state	2.428s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/memorymanager	1.251s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCheckpointStateRestore
=== RUN   TestCtestCheckpointStateRestore/Restore_non-existing_checkpoint
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.011174] Memory Manager state checkpoint: Initializing new in-memory state store
=== RUN   TestCtestCheckpointStateRestore/Restore_valid_checkpoint
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.034604] Memory Manager state checkpoint: Initializing new in-memory state store
    state_mem.go:77: I0209 18:45:24.034778] Memory Manager state checkpoint: Updated machine memory state
    state_mem.go:99: I0209 18:45:24.034889] Memory Manager state checkpoint: Updated Memory assignments assignments=<state.ContainerMemoryAssignments | len:1>: 
                pod:
                  container1:
                  - numaAffinity:
                    - 0
                    size: 512
                    type: memory
    state_checkpoint.go:85: I0209 18:45:24.034900] Memory Manager state checkpoint: State checkpoint: restored state from checkpoint
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_checksum
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.057454] Memory Manager state checkpoint: Initializing new in-memory state store
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_JSON
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.062388] Memory Manager state checkpoint: Initializing new in-memory state store
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.065391] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test3838988804/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: checksum missing"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.069646] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: json: cannot unmarshal number -100 into Go struct field Block.entries.size of type uint64, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test3838988804/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: invalid memory size"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.073324] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test3838988804/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: invalid memory size"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment
=== RUN   TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size
=== NAME  TestCtestCheckpointStateRestore
    state_mem.go:36: I0209 18:45:24.077462] Memory Manager state checkpoint: Initializing new in-memory state store
=== NAME  TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size
    ctest_state_checkpoint_test.go:155: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/memorymanager/state/ctest_state_checkpoint_test.go:155
        	Error:      	Error "could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete the memory manager checkpoint file \"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/memorymanager_state_test3838988804/memorymanager_checkpoint_test\" before restarting Kubelet" does not contain "could not restore state from checkpoint: memory size exceeds node capacity"
        	Test:       	TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size
--- FAIL: TestCtestCheckpointStateRestore (0.07s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_non-existing_checkpoint (0.01s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_valid_checkpoint (0.02s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_checksum (0.02s)
    --- PASS: TestCtestCheckpointStateRestore/Restore_checkpoint_with_invalid_JSON (0.00s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_missing_checksum_field (0.00s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_with_negative_memory_size (0.00s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_with_zero_size_assignment (0.00s)
    --- FAIL: TestCtestCheckpointStateRestore/Restore_checkpoint_with_extremely_large_size (0.00s)
FAIL
coverage: 54.3% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/memorymanager/state	0.864s
?   	k8s.io/kubernetes/pkg/kubelet/cm/resourceupdates	[no test files]
	k8s.io/kubernetes/pkg/kubelet/cm/testing		coverage: 0.0% of statements
=== RUN   TestCtestNewFakeManager

==================== CTEST START ====================
  I0209 18:45:30.340767   87918 fake_topology_manager.go:33] "NewFakeManager"

==================== CTEST END ======================
--- PASS: TestCtestNewFakeManager (0.00s)
=== RUN   TestCtestFakeGetAffinity

==================== CTEST START ====================
  I0209 18:45:30.341023   87918 fake_topology_manager.go:55] "GetAffinity" podUID="0aafa4c4-38e8-11e9-bcb1-a4bf01040474" containerName="nginx"
  I0209 18:45:30.341036   87918 fake_topology_manager.go:55] "GetAffinity" podUID="11111111-2222-3333-4444-555555555555" containerName=""
  I0209 18:45:30.341046   87918 fake_topology_manager.go:55] "GetAffinity" podUID="" containerName="busybox"
  I0209 18:45:30.341061   87918 fake_topology_manager.go:55] "GetAffinity" podUID="uid-\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00" containerName="container-\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"

==================== CTEST END ======================
--- PASS: TestCtestFakeGetAffinity (0.00s)
=== RUN   TestCtestFakeRemoveContainer

==================== CTEST START ====================
  I0209 18:45:30.341175   87918 fake_topology_manager.go:76] "RemoveContainer" containerID="nginx"
  I0209 18:45:30.341185   87918 fake_topology_manager.go:76] "RemoveContainer" containerID="Busy_Box"
  I0209 18:45:30.341194   87918 fake_topology_manager.go:76] "RemoveContainer" containerID=""
  I0209 18:45:30.341201   87918 fake_topology_manager.go:76] "RemoveContainer" containerID="!@#$%^&*()_+"
  I0209 18:45:30.341208   87918 fake_topology_manager.go:76] "RemoveContainer" containerID="\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"

==================== CTEST END ======================
--- PASS: TestCtestFakeRemoveContainer (0.00s)
=== RUN   TestCtestFakeAdmit

==================== CTEST START ====================
  I0209 18:45:30.341276   87918 fake_topology_manager.go:81] "Topology Admit Handler"
  I0209 18:45:30.341292   87918 fake_topology_manager.go:81] "Topology Admit Handler"
  I0209 18:45:30.341298   87918 fake_topology_manager.go:81] "Topology Admit Handler"
  I0209 18:45:30.341310   87918 fake_topology_manager.go:81] "Topology Admit Handler"
  I0209 18:45:30.341317   87918 fake_topology_manager.go:81] "Topology Admit Handler"

==================== CTEST END ======================
--- PASS: TestCtestFakeAdmit (0.00s)
=== RUN   TestCtestPolicyBestEffortMerge

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_best_effort_test.go:12]: Running TestPolicyBestEffortMerge
  I0209 18:45:30.341473   87918 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0209 18:45:30.341486   87918 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0209 18:45:30.341705   87918 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0209 18:45:30.341722   87918 policy.go:87] "Hint Provider has no preference for NUMA affinity with resource" resource="resource"
  I0209 18:45:30.341734   87918 policy.go:93] "Hint Provider has no possible NUMA affinities for resource" resource="resource"

==================== CTEST END ======================
--- PASS: TestCtestPolicyBestEffortMerge (0.00s)
=== RUN   TestCtestPolicyBestEffortMergeClosestNUMA

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_best_effort_test.go:29]: Running TestPolicyBestEffortMergeClosestNUMA
  I0209 18:45:30.342122   87918 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0209 18:45:30.342129   87918 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0209 18:45:30.342150   87918 policy.go:79] "Hint Provider has no preference for NUMA affinity with any resource"
  I0209 18:45:30.342155   87918 policy.go:87] "Hint Provider has no preference for NUMA affinity with resource" resource="resource"
  I0209 18:45:30.342163   87918 policy.go:93] "Hint Provider has no possible NUMA affinities for resource" resource="resource"

==================== CTEST END ======================
--- PASS: TestCtestPolicyBestEffortMergeClosestNUMA (0.00s)
=== RUN   TestCtestPolicyNoneName
--- PASS: TestCtestPolicyNoneName (0.00s)
=== RUN   TestCtestPolicyNoneCanAdmitPodResult
--- PASS: TestCtestPolicyNoneCanAdmitPodResult (0.00s)
=== RUN   TestCtestPolicyNoneMerge
--- PASS: TestCtestPolicyNoneMerge (0.00s)
=== RUN   TestCtestNewTopologyManagerOptions
=== RUN   TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_PreferClosestNUMA_set_to_true
=== RUN   TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_MaxAllowableNUMANodes_set_to_12
  I0209 18:45:30.342370   87918 policy_options.go:95] "WARNING: the value of max-allowable-numa-nodes is more than the default recommended value" max-allowable-numa-nodes=12 defaultMaxAllowableNUMANodes=8
=== RUN   TestCtestNewTopologyManagerOptions/fail_to_set_option_when_TopologyManagerPolicyBetaOptions_feature_gate_is_not_set
=== RUN   TestCtestNewTopologyManagerOptions/return_empty_TopologyManagerOptions_(default_values)
=== RUN   TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_PreferClosestNUMANodes
=== RUN   TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_MaxAllowableNUMANodes
=== RUN   TestCtestNewTopologyManagerOptions/test_beta_options_success
    ctest_policy_options_test.go:228: Unexpected error: unsupported topologymanager option: "fancy-new-option" (true)
=== RUN   TestCtestNewTopologyManagerOptions/test_beta_options_fail
=== RUN   TestCtestNewTopologyManagerOptions/test_alpha_options_success
    ctest_policy_options_test.go:228: Unexpected error: unsupported topologymanager option: "fancy-alpha-option" (true)
=== RUN   TestCtestNewTopologyManagerOptions/test_alpha_options_fail
=== RUN   TestCtestNewTopologyManagerOptions/empty_policyOptions_map_with_no_feature_gate
=== RUN   TestCtestNewTopologyManagerOptions/nil_policyOptions_map_(should_behave_like_empty)
=== RUN   TestCtestNewTopologyManagerOptions/unknown_option_key_should_be_ignored
    ctest_policy_options_test.go:228: Unexpected error: unknown Topology Manager Policy option: "unknownOption"
=== RUN   TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_empty_string
=== RUN   TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_empty_string
=== RUN   TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_negative_value
    ctest_policy_options_test.go:228: Unexpected error: the minimum value of "max-allowable-numa-nodes" should not be less than 8
=== RUN   TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_very_large_integer
  I0209 18:45:30.343636   87918 policy_options.go:95] "WARNING: the value of max-allowable-numa-nodes is more than the default recommended value" max-allowable-numa-nodes=1000000 defaultMaxAllowableNUMANodes=8
=== RUN   TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_case-insensitive_true
=== RUN   TestCtestNewTopologyManagerOptions/beta_option_present_while_beta_feature_disabled_but_alpha_enabled_(should_fail)
    ctest_policy_options_test.go:223: Unexpected error. Got: unsupported topologymanager option: "fancy-new-option" (true), want error containing: topology manager policy beta-level options not enabled,
=== RUN   TestCtestNewTopologyManagerOptions/both_beta_and_alpha_options_present,_both_feature_gates_enabled_(both_succeed)
    ctest_policy_options_test.go:228: Unexpected error: unsupported topologymanager option: "fancy-new-option" (true)
--- FAIL: TestCtestNewTopologyManagerOptions (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_PreferClosestNUMA_set_to_true (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/return_TopologyManagerOptions_with_MaxAllowableNUMANodes_set_to_12 (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/fail_to_set_option_when_TopologyManagerPolicyBetaOptions_feature_gate_is_not_set (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/return_empty_TopologyManagerOptions_(default_values) (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_PreferClosestNUMANodes (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/fail_to_parse_options_with_error_MaxAllowableNUMANodes (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/test_beta_options_success (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/test_beta_options_fail (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/test_alpha_options_success (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/test_alpha_options_fail (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/empty_policyOptions_map_with_no_feature_gate (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/nil_policyOptions_map_(should_behave_like_empty) (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/unknown_option_key_should_be_ignored (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_empty_string (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_empty_string (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_negative_value (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/MaxAllowableNUMANodes_very_large_integer (0.00s)
    --- PASS: TestCtestNewTopologyManagerOptions/PreferClosestNUMANodes_case-insensitive_true (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/beta_option_present_while_beta_feature_disabled_but_alpha_enabled_(should_fail) (0.00s)
    --- FAIL: TestCtestNewTopologyManagerOptions/both_beta_and_alpha_options_present,_both_feature_gates_enabled_(both_succeed) (0.00s)
=== RUN   TestCtestPolicyDefaultsAvailable
=== RUN   TestCtestPolicyDefaultsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyDefaultsAvailable/prefer-closest-numa-nodes
=== RUN   TestCtestPolicyDefaultsAvailable/max-allowable-numa-nodes
=== RUN   TestCtestPolicyDefaultsAvailable/#00
=== RUN   TestCtestPolicyDefaultsAvailable/___
=== RUN   TestCtestPolicyDefaultsAvailable/PREFER-CLOSEST-NUMA-NODES
--- PASS: TestCtestPolicyDefaultsAvailable (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/prefer-closest-numa-nodes (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/max-allowable-numa-nodes (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/___ (0.00s)
    --- PASS: TestCtestPolicyDefaultsAvailable/PREFER-CLOSEST-NUMA-NODES (0.00s)
=== RUN   TestCtestPolicyOptionsAvailable
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist
=== RUN   TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01
=== RUN   TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes
=== RUN   TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#01
=== RUN   TestCtestPolicyOptionsAvailable/fancy-alpha-option
=== RUN   TestCtestPolicyOptionsAvailable/fancy-alpha-option#01
=== RUN   TestCtestPolicyOptionsAvailable/fancy-new-option
=== RUN   TestCtestPolicyOptionsAvailable/fancy-new-option#01
=== RUN   TestCtestPolicyOptionsAvailable/#00
=== RUN   TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#02
--- PASS: TestCtestPolicyOptionsAvailable (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/this-option-does-not-exist#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-alpha-option (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-alpha-option#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-new-option (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/fancy-new-option#01 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/#00 (0.00s)
    --- PASS: TestCtestPolicyOptionsAvailable/prefer-closest-numa-nodes#02 (0.00s)
=== RUN   TestCtestPolicySingleNumaNodeCanAdmitPodResult

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_single_numa_node_test.go:13]: Running TestCtestPolicySingleNumaNodeCanAdmitPodResult
Running 0 th test case: Preferred is set to false in topology hints
Running 1 th test case: Preferred true with empty affinity mask (edge case)
    ctest_policy_single_numa_node_test.go:44: Test case "Preferred true with empty affinity mask (edge case)": expected result false, got true
Running 2 th test case: Preferred true with nil affinity (edge case)
    ctest_policy_single_numa_node_test.go:44: Test case "Preferred true with nil affinity (edge case)": expected result false, got true

==================== CTEST END ======================
--- FAIL: TestCtestPolicySingleNumaNodeCanAdmitPodResult (0.00s)
=== RUN   TestCtestPolicySingleNumaNodeFilterHints

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_policy_single_numa_node_test.go:52]: Running TestCtestPolicySingleNumaNodeFilterHints
Running 0 th test case: filter empty resources
Running 1 th test case: filter hints with nil socket mask 1/2
Running 2 th test case: filter hints with nil socket mask 2/2
Running 3 th test case: filter hints with empty resource socket mask
Running 4 th test case: filter hints with wide sockemask
Running 5 th test case: filter nil resources (edge case)
Running 6 th test case: filter resources with nil inner slice (edge case)

==================== CTEST END ======================
--- PASS: TestCtestPolicySingleNumaNodeFilterHints (0.00s)
=== RUN   TestCtestContainerCalculateAffinity

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_scope_container_test.go:143]: Number of test cases: 6
Running 0 th test case.
{No hint providers [] []}
  I0209 18:45:30.345379   87918 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 1 th test case.
{HintProvider returns empty non-nil map[string][]TopologyHint [0x1400040c448] [map[]]}
  I0209 18:45:30.345409   87918 scope_container.go:83] "TopologyHints" hints={} pod="" containerName=""
  I0209 18:45:30.345424   87918 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 2 th test case.
{HintProvider returns -nil map[string][]TopologyHint from provider [0x1400040c450] [map[resource:[]]]}
  I0209 18:45:30.345468   87918 scope_container.go:83] "TopologyHints" hints={"resource":null} pod="" containerName=""
  I0209 18:45:30.345485   87918 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 3 th test case.
{Assorted HintProviders [0x1400040c458 0x1400040c460 0x1400040c468] [map[resource-1/A:[{0x140006eb670 true} {0x140006eb678 false}] resource-1/B:[{0x140006eb680 true} {0x140006eb688 false}]] map[resource-2/A:[{0x140006eb690 true} {0x140006eb698 false}] resource-2/B:[{0x140006eb6a0 true} {0x140006eb6a8 false}]] map[resource-3:[]]]}
  I0209 18:45:30.345532   87918 scope_container.go:83] "TopologyHints" hints={"resource-1/A":[{"NUMANodeAffinity":1,"Preferred":true},{"NUMANodeAffinity":3,"Preferred":false}],"resource-1/B":[{"NUMANodeAffinity":2,"Preferred":true},{"NUMANodeAffinity":6,"Preferred":false}]} pod="" containerName=""
  I0209 18:45:30.345567   87918 scope_container.go:83] "TopologyHints" hints={"resource-2/A":[{"NUMANodeAffinity":4,"Preferred":true},{"NUMANodeAffinity":24,"Preferred":false}],"resource-2/B":[{"NUMANodeAffinity":4,"Preferred":true},{"NUMANodeAffinity":24,"Preferred":false}]} pod="" containerName=""
  I0209 18:45:30.345580   87918 scope_container.go:83] "TopologyHints" hints={"resource-3":null} pod="" containerName=""
  I0209 18:45:30.345599   87918 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName=""
Running 4 th test case.
{HintProvider returns map with empty resource name [0x1400040c470] [map[:[{0x140006eb6b8 true}]]]}
  I0209 18:45:30.345633   87918 scope_container.go:83] "TopologyHints" hints={"":[{"NUMANodeAffinity":1,"Preferred":true}]} pod="" containerName=""
  I0209 18:45:30.345643   87918 scope_container.go:91] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":null,"Preferred":false} pod="" containerName="" 
[signal SIGSEGV: segmentation violation code=0x2 addr=0x28 pc=0x10492c3ec]

goroutine 116 [running]:
testing.tRunner.func1.2({0x105094460, 0x106b7e3c0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x105094460?, 0x106b7e3c0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).accumulateProvidersHints(0x140006db918?, 0x14000558d88, 0x140000e71e0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/scope_container.go:81 +0x17c
k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).calculateAffinity(0x140006dbcf8, 0x14000558d88, 0x140000e71e0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/scope_container.go:89 +0x98
k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.TestCtestContainerCalculateAffinity(0x140007a0e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/cm/topologymanager/ctest_scope_container_test.go:157 +0x1114
testing.tRunner(0x140007a0e00, 0x1054d1410)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/cm/topologymanager	0.832s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/cm/topologymanager/bitmask	0.237s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/cm/util		coverage: 0.0% of statements
=== RUN   TestCtestNewSourceApiserver_UpdatesAndMultiplePods

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:29]: get default configs: {test_fixture.json [apiserver pods] containers [pods] {[] [] [{ image/placeholder [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:45:30 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:45:30 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"image":"image/placeholder","name":"","resources":{}}]})[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:37]: Skipping test execution. No new config objs found.
--- PASS: TestCtestNewSourceApiserver_UpdatesAndMultiplePods (0.00s)
=== RUN   TestCtestNewSourceApiserver_TwoNamespacesSameName

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:170]: get default configs: {test_fixture.json [apiserver two namespaces] containers [pods] {[] [] [{ image/placeholder [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:45:30 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:45:30 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"image":"image/placeholder","name":"","resources":{}}]})[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_apiserver_test.go:178]: Skipping test execution. No new config objs found.
--- PASS: TestCtestNewSourceApiserver_TwoNamespacesSameName (0.00s)
=== RUN   TestCtestDecodeSinglePod

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/config/ctest_common_test.go:36]: get default configs: {test_fixture.json [decode single pod] spec [pods deployments statefulsets daemonsets replicasets] &Pod{ObjectMeta:{test  mynamespace  12345  0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:image,Image:test/image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[],},Privileged:*false,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:*Default,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,AppArmorProfile:nil,SupplementalGroupsPolicy:nil,SELinuxChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:1.2.3.4,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:1.2.3.4,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:45:30 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:45:30 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/kubelet/config	1.246s
=== RUN   TestCtestCacheBasedConfigMapManager

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/configmap/ctest_configmap_manager_test.go:32]: get default configs: {test_fixture.json [cache based configmap manager test] containerEnvConfigMaps [pods] {[{[s1] []} {[] [s20]}] [s2]}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "containerEnvConfigMaps" in requested fixtures
2026/02/09 18:45:31 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:45:31 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={})[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:45:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/configmap/ctest_configmap_manager_test.go:41]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestCacheBasedConfigMapManager (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/configmap	2.012s	coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/container [build failed]
	k8s.io/kubernetes/pkg/kubelet/container/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/envvars	1.413s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/kubelet/events	[no test files]
testing: warning: no tests to run
PASS
coverage: 2.2% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/eviction	0.601s	coverage: 2.2% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/eviction/api		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/images	0.868s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/there_are_no_records_about_the_image_being_pulled
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/there_are_records_about_the_image_being_pulled
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_no_records
=== RUN   TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_with_records
--- PASS: TestCtestNeverVerifyPreloadedPullPolicy (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/there_are_no_records_about_the_image_being_pulled (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/there_are_records_about_the_image_being_pulled (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_no_records (0.00s)
    --- PASS: TestCtestNeverVerifyPreloadedPullPolicy/empty_image_name,_with_records (0.00s)
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_not_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_appears_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_appears_in_allowlist
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_in_the_middle
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_non-segment_wildcard_middle
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_path_segment_in_the_middle
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_only_wildcard_segment
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_ends_with_a_'/'
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_empty
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_asterisk
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_tag
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_digest
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_whitespace
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_different_repo_wildcard
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_org_wildcard
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_repo_wildcard
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_no_records
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_records
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_duplicate_entries,_image_not_in_list
=== RUN   TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_very_long_pattern_(max_length_255_chars)
    ctest_image_pull_policies_test.go:236: wanted error: false, got: failed to parse as an image name: repository name must not be more than 255 characters
--- FAIL: TestCtestNewNeverVerifyAllowListedPullPolicy (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_not_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_appears_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_records_about_the_image_being_pulled,_appears_in_allowlist (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_in_the_middle (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_non-segment_wildcard_middle (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_wildcard_path_segment_in_the_middle (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_only_wildcard_segment (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_ends_with_a_'/' (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_empty (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_asterisk (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_tag (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_image_with_a_digest (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/invalid_allowlist_pattern_-_trailing_whitespace (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_different_repo_wildcard (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_org_wildcard (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/there_are_no_records_about_the_image_being_pulled,_not_in_allowlist_-_matches_repo_wildcard (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_no_records (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/empty_allowlist_(nil_slice)_with_records (0.00s)
    --- PASS: TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_duplicate_entries,_image_not_in_list (0.00s)
    --- FAIL: TestCtestNewNeverVerifyAllowListedPullPolicy/allowlist_with_very_long_pattern_(max_length_255_chars) (0.00s)
FAIL
coverage: 7.6% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/images/pullmanager	1.290s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/kubeletconfig/configfiles	1.954s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/kubeletconfig/util/codec		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/kubeletconfig/util/test		coverage: 0.0% of statements
=== RUN   TestCtestConvertToKubeContainerImageSpec
=== TestCtestConvertToKubeContainerImageSpec start ===
Running 7 test cases.
Case #0: input=id:"test"
Case #1: input=id:"test" spec:{}
Case #2: input=id:"test" spec:{}
Case #3: input=id:"test" spec:{annotations:{key:"kubernetes.io/os" value:"linux"} annotations:{key:"kubernetes.io/runtimehandler" value:"handler"}}
Case #4: input=
Case #5: input=id:"test" spec:{annotations:{key:"" value:"emptykey"}}
Case #6: input=<nil>
--- FAIL: TestCtestConvertToKubeContainerImageSpec (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x68 pc=0x10712e458]

goroutine 137 [running]:
testing.tRunner.func1.2({0x107aac660, 0x10a1ecfc0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x107aac660?, 0x10a1ecfc0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/kuberuntime.toKubeContainerImageSpec(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/convert.go:33 +0xa8
k8s.io/kubernetes/pkg/kubelet/kuberuntime.TestCtestConvertToKubeContainerImageSpec(0x140004e5180)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/ctest_convert_test.go:129 +0x5e0
testing.tRunner(0x140004e5180, 0x108097cd0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/kuberuntime	3.449s
=== RUN   TestCtestPodSandboxChanged

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
=== RUN   TestCtestPodSandboxChanged/Pod_with_no_existing_sandboxes
=== RUN   TestCtestPodSandboxChanged/Pod_with_multiple_ready_sandbox_statuses
=== RUN   TestCtestPodSandboxChanged/Pod_with_no_ready_sandbox_statuses
=== RUN   TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_network_namespace_mismatch
=== RUN   TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_no_IP
=== RUN   TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_with_IP
=== RUN   TestCtestPodSandboxChanged/Pod_with_nil_status
--- FAIL: TestCtestPodSandboxChanged (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_no_existing_sandboxes (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_multiple_ready_sandbox_statuses (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_no_ready_sandbox_statuses (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_network_namespace_mismatch (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_but_no_IP (0.00s)
    --- PASS: TestCtestPodSandboxChanged/Pod_with_ready_sandbox_status_with_IP (0.00s)
    --- FAIL: TestCtestPodSandboxChanged/Pod_with_nil_status (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x80 pc=0x103cb1ce0]

goroutine 106 [running]:
testing.tRunner.func1.2({0x1043f4740, 0x105e82960})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1043f4740?, 0x105e82960?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/kuberuntime/util.PodSandboxChanged(0x1400067e488, 0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/util/util.go:33 +0xc0
k8s.io/kubernetes/pkg/kubelet/kuberuntime/util.TestCtestPodSandboxChanged.func1(0x14000582e00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/kuberuntime/util/ctest_util_test.go:170 +0x2c
testing.tRunner(0x14000582e00, 0x140005f0f30)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 99
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/kuberuntime/util	1.569s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/lifecycle	2.343s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/logs	1.891s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/metrics	1.068s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCollectResourceMetrics
=== RUN   TestCtestCollectResourceMetrics/error_getting_summary

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "summary" in requested fixtures
2026/02/09 18:45:47 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:45:47 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"node":{"nodeName":"","startTime":null},"pods":null})[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/metrics/collectors/ctest_resource_metrics_test.go:465]: Skipping test execution. No new configurations generated. 
  E0209 18:45:47.537717   88125 resource_metrics.go:169] "Error getting summary for resourceMetric prometheus endpoint" err="failed to get summary"
  E0209 18:45:47.537762   88125 resource_metrics.go:169] "Error getting summary for resourceMetric prometheus endpoint" err="failed to get summary"

==================== CTEST END ======================
=== RUN   TestCtestCollectResourceMetrics/arbitrary_node_metrics

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods nodes]
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods nodes], int=2)[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-09 18:45:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:45:47 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/pkg/kubelet/metrics/collectors	2.837s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/network/dns	0.722s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestMigrateConfig
[DEBUG-CTEST 2026-02-09 18:45:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/nodeshutdown/ctest_nodeshutdown_manager_test.go:116]: Running TestCtestMigrateConfig with 7 cases
Running 0 th test case: both shutdownGracePeriodRequested and shutdownGracePeriodCriticalPods
Running 1 th test case: only shutdownGracePeriodRequested
Running 2 th test case: empty configuration
Running 3 th test case: wrong configuration (critical > requested)
Running 4 th test case: negative shutdownGracePeriodRequested
Running 5 th test case: negative shutdownGracePeriodCriticalPods
Running 6 th test case: extremely large durations
    ctest_nodeshutdown_manager_test.go:119: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/nodeshutdown/ctest_nodeshutdown_manager_test.go:119
        	Error:      	Not equal: 
        	            	expected: []config.ShutdownGracePeriodByPodPriority{config.ShutdownGracePeriodByPodPriority{Priority:0, ShutdownGracePeriodSeconds:15768000}, config.ShutdownGracePeriodByPodPriority{Priority:2000000000, ShutdownGracePeriodSeconds:15552000}}
        	            	actual  : []config.ShutdownGracePeriodByPodPriority{config.ShutdownGracePeriodByPodPriority{Priority:0, ShutdownGracePeriodSeconds:15984000}, config.ShutdownGracePeriodByPodPriority{Priority:2000000000, ShutdownGracePeriodSeconds:15552000}}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -3,3 +3,3 @@
        	            	   Priority: (int32) 0,
        	            	-  ShutdownGracePeriodSeconds: (int64) 15768000
        	            	+  ShutdownGracePeriodSeconds: (int64) 15984000
        	            	  },
        	Test:       	TestCtestMigrateConfig
    ctest_nodeshutdown_manager_test.go:120: migrateConfig() = [{0 15984000} {2000000000 15552000}], want [{0 15768000} {2000000000 15552000}] (case extremely large durations)
--- FAIL: TestCtestMigrateConfig (0.00s)
=== RUN   TestCtestGroupByPriority
[DEBUG-CTEST 2026-02-09 18:45:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/nodeshutdown/ctest_nodeshutdown_manager_test.go:265]: Running TestCtestGroupByPriority with 5 cases
Running 0 th test case: migrate config with mixed priorities
Running 1 th test case: pod priority ordering
Running 2 th test case: empty pod list
    ctest_nodeshutdown_manager_test.go:268: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/nodeshutdown/ctest_nodeshutdown_manager_test.go:268
        	Error:      	Not equal: 
        	            	expected: []nodeshutdown.podShutdownGroup{}
        	            	actual  : []nodeshutdown.podShutdownGroup{nodeshutdown.podShutdownGroup{ShutdownGracePeriodByPodPriority:config.ShutdownGracePeriodByPodPriority{Priority:10, ShutdownGracePeriodSeconds:5}, Pods:[]*v1.Pod(nil)}}
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1,2 +1,9 @@
        	            	-([]nodeshutdown.podShutdownGroup) {
        	            	+([]nodeshutdown.podShutdownGroup) (len=1) {
        	            	+ (nodeshutdown.podShutdownGroup) {
        	            	+  ShutdownGracePeriodByPodPriority: (config.ShutdownGracePeriodByPodPriority) {
        	            	+   Priority: (int32) 10,
        	            	+   ShutdownGracePeriodSeconds: (int64) 5
        	            	+  },
        	            	+  Pods: ([]*v1.Pod) <nil>
        	            	+ }
        	            	 }
        	Test:       	TestCtestGroupByPriority
    ctest_nodeshutdown_manager_test.go:269: groupByPriority() = [{{10 5} []}], want [] (case empty pod list)
Running 3 th test case: nil shutdownGracePeriod config
--- FAIL: TestCtestGroupByPriority (0.00s)
panic: runtime error: index out of range [-1] [recovered]
	panic: runtime error: index out of range [-1]

goroutine 14 [running]:
testing.tRunner.func1.2({0x1053301e0, 0x140007a0138})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1053301e0?, 0x140007a0138?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/nodeshutdown.groupByPriority({0x0, 0x0, 0x1049f3dae?}, {0x14000506058, 0x1, 0x2?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/nodeshutdown/nodeshutdown_manager.go:291 +0x680
k8s.io/kubernetes/pkg/kubelet/nodeshutdown.TestCtestGroupByPriority(0x14000504c40)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/nodeshutdown/ctest_nodeshutdown_manager_test.go:268 +0x1288
testing.tRunner(0x14000504c40, 0x10550ad90)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/nodeshutdown	0.579s
?   	k8s.io/kubernetes/pkg/kubelet/nodeshutdown/systemd	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/nodestatus	0.455s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/oom		coverage: 0.0% of statements
=== RUN   TestCtestEventedPLEG_getPodIPs
[DEBUG-CTEST 2026-02-09 18:45:59 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/pleg/ctest_evented_test.go:169]: Starting TestEventedPLEG_getPodIPs with 8 cases
Running test case #0: status ips is not empty
=== RUN   TestCtestEventedPLEG_getPodIPs/status_ips_is_not_empty
Running test case #1: status ips is empty and SandboxStatuses has PodSandboxState_SANDBOX_READY state
=== RUN   TestCtestEventedPLEG_getPodIPs/status_ips_is_empty_and_SandboxStatuses_has_PodSandboxState_SANDBOX_READY_state
Running test case #2: status and cache ips are empty
=== RUN   TestCtestEventedPLEG_getPodIPs/status_and_cache_ips_are_empty
Running test case #3: sandbox state is no PodSandboxState_SANDBOX_READY
=== RUN   TestCtestEventedPLEG_getPodIPs/sandbox_state_is_no_PodSandboxState_SANDBOX_READY
Running test case #4: nil status
=== RUN   TestCtestEventedPLEG_getPodIPs/nil_status
--- FAIL: TestCtestEventedPLEG_getPodIPs (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/status_ips_is_not_empty (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/status_ips_is_empty_and_SandboxStatuses_has_PodSandboxState_SANDBOX_READY_state (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/status_and_cache_ips_are_empty (0.00s)
    --- PASS: TestCtestEventedPLEG_getPodIPs/sandbox_state_is_no_PodSandboxState_SANDBOX_READY (0.00s)
    --- FAIL: TestCtestEventedPLEG_getPodIPs/nil_status (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x38 pc=0x1028f9de0]

goroutine 91 [running]:
testing.tRunner.func1.2({0x10304e4e0, 0x104b1deb0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10304e4e0?, 0x104b1deb0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).getPodIPs(0x140000a2f08?, {0x10296109b?, 0x63?}, 0x66b?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/pleg/evented.go:298 +0x90
k8s.io/kubernetes/pkg/kubelet/pleg.TestCtestEventedPLEG_getPodIPs.func1(0x14000583340)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/pleg/ctest_evented_test.go:177 +0x38
testing.tRunner(0x14000583340, 0x14000599380)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 86
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/pleg	0.934s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager	1.846s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/cache	2.022s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/metrics	1.442s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestOperationExecutor_RegisterPlugin_ConcurrentRegisterPlugin
--- PASS: TestCtestOperationExecutor_RegisterPlugin_ConcurrentRegisterPlugin (0.00s)
=== RUN   TestCtestOperationExecutor_RegisterPlugin_SerialRegisterPlugin
--- PASS: TestCtestOperationExecutor_RegisterPlugin_SerialRegisterPlugin (5.00s)
=== RUN   TestCtestOperationExecutor_UnregisterPlugin_ConcurrentUnregisterPlugin
--- PASS: TestCtestOperationExecutor_UnregisterPlugin_ConcurrentUnregisterPlugin (0.00s)
=== RUN   TestCtestOperationExecutor_UnregisterPlugin_SerialUnregisterPlugin
--- PASS: TestCtestOperationExecutor_UnregisterPlugin_SerialUnregisterPlugin (5.00s)
PASS
coverage: 8.3% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/operationexecutor	11.252s	coverage: 8.3% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/pluginwatcher	0.996s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/pluginmanager/pluginwatcher/example_plugin_apis/v1beta1		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/kubelet/pluginmanager/pluginwatcher/example_plugin_apis/v1beta2		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pluginmanager/reconciler	0.482s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestParsePodFullName
--- PASS: TestCtestParsePodFullName (0.00s)
=== RUN   TestCtestCreateMirrorPod
=== RUN   TestCtestCreateMirrorPod/cannot_get_node
=== RUN   TestCtestCreateMirrorPod/node_missing_UID
=== RUN   TestCtestCreateMirrorPod/successfully_fetched_node
=== RUN   TestCtestCreateMirrorPod/node_UID_empty_string
=== RUN   TestCtestCreateMirrorPod/node_with_extra_irrelevant_field
=== RUN   TestCtestCreateMirrorPod/node_nil_(simulates_not_found)
--- PASS: TestCtestCreateMirrorPod (0.00s)
    --- PASS: TestCtestCreateMirrorPod/cannot_get_node (0.00s)
    --- PASS: TestCtestCreateMirrorPod/node_missing_UID (0.00s)
    --- PASS: TestCtestCreateMirrorPod/successfully_fetched_node (0.00s)
    --- PASS: TestCtestCreateMirrorPod/node_UID_empty_string (0.00s)
    --- PASS: TestCtestCreateMirrorPod/node_with_extra_irrelevant_field (0.00s)
    --- PASS: TestCtestCreateMirrorPod/node_nil_(simulates_not_found) (0.00s)
PASS
coverage: 13.8% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/pod	0.665s	coverage: 13.8% of statements
	k8s.io/kubernetes/pkg/kubelet/pod/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/podcertificate	1.084s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGetPodsToPreempt

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/ctest_preemption_test.go:192]: Number of test cases: 16
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
    ctest_preemption_test.go:202: equal pods and resources requirements: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 3 th test case.
Running 4 th test case.
    ctest_preemption_test.go:202: choose between bestEffort and burstable: expected [&Pod{ObjectMeta:{ bestEffort     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:bestEffort-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ bestEffort     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:bestEffort-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 5 th test case.
    ctest_preemption_test.go:202: choose between burstable and guaranteed: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 6 th test case.
    ctest_preemption_test.go:202: choose lower request burstable if it meets requirements: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 7 th test case.
    ctest_preemption_test.go:202: choose higher request burstable if lower does not meet requirements: expected [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 8 th test case.
    ctest_preemption_test.go:202: multiple pods required: expected [&Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 9 th test case.
    ctest_preemption_test.go:202: evict guaranteed when we have to, and dont evict the extra burstable: expected [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ high-request-guaranteed     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-guaranteed-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ high-request-burstable     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-burstable-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{ high-request-guaranteed     0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:high-request-guaranteed-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Requests:ResourceList{cpu: {{300 -3} {<nil>} 300m DecimalSI},memory: {{314572800 0} {<nil>} 300Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 10 th test case.
    ctest_preemption_test.go:202: evict cluster critical pod for node critical pod: expected [&Pod{ObjectMeta:{ cluster-critical kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:cluster-critical-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:system-cluster-critical,Priority:*2000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] but got [&Pod{ObjectMeta:{ cluster-critical kube-system    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{Container{Name:cluster-critical-container,Image:,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{104857600 0} {<nil>} 100Mi BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:,ImagePullPolicy:,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,RestartPolicyRules:[]ContainerRestartRule{},},},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:system-cluster-critical,Priority:*2000000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}]
Running 11 th test case.
Running 12 th test case.
--- FAIL: TestCtestGetPodsToPreempt (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0xb8 pc=0x10596c848]

goroutine 48 [running]:
testing.tRunner.func1.2({0x106fc3f00, 0x108b581f0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x106fc3f00?, 0x108b581f0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/types.GetPodSource(0x14000067198?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:106 +0x18
k8s.io/kubernetes/pkg/kubelet/types.IsStaticPod(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:155
k8s.io/kubernetes/pkg/kubelet/types.IsCriticalPod(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:161 +0x20
k8s.io/kubernetes/pkg/kubelet/types.Preemptable(0x0, 0x1400070bb08)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/pod_update.go:176 +0x24
k8s.io/kubernetes/pkg/kubelet/preemption.sortPodsByQOS(0x0, {0x140006506b8, 0x1, 0x14000067270?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/preemption.go:248 +0x174
k8s.io/kubernetes/pkg/kubelet/preemption.getPodsToPreempt(0x107429cc8?, {0x140006506b8?, 0x1068dbe45?, 0x19?}, {0x14000472ea0, 0x3, 0x4})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/preemption.go:134 +0xa4
k8s.io/kubernetes/pkg/kubelet/preemption.TestCtestGetPodsToPreempt(0x14000103c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/preemption/ctest_preemption_test.go:196 +0x1820
testing.tRunner(0x14000103c00, 0x10741c940)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/preemption	1.657s
=== RUN   TestCtestAddRemovePodsWithRestartableInitContainer

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:42]: matched config: {test_fixture.json [restartable init container probes] initContainers [pods] {[] [{restartable-init  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] &Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} &Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} &Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:1,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} nil    nil false false false}] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initContainers" in requested fixtures
2026/02/09 18:46:11 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:46:11 Base JSON size: 402 bytes
2026/02/09 18:46:11 Number of external values: 0
2026/02/09 18:46:11 BASE DATA (type: map[string]interface {}):
{
  "containers": null,
  "initContainers": [
    {
      "livenessProbe": {
        "exec": {},
        "failureThreshold": 3,
        "periodSeconds": 1,
        "successThreshold": 1,
        "timeoutSeconds": 1
      },
      "name": "restartable-init",
      "readinessProbe": {
        "exec": {},
        "failureThreshold": 3,
        "periodSeconds": 1,
        "successThreshold": 1,
        "timeoutSeconds": 1
      },
      "resources": {},
      "startupProbe": {
        "exec": {},
        "failureThreshold": 3,
        "periodSeconds": 1,
        "successThreshold": 1,
        "timeoutSeconds": 1
      }
    }
  ]
}
2026/02/09 18:46:11 
=== UNION COMPLETE ===
2026/02/09 18:46:11 Generated 0 result(s)
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"initContainers":[{"livenessProbe":{"exec":{},"failureThreshold":3,"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1},"name":"restartable-init","readinessProbe":{"exec":{},"failureThreshold":3,"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1},"resources":{},"startupProbe":{"exec":{},"failureThreshold":3,"periodSeconds":1,"successThreshold":1,"timeoutSeconds":1}}]})[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:50]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestAddRemovePodsWithRestartableInitContainer (0.01s)
=== RUN   TestCtestUpdatePodStatusWithInitContainers

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:189]: matched config: {test_fixture.json [init container spec without probes] initContainers [pods] {[] [{not_started_container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {started_container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {terminated_container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "initContainers" in requested fixtures
2026/02/09 18:46:11 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:46:11 Base JSON size: 177 bytes
2026/02/09 18:46:11 Number of external values: 0
2026/02/09 18:46:11 BASE DATA (type: map[string]interface {}):
{
  "containers": null,
  "initContainers": [
    {
      "name": "not_started_container",
      "resources": {}
    },
    {
      "name": "started_container",
      "resources": {}
    },
    {
      "name": "terminated_container",
      "resources": {}
    }
  ]
}
2026/02/09 18:46:11 
=== UNION COMPLETE ===
2026/02/09 18:46:11 Generated 0 result(s)
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null,"initContainers":[{"name":"not_started_container","resources":{}},{"name":"started_container","resources":{}},{"name":"terminated_container","resources":{}}]})[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/ctest_prober_manager_test.go:197]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestUpdatePodStatusWithInitContainers (0.00s)
=== RUN   TestCtestTCPPortExhaustion
    ctest_scale_test.go:33: skipping TCP port exhaustion tests
--- SKIP: TestCtestTCPPortExhaustion (0.00s)
PASS
coverage: 3.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/prober	2.878s	coverage: 3.0% of statements
=== RUN   TestCtestCacheOperations
--- PASS: TestCtestCacheOperations (0.00s)
=== RUN   TestCtestResult_ToPrometheusType
[DEBUG-CTEST 2026-02-09 18:46:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/results/ctest_results_manager_test.go:41]: Start TestResult_ToPrometheusType
[DEBUG-CTEST 2026-02-09 18:46:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/results/ctest_results_manager_test.go:74]: Number of test cases: 5
Running 0 th test case: result is Success
=== RUN   TestCtestResult_ToPrometheusType/result_is_Success
Running 1 th test case: result is Failure
=== RUN   TestCtestResult_ToPrometheusType/result_is_Failure
Running 2 th test case: result is other
=== RUN   TestCtestResult_ToPrometheusType/result_is_other
Running 3 th test case: negative unknown result
=== RUN   TestCtestResult_ToPrometheusType/negative_unknown_result
Running 4 th test case: large unknown result
=== RUN   TestCtestResult_ToPrometheusType/large_unknown_result
--- PASS: TestCtestResult_ToPrometheusType (0.00s)
    --- PASS: TestCtestResult_ToPrometheusType/result_is_Success (0.00s)
    --- PASS: TestCtestResult_ToPrometheusType/result_is_Failure (0.00s)
    --- PASS: TestCtestResult_ToPrometheusType/result_is_other (0.00s)
    --- PASS: TestCtestResult_ToPrometheusType/negative_unknown_result (0.00s)
    --- PASS: TestCtestResult_ToPrometheusType/large_unknown_result (0.00s)
=== RUN   TestCtestResult_String
[DEBUG-CTEST 2026-02-09 18:46:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/results/ctest_results_manager_test.go:86]: Start TestResult_String
[DEBUG-CTEST 2026-02-09 18:46:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/prober/results/ctest_results_manager_test.go:119]: Number of test cases: 5
Running 0 th test case: result is Success
=== RUN   TestCtestResult_String/result_is_Success
Running 1 th test case: result is Failure
=== RUN   TestCtestResult_String/result_is_Failure
Running 2 th test case: result is other
=== RUN   TestCtestResult_String/result_is_other
Running 3 th test case: positive unknown result
=== RUN   TestCtestResult_String/positive_unknown_result
Running 4 th test case: negative unknown result
=== RUN   TestCtestResult_String/negative_unknown_result
--- PASS: TestCtestResult_String (0.00s)
    --- PASS: TestCtestResult_String/result_is_Success (0.00s)
    --- PASS: TestCtestResult_String/result_is_Failure (0.00s)
    --- PASS: TestCtestResult_String/result_is_other (0.00s)
    --- PASS: TestCtestResult_String/positive_unknown_result (0.00s)
    --- PASS: TestCtestResult_String/negative_unknown_result (0.00s)
PASS
coverage: 92.3% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/prober/results	2.113s	coverage: 92.3% of statements
	k8s.io/kubernetes/pkg/kubelet/prober/testing		coverage: 0.0% of statements
=== RUN   TestCtestGetContainerOOMScoreAdjust

==================== CTEST START ====================
Running test case #0: cpu-limit
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [cpu-limit] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{cpu-limit  [] []  [] [] [] {map[cpu:{{10 0} {<nil>} 10 DecimalSI}] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"cpu-limit","resources":{"limits":{"cpu":"10"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #1: memory-limit-cpu-request
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [memory-limit-cpu-request] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{memory-limit-cpu-request  [] []  [] [] [] {map[memory:{{10 9} {<nil>} 10G DecimalSI}] map[cpu:{{0 0} {<nil>} 0 DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"memory-limit-cpu-request","resources":{"limits":{"memory":"10G"},"requests":{"cpu":"0"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #2: zero-memory-limit
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [zero-memory-limit] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{zero-memory-limit  [] []  [] [] [] {map[memory:{{0 0} {<nil>} 0 DecimalSI}] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"zero-memory-limit","resources":{"limits":{"memory":"0"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #3: no-request-limit
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [no-request-limit] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{no-request-limit  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"no-request-limit","resources":{}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #4: equal-request-limit-cpu-memory
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [equal-request-limit-cpu-memory] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{equal-request-limit-cpu-memory  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{10 9} {<nil>} 10G DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{10 9} {<nil>} 10G DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"equal-request-limit-cpu-memory","resources":{"limits":{"cpu":"5m","memory":"10G"},"requests":{"cpu":"5m","memory":"10G"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #5: cpu-unlimited-memory-limited-with-requests
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [cpu-unlimited-memory-limited-with-requests] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{cpu-unlimited-memory-limited-with-requests  [] []  [] [] [] {map[memory:{{10 9} {<nil>} 10G DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{4000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"cpu-unlimited-memory-limited-with-requests","resources":{"limits":{"memory":"10G"},"requests":{"cpu":"5m","memory":"4G"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #6: request-no-limit
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [request-no-limit] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{request-no-limit  [] []  [] [] [] {map[] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{7999999999 0} {<nil>} 7999999999 DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"request-no-limit","resources":{"requests":{"cpu":"5m","memory":"7999999999"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #7: cluster-critical
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [cluster-critical] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{cluster-critical  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] [] system-cluster-critical 0x107e942b8 nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"cluster-critical","resources":{}}],"priority":2000000000,"priorityClassName":"system-cluster-critical"},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #8: node-critical
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [node-critical] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{node-critical  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] [] system-node-critical 0x107e942bc nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"node-critical","resources":{}}],"priority":2000001000,"priorityClassName":"system-node-critical"},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #9: burstable-unique-container-pod
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-unique-container-pod] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{burstable-unique-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"burstable-unique-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #10: burstable-mixed-unique-main-container-pod
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-mixed-unique-main-container-pod] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{init-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] [{main-1  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"main-1","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}}],"initContainers":[{"name":"init-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #11: burstable-mixed-multi-container-small-sidecar-pod
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-mixed-multi-container-small-sidecar-pod] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{init-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {sidecar-small-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{400000000 0} {<nil>}  DecimalSI}] []} [] 0x107fa65d0 [] [] [] nil nil nil nil    nil false false false}] [{main-1  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"main-1","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}}],"initContainers":[{"name":"init-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}},{"name":"sidecar-small-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"400M"}},"restartPolicy":"Always"}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #12: burstable-mixed-multi-container-sample-request-pod
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-mixed-multi-container-sample-request-pod] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{init-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {sidecar-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] 0x107fa65d0 [] [] [] nil nil nil nil    nil false false false}] [{main-1  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"main-1","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}}],"initContainers":[{"name":"init-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}},{"name":"sidecar-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}},"restartPolicy":"Always"}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #13: burstable-mixed-multi-container-big-sidecar-container-pod
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-mixed-multi-container-big-sidecar-container-pod] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{init-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {sidecar-big-container  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{4000000000 0} {<nil>}  DecimalSI}] []} [] 0x107fa65d0 [] [] [] nil nil nil nil    nil false false false}] [{main-1  [] []  [] [] [] {map[memory:{{1000001000 0} {<nil>}  DecimalSI}] map[memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"main-1","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}}],"initContainers":[{"name":"init-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"1G"}}},{"name":"sidecar-big-container","resources":{"limits":{"memory":"1000001k"},"requests":{"memory":"4G"}},"restartPolicy":"Always"}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #14: guaranteed-pod-resources-no-container-resources
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [guaranteed-pod-resources-no-container-resources] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{no-request-limit-1  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {no-request-limit-2  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"no-request-limit-1","resources":{}},{"name":"no-request-limit-2","resources":{}}],"resources":{"limits":{"cpu":"5m","memory":"2G"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #15: guaranteed-pod-resources-equal-container-resources
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [guaranteed-pod-resources-equal-container-resources] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{guaranteed-container-1  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {guaranteed-container-2  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"guaranteed-container-1","resources":{"limits":{"cpu":"5m","memory":"1G"},"requests":{"cpu":"5m","memory":"1G"}}},{"name":"guaranteed-container-2","resources":{"limits":{"cpu":"5m","memory":"1G"},"requests":{"cpu":"5m","memory":"1G"}}}],"resources":{"limits":{"cpu":"5m","memory":"2G"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #16: guaranteed-pod-resources-unequal-container-requests
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [guaranteed-pod-resources-unequal-container-requests] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{burstable-container  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000001000 0} {<nil>}  DecimalSI}] map[cpu:{{3 -3} {<nil>} 3m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {best-effort-container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"burstable-container","resources":{"limits":{"cpu":"5m","memory":"1000001k"},"requests":{"cpu":"3m","memory":"1G"}}},{"name":"best-effort-container","resources":{}}],"resources":{"limits":{"cpu":"5m","memory":"2G"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #17: burstable-pod-resources-no-container-resources
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-pod-resources-no-container-resources] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{no-request-limit-1  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {no-request-limit-2  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000001000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"no-request-limit-1","resources":{}},{"name":"no-request-limit-2","resources":{}}],"resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #18: burstable-pod-resources-equal-container-requests
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-pod-resources-equal-container-requests] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{guaranteed-container-1  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {guaranteed-container-2  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000001000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"guaranteed-container-1","resources":{"limits":{"cpu":"5m","memory":"1G"},"requests":{"cpu":"5m","memory":"1G"}}},{"name":"guaranteed-container-2","resources":{"limits":{"cpu":"5m","memory":"1G"},"requests":{"cpu":"5m","memory":"1G"}}}],"resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #19: burstable-pod-resources-unequal-container-requests
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-pod-resources-unequal-container-requests] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{burstable-container  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] map[cpu:{{3 -3} {<nil>} 3m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {best-effort-container  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000001000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"burstable-container","resources":{"limits":{"cpu":"5m","memory":"1G"},"requests":{"cpu":"3m","memory":"1G"}}},{"name":"best-effort-container","resources":{}}],"resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #20: burstable-pod-resources-no-container-resources-with-sidecar
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-pod-resources-no-container-resources-with-sidecar] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{no-request-limit-sidecar  [] []  [] [] [] {map[] map[] []} [] 0x107fa65d0 [] [] [] nil nil nil nil    nil false false false}] [{no-request-limit  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000001000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"no-request-limit","resources":{}}],"initContainers":[{"name":"no-request-limit-sidecar","resources":{},"restartPolicy":"Always"}],"resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #21: burstable-pod-resources-equal-container-requests-with-sidecar
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-pod-resources-equal-container-requests-with-sidecar] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{burstable-sidecar  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000001000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] 0x107fa65d0 [] [] [] nil nil nil nil    nil false false false}] [{burstable-container  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000001000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000001000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"burstable-container","resources":{"limits":{"cpu":"5m","memory":"1000001k"},"requests":{"cpu":"5m","memory":"1G"}}}],"initContainers":[{"name":"burstable-sidecar","resources":{"limits":{"cpu":"5m","memory":"1000001k"},"requests":{"cpu":"5m","memory":"1G"}},"restartPolicy":"Always"}],"resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #22: burstable-pod-resources-unequal-container-requests-with-sidecar
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [burstable-pod-resources-unequal-container-requests-with-sidecar] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [{burstable-sidecar  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{2000001000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{200000000 0} {<nil>}  DecimalSI}] []} [] 0x107fa65d0 [] [] [] nil nil nil nil    nil false false false}] [{burstable-container-1  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{2000001000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{1000000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false} {burstable-container-2  [] []  [] [] [] {map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{2000001000 0} {<nil>}  DecimalSI}] map[cpu:{{5 -3} {<nil>} 5m DecimalSI} memory:{{500000000 0} {<nil>}  DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] &ResourceRequirements{Limits:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000001000 0} {<nil>}  DecimalSI},},Requests:ResourceList{cpu: {{5 -3} {<nil>} 5m DecimalSI},memory: {{2000000000 0} {<nil>}  DecimalSI},},Claims:[]ResourceClaim{},} <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"burstable-container-1","resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"1G"}}},{"name":"burstable-container-2","resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"500M"}}}],"initContainers":[{"name":"burstable-sidecar","resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"200M"}},"restartPolicy":"Always"}],"resources":{"limits":{"cpu":"5m","memory":"2000001k"},"requests":{"cpu":"5m","memory":"2G"}}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #23: empty-pod
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [empty-pod] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":null},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.
Running test case #24: negative-memory-limit
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:251]: Matched config item: {test_fixture.json [negative-memory-limit] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [{neg-mem  [] []  [] [] [] {map[memory:{{-1073741824 0} {<nil>} -1Gi BinarySI}] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"neg-mem","resources":{"limits":{"memory":"-1Gi"}}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/qos/ctest_policy_test.go:260]: No generated configurations – skipping test.

==================== CTEST END ======================
--- PASS: TestCtestGetContainerOOMScoreAdjust (0.02s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/qos	0.953s	coverage: 0.0% of statements
=== RUN   TestCtestLookupRuntimeHandler
=== RUN   TestCtestLookupRuntimeHandler/""->""(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"native"->""(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"sandbox"->"kata-containers"(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"phantom"->""(err:true)
=== RUN   TestCtestLookupRuntimeHandler/"nil"->""(err:false)
=== RUN   TestCtestLookupRuntimeHandler/"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"->""(err:true)
=== RUN   TestCtestLookupRuntimeHandler/"!@#$%^&*()"->""(err:true)
--- PASS: TestCtestLookupRuntimeHandler (0.11s)
    --- PASS: TestCtestLookupRuntimeHandler/""->""(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"native"->""(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"sandbox"->"kata-containers"(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"phantom"->""(err:true) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"nil"->""(err:false) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"->""(err:true) (0.00s)
    --- PASS: TestCtestLookupRuntimeHandler/"!@#$%^&*()"->""(err:true) (0.00s)
PASS
coverage: 93.3% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/runtimeclass	0.541s	coverage: 93.3% of statements
	k8s.io/kubernetes/pkg/kubelet/runtimeclass/testing		coverage: 0.0% of statements
=== RUN   TestCtestCacheBasedSecretManager

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/secret/ctest_secret_manager_test.go:86]: get default configs: {test_fixture.json [default pod spec with secrets] spec [pods] {[] [] [{container-base  [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:15 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"name":"container-base","resources":{}}]})[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:46:15 [DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [{container-base registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [{container-base registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/secret/ctest_secret_manager_test.go:98]: New Json Test Configs: [{"volumes":[{"name":"config","hostPath":{"path":"/etc/kubernetes"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":[{"name":"container-base","image":"registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0","args":["--socketpath=/kms/kms.sock","--cloud-config=/etc/kubernetes/cloud-config"],"resources":{},"volumeMounts":[{"name":"config","mountPath":"/etc/kubernetes/"},{"name":"socket-dir","mountPath":"/kms/"}]}]}]
[DEBUG-CTEST 2026-02-09 18:46:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/secret/ctest_secret_manager_test.go:99]: Number of test cases: 1
Running 0 th test cases.
{[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [{container-base registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{config false <nil> /etc/kubernetes/  <nil> } {socket-dir false <nil> /kms/  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    secret_manager_test.go:40: unexpected actions: &errors.errorString{s:"object \"edge-ns\"/\"dup\" not registered"}

==================== CTEST END ======================
--- FAIL: TestCtestCacheBasedSecretManager (0.00s)
FAIL
coverage: 17.9% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/secret	1.328s
=== RUN   TestCtestIsSubPath
    ctest_auth_test.go:46: subpath empty, path non-empty: expected true, got false
    ctest_auth_test.go:46: subpath non-empty, path empty: expected false, got true
    ctest_auth_test.go:46: no leading slash: expected true, got false
    ctest_auth_test.go:46: dot dot segment: expected false, got true
    ctest_auth_test.go:46: trailing    slash mismatch: expected true, got false
--- FAIL: TestCtestIsSubPath (0.00s)
=== RUN   TestCtestGetRequestAttributes
=== RUN   TestCtestGetRequestAttributes/POST:/statusz
=== RUN   TestCtestGetRequestAttributes/GET:/statusz
=== RUN   TestCtestGetRequestAttributes/PUT:/statusz
=== RUN   TestCtestGetRequestAttributes/PATCH:/statusz
=== RUN   TestCtestGetRequestAttributes/DELETE:/statusz
=== RUN   TestCtestGetRequestAttributes/POST:/containerLogs/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/containerLogs/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/containerLogs/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/containerLogs/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/containerLogs/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/debug/flags/v
=== RUN   TestCtestGetRequestAttributes/GET:/debug/flags/v
=== RUN   TestCtestGetRequestAttributes/PUT:/debug/flags/v
=== RUN   TestCtestGetRequestAttributes/PATCH:/debug/flags/v
=== RUN   TestCtestGetRequestAttributes/DELETE:/debug/flags/v
=== RUN   TestCtestGetRequestAttributes/POST:/healthz/ping
=== RUN   TestCtestGetRequestAttributes/GET:/healthz/ping
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz/ping
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz/ping
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz/ping
=== RUN   TestCtestGetRequestAttributes/POST:/logs/
=== RUN   TestCtestGetRequestAttributes/GET:/logs/
=== RUN   TestCtestGetRequestAttributes/PUT:/logs/
=== RUN   TestCtestGetRequestAttributes/PATCH:/logs/
=== RUN   TestCtestGetRequestAttributes/DELETE:/logs/
=== RUN   TestCtestGetRequestAttributes/POST:/logs/{logpath:*}
=== RUN   TestCtestGetRequestAttributes/GET:/logs/{logpath:*}
=== RUN   TestCtestGetRequestAttributes/PUT:/logs/{logpath:*}
=== RUN   TestCtestGetRequestAttributes/PATCH:/logs/{logpath:*}
=== RUN   TestCtestGetRequestAttributes/DELETE:/logs/{logpath:*}
=== RUN   TestCtestGetRequestAttributes/POST:/metrics
=== RUN   TestCtestGetRequestAttributes/GET:/metrics
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/cadvisor
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/cadvisor
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/cadvisor
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/cadvisor
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/cadvisor
=== RUN   TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/configz
=== RUN   TestCtestGetRequestAttributes/GET:/configz
=== RUN   TestCtestGetRequestAttributes/PUT:/configz
=== RUN   TestCtestGetRequestAttributes/PATCH:/configz
=== RUN   TestCtestGetRequestAttributes/DELETE:/configz
=== RUN   TestCtestGetRequestAttributes/POST:/pods/
=== RUN   TestCtestGetRequestAttributes/GET:/pods/
=== RUN   TestCtestGetRequestAttributes/PUT:/pods/
=== RUN   TestCtestGetRequestAttributes/PATCH:/pods/
=== RUN   TestCtestGetRequestAttributes/DELETE:/pods/
=== RUN   TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/healthz
=== RUN   TestCtestGetRequestAttributes/GET:/healthz
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz
=== RUN   TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}/{uid}
=== RUN   TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}/{uid}
=== RUN   TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}/{uid}
=== RUN   TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}/{uid}
=== RUN   TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}/{uid}
=== RUN   TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/healthz/syncloop
=== RUN   TestCtestGetRequestAttributes/GET:/healthz/syncloop
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz/syncloop
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz/syncloop
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz/syncloop
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/slis
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/slis
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/slis
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/slis
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/slis
=== RUN   TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}
=== RUN   TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}
=== RUN   TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}
=== RUN   TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}
=== RUN   TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}
=== RUN   TestCtestGetRequestAttributes/POST:/flagz
=== RUN   TestCtestGetRequestAttributes/GET:/flagz
=== RUN   TestCtestGetRequestAttributes/PUT:/flagz
=== RUN   TestCtestGetRequestAttributes/PATCH:/flagz
=== RUN   TestCtestGetRequestAttributes/DELETE:/flagz
=== RUN   TestCtestGetRequestAttributes/POST:/debug/pprof/{subpath:*}
=== RUN   TestCtestGetRequestAttributes/GET:/debug/pprof/{subpath:*}
=== RUN   TestCtestGetRequestAttributes/PUT:/debug/pprof/{subpath:*}
=== RUN   TestCtestGetRequestAttributes/PATCH:/debug/pprof/{subpath:*}
=== RUN   TestCtestGetRequestAttributes/DELETE:/debug/pprof/{subpath:*}
=== RUN   TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/resource
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/resource
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/resource
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/resource
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/resource
=== RUN   TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/checkpoint/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/checkpoint/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/checkpoint/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/checkpoint/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/checkpoint/{podNamespace}/{podID}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/healthz/log
=== RUN   TestCtestGetRequestAttributes/GET:/healthz/log
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz/log
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz/log
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz/log
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/probes
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/probes
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/probes
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/probes
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/probes
=== RUN   TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{uid}/{containerName}
=== RUN   TestCtestGetRequestAttributes/POST:/runningpods/
=== RUN   TestCtestGetRequestAttributes/GET:/runningpods/
=== RUN   TestCtestGetRequestAttributes/PUT:/runningpods/
=== RUN   TestCtestGetRequestAttributes/PATCH:/runningpods/
=== RUN   TestCtestGetRequestAttributes/DELETE:/runningpods/
=== RUN   TestCtestGetRequestAttributes/POST:/stats/
=== RUN   TestCtestGetRequestAttributes/GET:/stats/
=== RUN   TestCtestGetRequestAttributes/PUT:/stats/
=== RUN   TestCtestGetRequestAttributes/PATCH:/stats/
=== RUN   TestCtestGetRequestAttributes/DELETE:/stats/
=== RUN   TestCtestGetRequestAttributes/POST:/stats/summary
=== RUN   TestCtestGetRequestAttributes/GET:/stats/summary
=== RUN   TestCtestGetRequestAttributes/PUT:/stats/summary
=== RUN   TestCtestGetRequestAttributes/PATCH:/stats/summary
=== RUN   TestCtestGetRequestAttributes/DELETE:/stats/summary
=== RUN   TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}/{uid}#01
=== RUN   TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}/{uid}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}/{uid}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}/{uid}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}/{uid}#01
=== RUN   TestCtestGetRequestAttributes/POST:/stats/summary#01
=== RUN   TestCtestGetRequestAttributes/GET:/stats/summary#01
=== RUN   TestCtestGetRequestAttributes/PUT:/stats/summary#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/stats/summary#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/stats/summary#01
=== RUN   TestCtestGetRequestAttributes/POST:/checkpoint/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/checkpoint/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/checkpoint/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/checkpoint/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/checkpoint/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/healthz/ping#01
=== RUN   TestCtestGetRequestAttributes/GET:/healthz/ping#01
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz/ping#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz/ping#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz/ping#01
=== RUN   TestCtestGetRequestAttributes/POST:/logs/#01
=== RUN   TestCtestGetRequestAttributes/GET:/logs/#01
=== RUN   TestCtestGetRequestAttributes/PUT:/logs/#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/logs/#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/logs/#01
=== RUN   TestCtestGetRequestAttributes/POST:/runningpods/#01
=== RUN   TestCtestGetRequestAttributes/GET:/runningpods/#01
=== RUN   TestCtestGetRequestAttributes/PUT:/runningpods/#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/runningpods/#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/runningpods/#01
=== RUN   TestCtestGetRequestAttributes/POST:/healthz/syncloop#01
=== RUN   TestCtestGetRequestAttributes/GET:/healthz/syncloop#01
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz/syncloop#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz/syncloop#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz/syncloop#01
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/cadvisor#01
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/cadvisor#01
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/cadvisor#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/cadvisor#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/cadvisor#01
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/probes#01
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/probes#01
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/probes#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/probes#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/probes#01
=== RUN   TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/debug/flags/v#01
=== RUN   TestCtestGetRequestAttributes/GET:/debug/flags/v#01
=== RUN   TestCtestGetRequestAttributes/PUT:/debug/flags/v#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/debug/flags/v#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/debug/flags/v#01
=== RUN   TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/containerLogs/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/containerLogs/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/containerLogs/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/containerLogs/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/containerLogs/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/healthz#01
=== RUN   TestCtestGetRequestAttributes/GET:/healthz#01
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz#01
=== RUN   TestCtestGetRequestAttributes/POST:/configz#01
=== RUN   TestCtestGetRequestAttributes/GET:/configz#01
=== RUN   TestCtestGetRequestAttributes/PUT:/configz#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/configz#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/configz#01
=== RUN   TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/healthz/log#01
=== RUN   TestCtestGetRequestAttributes/GET:/healthz/log#01
=== RUN   TestCtestGetRequestAttributes/PUT:/healthz/log#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/healthz/log#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/healthz/log#01
=== RUN   TestCtestGetRequestAttributes/POST:/logs/{logpath:*}#01
=== RUN   TestCtestGetRequestAttributes/GET:/logs/{logpath:*}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/logs/{logpath:*}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/logs/{logpath:*}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/logs/{logpath:*}#01
=== RUN   TestCtestGetRequestAttributes/POST:/pods/#01
=== RUN   TestCtestGetRequestAttributes/GET:/pods/#01
=== RUN   TestCtestGetRequestAttributes/PUT:/pods/#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/pods/#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/pods/#01
=== RUN   TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}#01
=== RUN   TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}#01
=== RUN   TestCtestGetRequestAttributes/POST:/stats/#01
=== RUN   TestCtestGetRequestAttributes/GET:/stats/#01
=== RUN   TestCtestGetRequestAttributes/PUT:/stats/#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/stats/#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/stats/#01
=== RUN   TestCtestGetRequestAttributes/POST:/flagz#01
=== RUN   TestCtestGetRequestAttributes/GET:/flagz#01
=== RUN   TestCtestGetRequestAttributes/PUT:/flagz#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/flagz#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/flagz#01
=== RUN   TestCtestGetRequestAttributes/POST:/statusz#01
=== RUN   TestCtestGetRequestAttributes/GET:/statusz#01
=== RUN   TestCtestGetRequestAttributes/PUT:/statusz#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/statusz#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/statusz#01
=== RUN   TestCtestGetRequestAttributes/POST:/metrics#01
=== RUN   TestCtestGetRequestAttributes/GET:/metrics#01
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics#01
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/slis#01
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/slis#01
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/slis#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/slis#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/slis#01
=== RUN   TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/debug/pprof/{subpath:*}#01
=== RUN   TestCtestGetRequestAttributes/GET:/debug/pprof/{subpath:*}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/debug/pprof/{subpath:*}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/debug/pprof/{subpath:*}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/debug/pprof/{subpath:*}#01
=== RUN   TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01
=== RUN   TestCtestGetRequestAttributes/POST:/metrics/resource#01
=== RUN   TestCtestGetRequestAttributes/GET:/metrics/resource#01
=== RUN   TestCtestGetRequestAttributes/PUT:/metrics/resource#01
=== RUN   TestCtestGetRequestAttributes/PATCH:/metrics/resource#01
=== RUN   TestCtestGetRequestAttributes/DELETE:/metrics/resource#01
--- PASS: TestCtestGetRequestAttributes (0.01s)
    --- PASS: TestCtestGetRequestAttributes/POST:/statusz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/statusz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/statusz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/statusz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/statusz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/containerLogs/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/containerLogs/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/containerLogs/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/containerLogs/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/containerLogs/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/debug/flags/v (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/debug/flags/v (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/debug/flags/v (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/debug/flags/v (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/debug/flags/v (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz/ping (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz/ping (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz/ping (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz/ping (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz/ping (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/logs/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/logs/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/logs/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/logs/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/logs/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/logs/{logpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/logs/{logpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/logs/{logpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/logs/{logpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/logs/{logpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/cadvisor (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/cadvisor (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/cadvisor (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/cadvisor (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/cadvisor (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/configz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/configz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/configz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/configz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/configz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/pods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/pods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/pods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/pods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/pods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}/{uid} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}/{uid} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}/{uid} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}/{uid} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}/{uid} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz/syncloop (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz/syncloop (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz/syncloop (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz/syncloop (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz/syncloop (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/slis (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/slis (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/slis (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/slis (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/slis (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/flagz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/flagz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/flagz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/flagz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/flagz (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/debug/pprof/{subpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/debug/pprof/{subpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/debug/pprof/{subpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/debug/pprof/{subpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/debug/pprof/{subpath:*} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/resource (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/resource (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/resource (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/resource (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/resource (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/checkpoint/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/checkpoint/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/checkpoint/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/checkpoint/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/checkpoint/{podNamespace}/{podID}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz/log (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz/log (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz/log (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz/log (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz/log (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/probes (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/probes (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/probes (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/probes (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/probes (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{uid}/{containerName} (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/runningpods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/runningpods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/runningpods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/runningpods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/runningpods/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/stats/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/stats/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/stats/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/stats/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/stats/ (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/stats/summary (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/stats/summary (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/stats/summary (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/stats/summary (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/stats/summary (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}/{uid}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}/{uid}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}/{uid}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}/{uid}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}/{uid}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/stats/summary#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/stats/summary#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/stats/summary#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/stats/summary#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/stats/summary#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/checkpoint/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/checkpoint/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/checkpoint/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/checkpoint/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/checkpoint/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz/ping#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz/ping#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz/ping#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz/ping#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz/ping#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/logs/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/logs/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/logs/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/logs/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/logs/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/runningpods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/runningpods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/runningpods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/runningpods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/runningpods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz/syncloop#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz/syncloop#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz/syncloop#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz/syncloop#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz/syncloop#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/cadvisor#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/cadvisor#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/cadvisor#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/cadvisor#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/cadvisor#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/probes#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/probes#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/probes#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/probes#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/probes#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/debug/flags/v#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/debug/flags/v#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/debug/flags/v#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/debug/flags/v#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/debug/flags/v#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/attach/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/containerLogs/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/containerLogs/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/containerLogs/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/containerLogs/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/containerLogs/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/configz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/configz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/configz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/configz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/configz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/healthz/log#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/healthz/log#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/healthz/log#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/healthz/log#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/healthz/log#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/logs/{logpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/logs/{logpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/logs/{logpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/logs/{logpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/logs/{logpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/pods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/pods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/pods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/pods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/pods/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/portForward/{podNamespace}/{podID}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/portForward/{podNamespace}/{podID}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/portForward/{podNamespace}/{podID}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/portForward/{podNamespace}/{podID}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/portForward/{podNamespace}/{podID}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/stats/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/stats/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/stats/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/stats/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/stats/#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/flagz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/flagz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/flagz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/flagz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/flagz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/statusz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/statusz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/statusz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/statusz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/statusz#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/slis#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/slis#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/slis#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/slis#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/slis#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/run/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/run/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/run/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/run/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/run/{podNamespace}/{podID}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/debug/pprof/{subpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/debug/pprof/{subpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/debug/pprof/{subpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/debug/pprof/{subpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/debug/pprof/{subpath:*}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/exec/{podNamespace}/{podID}/{uid}/{containerName}#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/POST:/metrics/resource#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/GET:/metrics/resource#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PUT:/metrics/resource#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/PATCH:/metrics/resource#01 (0.00s)
    --- PASS: TestCtestGetRequestAttributes/DELETE:/metrics/resource#01 (0.00s)
=== RUN   TestCtestServeWSPortForward
=== RUN   TestCtestServeWSPortForward/no_port
E0209 18:46:16.604694   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
E0209 18:46:16.605079   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.605110   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.605852   88508 server.go:1007] "Unhandled Error" err="query parameter \"port\" cannot be empty" logger="UnhandledError"
=== RUN   TestCtestServeWSPortForward/none_number_port
E0209 18:46:16.606308   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.606324   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.606717   88508 server.go:1007] "Unhandled Error" err="unable to parse \"abc\" as a port: strconv.ParseUint: parsing \"abc\": invalid syntax" logger="UnhandledError"
=== RUN   TestCtestServeWSPortForward/normal_port_with_data_forward
E0209 18:46:16.607183   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.607194   88508 server.go:310] "Adding debug handlers to kubelet server"
writing the client data
E0209 18:46:16.608115   88508 conn.go:339] Error on socket receive: read tcp 127.0.0.1:49685->127.0.0.1:49688: use of closed network connection
=== RUN   TestCtestServeWSPortForward/port_with_whitespace
E0209 18:46:16.608702   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.608719   88508 server.go:310] "Adding debug handlers to kubelet server"
=== RUN   TestCtestServeWSPortForward/extremely_large_number
E0209 18:46:16.610242   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.610262   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.610719   88508 server.go:1007] "Unhandled Error" err="unable to parse \"9999999999\" as a port: strconv.ParseUint: parsing \"9999999999\": value out of range" logger="UnhandledError"
=== RUN   TestCtestServeWSPortForward/0_port
E0209 18:46:16.611014   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.611025   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.611413   88508 server.go:1007] "Unhandled Error" err="port \"0\" must be > 0" logger="UnhandledError"
=== RUN   TestCtestServeWSPortForward/normal_port_with_uid
E0209 18:46:16.612100   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.612135   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.612693   88508 conn.go:339] Error on socket receive: read tcp 127.0.0.1:49698->127.0.0.1:49701: use of closed network connection
=== RUN   TestCtestServeWSPortForward/min_port
E0209 18:46:16.613016   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.613029   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.613494   88508 conn.go:339] Error on socket receive: read tcp 127.0.0.1:49702->127.0.0.1:49705: use of closed network connection
=== RUN   TestCtestServeWSPortForward/max_port
E0209 18:46:16.614357   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.614370   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.614817   88508 conn.go:339] Error on socket receive: read tcp 127.0.0.1:49706->127.0.0.1:49709: use of closed network connection
=== RUN   TestCtestServeWSPortForward/port_with_leading_zeros
E0209 18:46:16.615123   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.615135   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.615716   88508 conn.go:339] Error on socket receive: read tcp 127.0.0.1:49710->127.0.0.1:49713: use of closed network connection
=== RUN   TestCtestServeWSPortForward/port_with_plus_sign
E0209 18:46:16.616052   88508 processstarttime.go:40] Could not get process start time, could not read "/proc": stat /proc: no such file or directory
I0209 18:46:16.616065   88508 server.go:310] "Adding debug handlers to kubelet server"
E0209 18:46:16.616347   88508 server.go:1007] "Unhandled Error" err="unable to parse \" 8000\" as a port: strconv.ParseUint: parsing \" 8000\": invalid syntax" logger="UnhandledError"
    ctest_server_websocket_test.go:98: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/server/ctest_server_websocket_test.go:98
        	Error:      	Not equal: 
        	            	expected: false
        	            	actual  : true
        	Test:       	TestCtestServeWSPortForward/port_with_plus_sign
        	Messages:   	websocket dial
    ctest_server_websocket_test.go:105: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/server/ctest_server_websocket_test.go:105
        	Error:      	Received unexpected error:
        	            	strconv.ParseUint: parsing "+8000": invalid syntax
        	Test:       	TestCtestServeWSPortForward/port_with_plus_sign
        	Messages:   	parse port
--- FAIL: TestCtestServeWSPortForward (0.01s)
    --- PASS: TestCtestServeWSPortForward/no_port (0.00s)
    --- PASS: TestCtestServeWSPortForward/none_number_port (0.00s)
    --- PASS: TestCtestServeWSPortForward/normal_port_with_data_forward (0.00s)
    --- PASS: TestCtestServeWSPortForward/port_with_whitespace (0.00s)
    --- PASS: TestCtestServeWSPortForward/extremely_large_number (0.00s)
    --- PASS: TestCtestServeWSPortForward/0_port (0.00s)
    --- PASS: TestCtestServeWSPortForward/normal_port_with_uid (0.00s)
    --- PASS: TestCtestServeWSPortForward/min_port (0.00s)
    --- PASS: TestCtestServeWSPortForward/max_port (0.00s)
    --- PASS: TestCtestServeWSPortForward/port_with_leading_zeros (0.00s)
    --- FAIL: TestCtestServeWSPortForward/port_with_plus_sign (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x68 pc=0x1054272bc]

goroutine 576 [running]:
testing.tRunner.func1.2({0x1066220e0, 0x107b49350})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1066220e0?, 0x107b49350?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
golang.org/x/net/websocket.(*Conn).Close(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/golang.org/x/net/websocket/websocket.go:235 +0x1c
runtime.Goexit()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:631 +0x60
testing.(*common).FailNow(0x14000705dc0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1041 +0x48
github.com/stretchr/testify/require.NoError({0x106967d78, 0x14000705dc0}, {0x10695e260, 0x1400082d8f0}, {0x14000903e78, 0x1, 0x1})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/github.com/stretchr/testify/require/require.go:1357 +0xcc
k8s.io/kubernetes/pkg/kubelet/server.TestCtestServeWSPortForward.func1(0x14000705dc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/server/ctest_server_websocket_test.go:105 +0x590
testing.tRunner(0x14000705dc0, 0x1400053d180)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 367
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/server	2.204s
	k8s.io/kubernetes/pkg/kubelet/server/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/server/stats	1.773s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/server/stats/testing		coverage: 0.0% of statements
=== RUN   TestCtestMergeProcessStats

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:72]: Number of test cases: 8
Running 0 th test case: both nil
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {both nil <nil> <nil> <nil>}
=== RUN   TestCtestMergeProcessStats/both_nil
Running 1 th test case: first non-nil, second nil
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {first non-nil, second nil 0x14000836b38 <nil> 0x14000836b40}
=== RUN   TestCtestMergeProcessStats/first_non-nil,_second_nil
Running 2 th test case: first nil, second non-nil
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {first nil, second non-nil <nil> 0x14000836b48 0x14000836b50}
=== RUN   TestCtestMergeProcessStats/first_nil,_second_non-nil
Running 3 th test case: both non-nil normal values
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {both non-nil normal values 0x14000836b58 0x14000836b60 0x14000836b68}
=== RUN   TestCtestMergeProcessStats/both_non-nil_normal_values
Running 4 th test case: both non-nil zero values
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {both non-nil zero values 0x14000836b70 0x14000836b78 0x14000836b80}
=== RUN   TestCtestMergeProcessStats/both_non-nil_zero_values
Running 5 th test case: first zero, second non-zero
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {first zero, second non-zero 0x14000836b88 0x14000836b90 0x14000836b98}
=== RUN   TestCtestMergeProcessStats/first_zero,_second_non-zero
Running 6 th test case: both non-nil large values without overflow
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {both non-nil large values without overflow 0x14000836ba0 0x14000836ba8 0x14000836bb0}
=== RUN   TestCtestMergeProcessStats/both_non-nil_large_values_without_overflow
Running 7 th test case: both non-nil max uint64 values (overflow wraps to max)
[DEBUG-CTEST 2026-02-09 18:46:23 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/stats/ctest_helper_test.go:75]: test case data: {both non-nil max uint64 values (overflow wraps to max) 0x14000836bb8 0x14000836bc0 0x14000836bc8}
=== RUN   TestCtestMergeProcessStats/both_non-nil_max_uint64_values_(overflow_wraps_to_max)
    ctest_helper_test.go:79: Unexpected diff on process stats (-want,+got):
          &v1alpha1.ProcessStats{
        - 	ProcessCount: &18446744073709551615,
        + 	ProcessCount: &0,
          }

==================== CTEST END ======================
--- FAIL: TestCtestMergeProcessStats (0.00s)
    --- PASS: TestCtestMergeProcessStats/both_nil (0.00s)
    --- PASS: TestCtestMergeProcessStats/first_non-nil,_second_nil (0.00s)
    --- PASS: TestCtestMergeProcessStats/first_nil,_second_non-nil (0.00s)
    --- PASS: TestCtestMergeProcessStats/both_non-nil_normal_values (0.00s)
    --- PASS: TestCtestMergeProcessStats/both_non-nil_zero_values (0.00s)
    --- PASS: TestCtestMergeProcessStats/first_zero,_second_non-zero (0.00s)
    --- PASS: TestCtestMergeProcessStats/both_non-nil_large_values_without_overflow (0.00s)
    --- FAIL: TestCtestMergeProcessStats/both_non-nil_max_uint64_values_(overflow_wraps_to_max) (0.00s)
FAIL
coverage: 1.2% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/stats	1.912s
	k8s.io/kubernetes/pkg/kubelet/stats/pidlimit		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/status	0.600s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/kubelet/status/testing		coverage: 0.0% of statements
=== RUN   TestCtestNewAllowlist

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/sysctl/ctest_allowlist_test.go:29]: get default configs: {test_fixture.json [allowlist sysctls] sysctls [pods] [kernel.msg* kernel.sem net.b.*]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "sysctls" in requested fixtures
2026/02/09 18:46:25 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:25 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=["kernel.msg*","kernel.sem","net.b.*"])[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/sysctl/ctest_allowlist_test.go:38]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestNewAllowlist (0.00s)
=== RUN   TestCtestAllowlist

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/sysctl/ctest_allowlist_test.go:92]: get default configs: {test_fixture.json [allowlist pod spec] spec [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,AppArmorProfile:nil,SupplementalGroupsPolicy:nil,SELinuxChangePolicy:nil,} []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>} {0  []     []  [] <nil> [] []  []  [] nil}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:25 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:25 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":null,"securityContext":{}},"status":{}})[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/sysctl/ctest_allowlist_test.go:101]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestAllowlist (0.00s)
=== RUN   TestCtest_getSafeSysctlAllowlist
=== RUN   TestCtest_getSafeSysctlAllowlist/failed_to_get_kernelVersion,_only_return_safeSysctls_with_no_kernelVersion_limit
=== NAME  TestCtest_getSafeSysctlAllowlist
    safe_sysctls.go:91: E0209 18:46:25.300174] failed to get kernel version, unable to determine which sysctls are available err="fork error"
    safe_sysctls.go:104: I0209 18:46:25.300230] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.ip_local_reserved_ports"
    safe_sysctls.go:104: I0209 18:46:25.300241] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_keepalive_time"
    safe_sysctls.go:104: I0209 18:46:25.300247] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_fin_timeout"
    safe_sysctls.go:104: I0209 18:46:25.300253] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_keepalive_intvl"
    safe_sysctls.go:104: I0209 18:46:25.300258] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_keepalive_probes"
    safe_sysctls.go:104: I0209 18:46:25.300263] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_rmem"
    safe_sysctls.go:104: I0209 18:46:25.300269] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_wmem"
=== RUN   TestCtest_getSafeSysctlAllowlist/kernelVersion_is_3.18.0,_return_safeSysctls_with_no_kernelVersion_limit_and_net.ipv4.ip_local_reserved_ports
=== NAME  TestCtest_getSafeSysctlAllowlist
    safe_sysctls.go:104: I0209 18:46:25.300293] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="3.18.0" sysctl="net.ipv4.tcp_keepalive_time"
    safe_sysctls.go:104: I0209 18:46:25.300313] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="3.18.0" sysctl="net.ipv4.tcp_fin_timeout"
    safe_sysctls.go:104: I0209 18:46:25.300321] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="3.18.0" sysctl="net.ipv4.tcp_keepalive_intvl"
    safe_sysctls.go:104: I0209 18:46:25.300329] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="3.18.0" sysctl="net.ipv4.tcp_keepalive_probes"
    safe_sysctls.go:104: I0209 18:46:25.300337] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="3.18.0" sysctl="net.ipv4.tcp_rmem"
    safe_sysctls.go:104: I0209 18:46:25.300344] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="3.18.0" sysctl="net.ipv4.tcp_wmem"
=== RUN   TestCtest_getSafeSysctlAllowlist/kernelVersion_is_5.15.0,_return_safeSysctls_with_no_kernelVersion_limit_and_kernelVersion_below_5.15.0
=== RUN   TestCtest_getSafeSysctlAllowlist/kernelVersion_is_5.14.0,_expect_base_list_plus_ip_local_reserved_ports_but_without_5.15+_sysctls
    ctest_safe_sysctls_test.go:114: getSafeSysctlAllowlist() = [kernel.shm_rmid_forced net.ipv4.ip_local_port_range net.ipv4.tcp_syncookies net.ipv4.ping_group_range net.ipv4.ip_unprivileged_port_start net.ipv4.ip_local_reserved_ports net.ipv4.tcp_keepalive_time net.ipv4.tcp_fin_timeout net.ipv4.tcp_keepalive_intvl net.ipv4.tcp_keepalive_probes net.ipv4.tcp_rmem net.ipv4.tcp_wmem], want [kernel.shm_rmid_forced net.ipv4.ip_local_port_range net.ipv4.tcp_syncookies net.ipv4.ping_group_range net.ipv4.ip_unprivileged_port_start net.ipv4.ip_local_reserved_ports]
=== RUN   TestCtest_getSafeSysctlAllowlist/kernelVersion_unparsable_string_returns_error,_fallback_to_safe_list_without_kernel_limits
=== NAME  TestCtest_getSafeSysctlAllowlist
    safe_sysctls.go:91: E0209 18:46:25.300393] failed to get kernel version, unable to determine which sysctls are available err="could not parse \"invalid-version\" as version"
    safe_sysctls.go:104: I0209 18:46:25.300403] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.ip_local_reserved_ports"
    safe_sysctls.go:104: I0209 18:46:25.300409] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_keepalive_time"
    safe_sysctls.go:104: I0209 18:46:25.300417] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_fin_timeout"
    safe_sysctls.go:104: I0209 18:46:25.300422] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_keepalive_intvl"
    safe_sysctls.go:104: I0209 18:46:25.300430] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_keepalive_probes"
    safe_sysctls.go:104: I0209 18:46:25.300437] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_rmem"
    safe_sysctls.go:104: I0209 18:46:25.300444] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="<nil>" sysctl="net.ipv4.tcp_wmem"
=== RUN   TestCtest_getSafeSysctlAllowlist/kernelVersion_zero_value,_treat_as_no_kernelVersion_limit
=== NAME  TestCtest_getSafeSysctlAllowlist
    safe_sysctls.go:104: I0209 18:46:25.300462] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.ip_local_reserved_ports"
    safe_sysctls.go:104: I0209 18:46:25.300472] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.tcp_keepalive_time"
    safe_sysctls.go:104: I0209 18:46:25.300478] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.tcp_fin_timeout"
    safe_sysctls.go:104: I0209 18:46:25.300484] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.tcp_keepalive_intvl"
    safe_sysctls.go:104: I0209 18:46:25.300490] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.tcp_keepalive_probes"
    safe_sysctls.go:104: I0209 18:46:25.300495] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.tcp_rmem"
    safe_sysctls.go:104: I0209 18:46:25.300501] kernel version is too old, dropping the sysctl from safe sysctl list kernelVersion="0.0.0" sysctl="net.ipv4.tcp_wmem"
--- FAIL: TestCtest_getSafeSysctlAllowlist (0.00s)
    --- PASS: TestCtest_getSafeSysctlAllowlist/failed_to_get_kernelVersion,_only_return_safeSysctls_with_no_kernelVersion_limit (0.00s)
    --- PASS: TestCtest_getSafeSysctlAllowlist/kernelVersion_is_3.18.0,_return_safeSysctls_with_no_kernelVersion_limit_and_net.ipv4.ip_local_reserved_ports (0.00s)
    --- PASS: TestCtest_getSafeSysctlAllowlist/kernelVersion_is_5.15.0,_return_safeSysctls_with_no_kernelVersion_limit_and_kernelVersion_below_5.15.0 (0.00s)
    --- FAIL: TestCtest_getSafeSysctlAllowlist/kernelVersion_is_5.14.0,_expect_base_list_plus_ip_local_reserved_ports_but_without_5.15+_sysctls (0.00s)
    --- PASS: TestCtest_getSafeSysctlAllowlist/kernelVersion_unparsable_string_returns_error,_fallback_to_safe_list_without_kernel_limits (0.00s)
    --- PASS: TestCtest_getSafeSysctlAllowlist/kernelVersion_zero_value,_treat_as_no_kernelVersion_limit (0.00s)
FAIL
coverage: 23.6% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/sysctl	3.336s
=== RUN   TestCtestTokenCachingAndExpiration
=== RUN   TestCtestTokenCachingAndExpiration/rotate_hour_token_expires_in_the_last_12_minutes
=== RUN   TestCtestTokenCachingAndExpiration/rotate_24_hour_token_that_expires_in_40_hours
=== RUN   TestCtestTokenCachingAndExpiration/rotate_hour_token_fails,_old_token_is_still_valid,_doesn't_error
E0209 18:46:23.337225   88617 token_manager.go:124] "Couldn't update token" err="err" cacheKey="\"b\"/\"a\"/[]string{\"foo1\", \"foo2\"}/2000/v1.BoundObjectReference{Kind:\"pod\", APIVersion:\"\", Name:\"foo-pod\", UID:\"foo-uid\"}/\"\""
=== RUN   TestCtestTokenCachingAndExpiration/service_account_recreated_-_cache_miss_due_to_different_UID
=== RUN   TestCtestTokenCachingAndExpiration/service_account_UID_consistent_-_cache_hit
=== RUN   TestCtestTokenCachingAndExpiration/zero_expiration_duration_(immediate_expiration)
=== RUN   TestCtestTokenCachingAndExpiration/very_large_expiration_(10_years)
    ctest_token_manager_test.go:176: expected no refresh for long-lived token, got 2
--- FAIL: TestCtestTokenCachingAndExpiration (0.00s)
    --- PASS: TestCtestTokenCachingAndExpiration/rotate_hour_token_expires_in_the_last_12_minutes (0.00s)
    --- PASS: TestCtestTokenCachingAndExpiration/rotate_24_hour_token_that_expires_in_40_hours (0.00s)
    --- PASS: TestCtestTokenCachingAndExpiration/rotate_hour_token_fails,_old_token_is_still_valid,_doesn't_error (0.00s)
    --- PASS: TestCtestTokenCachingAndExpiration/service_account_recreated_-_cache_miss_due_to_different_UID (0.00s)
    --- PASS: TestCtestTokenCachingAndExpiration/service_account_UID_consistent_-_cache_hit (0.00s)
    --- PASS: TestCtestTokenCachingAndExpiration/zero_expiration_duration_(immediate_expiration) (0.00s)
    --- FAIL: TestCtestTokenCachingAndExpiration/very_large_expiration_(10_years) (0.00s)
=== RUN   TestCtestRequiresRefresh
=== RUN   TestCtestRequiresRefresh/0
=== RUN   TestCtestRequiresRefresh/1
=== RUN   TestCtestRequiresRefresh/2
=== RUN   TestCtestRequiresRefresh/3
=== RUN   TestCtestRequiresRefresh/4
=== NAME  TestCtestRequiresRefresh
    token_manager.go:179: E0209 18:46:23.337971] Expiration seconds was nil for token request tokenRequest="&TokenRequest{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:TokenRequestSpec{Audiences:[],BoundObjectRef:nil,ExpirationSeconds:nil,},Status:TokenRequestStatus{Token:,ExpirationTimestamp:2026-02-09 19:46:23.33779 -0600 CST m=+3600.010600251,},}"
=== RUN   TestCtestRequiresRefresh/5
=== RUN   TestCtestRequiresRefresh/6
    ctest_token_manager_test.go:301: unexpected requiresRefresh result, got: false, want: true
=== RUN   TestCtestRequiresRefresh/7
--- FAIL: TestCtestRequiresRefresh (0.00s)
    --- PASS: TestCtestRequiresRefresh/0 (0.00s)
    --- PASS: TestCtestRequiresRefresh/1 (0.00s)
    --- PASS: TestCtestRequiresRefresh/2 (0.00s)
    --- PASS: TestCtestRequiresRefresh/3 (0.00s)
    --- PASS: TestCtestRequiresRefresh/4 (0.00s)
    --- PASS: TestCtestRequiresRefresh/5 (0.00s)
    --- FAIL: TestCtestRequiresRefresh/6 (0.00s)
    --- PASS: TestCtestRequiresRefresh/7 (0.00s)
=== RUN   TestCtestDeleteServiceAccountToken
=== RUN   TestCtestDeleteServiceAccountToken/delete_none_with_all_success_requests
=== RUN   TestCtestDeleteServiceAccountToken/delete_one_with_all_success_requests
=== RUN   TestCtestDeleteServiceAccountToken/delete_two_with_all_success_requests
=== RUN   TestCtestDeleteServiceAccountToken/delete_all_with_all_success_requests
=== RUN   TestCtestDeleteServiceAccountToken/delete_no_pod_with_failed_requests
=== RUN   TestCtestDeleteServiceAccountToken/delete_other_pod_with_failed_requests
=== RUN   TestCtestDeleteServiceAccountToken/delete_no_pod_with_request_which_success_after_failure
=== RUN   TestCtestDeleteServiceAccountToken/delete_the_pod_which_success_after_failure
=== RUN   TestCtestDeleteServiceAccountToken/delete_other_pod_with_request_which_success_after_failure
=== RUN   TestCtestDeleteServiceAccountToken/delete_some_pod_not_in_the_set
=== RUN   TestCtestDeleteServiceAccountToken/empty_request_list
=== RUN   TestCtestDeleteServiceAccountToken/delete_UID_that_does_not_exist_with_empty_request_list
--- PASS: TestCtestDeleteServiceAccountToken (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_none_with_all_success_requests (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_one_with_all_success_requests (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_two_with_all_success_requests (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_all_with_all_success_requests (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_no_pod_with_failed_requests (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_other_pod_with_failed_requests (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_no_pod_with_request_which_success_after_failure (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_the_pod_which_success_after_failure (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_other_pod_with_request_which_success_after_failure (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_some_pod_not_in_the_set (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/empty_request_list (0.00s)
    --- PASS: TestCtestDeleteServiceAccountToken/delete_UID_that_does_not_exist_with_empty_request_list (0.00s)
=== RUN   TestCtestCleanup
=== RUN   TestCtestCleanup/don't_cleanup_unexpired_tokens
=== RUN   TestCtestCleanup/cleanup_expired_tokens
=== RUN   TestCtestCleanup/cleanup_token_expiring_exactly_now
=== RUN   TestCtestCleanup/cleanup_token_far_in_past
--- PASS: TestCtestCleanup (0.00s)
    --- PASS: TestCtestCleanup/don't_cleanup_unexpired_tokens (0.00s)
    --- PASS: TestCtestCleanup/cleanup_expired_tokens (0.00s)
    --- PASS: TestCtestCleanup/cleanup_token_expiring_exactly_now (0.00s)
    --- PASS: TestCtestCleanup/cleanup_token_far_in_past (0.00s)
=== RUN   TestCtestKeyFunc
=== RUN   TestCtestKeyFunc/hit
=== RUN   TestCtestKeyFunc/not_hit_due_to_different_ExpirationSeconds
=== RUN   TestCtestKeyFunc/empty_name_and_namespace
=== RUN   TestCtestKeyFunc/nil_token_request_in_map_(should_not_panic)
--- FAIL: TestCtestKeyFunc (0.00s)
    --- PASS: TestCtestKeyFunc/hit (0.00s)
    --- PASS: TestCtestKeyFunc/not_hit_due_to_different_ExpirationSeconds (0.00s)
    --- PASS: TestCtestKeyFunc/empty_name_and_namespace (0.00s)
    --- FAIL: TestCtestKeyFunc/nil_token_request_in_map_(should_not_panic) (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x120 pc=0x1058572b8]

goroutine 135 [running]:
testing.tRunner.func1.2({0x105ba53e0, 0x1066509b0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x105ba53e0?, 0x1066509b0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/kubelet/token.keyFunc({0x1058a3c25?, 0x278d00?}, {0x1058a3959?, 0x9356907420000?}, 0x14000091f18?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/token/token_manager.go:200 +0x98
k8s.io/kubernetes/pkg/kubelet/token.TestCtestKeyFunc.func1(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/token/ctest_token_manager_test.go:569
k8s.io/kubernetes/pkg/kubelet/token.TestCtestKeyFunc.func2(0x140002ebc00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/token/ctest_token_manager_test.go:726 +0x180
testing.tRunner(0x140002ebc00, 0x14000592bd0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 128
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/kubelet/token	1.377s
=== RUN   TestCtestPodConditionByKubelet

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/ctest_pod_status_test.go:39]: Running true case checks
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/ctest_pod_status_test.go:46]: Running false case checks

==================== CTEST END ======================
--- PASS: TestCtestPodConditionByKubelet (0.00s)
=== RUN   TestCtestPodConditionSharedByKubelet

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/ctest_pod_status_test.go:74]: Running true case checks
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/types/ctest_pod_status_test.go:81]: Running false case checks

==================== CTEST END ======================
--- PASS: TestCtestPodConditionSharedByKubelet (0.00s)
PASS
coverage: 11.3% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/types	2.851s	coverage: 11.3% of statements
=== RUN   TestCtestMakeUserNsManagerDisabled

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:24]: Running TestCtestMakeUserNsManagerDisabled

==================== CTEST END ======================
--- PASS: TestCtestMakeUserNsManagerDisabled (0.00s)
=== RUN   TestCtestReleaseDisabled

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:35]: Running TestCtestReleaseDisabled

==================== CTEST END ======================
--- PASS: TestCtestReleaseDisabled (0.00s)
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled

==================== CTEST OVERRIDE ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:48]: Running TestCtestGetOrCreateUserNamespaceMappingsDisabled
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:53]: Loading hard‑coded base config
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:59]: Base config item: {test_fixture.json [default pod spec hostUsers] hostUsers [pods] {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "hostUsers" in requested fixtures
2026/02/09 18:46:24 [DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:46:24 Mode: 1
2026/02/09 18:46:24 Base JSON size: 19 bytes
2026/02/09 18:46:24 Number of external values: 0
2026/02/09 18:46:24 [DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:46:24 [DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null})[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:67]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:68]: Num of base configs: 0
Running 0 th test case: pod is nil
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled/pod_is_nil
Running 1 th test case: hostUsers is nil (base config)
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_is_nil_(base_config)
Running 2 th test case: hostUsers is true
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_is_true
Running 3 th test case: hostUsers is false
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_is_false
Running 4 th test case: hostUsers true with extra field (restartPolicy)
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_true_with_extra_field_(restartPolicy)
Running 5 th test case: hostUsers false with extra field (restartPolicy)
=== RUN   TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_false_with_extra_field_(restartPolicy)
    ctest_userns_manager_disabled_test.go:151: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:151
        	Error:      	Received unexpected error:
        	            	the feature gate "UserNamespacesSupport" is disabled: can't set spec.HostUsers
        	Test:       	TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_false_with_extra_field_(restartPolicy)

==================== CTEST END ======================
--- FAIL: TestCtestGetOrCreateUserNamespaceMappingsDisabled (0.00s)
    --- PASS: TestCtestGetOrCreateUserNamespaceMappingsDisabled/pod_is_nil (0.00s)
    --- PASS: TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_is_nil_(base_config) (0.00s)
    --- PASS: TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_is_true (0.00s)
    --- PASS: TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_is_false (0.00s)
    --- PASS: TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_true_with_extra_field_(restartPolicy) (0.00s)
    --- FAIL: TestCtestGetOrCreateUserNamespaceMappingsDisabled/hostUsers_false_with_extra_field_(restartPolicy) (0.00s)
=== RUN   TestCtestCleanupOrphanedPodUsernsAllocationsDisabled

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_disabled_test.go:162]: Running TestCtestCleanupOrphanedPodUsernsAllocationsDisabled

==================== CTEST END ======================
--- PASS: TestCtestCleanupOrphanedPodUsernsAllocationsDisabled (0.00s)
=== RUN   TestCtestGetOrCreateUserNamespaceMappings

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_test.go:37]: get default configs: {test_fixture.json [default pod spec for GetOrCreateUserNamespaceMappings] spec [pods] {[] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:24 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:24 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":null})[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:46:24 [DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_test.go:47]: New Json Test Configs: [{"volumes":[{"name":"config","hostPath":{"path":"/etc/kubernetes"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}],"containers":null}]
[DEBUG-CTEST 2026-02-09 18:46:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/userns/ctest_userns_manager_test.go:48]: Number of test cases: 1
Running config #0
{[{config {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}] [] [] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
  Test case 0: no user namespace
    ctest_userns_manager_test.go:158: MakeUserNsManager failed: kubelet user assigned IDs are not enough to support 110 pods
--- FAIL: TestCtestGetOrCreateUserNamespaceMappings (0.00s)
FAIL
coverage: 12.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/userns	2.383s
=== RUN   TestCtestGetNodenameForKernel
    ctest_util_test.go:97: TestCase: "no hostDomain, setHostnameAsFQDN false"
    ctest_util_test.go:97: TestCase: "no hostDomain, setHostnameAsFQDN true"
    ctest_util_test.go:97: TestCase: "valid hostDomain, setHostnameAsFQDN false"
    ctest_util_test.go:97: TestCase: "valid hostDomain, setHostnameAsFQDN true"
    ctest_util_test.go:97: TestCase: "FQDN is too long, setHostnameAsFQDN false"
    ctest_util_test.go:97: TestCase: "FQDN is too long, setHostnameAsFQDN true"
    ctest_util_test.go:97: TestCase: "empty hostname, no domain, FQDN false"
    ctest_util_test.go:97: TestCase: "empty hostname, no domain, FQDN true"
    ctest_util_test.go:97: TestCase: "empty hostname with long domain, FQDN true (should error due to length)"
    ctest_util_test.go:100: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_test.go:100
        	Error:      	An error is expected but got nil.
        	Test:       	TestCtestGetNodenameForKernel
    ctest_util_test.go:104: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_test.go:104
        	Error:      	Not equal: 
        	            	expected: ""
        	            	actual  : ".a.b.c.d.e.f.g.h.i.j.k.l.m.n.o.p.q.r.s.t.u.v.w.x.y.z"
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1 +1 @@
        	            	-
        	            	+.a.b.c.d.e.f.g.h.i.j.k.l.m.n.o.p.q.r.s.t.u.v.w.x.y.z
        	Test:       	TestCtestGetNodenameForKernel
--- FAIL: TestCtestGetNodenameForKernel (0.00s)
=== RUN   TestCtestGetContainerByIndex
--- PASS: TestCtestGetContainerByIndex (0.00s)
=== RUN   TestCtestLocalEndpoint

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_unix_test.go:47]: Number of test cases: 4
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_unix_test.go:50]: test: {path file false unix:/path/file.sock}
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_unix_test.go:50]: test: {a/very/long/path/that/should/be/handled socket false unix:/a/very/long/path/that/should/be/handled/socket.sock}
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_unix_test.go:50]: test: { emptyPath false unix:/emptyPath.sock}
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/ctest_util_unix_test.go:50]: test: {emptyFilePath  false unix:/emptyFilePath/.sock}

==================== CTEST END ======================
--- PASS: TestCtestLocalEndpoint (0.00s)
FAIL
coverage: 12.7% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/util	1.159s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/util/cache	0.672s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/util/env	0.249s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPod

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:68]: Number of test cases: 5
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_empty_case
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_normal_case
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: nil_pod_case
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_long_name_case
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:72]: Test case: field_special_chars_case
    ctest_pod_test.go:74: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:74
        	Error:      	Not equal: 
        	            	expected: "pod!@#$%^&*()_+_ns!@#$ (uid!@#$)"
        	            	actual  : "pod!@#$%^&*()_+_ns!@#$(uid!@#$)"
        	            	
        	            	Diff:
        	            	--- Expected
        	            	+++ Actual
        	            	@@ -1 +1 @@
        	            	-pod!@#$%^&*()_+_ns!@#$ (uid!@#$)
        	            	+pod!@#$%^&*()_+_ns!@#$(uid!@#$)
        	Test:       	TestCtestPod
        	Messages:   	Failed to test: field_special_chars_case

==================== CTEST END ======================
--- FAIL: TestCtestPod (0.00s)
=== RUN   TestCtestPodAndPodDesc

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:120]: Number of test cases: 4
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_empty_case
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_normal_case
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_long_strings_case
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:46:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/format/ctest_pod_test.go:124]: Test case: field_special_chars_case

==================== CTEST END ======================
--- PASS: TestCtestPodAndPodDesc (0.00s)
FAIL
coverage: 100.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/util/format	1.638s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/util/ioutils	0.453s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestParseNodeAnnotation

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/manager/ctest_cache_based_manager_test.go:26]: get default configs: {test_fixture.json [default node ttl] annotations [nodes] &Node{ObjectMeta:{node      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{},Allocatable:ResourceList{},Phase:,Conditions:[]NodeCondition{},Addresses:[]NodeAddress{},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:0,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:,KubeProxyVersion:,OperatingSystem:,Architecture:,Swap:nil,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,RuntimeHandlers:[]NodeRuntimeHandler{},Features:nil,},}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-09 18:46:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-09 18:46:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-09 18:46:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:46:32 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/pkg/kubelet/util/manager	2.002s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/util/queue	1.372s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPodsByCreationTimeLen
[DEBUG] Starting TestPodsByCreationTimeLen
[DEBUG] Running Len case #0: pods=[] expected=0
[DEBUG] Running Len case #1: pods=[&Pod{ObjectMeta:{foo1  default    0 2026-02-09 18:46:33.092644 -0600 CST m=+0.041263126 <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{foo2  default    0 2026-02-09 19:46:33.092644 -0600 CST m=+3600.041263334 <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{foo3  default    0 2026-02-09 20:46:33.092644 -0600 CST m=+7200.041263626 <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] expected=3
[DEBUG] Running Len case #2: pods=[nil &Pod{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] expected=2
[DEBUG] Running Len case #3: pods=[] expected=0
[DEBUG] Running Len case #4: pods=[&Pod{ObjectMeta:{same1  default    0 2026-02-09 18:46:33.092646 -0600 CST m=+0.041265834 <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},} &Pod{ObjectMeta:{same2  default    0 2026-02-09 18:46:33.092646 -0600 CST m=+0.041265834 <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}] expected=2
--- PASS: TestCtestPodsByCreationTimeLen (0.00s)
=== RUN   TestCtestPodsByCreationTimeSwap
[DEBUG] Starting TestPodsByCreationTimeSwap
[DEBUG] Running Swap case #0: i=0 j=1
[DEBUG] Running Swap case #1: i=2 j=1
[DEBUG] Running Swap case #2: i=1 j=1
--- PASS: TestCtestPodsByCreationTimeSwap (0.00s)
=== RUN   TestCtestPodsByCreationTimeLess
[DEBUG] Starting TestPodsByCreationTimeLess
[DEBUG] Running Less case #0: i=0 j=2 expected=true
[DEBUG] Running Less case #1: i=1 j=0 expected=false
[DEBUG] Running Less case #2: i=0 j=1 expected=false
--- PASS: TestCtestPodsByCreationTimeLess (0.00s)
=== RUN   TestCtestByImageSizeLen
[DEBUG] Starting TestByImageSizeLen
[DEBUG] Running Len case #0: expected=0
[DEBUG] Running Len case #1: expected=4
[DEBUG] Running Len case #2: expected=0
[DEBUG] Running Len case #3: expected=2
--- PASS: TestCtestByImageSizeLen (0.00s)
=== RUN   TestCtestByImageSizeSwap
[DEBUG] Starting TestByImageSizeSwap
[DEBUG] Running Swap case #0: i=0 j=1
[DEBUG] Running Swap case #1: i=2 j=1
[DEBUG] Running Swap case #2: i=3 j=3
--- PASS: TestCtestByImageSizeSwap (0.00s)
=== RUN   TestCtestByImageSizeLess
[DEBUG] Starting TestByImageSizeLess
[DEBUG] Running Less case #0: i=0 j=2 expected=false
[DEBUG] Running Less case #1: i=1 j=0 expected=true
[DEBUG] Running Less case #2: i=3 j=2 expected=true
[DEBUG] Running Less case #3: i=0 j=1 expected=false
    ctest_sliceutils_test.go:179: returned true but expected false for the foo=[{zero [] [] 0 {  []} false} {negative [] [] -1 {  []} false}]
--- FAIL: TestCtestByImageSizeLess (0.00s)
FAIL
coverage: 100.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/util/sliceutils	2.519s
=== RUN   TestCtestFileStore
    ctest_filestore_test.go:44: test case:  {id1 data1 false}
    ctest_filestore_test.go:44: test case:  {id2 data2 false}
    ctest_filestore_test.go:44: test case:  {/id1 data1 true}
    ctest_filestore_test.go:44: test case:  {.id1 data1 true}
    ctest_filestore_test.go:44: test case:  {    data2 true}
    ctest_filestore_test.go:44: test case:  {___ data2 true}
    ctest_filestore_test.go:44: test case:  { emptykey true}
    ctest_filestore_test.go:44: test case:  {aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa longkey false}
    ctest_filestore_test.go:53: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/util/store/ctest_filestore_test.go:53
        	Error:      	Received unexpected error:
        	            	invalid key: "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
        	Test:       	TestCtestFileStore
--- FAIL: TestCtestFileStore (0.04s)
FAIL
coverage: 52.7% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/util/store	2.570s
=== RUN   TestCtestIsSwapEnabled
=== RUN   TestCtestIsSwapEnabled/empty
=== RUN   TestCtestIsSwapEnabled/with_swap_enabled,_one_partition
I0209 18:46:33.486248   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428	0		-2
 >
=== RUN   TestCtestIsSwapEnabled/with_swap_enabled,_2_partitions
I0209 18:46:33.486333   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428	0		-2
	/dev/zram0              partition	8388604		0		100
 >
=== RUN   TestCtestIsSwapEnabled/empty_lines
=== RUN   TestCtestIsSwapEnabled/missing_header_line
=== RUN   TestCtestIsSwapEnabled/malformed_entry,_not_enough_columns
I0209 18:46:33.486356   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428
 >
    ctest_swap_util_test.go:108: expected false, got true
=== RUN   TestCtestIsSwapEnabled/non‑numeric_size_field
I0209 18:46:33.486456   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	NaN	0		-2
 >
    ctest_swap_util_test.go:108: expected false, got true
=== RUN   TestCtestIsSwapEnabled/negative_size_field
I0209 18:46:33.486470   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	-1024	0		-2
 >
    ctest_swap_util_test.go:108: expected false, got true
=== RUN   TestCtestIsSwapEnabled/zero_size_but_valid_entry
I0209 18:46:33.486480   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/zero-swap          partition	0	0		-1
 >
=== RUN   TestCtestIsSwapEnabled/extremely_large_size_value
I0209 18:46:33.486487   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/huge-swap          partition	18446744073709551615	0		-1
 >
=== RUN   TestCtestIsSwapEnabled/duplicate_entries
I0209 18:46:33.486495   88734 swap_util.go:115] "Swap is on" /proc/swaps contents=<
	Filename				Type		Size		Used		Priority
	/dev/dm-1               partition	33554428	0		-2
	/dev/dm-1               partition	33554428	0		-2
 >
=== RUN   TestCtestIsSwapEnabled/header_only,_no_entries
--- FAIL: TestCtestIsSwapEnabled (0.00s)
    --- PASS: TestCtestIsSwapEnabled/empty (0.00s)
    --- PASS: TestCtestIsSwapEnabled/with_swap_enabled,_one_partition (0.00s)
    --- PASS: TestCtestIsSwapEnabled/with_swap_enabled,_2_partitions (0.00s)
    --- PASS: TestCtestIsSwapEnabled/empty_lines (0.00s)
    --- PASS: TestCtestIsSwapEnabled/missing_header_line (0.00s)
    --- FAIL: TestCtestIsSwapEnabled/malformed_entry,_not_enough_columns (0.00s)
    --- FAIL: TestCtestIsSwapEnabled/non‑numeric_size_field (0.00s)
    --- FAIL: TestCtestIsSwapEnabled/negative_size_field (0.00s)
    --- PASS: TestCtestIsSwapEnabled/zero_size_but_valid_entry (0.00s)
    --- PASS: TestCtestIsSwapEnabled/extremely_large_size_value (0.00s)
    --- PASS: TestCtestIsSwapEnabled/duplicate_entries (0.00s)
    --- PASS: TestCtestIsSwapEnabled/header_only,_no_entries (0.00s)
FAIL
coverage: 13.0% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/util/swap	2.715s
# k8s.io/kubernetes/pkg/probe/exec
# [k8s.io/kubernetes/pkg/probe/exec]
pkg/probe/exec/ctest_exec_test.go:103:4: (*testing.common).Errorf format %v reads arg #4, but call has 3 args
pkg/probe/exec/ctest_exec_test.go:106:4: (*testing.common).Errorf format %v reads arg #3, but call has 2 args
pkg/probe/exec/ctest_exec_test.go:109:4: (*testing.common).Errorf format %s reads arg #2, but call has 1 arg
pkg/probe/exec/ctest_exec_test.go:112:4: (*testing.common).Errorf format %q reads arg #4, but call has 3 args
=== RUN   TestCtestWaitForAttachAndMountError

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/volumemanager/ctest_volume_manager_test.go:57]: matched config item: {test_fixture.json [wait for attach and mount error pod spec] spec [pods] {[{fail-mount-device-volume-name {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ConfigMapVolumeSource{LocalObjectReference:LocalObjectReference{Name:,},Items:[]KeyToPath{},DefaultMode:nil,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil}} {vol2 {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol02 {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol3 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}} {vol03 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}}] [] [{container1  [] []  [] [] [] {map[] map[] []} [] <nil> [] [{fail-mount-device-volume-name false <nil> /vol1  <nil> } {vol2 false <nil> /vol2  <nil> } {vol02 false <nil> /vol02  <nil> } {vol3 false <nil> /vol3  <nil> } {vol03 false <nil> /vol03  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:39 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:39 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"name":"container1","resources":{},"volumeMounts":[{"mountPath":"/vol1","name":"fail-mount-device-volume-name"},{"mountPath":"/vol2","name":"vol2"},{"mountPath":"/vol02","name":"vol02"},{"mountPath":"/vol3","name":"vol3"},{"mountPath":"/vol03","name":"vol03"}]}],"volumes":[{"configMap":{},"name":"fail-mount-device-volume-name"},{"name":"vol2","rbd":{"image":"","monitors":null}},{"name":"vol02","rbd":{"image":"","monitors":null}},{"azureDisk":{"diskName":"","diskURI":""},"name":"vol3"},{"azureDisk":{"diskName":"","diskURI":""},"name":"vol03"}]})[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:46:39 [DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, v1.PodSpec={[{fail-mount-device-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ConfigMapVolumeSource{LocalObjectReference:LocalObjectReference{Name:,},Items:[]KeyToPath{},DefaultMode:nil,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil}} {vol2 {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol02 {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol3 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}} {vol03 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}}] [] [{container1 registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{fail-mount-device-volume-name false <nil> /vol1  <nil> } {vol2 false <nil> /vol2  <nil> } {vol02 false <nil> /vol02  <nil> } {vol3 false <nil> /vol3  <nil> } {vol03 false <nil> /vol03  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>})
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 {[{fail-mount-device-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ConfigMapVolumeSource{LocalObjectReference:LocalObjectReference{Name:,},Items:[]KeyToPath{},DefaultMode:nil,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil}} {vol2 {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol02 {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol3 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}} {vol03 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}}] [] [{container1 registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{fail-mount-device-volume-name false <nil> /vol1  <nil> } {vol2 false <nil> /vol2  <nil> } {vol02 false <nil> /vol02  <nil> } {vol3 false <nil> /vol3  <nil> } {vol03 false <nil> /vol03  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/volumemanager/ctest_volume_manager_test.go:69]: New Json Test Configs: [{"volumes":[{"name":"fail-mount-device-volume-name","hostPath":{"path":"/etc/kubernetes"},"configMap":{}},{"name":"vol2","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"},"rbd":{"monitors":null,"image":""}},{"name":"vol02","rbd":{"monitors":null,"image":""}},{"name":"vol3","azureDisk":{"diskName":"","diskURI":""}},{"name":"vol03","azureDisk":{"diskName":"","diskURI":""}}],"containers":[{"name":"container1","image":"registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0","args":["--socketpath=/kms/kms.sock","--cloud-config=/etc/kubernetes/cloud-config"],"resources":{},"volumeMounts":[{"name":"fail-mount-device-volume-name","mountPath":"/vol1"},{"name":"vol2","mountPath":"/vol2"},{"name":"vol02","mountPath":"/vol02"},{"name":"vol3","mountPath":"/vol3"},{"name":"vol03","mountPath":"/vol03"}]}]}]
[DEBUG-CTEST 2026-02-09 18:46:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/volumemanager/ctest_volume_manager_test.go:70]: Number of test cases: 1
Running 0 th test case.
{[{fail-mount-device-volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &ConfigMapVolumeSource{LocalObjectReference:LocalObjectReference{Name:,},Items:[]KeyToPath{},DefaultMode:nil,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil}} {vol2 {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol02 {nil nil nil nil nil nil nil nil nil nil &RBDVolumeSource{CephMonitors:[],RBDImage:,FSType:,RBDPool:,RadosUser:,Keyring:,SecretRef:nil,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {vol3 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}} {vol03 {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &AzureDiskVolumeSource{DiskName:,DataDiskURI:,CachingMode:nil,FSType:nil,ReadOnly:nil,Kind:nil,} nil nil nil nil nil nil nil nil}}] [] [{container1 registry.k8s.io/provider-os/barbican-kms-plugin:v1.34.0 [] [--socketpath=/kms/kms.sock --cloud-config=/etc/kubernetes/cloud-config]  [] [] [] {map[] map[] []} [] <nil> [] [{fail-mount-device-volume-name false <nil> /vol1  <nil> } {vol2 false <nil> /vol2  <nil> } {vol02 false <nil> /vol02  <nil> } {vol3 false <nil> /vol3  <nil> } {vol03 false <nil> /vol03  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}
    volume_manager.go:313: I0209 18:46:39.661732] Starting Kubelet Volume Manager
    desired_state_of_world_populator.go:146: I0209 18:46:39.661910] Desired state populator starts to run
    reconstruct.go:54: E0209 18:46:39.662067] Cannot get volumes from disk, skip sync states for volume reconstruction err="open : no such file or directory"
    reconciler.go:29: I0209 18:46:39.662103] Reconciler: start to sync state
    desired_state_of_world_populator.go:312: E0209 18:46:39.762828] Failed to add volume to desiredStateOfWorld err="failed to get Plugin from volumeSpec for volume \"\" err=no volume plugin matched" pod="nsA/abc" volumeName="vol3" volumeSpecName=""
    desired_state_of_world_populator.go:312: E0209 18:46:39.762901] Failed to add volume to desiredStateOfWorld err="failed to get Plugin from volumeSpec for volume \"\" err=no volume plugin matched" pod="nsA/abc" volumeName="vol03" volumeSpecName=""
    desired_state_of_world_populator.go:154: I0209 18:46:39.762913] Finished populating initial desired state of world
    reconciler_common.go:251: I0209 18:46:39.862888] operationExecutor.VerifyControllerAttachedVolume started for volume "fail-mount-device-volume-name" (UniqueName: "unattachable-fake-plugin/fail-mount-device-volume-name") pod "abc" (UID: "1234")  pod="nsA/abc"

==================== CTEST END ======================
    volume_manager.go:319: I0209 18:46:39.962148] Shutting down Kubelet Volume Manager
--- PASS: TestCtestWaitForAttachAndMountError (0.31s)
PASS
coverage: 37.2% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/volumemanager	2.462s	coverage: 37.2% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/volumemanager/cache	2.735s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestMetricCollection

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/volumemanager/metrics/ctest_metrics_test.go:33]: get default configs: {test_fixture.json [default volume config] volumes [pods] [{volume-name {nil nil &GCEPersistentDiskVolumeSource{PDName:fake-device1,FSType:,Partition:0,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:46:38 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:46:38 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=[{"gcePersistentDisk":{"pdName":"fake-device1"},"name":"volume-name"}])[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:46:38 [DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, []v1.Volume=[{volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil &GCEPersistentDiskVolumeSource{PDName:fake-device1,FSType:,Partition:0,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}])
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 [{volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil &GCEPersistentDiskVolumeSource{PDName:fake-device1,FSType:,Partition:0,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 1
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/volumemanager/metrics/ctest_metrics_test.go:45]: New Json Test Configs: [[{"name":"volume-name","hostPath":{"path":"/etc/kubernetes"},"gcePersistentDisk":{"pdName":"fake-device1"}},{"name":"socket-dir","hostPath":{"path":"/var/lib/kms/","type":"DirectoryOrCreate"}}]]
[DEBUG-CTEST 2026-02-09 18:46:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/kubelet/volumemanager/metrics/ctest_metrics_test.go:46]: Number of test cases: 1
Running 0 th test case.
Config volume slice: [{volume-name {&HostPathVolumeSource{Path:/etc/kubernetes,Type:nil,} nil &GCEPersistentDiskVolumeSource{PDName:fake-device1,FSType:,Partition:0,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}} {socket-dir {&HostPathVolumeSource{Path:/var/lib/kms/,Type:*DirectoryOrCreate,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    desired_state_of_world.go:309: I0209 18:46:38.575887] expected volume SELinux label context volume="volume-name" label=""
    desired_state_of_world.go:329: I0209 18:46:38.575950] volume does not support SELinux context mount, clearing the expected label volume="volume-name"
Running 1 th test case.
Config volume slice: [{edge-volume-empty-pdname {nil nil &GCEPersistentDiskVolumeSource{PDName:,FSType:,Partition:0,ReadOnly:false,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    desired_state_of_world.go:309: I0209 18:46:38.576056] expected volume SELinux label context volume="edge-volume-empty-pdname" label=""
    desired_state_of_world.go:329: I0209 18:46:38.576065] volume does not support SELinux context mount, clearing the expected label volume="edge-volume-empty-pdname"
Running 2 th test case.
Config volume slice: [{edge-volume-emptydir {nil &EmptyDirVolumeSource{Medium:,SizeLimit:<nil>,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}]
    desired_state_of_world.go:309: I0209 18:46:38.576140] expected volume SELinux label context volume="edge-volume-emptydir" label=""
    desired_state_of_world.go:329: I0209 18:46:38.576151] volume does not support SELinux context mount, clearing the expected label volume="edge-volume-emptydir"
--- PASS: TestCtestMetricCollection (0.00s)
PASS
coverage: 59.3% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/volumemanager/metrics	1.076s	coverage: 59.3% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/kubelet/volumemanager/populator	0.575s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestReconstructVolumes
=== RUN   TestCtestReconstructVolumes/when_two_pods_are_using_same_volume_and_both_are_deleted
=== NAME  TestCtestReconstructVolumes
    reconstruct_common.go:236: I0209 18:46:39.049670] Volume path from volume plugin directory podName="pod1" volumePath="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1242673636/pods/pod1/volumes/fake-plugin/pvc-abcdef"
    reconstruct_common.go:236: I0209 18:46:39.049843] Volume path from volume plugin directory podName="pod2" volumePath="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1242673636/pods/pod2/volumes/fake-plugin/pvc-abcdef"
    reconstruct_common.go:249: I0209 18:46:39.049883] Get volume from pod directory path="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1242673636/pods" volume={"podName":"pod1","volumeSpecName":"pvc-abcdef","volumePath":"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1242673636/pods/pod1/volumes/fake-plugin/pvc-abcdef","pluginName":"fake-plugin","volumeMode":"Filesystem"}
    reconstruct_common.go:249: I0209 18:46:39.049893] Get volume from pod directory path="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1242673636/pods" volume={"podName":"pod2","volumeSpecName":"pvc-abcdef","volumePath":"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1242673636/pods/pod2/volumes/fake-plugin/pvc-abcdef","pluginName":"fake-plugin","volumeMode":"Filesystem"}
    reconstruct.go:72: I0209 18:46:39.049949] Adding reconstructed volume to actual state and node status podName="pod1" volumeSpecName="pvc-abcdef"
    reconstruct.go:72: I0209 18:46:39.049959] Adding reconstructed volume to actual state and node status podName="pod2" volumeSpecName="pvc-abcdef"
    reconstruct.go:130: I0209 18:46:39.049980] Volume is marked as uncertain and added into the actual state pod="" podName="pod1" volumeName="fake-plugin/pvc-abcdef" seLinuxMountContext=""
    reconstruct.go:130: I0209 18:46:39.049990] Volume is marked as uncertain and added into the actual state pod="" podName="pod2" volumeName="fake-plugin/pvc-abcdef" seLinuxMountContext=""
    reconstruct.go:144: I0209 18:46:39.050006] Volume is marked device as uncertain and added into the actual state volumeName="fake-plugin/pvc-abcdef" deviceMountPath=""
    reconstruct.go:97: I0209 18:46:39.050013] Volume reconstruction finished
=== RUN   TestCtestReconstructVolumes/when_reconstruction_fails_for_a_volume,_volumes_should_be_cleaned_up
=== NAME  TestCtestReconstructVolumes
    reconstruct_common.go:236: I0209 18:46:39.050920] Volume path from volume plugin directory podName="pod1" volumePath="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/413181878/pods/pod1/volumes/missing-plugin/pvc-abcdef"
    reconstruct_common.go:249: I0209 18:46:39.050931] Get volume from pod directory path="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/413181878/pods" volume={"podName":"pod1","volumeSpecName":"pvc-abcdef","volumePath":"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/413181878/pods/pod1/volumes/missing-plugin/pvc-abcdef","pluginName":"missing-plugin","volumeMode":"Filesystem"}
    reconstruct.go:67: I0209 18:46:39.050939] Could not construct volume information podName="pod1" volumeSpecName="pvc-abcdef" err="no volume plugin matched name: missing-plugin"
    reconstruct.go:97: I0209 18:46:39.050944] Volume reconstruction finished
=== RUN   TestCtestReconstructVolumes/edge:_empty_volume_paths_list
=== NAME  TestCtestReconstructVolumes
    reconstruct.go:97: I0209 18:46:39.051335] Volume reconstruction finished
=== RUN   TestCtestReconstructVolumes/edge:_non‑existent_volume_path_triggers_reconstruction_failure
=== NAME  TestCtestReconstructVolumes
    reconstruct_common.go:236: I0209 18:46:39.051722] Volume path from volume plugin directory podName="nonexistent" volumePath="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1064755335/pods/nonexistent/volumes/fake-plugin/pvc-xyz"
    reconstruct_common.go:249: I0209 18:46:39.051736] Get volume from pod directory path="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1064755335/pods" volume={"podName":"nonexistent","volumeSpecName":"pvc-xyz","volumePath":"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1064755335/pods/nonexistent/volumes/fake-plugin/pvc-xyz","pluginName":"fake-plugin","volumeMode":"Filesystem"}
    reconstruct.go:72: I0209 18:46:39.051745] Adding reconstructed volume to actual state and node status podName="nonexistent" volumeSpecName="pvc-xyz"
    reconstruct.go:130: I0209 18:46:39.051753] Volume is marked as uncertain and added into the actual state pod="" podName="nonexistent" volumeName="fake-plugin/pvc-xyz" seLinuxMountContext=""
    reconstruct.go:144: I0209 18:46:39.051761] Volume is marked device as uncertain and added into the actual state volumeName="fake-plugin/pvc-xyz" deviceMountPath=""
    reconstruct.go:97: I0209 18:46:39.051766] Volume reconstruction finished
=== NAME  TestCtestReconstructVolumes/edge:_non‑existent_volume_path_triggers_reconstruction_failure
    ctest_reconstruct_test.go:104: Expected expectedVolumesNeedDevicePath:
        []
         got:
        [fake-plugin/pvc-xyz]
    ctest_reconstruct_test.go:111: Expected volumesFailedReconstruction:
        [pvc-xyz]
         got:
        []
--- FAIL: TestCtestReconstructVolumes (0.00s)
    --- PASS: TestCtestReconstructVolumes/when_two_pods_are_using_same_volume_and_both_are_deleted (0.00s)
    --- PASS: TestCtestReconstructVolumes/when_reconstruction_fails_for_a_volume,_volumes_should_be_cleaned_up (0.00s)
    --- PASS: TestCtestReconstructVolumes/edge:_empty_volume_paths_list (0.00s)
    --- FAIL: TestCtestReconstructVolumes/edge:_non‑existent_volume_path_triggers_reconstruction_failure (0.00s)
=== RUN   TestCtestCleanOrphanVolumes
=== RUN   TestCtestCleanOrphanVolumes/volume_is_in_DSW_and_is_not_cleaned
    desired_state_of_world.go:309: I0209 18:46:39.052249] expected volume SELinux label context volume="volume-name" label=""
    desired_state_of_world.go:329: I0209 18:46:39.052275] volume does not support SELinux context mount, clearing the expected label volume="volume-name"
    reconstruct.go:159: I0209 18:46:39.052294] Volume exists in desired state, skip cleaning up mounts podName="pod1uid" volumeSpecName="volume-name"
    reconstruct.go:166: I0209 18:46:39.052304] Orphan volume cleanup finished
=== RUN   TestCtestCleanOrphanVolumes/volume_is_not_in_DSW_and_is_cleaned
    reconstruct.go:162: I0209 18:46:39.052511] Cleaning up mounts for volume that could not be reconstructed podName="pod1uid" volumeSpecName="volume-name"
    reconstruct_common.go:158: I0209 18:46:39.052524] Reconciler sync states: could not find volume information in desired state, clean up the mount points podName="pod1uid" volumeSpecName="volume-name"
    reconstruct.go:166: I0209 18:46:39.052548] Orphan volume cleanup finished
I0209 18:46:39.052652   88816 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "volume-name" (OuterVolumeSpecName: "") pod "pod1uid" (UID: "pod1uid"). InnerVolumeSpecName "volume-name". PluginName "fake-plugin", VolumeGIDValue ""
E0209 18:46:39.052687   88816 operation_generator.go:796] UnmountVolume.MarkVolumeAsUnmounted failed for volume "" (UniqueName: "volume-name") pod "pod1uid" (UID: "pod1uid") : no volume with the name "volume-name" exists in the list of attached volumes
=== RUN   TestCtestCleanOrphanVolumes/edge:_no_failed_reconstructions,_nothing_to_clean
=== RUN   TestCtestCleanOrphanVolumes/edge:_multiple_pods_with_same_volume,_only_one_unmount_expected
    desired_state_of_world.go:309: I0209 18:46:39.155660] expected volume SELinux label context volume="volume-name" label=""
    desired_state_of_world.go:329: I0209 18:46:39.155742] volume does not support SELinux context mount, clearing the expected label volume="volume-name"
    desired_state_of_world.go:309: I0209 18:46:39.155764] expected volume SELinux label context volume="volume-name" label=""
    actual_state_of_world.go:721: I0209 18:46:39.155812] Volume is already added to attachedVolume list, update device path volumeName="fake-plugin/volume-name" path=""
    reconstruct.go:159: I0209 18:46:39.155938] Volume exists in desired state, skip cleaning up mounts podName="pod1uid" volumeSpecName="volume-name"
    reconstruct.go:166: I0209 18:46:39.155959] Orphan volume cleanup finished
    ctest_reconstruct_test.go:219: Error waiting for volumes to get unmounted: timed out waiting for the condition: expected TearDown calls 1, got 0
--- FAIL: TestCtestCleanOrphanVolumes (12.22s)
    --- PASS: TestCtestCleanOrphanVolumes/volume_is_in_DSW_and_is_not_cleaned (0.00s)
    --- PASS: TestCtestCleanOrphanVolumes/volume_is_not_in_DSW_and_is_cleaned (0.10s)
    --- PASS: TestCtestCleanOrphanVolumes/edge:_no_failed_reconstructions,_nothing_to_clean (0.00s)
    --- FAIL: TestCtestCleanOrphanVolumes/edge:_multiple_pods_with_same_volume,_only_one_unmount_expected (12.12s)
=== RUN   TestCtestReconstructVolumesMount
=== RUN   TestCtestReconstructVolumesMount/reconstructed_volume_is_mounted
=== NAME  TestCtestReconstructVolumesMount
    reconstruct_common.go:236: I0209 18:46:51.280837] Volume path from volume plugin directory podName="pod1uid" volumePath="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1970547792/pods/pod1uid/volumes/fake-plugin/volumename"
    reconstruct_common.go:249: I0209 18:46:51.282880] Get volume from pod directory path="/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1970547792/pods" volume={"podName":"pod1uid","volumeSpecName":"volumename","volumePath":"/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/1970547792/pods/pod1uid/volumes/fake-plugin/volumename","pluginName":"fake-plugin","volumeMode":"Filesystem"}
    reconstruct.go:72: I0209 18:46:51.284293] Adding reconstructed volume to actual state and node status podName="pod1uid" volumeSpecName="volumename"
    reconstruct.go:130: I0209 18:46:51.284636] Volume is marked as uncertain and added into the actual state pod="" podName="pod1uid" volumeName="fake-plugin/volumename" seLinuxMountContext=""
    reconstruct.go:144: I0209 18:46:51.284651] Volume is marked device as uncertain and added into the actual state volumeName="fake-plugin/volumename" deviceMountPath=""
    reconstruct.go:97: I0209 18:46:51.284660] Volume reconstruction finished
    desired_state_of_world.go:309: I0209 18:46:51.285154] expected volume SELinux label context volume="volumename" label=""
    desired_state_of_world.go:329: I0209 18:46:51.285634] volume does not support SELinux context mount, clearing the expected label volume="volumename"
=== NAME  TestCtestReconstructVolumesMount/reconstructed_volume_is_mounted
    actual_state_of_world.go:721: I0209 18:46:51.285654] Volume is already added to attachedVolume list, update device path volumeName="fake-plugin/volumename" path=""
=== NAME  TestCtestReconstructVolumesMount
    reconciler_common.go:214: I0209 18:46:51.285882] Starting operationExecutor.MountVolume for volume "volumename" (UniqueName: "fake-plugin/volumename") pod "pod1" (UID: "pod1uid")  pod="pod1"
    reconciler_common.go:225: I0209 18:46:51.286844] operationExecutor.MountVolume started for volume "volumename" (UniqueName: "fake-plugin/volumename") pod "pod1" (UID: "pod1uid")  pod="pod1"
I0209 18:46:51.287955   88816 operation_generator.go:515] "MountVolume.WaitForAttach entering for volume \"volumename\" (UniqueName: \"fake-plugin/volumename\") pod \"pod1\" (UID: \"pod1uid\") DevicePath \"\"" pod="pod1"
I0209 18:46:51.288224   88816 operation_generator.go:525] "MountVolume.WaitForAttach succeeded for volume \"volumename\" (UniqueName: \"fake-plugin/volumename\") pod \"pod1\" (UID: \"pod1uid\") DevicePath \"/dev/sdb\"" pod="pod1"
I0209 18:46:51.288233   88816 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"volumename\" (UniqueName: \"fake-plugin/volumename\") pod \"pod1\" (UID: \"pod1uid\") device mount path \"\"" pod="pod1"
=== RUN   TestCtestReconstructVolumesMount/reconstructed_volume_fails_to_mount
    actual_state_of_world.go:721: I0209 18:46:51.392695] Volume is already added to attachedVolume list, update device path volumeName="fake-plugin/fail-setup-volume" path=""
=== NAME  TestCtestReconstructVolumesMount
    reconciler_common.go:214: I0209 18:46:51.392723] Starting operationExecutor.MountVolume for volume "fail-setup-volume" (UniqueName: "fake-plugin/fail-setup-volume") pod "pod1" (UID: "pod1uid")  pod="pod1"
    reconciler_common.go:225: I0209 18:46:51.392796] operationExecutor.MountVolume started for volume "fail-setup-volume" (UniqueName: "fake-plugin/fail-setup-volume") pod "pod1" (UID: "pod1uid")  pod="pod1"
I0209 18:46:51.392831   88816 operation_generator.go:515] "MountVolume.WaitForAttach entering for volume \"fail-setup-volume\" (UniqueName: \"fake-plugin/fail-setup-volume\") pod \"pod1\" (UID: \"pod1uid\") DevicePath \"\"" pod="pod1"
I0209 18:46:51.392851   88816 operation_generator.go:525] "MountVolume.WaitForAttach succeeded for volume \"fail-setup-volume\" (UniqueName: \"fake-plugin/fail-setup-volume\") pod \"pod1\" (UID: \"pod1uid\") DevicePath \"/dev/sdb\"" pod="pod1"
I0209 18:46:51.392858   88816 operation_generator.go:557] "MountVolume.MountDevice succeeded for volume \"fail-setup-volume\" (UniqueName: \"fake-plugin/fail-setup-volume\") pod \"pod1\" (UID: \"pod1uid\") device mount path \"\"" pod="pod1"
E0209 18:46:51.394135   88816 nestedpendingoperations.go:348] Operation for "{volumeName:fake-plugin/fail-setup-volume podName: nodeName:}" failed. No retries permitted until 2026-02-09 18:46:51.893268 -0600 CST m=+12.864842918 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "fail-setup-volume" (UniqueName: "fake-plugin/fail-setup-volume") pod "pod1" (UID: "pod1uid") : mounting volume failed
=== RUN   TestCtestReconstructVolumesMount/reconstructed_volume_device_map_fails
    actual_state_of_world.go:721: I0209 18:46:51.495704] Volume is already added to attachedVolume list, update device path volumeName="fake-plugin/fail-mount-device-volume-name" path=""
=== NAME  TestCtestReconstructVolumesMount
    reconciler_common.go:214: I0209 18:46:51.495727] Starting operationExecutor.MountVolume for volume "fail-mount-device-volume-name" (UniqueName: "fake-plugin/fail-mount-device-volume-name") pod "pod1" (UID: "pod1uid")  pod="pod1"
    reconciler_common.go:225: I0209 18:46:51.495744] operationExecutor.MountVolume started for volume "fail-mount-device-volume-name" (UniqueName: "fake-plugin/fail-mount-device-volume-name") pod "pod1" (UID: "pod1uid")  pod="pod1"
I0209 18:46:51.496465   88816 operation_generator.go:992] "MapVolume.WaitForAttach entering for volume \"fail-mount-device-volume-name\" (UniqueName: \"fake-plugin/fail-mount-device-volume-name\") pod \"pod1\" (UID: \"pod1uid\") DevicePath \"\"" pod="pod1"
I0209 18:46:51.496484   88816 operation_generator.go:1002] "MapVolume.WaitForAttach succeeded for volume \"fail-mount-device-volume-name\" (UniqueName: \"fake-plugin/fail-mount-device-volume-name\") pod \"pod1\" (UID: \"pod1uid\") DevicePath \"/dev/sdb\"" pod="pod1"
E0209 18:46:51.498203   88816 nestedpendingoperations.go:348] Operation for "{volumeName:fake-plugin/fail-mount-device-volume-name podName: nodeName:}" failed. No retries permitted until 2026-02-09 18:46:51.998191 -0600 CST m=+12.969766543 (durationBeforeRetry 500ms). Error: MapVolume.SetUpDevice failed for volume "fail-mount-device-volume-name" (UniqueName: "fake-plugin/fail-mount-device-volume-name") pod "pod1" (UID: "pod1uid") : error mapping disk: fail-mount-device-volume-name
=== RUN   TestCtestReconstructVolumesMount/edge:_empty_volume_path_should_result_in_no_reconstruction
--- PASS: TestCtestReconstructVolumesMount (0.33s)
    --- PASS: TestCtestReconstructVolumesMount/reconstructed_volume_is_mounted (0.11s)
    --- PASS: TestCtestReconstructVolumesMount/reconstructed_volume_fails_to_mount (0.11s)
    --- PASS: TestCtestReconstructVolumesMount/reconstructed_volume_device_map_fails (0.10s)
    --- PASS: TestCtestReconstructVolumesMount/edge:_empty_volume_path_should_result_in_no_reconstruction (0.00s)
FAIL
coverage: 55.8% of statements
FAIL	k8s.io/kubernetes/pkg/kubelet/volumemanager/reconciler	14.101s
	k8s.io/kubernetes/pkg/kubelet/watchdog		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/kubelet/winstats	[no test files]
	k8s.io/kubernetes/pkg/kubemark		coverage: 0.0% of statements
=== RUN   TestCtestCustomTypePrinting

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/printers/ctest_tablegenerator_test.go:27]: get default configs: {test_fixture.json [custom type printing columns] columns [pods deployments statefulsets daemonsets replicasets] [{Data    0}]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:46:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:46:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:46:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:46:42 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/printers	0.591s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/printers/internalversion	1.210s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/printers/storage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/probe	0.790s	coverage: 0.0% of statements [no tests to run]
FAIL	k8s.io/kubernetes/pkg/probe/exec [build failed]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/probe/grpc	1.444s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestFormatURL

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/probe/http/ctest_request_test.go:31]: Number of test cases: 7
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
    ctest_request_test.go:36: Expected ://example.com:80, got //example.com:80
Running 5 th test case.
Running 6 th test case.

==================== CTEST END ======================
--- FAIL: TestCtestFormatURL (0.00s)
=== RUN   TestCtest_v1HeaderToHTTPHeader

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/probe/http/ctest_request_test.go:104]: Number of test cases: 6
=== RUN   TestCtest_v1HeaderToHTTPHeader/not_empty_input
=== RUN   TestCtest_v1HeaderToHTTPHeader/case_insensitive
=== RUN   TestCtest_v1HeaderToHTTPHeader/empty_input
=== RUN   TestCtest_v1HeaderToHTTPHeader/empty_header_name
=== RUN   TestCtest_v1HeaderToHTTPHeader/duplicate_names_with_different_case
=== RUN   TestCtest_v1HeaderToHTTPHeader/nil_slice_input

==================== CTEST END ======================
--- PASS: TestCtest_v1HeaderToHTTPHeader (0.00s)
    --- PASS: TestCtest_v1HeaderToHTTPHeader/not_empty_input (0.00s)
    --- PASS: TestCtest_v1HeaderToHTTPHeader/case_insensitive (0.00s)
    --- PASS: TestCtest_v1HeaderToHTTPHeader/empty_input (0.00s)
    --- PASS: TestCtest_v1HeaderToHTTPHeader/empty_header_name (0.00s)
    --- PASS: TestCtest_v1HeaderToHTTPHeader/duplicate_names_with_different_case (0.00s)
    --- PASS: TestCtest_v1HeaderToHTTPHeader/nil_slice_input (0.00s)
=== RUN   TestCtestHeaderConversion

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/probe/http/ctest_request_test.go:175]: Number of test cases: 6
Running 0 th test case.
Running 1 th test case.
Running 2 th test case.
Running 3 th test case.
Running 4 th test case.
Running 5 th test case.

==================== CTEST END ======================
--- PASS: TestCtestHeaderConversion (0.00s)
FAIL
coverage: 12.0% of statements
FAIL	k8s.io/kubernetes/pkg/probe/http	0.527s
=== RUN   TestCtestTcpHealthChecker

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/probe/tcp/ctest_tcp_test.go:62]: Number of test cases: 7
Running test case #0: host="127.0.0.1" port=49717
Running test case #1: host="127.0.0.1" port=-1
Running test case #2: host="127.0.0.1" port=0
Running test case #3: host="127.0.0.1" port=65536
Running test case #4: host="" port=49717
    ctest_tcp_test.go:69: #4: expected status=failure, got=success
Running test case #5: host="invalid.host" port=49717
Running test case #6: host="127.0.0.1" port=-9999

==================== CTEST END ======================
--- FAIL: TestCtestTcpHealthChecker (0.10s)
FAIL
coverage: 90.9% of statements
FAIL	k8s.io/kubernetes/pkg/probe/tcp	0.707s
=== RUN   TestCtestNewNodeManager

==================== CTEST START ====================
Running test case 0: node object doesn't exist
=== RUN   TestCtestNewNodeManager/node_object_doesn't_exist
  I0209 18:46:48.227801   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:48.329587   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
    node.go:121: E0209 18:46:49.330839] Timed out waiting for node %q to exist test-node="(MISSING)"
Running test case 1: node object doesn't exist, with watchPodCIDRs
=== RUN   TestCtestNewNodeManager/node_object_doesn't_exist,_with_watchPodCIDRs
  I0209 18:46:49.331243   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:49.433292   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Running test case 2: node object exist without NodeIP
=== RUN   TestCtestNewNodeManager/node_object_exist_without_NodeIP
  I0209 18:46:51.436324   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:51.538403   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
    node.go:123: E0209 18:46:52.540495] Timed out waiting for node %q to be assigned IPs test-node="(MISSING)"
Running test case 3: node object exist with NodeIP
=== RUN   TestCtestNewNodeManager/node_object_exist_with_NodeIP
  I0209 18:46:52.540902   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:52.641420   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Running test case 4: watchPodCIDRs and node object exist without PodCIDRs
=== RUN   TestCtestNewNodeManager/watchPodCIDRs_and_node_object_exist_without_PodCIDRs
  I0209 18:46:52.683413   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:52.785451   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Running test case 5: watchPodCIDRs and node object exist with NodeIP and PodCIDR
=== RUN   TestCtestNewNodeManager/watchPodCIDRs_and_node_object_exist_with_NodeIP_and_PodCIDR
  I0209 18:46:53.828068   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:53.929895   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Running test case 6: watchPodCIDRs and node object exist without NodeIP and with PodCIDR
=== RUN   TestCtestNewNodeManager/watchPodCIDRs_and_node_object_exist_without_NodeIP_and_with_PodCIDR
  I0209 18:46:53.990264   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:54.092313   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
    node.go:123: E0209 18:46:55.093692] Timed out waiting for node %q to be assigned IPs test-node="(MISSING)"
Running test case 7: node object exist with empty NodeIP
=== RUN   TestCtestNewNodeManager/node_object_exist_with_empty_NodeIP
  I0209 18:46:55.093822   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:55.195864   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
    node.go:123: E0209 18:46:56.197945] Timed out waiting for node %q to be assigned IPs test-node="(MISSING)"
Running test case 8: node object exist with invalid NodeIP
=== RUN   TestCtestNewNodeManager/node_object_exist_with_invalid_NodeIP
  I0209 18:46:56.198191   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:56.300213   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
    node.go:123: E0209 18:46:57.301738] Timed out waiting for node %q to be assigned IPs test-node="(MISSING)"
Running test case 9: watchPodCIDRs with invalid PodCIDR
=== RUN   TestCtestNewNodeManager/watchPodCIDRs_with_invalid_PodCIDR
  I0209 18:46:57.301967   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:57.405117   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
    ctest_node_test.go:224: 
        	Error Trace:	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/proxy/ctest_node_test.go:224
        	Error:      	Expected nil, but got: &proxy.NodeManager{nodeInformer:(*v1.nodeInformer)(0x140009ce540), nodeLister:(*v1.nodeLister)(0x14000f0a280), exitFunc:(func(int))(0x102fde500), watchPodCIDRs:true, nodeIPs:[]net.IP{net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xc0, 0xa8, 0x1, 0x1}}, podCIDRs:[]string{"invalid-cidr"}, mu:sync.Mutex{_:sync.noCopy{}, mu:sync.Mutex{state:0, sema:0x0}}, node:(*v1.Node)(0x14000bbc308)}
        	Test:       	TestCtestNewNodeManager/watchPodCIDRs_with_invalid_PodCIDR

==================== CTEST END ======================
--- FAIL: TestCtestNewNodeManager (9.24s)
    --- PASS: TestCtestNewNodeManager/node_object_doesn't_exist (1.15s)
    --- PASS: TestCtestNewNodeManager/node_object_doesn't_exist,_with_watchPodCIDRs (2.11s)
    --- PASS: TestCtestNewNodeManager/node_object_exist_without_NodeIP (1.10s)
    --- PASS: TestCtestNewNodeManager/node_object_exist_with_NodeIP (0.14s)
    --- PASS: TestCtestNewNodeManager/watchPodCIDRs_and_node_object_exist_without_PodCIDRs (1.14s)
    --- PASS: TestCtestNewNodeManager/watchPodCIDRs_and_node_object_exist_with_NodeIP_and_PodCIDR (0.16s)
    --- PASS: TestCtestNewNodeManager/watchPodCIDRs_and_node_object_exist_without_NodeIP_and_with_PodCIDR (1.10s)
    --- PASS: TestCtestNewNodeManager/node_object_exist_with_empty_NodeIP (1.10s)
    --- PASS: TestCtestNewNodeManager/node_object_exist_with_invalid_NodeIP (1.10s)
    --- FAIL: TestCtestNewNodeManager/watchPodCIDRs_with_invalid_PodCIDR (0.12s)
=== RUN   TestCtestNodeManagerOnNodeChange

==================== CTEST START ====================
Running node change test 0: node updated with same NodeIPs
=== RUN   TestCtestNodeManagerOnNodeChange/node_updated_with_same_NodeIPs
  I0209 18:46:57.423051   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:57.525228   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Running node change test 1: node updated with different NodeIPs
=== RUN   TestCtestNodeManagerOnNodeChange/node_updated_with_different_NodeIPs
  I0209 18:46:57.527945   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:57.628837   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
  I0209 18:46:57.630863   88995 node.go:197] "NodeIPs changed for the node" node="test-node" newNodeIPs=["10.0.1.1","fd00:3:2:1::2"] oldNodeIPs=["192.168.1.1","fd00:1:2:3::1"]
Running node change test 2: watchPodCIDR and node updated with same PodCIDRs
=== RUN   TestCtestNodeManagerOnNodeChange/watchPodCIDR_and_node_updated_with_same_PodCIDRs
  I0209 18:46:57.632787   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:57.734568   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
Running node change test 3: watchPodCIDR and node updated with different PodCIDRs
=== RUN   TestCtestNodeManagerOnNodeChange/watchPodCIDR_and_node_updated_with_different_PodCIDRs
  I0209 18:46:57.737408   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:57.842674   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
  I0209 18:46:57.842756   88995 node.go:185] "PodCIDRs changed for the node" node="test-node" newPodCIDRs=["172.16.10.0/24","fd01:5422::/64"] oldPodCIDRs=["10.0.0.0/8","fd01:2345::/64"]
Running node change test 4: node updated with empty NodeIPs
=== RUN   TestCtestNodeManagerOnNodeChange/node_updated_with_empty_NodeIPs
  I0209 18:46:57.846081   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:57.947027   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
  I0209 18:46:57.947070   88995 node.go:197] "NodeIPs changed for the node" node="test-node" newNodeIPs=null oldNodeIPs=["192.168.1.1"]
Running node change test 5: watchPodCIDR with invalid updated PodCIDR
=== RUN   TestCtestNodeManagerOnNodeChange/watchPodCIDR_with_invalid_updated_PodCIDR
  I0209 18:46:57.948791   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:58.050841   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
  I0209 18:46:58.050875   88995 node.go:185] "PodCIDRs changed for the node" node="test-node" newPodCIDRs=["invalid-cidr"] oldPodCIDRs=["10.0.0.0/8"]
  I0209 18:46:58.050894   88995 node.go:197] "NodeIPs changed for the node" node="test-node" newNodeIPs=null oldNodeIPs=["192.168.1.1"]

==================== CTEST END ======================
--- PASS: TestCtestNodeManagerOnNodeChange (0.63s)
    --- PASS: TestCtestNodeManagerOnNodeChange/node_updated_with_same_NodeIPs (0.11s)
    --- PASS: TestCtestNodeManagerOnNodeChange/node_updated_with_different_NodeIPs (0.11s)
    --- PASS: TestCtestNodeManagerOnNodeChange/watchPodCIDR_and_node_updated_with_same_PodCIDRs (0.10s)
    --- PASS: TestCtestNodeManagerOnNodeChange/watchPodCIDR_and_node_updated_with_different_PodCIDRs (0.11s)
    --- PASS: TestCtestNodeManagerOnNodeChange/node_updated_with_empty_NodeIPs (0.10s)
    --- PASS: TestCtestNodeManagerOnNodeChange/watchPodCIDR_with_invalid_updated_PodCIDR (0.10s)
=== RUN   TestCtestNodeManagerOnNodeDelete

==================== CTEST START ====================
  I0209 18:46:58.053356   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:58.158607   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
  I0209 18:46:58.158645   88995 node.go:207] "Node is being deleted" node="test-node"

==================== CTEST END ======================
--- PASS: TestCtestNodeManagerOnNodeDelete (0.11s)
=== RUN   TestCtestNodeManagerNode

==================== CTEST START ====================
  I0209 18:46:58.169836   88995 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
  I0209 18:46:58.277632   88995 shared_informer.go:356] "Caches are synced" controller="node informer cache"
  I0209 18:46:58.281914   88995 node.go:197] "NodeIPs changed for the node" node="test-node" newNodeIPs=null oldNodeIPs=["192.168.1.1"]
  I0209 18:46:58.282280   88995 node.go:197] "NodeIPs changed for the node" node="test-node" newNodeIPs=null oldNodeIPs=["192.168.1.1"]

==================== CTEST END ======================
--- PASS: TestCtestNodeManagerNode (0.12s)
FAIL
coverage: 10.9% of statements
FAIL	k8s.io/kubernetes/pkg/proxy	10.998s
?   	k8s.io/kubernetes/pkg/proxy/apis	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/proxy/apis/config	0.306s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/proxy/apis/config/fuzzer		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/proxy/apis/config/scheme	1.969s	coverage: 100.0% of statements [no tests to run]
=== RUN   TestCtestDefaultsKubeProxyConfiguration
=== Starting TestCtestDefaultsKubeProxyConfiguration ===
Number of test cases: 3
Running test case #0: empty-config
=== RUN   TestCtestDefaultsKubeProxyConfiguration/empty-config
Running test case #1: metrics and healthz address with no port
=== RUN   TestCtestDefaultsKubeProxyConfiguration/metrics_and_healthz_address_with_no_port
Running test case #2: zero duration fields should default
=== RUN   TestCtestDefaultsKubeProxyConfiguration/zero_duration_fields_should_default
=== Finished TestCtestDefaultsKubeProxyConfiguration ===
--- PASS: TestCtestDefaultsKubeProxyConfiguration (0.00s)
    --- PASS: TestCtestDefaultsKubeProxyConfiguration/empty-config (0.00s)
    --- PASS: TestCtestDefaultsKubeProxyConfiguration/metrics_and_healthz_address_with_no_port (0.00s)
    --- PASS: TestCtestDefaultsKubeProxyConfiguration/zero_duration_fields_should_default (0.00s)
PASS
coverage: 20.4% of statements
ok  	k8s.io/kubernetes/pkg/proxy/apis/config/v1alpha1	1.289s	coverage: 20.4% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/proxy/apis/config/validation	1.532s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/proxy/config	0.441s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/proxy/conntrack	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/proxy/healthcheck	0.500s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/proxy/iptables	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/ipset	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/ipset/testing	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/testing	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/util	[no test files]
?   	k8s.io/kubernetes/pkg/proxy/ipvs/util/testing	[no test files]
	k8s.io/kubernetes/pkg/proxy/kubemark		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/proxy/metaproxier		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/proxy/metrics		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/proxy/nftables	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/proxy/runner	0.399s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGetNodeIPsEdge

==================== CTEST EXTEND ONLY START ==================== Running edge case tests for GetNodeIPs
=== RUN   TestCtestGetNodeIPsEdge/invalid_CIDR_format
    ctest_nodeport_addresses_test.go:98: unexpected MatchAll(IPv4), expected: false
    ctest_nodeport_addresses_test.go:106: unexpected mismatch for IPv4: not found: [], extra: [10.0.0.1]
--- FAIL: TestCtestGetNodeIPsEdge (0.00s)
    --- FAIL: TestCtestGetNodeIPsEdge/invalid_CIDR_format (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x8 pc=0x1003c8220]

goroutine 60 [running]:
testing.tRunner.func1.2({0x10223c940, 0x103c316e0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10223c940?, 0x103c316e0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
net.networkNumberAndMask(0x103c97780?)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/ip.go:457
net.(*IPNet).Contains(0x1400080c000?, {0x140002ca390, 0x10, 0x10022d2c0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/net/ip.go:481 +0x24
k8s.io/kubernetes/pkg/proxy/util.(*NodePortAddresses).GetNodeIPs(0x14000516f40, {0x102653100?, 0x1400011f6c0?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/proxy/util/nodeport_addresses.go:117 +0x600
k8s.io/kubernetes/pkg/proxy/util.TestCtestGetNodeIPsEdge.func1(0x14000485880)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/proxy/util/ctest_nodeport_addresses_test.go:100 +0x4e8
testing.tRunner(0x14000485880, 0x1400068a320)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 59
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/proxy/util	0.570s
	k8s.io/kubernetes/pkg/proxy/util/nfacct		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/proxy/util/testing		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/proxy/winkernel	[no test files]
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:27]: Running MatchingResources with edge cases
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/empty-list
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case empty-list
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/nil-list
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case nil-list
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/supported-resources
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case supported-resources
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/unsupported-resources
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case unsupported-resources
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/invalid-empty-string
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case invalid-empty-string
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/whitespace-string
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case whitespace-string
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/duplicate-supported
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case duplicate-supported
=== RUN   TestCtestPersistentVolumeClaimEvaluatorMatchingResources/very-long-string
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:114]: Running test case very-long-string

==================== CTEST END ======================
--- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/empty-list (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/nil-list (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/supported-resources (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/unsupported-resources (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/invalid-empty-string (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/whitespace-string (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/duplicate-supported (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorMatchingResources/very-long-string (0.00s)
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:126]: Running Handles with edge cases
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/create
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case create
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/update
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case update
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/delete
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case delete
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/connect
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case connect
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/create-subresource
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case create-subresource
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/update-subresource
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case update-subresource
=== RUN   TestCtestPersistentVolumeClaimEvaluatorHandles/nil-attributes
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:197]: Running Handles case nil-attributes
--- FAIL: TestCtestPersistentVolumeClaimEvaluatorHandles (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/create (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/update (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/delete (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/connect (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/create-subresource (0.00s)
    --- PASS: TestCtestPersistentVolumeClaimEvaluatorHandles/update-subresource (0.00s)
    --- FAIL: TestCtestPersistentVolumeClaimEvaluatorHandles/nil-attributes (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x50 pc=0x1062bc98c]

goroutine 80 [running]:
testing.tRunner.func1.2({0x106a375e0, 0x1084be390})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x106a375e0?, 0x1084be390?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/quota/v1/evaluator/core.(*pvcEvaluator).Handles(0x106e63488?, {0x0, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/persistent_volume_claims.go:97 +0x9c
k8s.io/kubernetes/pkg/quota/v1/evaluator/core.TestCtestPersistentVolumeClaimEvaluatorHandles.func1(0x140004b5c00)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/evaluator/core/ctest_persistent_volume_claims_test.go:198 +0x12c
testing.tRunner(0x140004b5c00, 0x1400006f080)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 73
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/quota/v1/evaluator/core	1.015s
=== RUN   TestCtestHasResourcesChanged

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/quota/v1/install/ctest_update_filter_test.go:25]: get default configs: {test_fixture.json [default pod containers] containers [pods deployments statefulsets daemonsets replicasets] [{foo  [] []  [] [] [] {map[] map[cpu:{{2 0} {<nil>} 2 DecimalSI}] []} [] <nil> [] [] [] nil nil nil nil    nil false false false}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:46:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:46:55 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/quota/v1/install	1.497s
?   	k8s.io/kubernetes/pkg/registry	[no test files]
=== RUN   TestCtestAuthorization
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:22]: 
==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:157]: Number of test cases: 9
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:160]: test case: superuser
=== RUN   TestCtestAuthorization/superuser
=== RUN   TestCtestAuthorization/superuser/create
=== RUN   TestCtestAuthorization/superuser/update
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:160]: test case: authorized
=== RUN   TestCtestAuthorization/authorized
=== RUN   TestCtestAuthorization/authorized/create
=== RUN   TestCtestAuthorization/authorized/update
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:160]: test case: denied
=== RUN   TestCtestAuthorization/denied
=== RUN   TestCtestAuthorization/denied/create
=== RUN   TestCtestAuthorization/denied/update
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:160]: test case: param not found
=== RUN   TestCtestAuthorization/param_not_found
=== RUN   TestCtestAuthorization/param_not_found/create
=== RUN   TestCtestAuthorization/param_not_found/update
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:47:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:160]: test case: nil object
=== RUN   TestCtestAuthorization/nil_object
=== RUN   TestCtestAuthorization/nil_object/create
--- FAIL: TestCtestAuthorization (0.04s)
    --- PASS: TestCtestAuthorization/superuser (0.01s)
        --- PASS: TestCtestAuthorization/superuser/create (0.01s)
        --- PASS: TestCtestAuthorization/superuser/update (0.00s)
    --- PASS: TestCtestAuthorization/authorized (0.01s)
        --- PASS: TestCtestAuthorization/authorized/create (0.00s)
        --- PASS: TestCtestAuthorization/authorized/update (0.01s)
    --- PASS: TestCtestAuthorization/denied (0.01s)
        --- PASS: TestCtestAuthorization/denied/create (0.00s)
        --- PASS: TestCtestAuthorization/denied/update (0.01s)
    --- PASS: TestCtestAuthorization/param_not_found (0.01s)
        --- PASS: TestCtestAuthorization/param_not_found/create (0.00s)
        --- PASS: TestCtestAuthorization/param_not_found/update (0.01s)
    --- FAIL: TestCtestAuthorization/nil_object (0.00s)
        --- FAIL: TestCtestAuthorization/nil_object/create (0.00s)
panic: interface conversion: runtime.Object is nil, not *admissionregistration.MutatingAdmissionPolicy [recovered]
	panic: interface conversion: runtime.Object is nil, not *admissionregistration.MutatingAdmissionPolicy

goroutine 100 [running]:
testing.tRunner.func1.2({0x102fb5be0, 0x1400083efc0})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x102fb5be0?, 0x1400083efc0?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy.(*mutatingAdmissionPolicyStrategy).Validate(0x1033d99f8?, {0x1033d9ad8?, 0x1400083ef90?}, {0x0?, 0x0?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/strategy.go:77 +0x504
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy.TestCtestAuthorization.func13.1(0x140004bb340)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicy/ctest_authz_test.go:168 +0x130
testing.tRunner(0x140004bb340, 0x14000a3f2c0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 99
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy	0.683s
=== RUN   TestCtestCreate
  W0209 18:47:02.183736   89202 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:02.185945   89202 logging.go:55] [core] [Channel #12 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:36: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:02.186-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:02.186-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  E0209 18:47:02.186933   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:02.186949   89202 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:02.188-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49725: use of closed network connection"}
  W0209 18:47:02.191387   89202 logging.go:55] [core] [Channel #1 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49725: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:02.195-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49726: use of closed network connection"}
--- FAIL: TestCtestCreate (0.42s)
=== RUN   TestCtestUpdate
  W0209 18:47:02.641151   89202 logging.go:55] [core] [Channel #19 SubChannel #21]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49735", ServerName: "localhost:49735", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:02.689455   89202 logging.go:55] [core] [Channel #30 SubChannel #31]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49735", ServerName: "localhost:49735", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49735: operation was canceled"

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:65: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:02.692-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:02.692-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  W0209 18:47:02.698189   89202 logging.go:55] [core] [Channel #19 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49735", ServerName: "localhost:49735", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49735: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:02.693-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49735: use of closed network connection"}
  E0209 18:47:02.703648   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:02.704116   89202 logging.go:55] [core] [Channel #35 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49735", ServerName: "localhost:49735", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49735: operation was canceled"
    logger.go:146: 2026-02-09T18:47:02.710-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49736: use of closed network connection"}
--- FAIL: TestCtestUpdate (0.50s)
=== RUN   TestCtestGet
  W0209 18:47:03.200951   89202 logging.go:55] [core] [Channel #1 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49725: connect: connection refused"
  W0209 18:47:03.708227   89202 logging.go:55] [core] [Channel #19 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49735", ServerName: "localhost:49735", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49735: connect: connection refused"
  W0209 18:47:03.760862   89202 logging.go:55] [core] [Channel #37 SubChannel #39]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49745", ServerName: "localhost:49745", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:03.787754   89202 logging.go:55] [core] [Channel #48 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49745", ServerName: "localhost:49745", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:100: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:03.791-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
  E0209 18:47:03.791829   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:03.791866   89202 logging.go:55] [core] [Channel #53 SubChannel #54]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49745", ServerName: "localhost:49745", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:03.792-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
    logger.go:146: 2026-02-09T18:47:03.792-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49745: use of closed network connection"}
  W0209 18:47:03.794338   89202 logging.go:55] [core] [Channel #37 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49745", ServerName: "localhost:49745", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49745: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:03.801-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49746: use of closed network connection"}
--- FAIL: TestCtestGet (1.09s)
=== RUN   TestCtestList
  W0209 18:47:04.286178   89202 logging.go:55] [core] [Channel #55 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49759", ServerName: "localhost:49759", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:04.286977   89202 logging.go:55] [core] [Channel #66 SubChannel #67]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49759", ServerName: "localhost:49759", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:121: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:04.287-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
  E0209 18:47:04.287536   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
    logger.go:146: 2026-02-09T18:47:04.287-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  W0209 18:47:04.287550   89202 logging.go:55] [core] [Channel #71 SubChannel #72]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49759", ServerName: "localhost:49759", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:04.287-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49759: use of closed network connection"}
  W0209 18:47:04.288126   89202 logging.go:55] [core] [Channel #55 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49759", ServerName: "localhost:49759", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49759: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:04.295-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49760: use of closed network connection"}
--- FAIL: TestCtestList (0.49s)
=== RUN   TestCtestDelete
  W0209 18:47:04.735996   89202 logging.go:55] [core] [Channel #1 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49725: connect: connection refused"
  W0209 18:47:04.795298   89202 logging.go:55] [core] [Channel #37 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49745", ServerName: "localhost:49745", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49745: connect: connection refused"
  W0209 18:47:05.290954   89202 logging.go:55] [core] [Channel #55 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49759", ServerName: "localhost:49759", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49759: connect: connection refused"
  W0209 18:47:05.481658   89202 logging.go:55] [core] [Channel #19 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49735", ServerName: "localhost:49735", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49735: connect: connection refused"
  W0209 18:47:05.529397   89202 logging.go:55] [core] [Channel #73 SubChannel #76]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49769", ServerName: "localhost:49769", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:05.532117   89202 logging.go:55] [core] [Channel #84 SubChannel #85]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49769", ServerName: "localhost:49769", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:142: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:05.532-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:05.532-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
    logger.go:146: 2026-02-09T18:47:05.532-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49769: use of closed network connection"}
  E0209 18:47:05.533001   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:05.533011   89202 logging.go:55] [core] [Channel #89 SubChannel #90]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49769", ServerName: "localhost:49769", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:05.533560   89202 logging.go:55] [core] [Channel #73 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49769", ServerName: "localhost:49769", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49769: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:05.545-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49770: use of closed network connection"}
--- FAIL: TestCtestDelete (1.25s)
=== RUN   TestCtestWatch
  W0209 18:47:06.196742   89202 logging.go:55] [core] [Channel #37 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49745", ServerName: "localhost:49745", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49745: connect: connection refused"
  W0209 18:47:06.538703   89202 logging.go:55] [core] [Channel #73 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49769", ServerName: "localhost:49769", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49769: connect: connection refused"
  W0209 18:47:06.753002   89202 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49787", ServerName: "localhost:49787", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:06.764028   89202 logging.go:55] [core] [Channel #55 SubChannel #60]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49759", ServerName: "localhost:49759", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49759: connect: connection refused"
  W0209 18:47:06.767559   89202 logging.go:55] [core] [Channel #102 SubChannel #103]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49787", ServerName: "localhost:49787", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:163: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:06.773-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:06.774-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  W0209 18:47:06.776890   89202 logging.go:55] [core] [Channel #107 SubChannel #108]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49787", ServerName: "localhost:49787", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:06.777-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49787: use of closed network connection"}
  W0209 18:47:06.777549   89202 logging.go:55] [core] [Channel #91 SubChannel #94]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49787", ServerName: "localhost:49787", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49787: connect: connection refused"
  E0209 18:47:06.779410   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
    logger.go:146: 2026-02-09T18:47:06.784-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49788: use of closed network connection"}
--- FAIL: TestCtestWatch (1.24s)
=== RUN   TestCtestCategories
  W0209 18:47:06.935320   89202 logging.go:55] [core] [Channel #1 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49725", ServerName: "localhost:49725", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49725: connect: connection refused"
  W0209 18:47:07.377036   89202 logging.go:55] [core] [Channel #120 SubChannel #121]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49803", ServerName: "localhost:49803", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:07.384-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:07.385-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
    logger.go:146: 2026-02-09T18:47:07.386-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49803: use of closed network connection"}
  E0209 18:47:07.386589   89202 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:07.386665   89202 logging.go:55] [core] [Channel #125 SubChannel #126]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49803", ServerName: "localhost:49803", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:07.388645   89202 logging.go:55] [core] [Channel #109 SubChannel #112]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49803", ServerName: "localhost:49803", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49803: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:07.419-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49804: use of closed network connection"}
--- PASS: TestCtestCategories (0.65s)
FAIL
coverage: 75.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicy/storage	6.817s
=== RUN   TestCtestAuthorization
=== RUN   TestCtestAuthorization/superuser
=== RUN   TestCtestAuthorization/superuser/create
=== RUN   TestCtestAuthorization/superuser/update
=== RUN   TestCtestAuthorization/authorized
=== RUN   TestCtestAuthorization/authorized/create
=== RUN   TestCtestAuthorization/authorized/update
=== RUN   TestCtestAuthorization/denied
=== RUN   TestCtestAuthorization/denied/create
=== RUN   TestCtestAuthorization/denied/update
=== RUN   TestCtestAuthorization/deny_but_relevant_fields_not_updated
=== RUN   TestCtestAuthorization/deny_but_relevant_fields_not_updated/create
=== RUN   TestCtestAuthorization/deny_but_relevant_fields_not_updated/update
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef/create
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef/update
=== RUN   TestCtestAuthorization/unable_to_resolve_param
=== RUN   TestCtestAuthorization/unable_to_resolve_param/create
=== RUN   TestCtestAuthorization/unable_to_resolve_param/update
=== RUN   TestCtestAuthorization/unable_to_get_policy
=== RUN   TestCtestAuthorization/unable_to_get_policy/create
=== RUN   TestCtestAuthorization/unable_to_get_policy/update
=== RUN   TestCtestAuthorization/nil_userInfo
=== RUN   TestCtestAuthorization/nil_userInfo/create
=== RUN   TestCtestAuthorization/nil_userInfo/update
=== RUN   TestCtestAuthorization/auth_returns_error
=== RUN   TestCtestAuthorization/auth_returns_error/create
=== RUN   TestCtestAuthorization/auth_returns_error/update
=== RUN   TestCtestAuthorization/policyGetter_returns_nil_without_error
=== RUN   TestCtestAuthorization/policyGetter_returns_nil_without_error/create
--- FAIL: TestCtestAuthorization (0.00s)
    --- PASS: TestCtestAuthorization/superuser (0.00s)
        --- PASS: TestCtestAuthorization/superuser/create (0.00s)
        --- PASS: TestCtestAuthorization/superuser/update (0.00s)
    --- PASS: TestCtestAuthorization/authorized (0.00s)
        --- PASS: TestCtestAuthorization/authorized/create (0.00s)
        --- PASS: TestCtestAuthorization/authorized/update (0.00s)
    --- PASS: TestCtestAuthorization/denied (0.00s)
        --- PASS: TestCtestAuthorization/denied/create (0.00s)
        --- PASS: TestCtestAuthorization/denied/update (0.00s)
    --- PASS: TestCtestAuthorization/deny_but_relevant_fields_not_updated (0.00s)
        --- PASS: TestCtestAuthorization/deny_but_relevant_fields_not_updated/create (0.00s)
        --- PASS: TestCtestAuthorization/deny_but_relevant_fields_not_updated/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_parse_paramRef (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_parse_paramRef/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_parse_paramRef/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_resolve_param (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_resolve_param/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_resolve_param/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_get_policy (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_get_policy/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_get_policy/update (0.00s)
    --- PASS: TestCtestAuthorization/nil_userInfo (0.00s)
        --- PASS: TestCtestAuthorization/nil_userInfo/create (0.00s)
        --- PASS: TestCtestAuthorization/nil_userInfo/update (0.00s)
    --- PASS: TestCtestAuthorization/auth_returns_error (0.00s)
        --- PASS: TestCtestAuthorization/auth_returns_error/create (0.00s)
        --- PASS: TestCtestAuthorization/auth_returns_error/update (0.00s)
    --- FAIL: TestCtestAuthorization/policyGetter_returns_nil_without_error (0.00s)
        --- FAIL: TestCtestAuthorization/policyGetter_returns_nil_without_error/create (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x108 pc=0x10228d5b0]

goroutine 98 [running]:
testing.tRunner.func1.2({0x1029cac20, 0x104442710})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x1029cac20?, 0x104442710?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.(*mutatingAdmissionPolicyBindingStrategy).authorize(0x1400016c4b0, {0x102e1a8e8, 0x14000195980}, 0x140007fedc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/authz.go:82 +0x370
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.(*mutatingAdmissionPolicyBindingStrategy).authorizeCreate(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/authz.go:38
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.(*mutatingAdmissionPolicyBindingStrategy).Validate(0x1400016c4b0, {0x102e1a8e8, 0x14000195980}, {0x102df9878?, 0x140007fedc0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/strategy.go:88 +0x1d8
k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding.TestCtestAuthorization.func29.1(0x140002421c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/ctest_authz_test.go:251 +0xbc
testing.tRunner(0x140002421c0, 0x140000c6b40)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 97
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding	1.663s
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/storage/ctest_storage_test.go:36]: get default configs: {test_fixture.json [valid policy bindings] spec [mutatingadmissionpolicybindings] 0x14000966000}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[mutatingadmissionpolicybindings]
[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[mutatingadmissionpolicybindings], int=1)[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [mutatingadmissionpolicybindings]
[DEBUG-CTEST 2026-02-09 18:47:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:47:03 load all fixtures failed: requested fixture keys not found in test_fixtures.json: mutatingadmissionpolicybindings
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingadmissionpolicybinding/storage	2.356s
	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingwebhookconfiguration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/admissionregistration/mutatingwebhookconfiguration/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/admissionregistration/resolver		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/admissionregistration/rest		coverage: 0.0% of statements
=== RUN   TestCtestAuthorization

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:127]: Number of test cases: 7
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:130]: test case: superuser
=== RUN   TestCtestAuthorization/superuser
=== RUN   TestCtestAuthorization/superuser/create
=== RUN   TestCtestAuthorization/superuser/update
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:130]: test case: authorized
=== RUN   TestCtestAuthorization/authorized
=== RUN   TestCtestAuthorization/authorized/create
=== RUN   TestCtestAuthorization/authorized/update
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:130]: test case: denied
=== RUN   TestCtestAuthorization/denied
=== RUN   TestCtestAuthorization/denied/create
=== RUN   TestCtestAuthorization/denied/update
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:130]: test case: param not found
=== RUN   TestCtestAuthorization/param_not_found
=== RUN   TestCtestAuthorization/param_not_found/create
=== RUN   TestCtestAuthorization/param_not_found/update
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:130]: test case: nil userInfo
=== RUN   TestCtestAuthorization/nil_userInfo
=== RUN   TestCtestAuthorization/nil_userInfo/create
=== RUN   TestCtestAuthorization/nil_userInfo/update
Running 5 th test case.
[DEBUG-CTEST 2026-02-09 18:47:11 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:130]: test case: nil policy object
=== RUN   TestCtestAuthorization/nil_policy_object
=== RUN   TestCtestAuthorization/nil_policy_object/create
--- FAIL: TestCtestAuthorization (0.01s)
    --- PASS: TestCtestAuthorization/superuser (0.00s)
        --- PASS: TestCtestAuthorization/superuser/create (0.00s)
        --- PASS: TestCtestAuthorization/superuser/update (0.00s)
    --- PASS: TestCtestAuthorization/authorized (0.00s)
        --- PASS: TestCtestAuthorization/authorized/create (0.00s)
        --- PASS: TestCtestAuthorization/authorized/update (0.00s)
    --- PASS: TestCtestAuthorization/denied (0.00s)
        --- PASS: TestCtestAuthorization/denied/create (0.00s)
        --- PASS: TestCtestAuthorization/denied/update (0.00s)
    --- PASS: TestCtestAuthorization/param_not_found (0.00s)
        --- PASS: TestCtestAuthorization/param_not_found/create (0.00s)
        --- PASS: TestCtestAuthorization/param_not_found/update (0.00s)
    --- PASS: TestCtestAuthorization/nil_userInfo (0.00s)
        --- PASS: TestCtestAuthorization/nil_userInfo/create (0.00s)
        --- PASS: TestCtestAuthorization/nil_userInfo/update (0.00s)
    --- FAIL: TestCtestAuthorization/nil_policy_object (0.00s)
        --- FAIL: TestCtestAuthorization/nil_policy_object/create (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x101bb7b40]

goroutine 119 [running]:
testing.tRunner.func1.2({0x10236ac60, 0x103dada40})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x10236ac60?, 0x103dada40?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/apis/admissionregistration/validation.validateValidatingAdmissionPolicy(0x0, {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, {0x0, 0x0, 0x0, ...}, ...})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/validation/validation.go:774 +0x80
k8s.io/kubernetes/pkg/apis/admissionregistration/validation.ValidateValidatingAdmissionPolicy(0x0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/apis/admissionregistration/validation/validation.go:770 +0x94
k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy.(*validatingAdmissionPolicyStrategy).Validate(0x140003cc180, {0x1027b5c38, 0x14000025860}, {0x102794fd8?, 0x0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/strategy.go:84 +0xc8
k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy.TestCtestAuthorization.func12.1(0x140008dc700)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/ctest_authz_test.go:135 +0x80
testing.tRunner(0x140008dc700, 0x140008cd6d0)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 118
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy	0.628s
=== RUN   TestCtestCreate

==================== CTEST OVERRIDE ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:47:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicy/storage/ctest_storage_test.go:37]: get default configs: {test_fixture.json [valid policy] spec [customresourcedefinitions] 0x14000505a40}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:47:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:47:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:47:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:47:13 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:47:13 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicy/storage	1.626s
=== RUN   TestCtestAuthorization

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicybinding/ctest_authz_test.go:237]: Number of test cases: 11
=== RUN   TestCtestAuthorization/superuser
=== RUN   TestCtestAuthorization/superuser/create
=== RUN   TestCtestAuthorization/superuser/update
=== RUN   TestCtestAuthorization/authorized
=== RUN   TestCtestAuthorization/authorized/create
=== RUN   TestCtestAuthorization/authorized/update
=== RUN   TestCtestAuthorization/denied
=== RUN   TestCtestAuthorization/denied/create
=== RUN   TestCtestAuthorization/denied/update
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef/create
=== RUN   TestCtestAuthorization/unable_to_parse_paramRef/update
=== RUN   TestCtestAuthorization/unable_to_resolve_param
=== RUN   TestCtestAuthorization/unable_to_resolve_param/create
=== RUN   TestCtestAuthorization/unable_to_resolve_param/update
=== RUN   TestCtestAuthorization/unable_to_get_policy
=== RUN   TestCtestAuthorization/unable_to_get_policy/create
=== RUN   TestCtestAuthorization/unable_to_get_policy/update
=== RUN   TestCtestAuthorization/nil_userInfo
=== RUN   TestCtestAuthorization/nil_userInfo/create
    ctest_authz_test.go:249: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
    ctest_authz_test.go:249: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
    ctest_authz_test.go:249: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
    ctest_authz_test.go:249: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
=== RUN   TestCtestAuthorization/nil_userInfo/update
    ctest_authz_test.go:282: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
    ctest_authz_test.go:282: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
    ctest_authz_test.go:282: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
    ctest_authz_test.go:282: expected error to contain: user must be set but got error: [spec.paramRef: Forbidden: cannot identify user to authorize read access to paramRef object]
=== RUN   TestCtestAuthorization/auth_returns_error
=== RUN   TestCtestAuthorization/auth_returns_error/create
=== RUN   TestCtestAuthorization/auth_returns_error/update
=== RUN   TestCtestAuthorization/policyGetter_returns_nil_without_error
=== RUN   TestCtestAuthorization/policyGetter_returns_nil_without_error/create
--- FAIL: TestCtestAuthorization (0.00s)
    --- PASS: TestCtestAuthorization/superuser (0.00s)
        --- PASS: TestCtestAuthorization/superuser/create (0.00s)
        --- PASS: TestCtestAuthorization/superuser/update (0.00s)
    --- PASS: TestCtestAuthorization/authorized (0.00s)
        --- PASS: TestCtestAuthorization/authorized/create (0.00s)
        --- PASS: TestCtestAuthorization/authorized/update (0.00s)
    --- PASS: TestCtestAuthorization/denied (0.00s)
        --- PASS: TestCtestAuthorization/denied/create (0.00s)
        --- PASS: TestCtestAuthorization/denied/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_parse_paramRef (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_parse_paramRef/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_parse_paramRef/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_resolve_param (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_resolve_param/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_resolve_param/update (0.00s)
    --- PASS: TestCtestAuthorization/unable_to_get_policy (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_get_policy/create (0.00s)
        --- PASS: TestCtestAuthorization/unable_to_get_policy/update (0.00s)
    --- FAIL: TestCtestAuthorization/nil_userInfo (0.00s)
        --- FAIL: TestCtestAuthorization/nil_userInfo/create (0.00s)
        --- FAIL: TestCtestAuthorization/nil_userInfo/update (0.00s)
    --- PASS: TestCtestAuthorization/auth_returns_error (0.00s)
        --- PASS: TestCtestAuthorization/auth_returns_error/create (0.00s)
        --- PASS: TestCtestAuthorization/auth_returns_error/update (0.00s)
    --- FAIL: TestCtestAuthorization/policyGetter_returns_nil_without_error (0.00s)
        --- FAIL: TestCtestAuthorization/policyGetter_returns_nil_without_error/create (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x108 pc=0x10639d690]

goroutine 117 [running]:
testing.tRunner.func1.2({0x106aa8860, 0x1084dda00})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x106aa8860?, 0x1084dda00?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding.(*validatingAdmissionPolicyBindingStrategy).authorize(0x1400041e410, {0x106eefaf8, 0x14000640870}, 0x1400063e3c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicybinding/authz.go:82 +0x370
k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding.(*validatingAdmissionPolicyBindingStrategy).authorizeCreate(...)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicybinding/authz.go:38
k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding.(*validatingAdmissionPolicyBindingStrategy).Validate(0x1400041e410, {0x106eefaf8, 0x14000640870}, {0x106ecf078?, 0x1400063e3c0})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicybinding/strategy.go:88 +0x1d8
k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding.TestCtestAuthorization.func32.1(0x1400061ddc0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/admissionregistration/validatingadmissionpolicybinding/ctest_authz_test.go:247 +0xbc
testing.tRunner(0x1400061ddc0, 0x1400062d320)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 116
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding	0.535s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingadmissionpolicybinding/storage	2.119s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingwebhookconfiguration	2.712s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/admissionregistration/validatingwebhookconfiguration/storage	1.030s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/apiserverinternal/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apiserverinternal/storageversion	2.892s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreate

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:47:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{},"Status":{"CommonEncodingVersion":null,"Conditions":[{"LastTransitionTime":null,"Message":"Common encoding version unset","ObservedGeneration":0,"Reason":"CommonEncodingVersionUnset","Status":"False","Type":"AllEncodingVersionsEqual"}],"StorageVersions":[{"APIServerID":"1","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"2","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"3","DecodableVersions":["v1","v2"],"EncodingVersion":"v2","ServedVersions":null}]},"name":"core.pods"})[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apiserverinternal/storageversion/storage/ctest_storage_test.go:33]: New Json Test Configs: 
    ctest_storage_test.go:35: no valid config objects generated
--- FAIL: TestCtestCreate (0.00s)
=== RUN   TestCtestUpdate

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:47:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{},"Status":{"CommonEncodingVersion":null,"Conditions":[{"LastTransitionTime":null,"Message":"Common encoding version unset","ObservedGeneration":0,"Reason":"CommonEncodingVersionUnset","Status":"False","Type":"AllEncodingVersionsEqual"}],"StorageVersions":[{"APIServerID":"1","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"2","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"3","DecodableVersions":["v1","v2"],"EncodingVersion":"v2","ServedVersions":null}]},"name":"core.pods"})[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apiserverinternal/storageversion/storage/ctest_storage_test.go:77]: New Json Test Configs: 
    ctest_storage_test.go:79: no config objects generated
--- FAIL: TestCtestUpdate (0.00s)
=== RUN   TestCtestGet

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:47:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{},"Status":{"CommonEncodingVersion":null,"Conditions":[{"LastTransitionTime":null,"Message":"Common encoding version unset","ObservedGeneration":0,"Reason":"CommonEncodingVersionUnset","Status":"False","Type":"AllEncodingVersionsEqual"}],"StorageVersions":[{"APIServerID":"1","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"2","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"3","DecodableVersions":["v1","v2"],"EncodingVersion":"v2","ServedVersions":null}]},"name":"core.pods"})[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apiserverinternal/storageversion/storage/ctest_storage_test.go:120]: New Json Test Configs: 
    ctest_storage_test.go:122: no config objects generated
--- FAIL: TestCtestGet (0.00s)
=== RUN   TestCtestList

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:47:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{},"Status":{"CommonEncodingVersion":null,"Conditions":[{"LastTransitionTime":null,"Message":"Common encoding version unset","ObservedGeneration":0,"Reason":"CommonEncodingVersionUnset","Status":"False","Type":"AllEncodingVersionsEqual"}],"StorageVersions":[{"APIServerID":"1","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"2","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"3","DecodableVersions":["v1","v2"],"EncodingVersion":"v2","ServedVersions":null}]},"name":"core.pods"})[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apiserverinternal/storageversion/storage/ctest_storage_test.go:146]: New Json Test Configs: 
    ctest_storage_test.go:148: no config objects generated
--- FAIL: TestCtestList (0.00s)
=== RUN   TestCtestDelete

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:27 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:47:27 
=== COMPLETE: Generated 1 results ===
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{},"Status":{"CommonEncodingVersion":null,"Conditions":[{"LastTransitionTime":null,"Message":"Common encoding version unset","ObservedGeneration":0,"Reason":"CommonEncodingVersionUnset","Status":"False","Type":"AllEncodingVersionsEqual"}],"StorageVersions":[{"APIServerID":"1","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"2","DecodableVersions":["v1","v2"],"EncodingVersion":"v1","ServedVersions":null},{"APIServerID":"3","DecodableVersions":["v1","v2"],"EncodingVersion":"v2","ServedVersions":null}]},"name":"core.pods"})[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apiserverinternal/storageversion/storage/ctest_storage_test.go:172]: New Json Test Configs: 
    ctest_storage_test.go:174: no config objects generated
--- FAIL: TestCtestDelete (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/apiserverinternal/storageversion/storage	0.978s
=== RUN   TestCtestStrategy_Validate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apps/controllerrevision/ctest_strategy_test.go:30]: get default configs: {test_fixture.json [default statefulset] spec [statefulsets] {{ } {abc  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {0 &LabelSelector{MatchLabels:map[string]string{foo: bar,},MatchExpressions:[]LabelSelectorRequirement{},} {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[foo:bar] map[] [] [] []} {[] [] [] [] Always <nil> <nil> ClusterFirst map[]  <nil>  <nil> []   <nil> <nil>  [] []  <nil> <nil> <nil> [] <nil> map[] <nil> [] <nil> [] [] <nil> <nil>}} []   { <nil>} <nil> 0 <nil> <nil>} {<nil> 0 0 0 0   <nil> [] 0}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[statefulsets]
[DEBUG-CTEST 2026-02-09 18:47:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[statefulsets], int=1)[DEBUG-CTEST 2026-02-09 18:47:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:47:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets]
[DEBUG-CTEST 2026-02-09 18:47:29 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:47:29 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets
FAIL	k8s.io/kubernetes/pkg/registry/apps/controllerrevision	2.903s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/controllerrevision/storage	3.373s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestSelectorImmutability

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apps/daemonset/ctest_strategy_test.go:29]: get default configs: {test_fixture.json [selector immutability] selector [daemonsets deployments statefulsets replicasets] {{ } {test-daemonset  test-namespace    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {&LabelSelector{MatchLabels:map[string]string{placeholder: value,},MatchExpressions:[]LabelSelectorRequirement{},} {{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[placeholder:value] map[] [] [] []} {[] [] [{ctr image [] []  [] [] [] {map[] map[] []} [] <nil> [] [] [] <nil> <nil> <nil> <nil> /dev/termination-log File IfNotPresent <nil> false false false}] [] Always 0x14000148b08 <nil> ClusterFirst map[]  <nil>  <nil> []   <nil> <nil>  [] []  <nil> <nil> <nil> [] <nil> map[] <nil> [] <nil> [] [] <nil> <nil>}} {OnDelete <nil>} 0 1 <nil>} {0 0 0 0 0 0 0 0 <nil> []}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[daemonsets deployments statefulsets replicasets]
[DEBUG-CTEST 2026-02-09 18:47:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[daemonsets deployments statefulsets replicasets], int=4)[DEBUG-CTEST 2026-02-09 18:47:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:47:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [daemonsets statefulsets replicasets]
[DEBUG-CTEST 2026-02-09 18:47:28 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:47:28 load all fixtures failed: requested fixture keys not found in test_fixtures.json: daemonsets, statefulsets, replicasets
FAIL	k8s.io/kubernetes/pkg/registry/apps/daemonset	1.873s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/daemonset/storage	1.410s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/deployment	2.395s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/deployment/storage	1.102s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas
[DEBUG-CTEST 2026-02-09 18:47:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apps/replicaset/ctest_strategy_test.go:24]: Running TestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas - including edge cases
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/should_not_allow_updates_when_feature_gate_is_disabled
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/should_allow_update_when_the_field_is_in_use_when_feature_gate_is_disabled
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/should_allow_updates_when_feature_gate_is_enabled
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_enabled,_both_nil_(no_change)
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_disabled,_negative_terminatingReplicas
    ctest_strategy_test.go:111: Unexpected error [status.terminatingReplicas: Invalid value: -5: must be greater than or equal to 0]
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_enabled,_max_int32_value
=== RUN   TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_enabled,_transition_from_nil_to_max_int32
--- FAIL: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas (0.00s)
    --- PASS: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/should_not_allow_updates_when_feature_gate_is_disabled (0.00s)
    --- PASS: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/should_allow_update_when_the_field_is_in_use_when_feature_gate_is_disabled (0.00s)
    --- PASS: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/should_allow_updates_when_feature_gate_is_enabled (0.00s)
    --- PASS: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_enabled,_both_nil_(no_change) (0.00s)
    --- FAIL: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_disabled,_negative_terminatingReplicas (0.00s)
    --- PASS: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_enabled,_max_int32_value (0.00s)
    --- PASS: TestCtestReplicaSetStatusStrategyWithDeploymentReplicaSetTerminatingReplicas/feature_enabled,_transition_from_nil_to_max_int32 (0.00s)
=== RUN   TestCtestSelectorImmutability
[DEBUG-CTEST 2026-02-09 18:47:36 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/apps/replicaset/ctest_strategy_test.go:118]: Running TestSelectorImmutability - including edge cases
    ctest_strategy_test.go:215: Unexpected error list, expected: [], actual: [spec.selector: Invalid value: {}: empty selector is invalid for deployment]
--- FAIL: TestCtestSelectorImmutability (0.00s)
FAIL
coverage: 21.2% of statements
FAIL	k8s.io/kubernetes/pkg/registry/apps/replicaset	0.711s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/replicaset/storage	0.952s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/apps/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/statefulset	0.387s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/apps/statefulset/storage	1.533s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/authentication/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authentication/selfsubjectreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authentication/tokenreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/localsubjectaccessreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/selfsubjectaccessreview		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/authorization/selfsubjectrulesreview		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/authorization/subjectaccessreview	0.768s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/authorization/util	0.561s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestValidationOptionsForHorizontalPodAutoscaler

==================== CTEST START ====================
Running 0 th test case: scale to zero disabled, no old hpa
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale to zero disabled, no old hpa 0x14000502e00 <nil> false 1 {false false} {true false}}
Running 1 th test case: scale to zero disabled, old hpa has minReplicas=1
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale to zero disabled, old hpa has minReplicas=1 0x14000502fc0 0x14000503180 false 1 {false true} {true false}}
Running 2 th test case: scale to zero disabled, old hpa has minReplicas=0
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale to zero disabled, old hpa has minReplicas=0 0x14000503340 0x14000503500 false 0 {false true} {true false}}
Running 3 th test case: scale to zero enabled
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale to zero enabled 0x140005036c0 <nil> true 0 {false false} {true false}}
Running 4 th test case: ReplicationController with the legacy API Version
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {ReplicationController with the legacy API Version 0x14000503880 <nil> false 1 {true false} {true false}}
Running 5 th test case: scale target ref api version changed
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale target ref api version changed 0x14000503a40 0x14000503c00 false 1 {false false} {true false}}
Running 6 th test case: scale target ref api version unchanged
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale target ref api version unchanged 0x14000503dc0 0x14000387180 false 1 {false true} {true false}}
Running 7 th test case: scale target ref api and Kind are changed to ReplicationController
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {scale target ref api and Kind are changed to ReplicationController 0x14000387340 0x14000387500 false 1 {true false} {true false}}
Running 8 th test case: Kind changed
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {Kind changed 0x140003876c0 0x14000387880 false 1 {false false} {true false}}
Running 9 th test case: no metrics
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {no metrics 0x14000387a40 <nil> false 1 {false false} {true false}}
Running 10 th test case: non-object metric
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {non-object metric 0x14000387c00 <nil> false 1 {false false} {true false}}
Running 11 th test case: new object metric with valid api version
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {new object metric with valid api version 0x14000387dc0 <nil> false 1 {false false} {true false}}
Running 12 th test case: old object metric with invalid api version
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {old object metric with invalid api version 0x14000582380 0x14000582540 false 1 {false true} {true true}}
Running 13 th test case: new object metric with invalid api version
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {new object metric with invalid api version 0x14000582700 <nil> false 1 {false false} {true false}}
Running 14 th test case: nil object Metrics
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {nil object Metrics 0x140005828c0 <nil> false 1 {false false} {true false}}
Running 15 th test case: negative minReplicas
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {negative minReplicas 0x14000582a80 <nil> false -1 {false false} {true false}}
    ctest_strategy_test.go:320: expected MinReplicasLowerBound -1, got 1
Running 16 th test case: empty ScaleTargetRef fields
[DEBUG-CTEST 2026-02-09 18:47:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/autoscaling/horizontalpodautoscaler/ctest_strategy_test.go:315]: Test case details: {empty ScaleTargetRef fields 0x14000582c40 <nil> false 1 {true false} {true false}}
    ctest_strategy_test.go:323: expected ScaleTargetRefValidationOptions {true false}, got {false false}

==================== CTEST END ======================
--- FAIL: TestCtestValidationOptionsForHorizontalPodAutoscaler (0.00s)
FAIL
coverage: 24.1% of statements
FAIL	k8s.io/kubernetes/pkg/registry/autoscaling/horizontalpodautoscaler	0.576s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/autoscaling/horizontalpodautoscaler/storage	1.046s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/autoscaling/rest		coverage: 0.0% of statements
=== RUN   TestCtestCronJobStrategy_WarningsOnCreate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[cronjobs]
[DEBUG-CTEST 2026-02-09 18:47:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[cronjobs], int=1)[DEBUG-CTEST 2026-02-09 18:47:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:47:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [cronjobs]
[DEBUG-CTEST 2026-02-09 18:47:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:47:47 load all fixtures failed: requested fixture keys not found in test_fixtures.json: cronjobs
FAIL	k8s.io/kubernetes/pkg/registry/batch/cronjob	1.422s
=== RUN   TestCtestCreate

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[cronjobs]
[DEBUG-CTEST 2026-02-09 18:47:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[cronjobs], int=1)[DEBUG-CTEST 2026-02-09 18:47:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:47:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [cronjobs]
[DEBUG-CTEST 2026-02-09 18:47:48 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:47:48 load all fixtures failed: requested fixture keys not found in test_fixtures.json: cronjobs
FAIL	k8s.io/kubernetes/pkg/registry/batch/cronjob/storage	2.012s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/batch/job	3.229s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreate
  W0209 18:47:50.044268   89687 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49817", ServerName: "localhost:49817", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:50.048309   89687 logging.go:55] [core] [Channel #12 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49817", ServerName: "localhost:49817", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:41: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:50.050-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:50.050-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  W0209 18:47:50.051245   89687 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49817", ServerName: "localhost:49817", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:50.051-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49817: use of closed network connection"}
  W0209 18:47:50.052261   89687 logging.go:55] [core] [Channel #2 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49817", ServerName: "localhost:49817", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49817: connect: connection refused"
  E0209 18:47:50.057811   89687 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
    logger.go:146: 2026-02-09T18:47:50.059-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49818: use of closed network connection"}
--- FAIL: TestCtestCreate (1.11s)
=== RUN   TestCtestUpdate
  W0209 18:47:50.896569   89687 logging.go:55] [core] [Channel #19 SubChannel #21]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49827", ServerName: "localhost:49827", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:50.899455   89687 logging.go:55] [core] [Channel #30 SubChannel #31]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49827", ServerName: "localhost:49827", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:78: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:50.901-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
  E0209 18:47:50.901337   89687 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
    logger.go:146: 2026-02-09T18:47:50.901-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  W0209 18:47:50.901417   89687 logging.go:55] [core] [Channel #35 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49827", ServerName: "localhost:49827", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:50.901-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49827: use of closed network connection"}
  W0209 18:47:50.902452   89687 logging.go:55] [core] [Channel #19 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49827", ServerName: "localhost:49827", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49827: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:50.910-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49828: use of closed network connection"}
--- FAIL: TestCtestUpdate (0.86s)
=== RUN   TestCtestDelete
  W0209 18:47:51.054764   89687 logging.go:55] [core] [Channel #2 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49817", ServerName: "localhost:49817", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49817: connect: connection refused"
  W0209 18:47:51.904485   89687 logging.go:55] [core] [Channel #19 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49827", ServerName: "localhost:49827", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49827: connect: connection refused"
  W0209 18:47:52.194274   89687 logging.go:55] [core] [Channel #37 SubChannel #40]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49837", ServerName: "localhost:49837", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49837: connect: connection refused"
  W0209 18:47:52.216147   89687 logging.go:55] [core] [Channel #48 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49837", ServerName: "localhost:49837", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:125: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:52.226-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:52.226-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
    logger.go:146: 2026-02-09T18:47:52.228-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49837: use of closed network connection"}
  W0209 18:47:52.228356   89687 logging.go:55] [core] [Channel #53 SubChannel #54]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49837", ServerName: "localhost:49837", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49837: connect: connection refused"
  W0209 18:47:52.228369   89687 logging.go:55] [core] [Channel #37 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49837", ServerName: "localhost:49837", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49837: connect: connection refused"
{"level":"warn","ts":"2026-02-09T18:47:52.226375-0600","logger":"etcd-client","caller":"v3/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0x140015d01e0/localhost:49837","method":"/etcdserverpb.Maintenance/Status","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
  E0209 18:47:52.230570   89687 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
    logger.go:146: 2026-02-09T18:47:52.238-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49838: use of closed network connection"}
--- FAIL: TestCtestDelete (1.32s)
=== RUN   TestCtestJobDeletion

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:152: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestJobDeletion (0.00s)
=== RUN   TestCtestGet
  W0209 18:47:52.556291   89687 logging.go:55] [core] [Channel #2 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49817", ServerName: "localhost:49817", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49817: connect: connection refused"
  W0209 18:47:53.230121   89687 logging.go:55] [core] [Channel #37 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49837", ServerName: "localhost:49837", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49837: connect: connection refused"
  W0209 18:47:53.354333   89687 logging.go:55] [core] [Channel #57 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49857", ServerName: "localhost:49857", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:53.356972   89687 logging.go:55] [core] [Channel #68 SubChannel #69]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49857", ServerName: "localhost:49857", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:293: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:53.357-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:53.357-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  E0209 18:47:53.357774   89687 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:53.357778   89687 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49857", ServerName: "localhost:49857", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:53.357-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49857: use of closed network connection"}
  W0209 18:47:53.358177   89687 logging.go:55] [core] [Channel #57 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49857", ServerName: "localhost:49857", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49857: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:53.502-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49858: use of closed network connection"}
  W0209 18:47:53.506821   89687 logging.go:55] [core] [Channel #19 SubChannel #22]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49827", ServerName: "localhost:49827", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49827: connect: connection refused"
--- FAIL: TestCtestGet (1.28s)
=== RUN   TestCtestList
  W0209 18:47:53.976670   89687 logging.go:55] [core] [Channel #75 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49873", ServerName: "localhost:49873", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:53.978510   89687 logging.go:55] [core] [Channel #86 SubChannel #87]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49873", ServerName: "localhost:49873", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:53 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:321: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:53.979-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:53.979-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  E0209 18:47:53.979184   89687 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
  W0209 18:47:53.979217   89687 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49873", ServerName: "localhost:49873", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
    logger.go:146: 2026-02-09T18:47:53.979-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49873: use of closed network connection"}
  W0209 18:47:53.979813   89687 logging.go:55] [core] [Channel #75 SubChannel #78]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49873", ServerName: "localhost:49873", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49873: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:53.988-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49874: use of closed network connection"}
--- FAIL: TestCtestList (0.47s)
=== RUN   TestCtestWatch
  W0209 18:47:54.358997   89687 logging.go:55] [core] [Channel #57 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49857", ServerName: "localhost:49857", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49857: connect: connection refused"
  W0209 18:47:54.527879   89687 logging.go:55] [core] [Channel #37 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49837", ServerName: "localhost:49837", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49837: connect: connection refused"
  W0209 18:47:54.550816   89687 logging.go:55] [core] [Channel #93 SubChannel #94]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49883", ServerName: "localhost:49883", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:47:54.552300   89687 logging.go:55] [core] [Channel #104 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49883", ServerName: "localhost:49883", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:349: GenerateEffectiveConfigReturnType failed: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
    logger.go:146: 2026-02-09T18:47:54.553-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:47:54.553-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
  W0209 18:47:54.553228   89687 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49883", ServerName: "localhost:49883", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  E0209 18:47:54.553231   89687 feature_support_checker.go:118] "Failed to check if RequestWatchProgress is supported by etcd after retrying" err="context canceled"
    logger.go:146: 2026-02-09T18:47:54.553-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49883: use of closed network connection"}
  W0209 18:47:54.554015   89687 logging.go:55] [core] [Channel #93 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49883", ServerName: "localhost:49883", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49883: connect: connection refused"
    logger.go:146: 2026-02-09T18:47:54.561-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49884: use of closed network connection"}
--- FAIL: TestCtestWatch (0.57s)
FAIL
coverage: 42.3% of statements
FAIL	k8s.io/kubernetes/pkg/registry/batch/job/storage	8.235s
	k8s.io/kubernetes/pkg/registry/batch/rest		coverage: 0.0% of statements
=== RUN   TestCtestDeclarativeValidateForDeclarative
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Required value", Field="status.conditions[0].type", Value="", Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: unmatched error:
        {Type="Duplicate value", Field="status.conditions[1].type", Value=Approved, Origin="", Detail=""}
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:113: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
--- FAIL: TestCtestDeclarativeValidateForDeclarative (0.01s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/failed_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+denied_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/empty_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/duplicate_approved_condition_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved_condition_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/approved+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+failed_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/denied+approved_conditions_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/unknown_condition_type_-_invalid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01 (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestDeclarativeValidateForDeclarative/no_conditions_-_valid#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
=== RUN   TestCtestValidateUpdateForDeclarative
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
    ctest_declarative_validation_test.go:258: expected an error matching:
        {Type="Invalid value", Field="status.conditions", Origin="zeroOrOneOf"}
    ctest_declarative_validation_test.go:258: unmatched error:
        {Type="Forbidden", Field="status.conditions", Value="", Origin="", Detail="updates may not remove a condition of type \"Approved\""}
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest
=== RUN   TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest
--- FAIL: TestCtestValidateUpdateForDeclarative (0.02s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/ (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=//certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_approved+denied_conditions_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_modify_condition_reason_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/add_unknown_condition_type_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/no_change_in_conditions_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_swapped_order_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- FAIL: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/remove_all_conditions_via_update_-_invalid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions_unchanged_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_change_spec_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/approval#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
    --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01 (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1,_Kind=CertificateSigningRequest (0.00s)
        --- PASS: TestCtestValidateUpdateForDeclarative/ratcheting:_approved+denied_conditions,_add_failed_condition_-_valid_subresource=/status#01/certificates.k8s.io/v1beta1,_Kind=CertificateSigningRequest (0.00s)
FAIL
coverage: 34.2% of statements
FAIL	k8s.io/kubernetes/pkg/registry/certificates/certificates	0.584s
=== RUN   TestCtestCountCSRDurationMetric

==================== CTEST START ====================
=== RUN   TestCtestCountCSRDurationMetric/cert_parse_failure
=== PAUSE TestCtestCountCSRDurationMetric/cert_parse_failure
=== RUN   TestCtestCountCSRDurationMetric/kube_signer_honors_duration_exactly
=== PAUSE TestCtestCountCSRDurationMetric/kube_signer_honors_duration_exactly
=== RUN   TestCtestCountCSRDurationMetric/signer_honors_duration_exactly
=== PAUSE TestCtestCountCSRDurationMetric/signer_honors_duration_exactly
=== RUN   TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_less
=== PAUSE TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_less
=== RUN   TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_more
=== PAUSE TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_more
=== RUN   TestCtestCountCSRDurationMetric/honors_duration_lower_bound
=== PAUSE TestCtestCountCSRDurationMetric/honors_duration_lower_bound
=== RUN   TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_lower_bound
=== PAUSE TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_lower_bound
=== RUN   TestCtestCountCSRDurationMetric/honors_duration_upper_bound
=== PAUSE TestCtestCountCSRDurationMetric/honors_duration_upper_bound
=== RUN   TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_upper_bound
=== PAUSE TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_upper_bound
=== RUN   TestCtestCountCSRDurationMetric/failed_update_is_ignored
=== PAUSE TestCtestCountCSRDurationMetric/failed_update_is_ignored
=== RUN   TestCtestCountCSRDurationMetric/dry_run_is_ignored
=== PAUSE TestCtestCountCSRDurationMetric/dry_run_is_ignored
=== RUN   TestCtestCountCSRDurationMetric/old_CSR_already_has_a_cert_so_it_is_ignored
=== PAUSE TestCtestCountCSRDurationMetric/old_CSR_already_has_a_cert_so_it_is_ignored
=== RUN   TestCtestCountCSRDurationMetric/CSRs_with_no_duration_are_ignored
=== PAUSE TestCtestCountCSRDurationMetric/CSRs_with_no_duration_are_ignored
=== RUN   TestCtestCountCSRDurationMetric/unissued_CSRs_are_ignored
=== PAUSE TestCtestCountCSRDurationMetric/unissued_CSRs_are_ignored
=== RUN   TestCtestCountCSRDurationMetric/invalid_data_-_nil_old_object
=== PAUSE TestCtestCountCSRDurationMetric/invalid_data_-_nil_old_object
=== RUN   TestCtestCountCSRDurationMetric/invalid_data_-_nil_new_object
=== PAUSE TestCtestCountCSRDurationMetric/invalid_data_-_nil_new_object
=== RUN   TestCtestCountCSRDurationMetric/invalid_data_-_junk_old_object
=== PAUSE TestCtestCountCSRDurationMetric/invalid_data_-_junk_old_object
=== RUN   TestCtestCountCSRDurationMetric/invalid_data_-_junk_new_object
=== PAUSE TestCtestCountCSRDurationMetric/invalid_data_-_junk_new_object
=== RUN   TestCtestCountCSRDurationMetric/empty_signer_name_in_old_Spec
=== PAUSE TestCtestCountCSRDurationMetric/empty_signer_name_in_old_Spec
=== RUN   TestCtestCountCSRDurationMetric/negative_expiration_seconds
=== PAUSE TestCtestCountCSRDurationMetric/negative_expiration_seconds
=== RUN   TestCtestCountCSRDurationMetric/extremely_large_expiration_seconds
=== PAUSE TestCtestCountCSRDurationMetric/extremely_large_expiration_seconds

==================== CTEST END ======================
=== CONT  TestCtestCountCSRDurationMetric/cert_parse_failure
=== CONT  TestCtestCountCSRDurationMetric/old_CSR_already_has_a_cert_so_it_is_ignored
=== CONT  TestCtestCountCSRDurationMetric/extremely_large_expiration_seconds
=== CONT  TestCtestCountCSRDurationMetric/negative_expiration_seconds
=== CONT  TestCtestCountCSRDurationMetric/empty_signer_name_in_old_Spec
=== CONT  TestCtestCountCSRDurationMetric/invalid_data_-_junk_new_object
=== CONT  TestCtestCountCSRDurationMetric/invalid_data_-_junk_old_object
=== CONT  TestCtestCountCSRDurationMetric/invalid_data_-_nil_new_object
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: cert parse failure
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: old CSR already has a cert so it is ignored
=== CONT  TestCtestCountCSRDurationMetric/invalid_data_-_nil_old_object
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: invalid data - nil old object
  E0209 18:47:54.223925   89783 metrics.go:130] "Unhandled Error" err="metrics recording failed to parse certificate for CSR : data does not contain any valid RSA or ECDSA certificates" logger="UnhandledError"
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: extremely large expiration seconds
=== CONT  TestCtestCountCSRDurationMetric/unissued_CSRs_are_ignored
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: unissued CSRs are ignored
=== CONT  TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_lower_bound
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: does not honor duration just outside of lower bound
=== CONT  TestCtestCountCSRDurationMetric/CSRs_with_no_duration_are_ignored
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: CSRs with no duration are ignored
=== CONT  TestCtestCountCSRDurationMetric/dry_run_is_ignored
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: dry run is ignored
=== CONT  TestCtestCountCSRDurationMetric/failed_update_is_ignored
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: failed update is ignored
=== CONT  TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_upper_bound
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: does not honor duration just outside of upper bound
=== CONT  TestCtestCountCSRDurationMetric/honors_duration_upper_bound
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: honors duration upper bound
=== CONT  TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_less
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: signer honors duration but just a little bit less
=== CONT  TestCtestCountCSRDurationMetric/honors_duration_lower_bound
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: honors duration lower bound
=== CONT  TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_more
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: signer honors duration but just a little bit more
=== CONT  TestCtestCountCSRDurationMetric/signer_honors_duration_exactly
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: signer honors duration exactly
=== CONT  TestCtestCountCSRDurationMetric/kube_signer_honors_duration_exactly
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: kube signer honors duration exactly
=== NAME  TestCtestCountCSRDurationMetric/extremely_large_expiration_seconds
    ctest_metrics_test.go:456: honored signer: want other, got 
    ctest_metrics_test.go:464: honored inc: want true, got false
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: negative expiration seconds
=== NAME  TestCtestCountCSRDurationMetric/negative_expiration_seconds
    ctest_metrics_test.go:460: requested inc: want false, got true
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: empty signer name in old Spec
=== NAME  TestCtestCountCSRDurationMetric/empty_signer_name_in_old_Spec
    ctest_metrics_test.go:460: requested inc: want false, got true
    ctest_metrics_test.go:464: honored inc: want false, got true
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: invalid data - junk new object
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: invalid data - junk old object
[DEBUG-CTEST 2026-02-09 18:47:54 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/certificates/storage/ctest_metrics_test.go:439]: Running test case: invalid data - nil new object
--- FAIL: TestCtestCountCSRDurationMetric (0.01s)
    --- PASS: TestCtestCountCSRDurationMetric/old_CSR_already_has_a_cert_so_it_is_ignored (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/invalid_data_-_nil_old_object (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/cert_parse_failure (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/unissued_CSRs_are_ignored (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/CSRs_with_no_duration_are_ignored (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/dry_run_is_ignored (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/failed_update_is_ignored (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_upper_bound (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/honors_duration_upper_bound (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_less (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/honors_duration_lower_bound (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/signer_honors_duration_but_just_a_little_bit_more (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/signer_honors_duration_exactly (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/kube_signer_honors_duration_exactly (0.00s)
    --- FAIL: TestCtestCountCSRDurationMetric/extremely_large_expiration_seconds (0.00s)
    --- FAIL: TestCtestCountCSRDurationMetric/negative_expiration_seconds (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/does_not_honor_duration_just_outside_of_lower_bound (0.00s)
    --- FAIL: TestCtestCountCSRDurationMetric/empty_signer_name_in_old_Spec (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/invalid_data_-_junk_new_object (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/invalid_data_-_junk_old_object (0.00s)
    --- PASS: TestCtestCountCSRDurationMetric/invalid_data_-_nil_new_object (0.00s)
FAIL
coverage: 63.1% of statements
FAIL	k8s.io/kubernetes/pkg/registry/certificates/certificates/storage	0.927s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/certificates/clustertrustbundle	0.396s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/certificates/clustertrustbundle/storage	1.795s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPrepareForCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:35]: matched config: {test_fixture.json [prepare for create] spec [pods] {foo.com/abc pod-1 pod-1-uid sa-1 sa-uid-1 node-1 node-uid-1 0x14000980390 [112 107 105 120] [112 114 111 111 102]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:47:55 Mode: 1
2026/02/09 18:47:55 Base JSON size: 256 bytes
2026/02/09 18:47:55 Number of external values: 1
2026/02/09 18:47:55   [KEEP] PodName: pod-1 (missing in external)
2026/02/09 18:47:55   [KEEP] ServiceAccountName: sa-1 (missing in external)
2026/02/09 18:47:55   [KEEP] ServiceAccountUID: sa-uid-1 (missing in external)
2026/02/09 18:47:55   [KEEP] NodeUID: node-uid-1 (missing in external)
2026/02/09 18:47:55   [KEEP] MaxExpirationSeconds: 86400 (missing in external)
2026/02/09 18:47:55   [KEEP] ProofOfPossession: cHJvb2Y= (missing in external)
2026/02/09 18:47:55   [KEEP] SignerName: foo.com/abc (missing in external)
2026/02/09 18:47:55   [KEEP] PodUID: pod-1-uid (missing in external)
2026/02/09 18:47:55   [KEEP] NodeName: node-1 (missing in external)
2026/02/09 18:47:55   [KEEP] PKIXPublicKey: cGtpeA== (missing in external)
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"MaxExpirationSeconds":86400,"NodeName":"node-1","NodeUID":"node-uid-1","PKIXPublicKey":"cGtpeA==","PodName":"pod-1","PodUID":"pod-1-uid","ProofOfPossession":"cHJvb2Y=","ServiceAccountName":"sa-1","ServiceAccountUID":"sa-uid-1","SignerName":"foo.com/abc"})[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:40]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:42]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestPrepareForCreate (0.00s)
=== RUN   TestCtestPrepareForUpdate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:89]: matched config: {test_fixture.json [prepare for update] spec [pods] {foo.com/abc pod-1 pod-1-uid sa-1 sa-uid-1 node-1 node-uid-1 0x14000980f80 [112 107 105 120] [112 114 111 111 102]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:47:55 Mode: 1
2026/02/09 18:47:55 Base JSON size: 256 bytes
2026/02/09 18:47:55 Number of external values: 1
2026/02/09 18:47:55   [KEEP] ServiceAccountName: sa-1 (missing in external)
2026/02/09 18:47:55   [KEEP] ServiceAccountUID: sa-uid-1 (missing in external)
2026/02/09 18:47:55   [KEEP] NodeName: node-1 (missing in external)
2026/02/09 18:47:55   [KEEP] NodeUID: node-uid-1 (missing in external)
2026/02/09 18:47:55   [KEEP] ProofOfPossession: cHJvb2Y= (missing in external)
2026/02/09 18:47:55   [KEEP] PodUID: pod-1-uid (missing in external)
2026/02/09 18:47:55   [KEEP] MaxExpirationSeconds: 86400 (missing in external)
2026/02/09 18:47:55   [KEEP] PKIXPublicKey: cGtpeA== (missing in external)
2026/02/09 18:47:55   [KEEP] SignerName: foo.com/abc (missing in external)
2026/02/09 18:47:55   [KEEP] PodName: pod-1 (missing in external)
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"MaxExpirationSeconds":86400,"NodeName":"node-1","NodeUID":"node-uid-1","PKIXPublicKey":"cGtpeA==","PodName":"pod-1","PodUID":"pod-1-uid","ProofOfPossession":"cHJvb2Y=","ServiceAccountName":"sa-1","ServiceAccountUID":"sa-uid-1","SignerName":"foo.com/abc"})[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:94]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:96]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestPrepareForUpdate (0.00s)
=== RUN   TestCtestStatusPrepareForUpdate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:170]: matched config: {test_fixture.json [status prepare for update] spec [pods] {foo.com/abc pod-1 pod-1-uid sa-1 sa-uid-1 node-1 node-uid-1 0x14000981710 [112 107 105 120] [112 114 111 111 102]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:47:55 Mode: 1
2026/02/09 18:47:55 Base JSON size: 256 bytes
2026/02/09 18:47:55 Number of external values: 1
2026/02/09 18:47:55   [KEEP] SignerName: foo.com/abc (missing in external)
2026/02/09 18:47:55   [KEEP] PodName: pod-1 (missing in external)
2026/02/09 18:47:55   [KEEP] ServiceAccountName: sa-1 (missing in external)
2026/02/09 18:47:55   [KEEP] ServiceAccountUID: sa-uid-1 (missing in external)
2026/02/09 18:47:55   [KEEP] NodeUID: node-uid-1 (missing in external)
2026/02/09 18:47:55   [KEEP] MaxExpirationSeconds: 86400 (missing in external)
2026/02/09 18:47:55   [KEEP] PKIXPublicKey: cGtpeA== (missing in external)
2026/02/09 18:47:55   [KEEP] ProofOfPossession: cHJvb2Y= (missing in external)
2026/02/09 18:47:55   [KEEP] PodUID: pod-1-uid (missing in external)
2026/02/09 18:47:55   [KEEP] NodeName: node-1 (missing in external)
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:47:55 [DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"MaxExpirationSeconds":86400,"NodeName":"node-1","NodeUID":"node-uid-1","PKIXPublicKey":"cGtpeA==","PodName":"pod-1","PodUID":"pod-1-uid","ProofOfPossession":"cHJvb2Y=","ServiceAccountName":"sa-1","ServiceAccountUID":"sa-uid-1","SignerName":"foo.com/abc"})[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:175]: New Json Test Configs: 
[DEBUG-CTEST 2026-02-09 18:47:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/certificates/podcertificaterequest/ctest_strategy_test.go:177]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestStatusPrepareForUpdate (0.00s)
=== RUN   TestCtestStatusValidateUpdate

==================== CTEST START ====================
=== RUN   TestCtestStatusValidateUpdate/authorized_caller,_unchanged_spec
=== RUN   TestCtestStatusValidateUpdate/unauthorized_caller
=== RUN   TestCtestStatusValidateUpdate/authorized_caller,_signerName_changed_(should_be_allowed)

==================== CTEST END ======================
--- PASS: TestCtestStatusValidateUpdate (0.00s)
    --- PASS: TestCtestStatusValidateUpdate/authorized_caller,_unchanged_spec (0.00s)
    --- PASS: TestCtestStatusValidateUpdate/unauthorized_caller (0.00s)
    --- PASS: TestCtestStatusValidateUpdate/authorized_caller,_signerName_changed_(should_be_allowed) (0.00s)
PASS
coverage: 35.1% of statements
ok  	k8s.io/kubernetes/pkg/registry/certificates/podcertificaterequest	1.282s	coverage: 35.1% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/certificates/podcertificaterequest/storage	0.745s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/certificates/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/lease		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/lease/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/leasecandidate		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/leasecandidate/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/coordination/rest		coverage: 0.0% of statements
=== RUN   TestCtestValidate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/componentstatus/ctest_validator_test.go:40]: Number of test cases: 8
Running 0 th test case.
{unknown  0x140005100a0 unknown  true <nil>}
Running 1 th test case.
{failure  <nil> failure  false <nil>}
Running 2 th test case.
{success foo <nil> failure foo true 0x106523c10}
Running 3 th test case.
{success foo <nil> success foo false <nil>}
Running 4 th test case.
{success bar <nil> success bar false 0x106523c10}
Running 5 th test case.
{success baz <nil> success baz false <nil>}
Running 6 th test case.
{unknown any <nil> unknown any false <nil>}
Running 7 th test case.
{failure data 0x140005100b0 failure data true <nil>}
    ctest_validator_test.go:62: expected data, got 
    ctest_validator_test.go:65: expected failure, got unknown

==================== CTEST END ======================
--- FAIL: TestCtestValidate (0.00s)
FAIL
coverage: 21.5% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/componentstatus	0.710s
=== RUN   TestCtestConfigMapStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/configmap/ctest_strategy_test.go:27]: valid config item: {test_fixture.json [valid configmap data] data [configmaps] map[foo:bar]}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:48:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:03 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/registry/core/configmap	1.224s
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/configmap/storage/ctest_storage_test.go:35]: get default configs: {test_fixture.json [valid configmap for create] metadata [configmaps] 0x14000294b40}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[configmaps]
[DEBUG-CTEST 2026-02-09 18:48:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[configmaps], int=1)[DEBUG-CTEST 2026-02-09 18:48:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [configmaps]
[DEBUG-CTEST 2026-02-09 18:48:06 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:06 load all fixtures failed: requested fixture keys not found in test_fixtures.json: configmaps
FAIL	k8s.io/kubernetes/pkg/registry/core/configmap/storage	1.806s
=== RUN   TestCtestEndpointsWarning

==================== CTEST START ====================
Running 0 th test case.
=== RUN   TestCtestEndpointsWarning/empty_Endpoints
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:78]: get default configs: {test_fixture.json [empty endpoints] subsets [services] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} []}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "subsets" in requested fixtures
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:05 Mode: 1
2026/02/09 18:48:05 Base JSON size: 16 bytes
2026/02/09 18:48:05 Number of external values: 0
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Subsets":null})[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:86]: Skipping test execution. No new configurations generated.
Running 1 th test case.
=== RUN   TestCtestEndpointsWarning/valid_Endpoints
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:78]: get default configs: {test_fixture.json [valid endpoints] subsets [services] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[{1.2.3.4  <nil> <nil>} {fd00::1234  <nil> <nil>}] [] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "subsets" in requested fixtures
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:05 Mode: 1
2026/02/09 18:48:05 Base JSON size: 198 bytes
2026/02/09 18:48:05 Number of external values: 0
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Subsets":[{"Addresses":[{"Hostname":"","IP":"1.2.3.4","NodeName":null,"TargetRef":null},{"Hostname":"","IP":"fd00::1234","NodeName":null,"TargetRef":null}],"NotReadyAddresses":null,"Ports":null}]})[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:86]: Skipping test execution. No new configurations generated.
Running 2 th test case.
=== RUN   TestCtestEndpointsWarning/bad_Endpoints
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:78]: get default configs: {test_fixture.json [bad endpoints] subsets [services] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[{fd00::1234  <nil> <nil>} {01.02.03.04  <nil> <nil>}] [] []} {[{::ffff:1.2.3.4  <nil> <nil>}] [] []} {[{1.2.3.4  <nil> <nil>}] [{::ffff:1.2.3.4  <nil> <nil>}] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "subsets" in requested fixtures
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:05 Mode: 1
2026/02/09 18:48:05 Base JSON size: 513 bytes
2026/02/09 18:48:05 Number of external values: 0
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Subsets":[{"Addresses":[{"Hostname":"","IP":"fd00::1234","NodeName":null,"TargetRef":null},{"Hostname":"","IP":"01.02.03.04","NodeName":null,"TargetRef":null}],"NotReadyAddresses":null,"Ports":null},{"Addresses":[{"Hostname":"","IP":"::ffff:1.2.3.4","NodeName":null,"TargetRef":null}],"NotReadyAddresses":null,"Ports":null},{"Addresses":[{"Hostname":"","IP":"1.2.3.4","NodeName":null,"TargetRef":null}],"NotReadyAddresses":[{"Hostname":"","IP":"::ffff:1.2.3.4","NodeName":null,"TargetRef":null}],"Ports":null}]})[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:86]: Skipping test execution. No new configurations generated.
Running 3 th test case.
=== RUN   TestCtestEndpointsWarning/bad_Endpoints_ignored_because_of_label
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:78]: get default configs: {test_fixture.json [bad endpoints ignored because of label] subsets [services] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[endpoints.kubernetes.io/managed-by:endpoint-controller] map[] [] [] []} [{[{fd00::1234  <nil> <nil>} {01.02.03.04  <nil> <nil>}] [] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "subsets" in requested fixtures
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:05 Mode: 1
2026/02/09 18:48:05 Base JSON size: 272 bytes
2026/02/09 18:48:05 Number of external values: 0
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Subsets":[{"Addresses":[{"Hostname":"","IP":"fd00::1234","NodeName":null,"TargetRef":null},{"Hostname":"","IP":"01.02.03.04","NodeName":null,"TargetRef":null}],"NotReadyAddresses":null,"Ports":null}],"labels":{"endpoints.kubernetes.io/managed-by":"endpoint-controller"}})[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:86]: Skipping test execution. No new configurations generated.
Running 4 th test case.
=== RUN   TestCtestEndpointsWarning/empty_IP_address
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:78]: get default configs: {test_fixture.json [empty ip address] subsets [services] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[{  <nil> <nil>}] [] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "subsets" in requested fixtures
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:05 Mode: 1
2026/02/09 18:48:05 Base JSON size: 124 bytes
2026/02/09 18:48:05 Number of external values: 0
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Subsets":[{"Addresses":[{"Hostname":"","IP":"","NodeName":null,"TargetRef":null}],"NotReadyAddresses":null,"Ports":null}]})[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:86]: Skipping test execution. No new configurations generated.
Running 5 th test case.
=== RUN   TestCtestEndpointsWarning/invalid_IPv4_segment
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:78]: get default configs: {test_fixture.json [invalid ipv4 segment] subsets [services] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[{300.300.300.300  <nil> <nil>}] [] []}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "subsets" in requested fixtures
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:05 Mode: 1
2026/02/09 18:48:05 Base JSON size: 139 bytes
2026/02/09 18:48:05 Number of external values: 0
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:05 [DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Subsets":[{"Addresses":[{"Hostname":"","IP":"300.300.300.300","NodeName":null,"TargetRef":null}],"NotReadyAddresses":null,"Ports":null}]})[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/ctest_strategy_test.go:86]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestEndpointsWarning (0.01s)
    --- PASS: TestCtestEndpointsWarning/empty_Endpoints (0.00s)
    --- PASS: TestCtestEndpointsWarning/valid_Endpoints (0.00s)
    --- PASS: TestCtestEndpointsWarning/bad_Endpoints (0.00s)
    --- PASS: TestCtestEndpointsWarning/bad_Endpoints_ignored_because_of_label (0.00s)
    --- PASS: TestCtestEndpointsWarning/empty_IP_address (0.00s)
    --- PASS: TestCtestEndpointsWarning/invalid_IPv4_segment (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/endpoint	0.613s	coverage: 0.0% of statements
=== RUN   TestCtestCreate
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/storage/ctest_storage_test.go:46]: Start TestCreate
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/endpoint/storage/ctest_storage_test.go:54]: Matched config: {test_fixture.json [valid endpoints] subsets [endpoints] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [{[{1.2.3.4  <nil> <nil>}] [] [{ 80 TCP <nil>}]}]}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[endpoints]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[endpoints], int=1)[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [endpoints]
[DEBUG-CTEST 2026-02-09 18:48:05 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:05 load all fixtures failed: requested fixture keys not found in test_fixtures.json: endpoints
FAIL	k8s.io/kubernetes/pkg/registry/core/endpoint/storage	1.227s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/event	2.405s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/event/storage/ctest_storage_test.go:61]: get default configs: {test_fixture.json [valid event] event [] 0x140007cf908}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "event" in requested fixtures
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:07 Mode: 1
2026/02/09 18:48:07 Base JSON size: 514 bytes
2026/02/09 18:48:07 Number of external values: 0
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Action":"Do","Count":0,"EventTime":"2017-09-19T13:49:16.000000Z","FirstTimestamp":null,"InvolvedObject":{"APIVersion":"","FieldPath":"","Kind":"","Name":"bar","Namespace":"default","ResourceVersion":"","UID":""},"LastTimestamp":null,"Message":"","Reason":"forTesting","Related":null,"ReportingController":"test-controller","ReportingInstance":"test-node","Series":{"Count":2,"LastObservedTime":"2017-09-19T13:49:16.000000Z"},"Source":{"Component":"","Host":""},"Type":"Normal","name":"foo","namespace":"default"})[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/event/storage/ctest_storage_test.go:70]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestCreate (0.00s)
=== RUN   TestCtestUpdate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/event/storage/ctest_storage_test.go:111]: get default configs: {test_fixture.json [valid event] event [] 0x1400072c288}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "event" in requested fixtures
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:07 Mode: 1
2026/02/09 18:48:07 Base JSON size: 514 bytes
2026/02/09 18:48:07 Number of external values: 0
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Action":"Do","Count":0,"EventTime":"2017-09-19T13:49:16.000000Z","FirstTimestamp":null,"InvolvedObject":{"APIVersion":"","FieldPath":"","Kind":"","Name":"bar","Namespace":"default","ResourceVersion":"","UID":""},"LastTimestamp":null,"Message":"","Reason":"forTesting","Related":null,"ReportingController":"test-controller","ReportingInstance":"test-node","Series":{"Count":2,"LastObservedTime":"2017-09-19T13:49:16.000000Z"},"Source":{"Component":"","Host":""},"Type":"Normal","name":"foo","namespace":"default"})[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/event/storage/ctest_storage_test.go:120]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestUpdate (0.00s)
=== RUN   TestCtestDelete

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/event/storage/ctest_storage_test.go:166]: get default configs: {test_fixture.json [valid event] event [] 0x1400072c788}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "event" in requested fixtures
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:48:07 Mode: 1
2026/02/09 18:48:07 Base JSON size: 514 bytes
2026/02/09 18:48:07 Number of external values: 0
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:48:07 [DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Action":"Do","Count":0,"EventTime":"2017-09-19T13:49:16.000000Z","FirstTimestamp":null,"InvolvedObject":{"APIVersion":"","FieldPath":"","Kind":"","Name":"bar","Namespace":"default","ResourceVersion":"","UID":""},"LastTimestamp":null,"Message":"","Reason":"forTesting","Related":null,"ReportingController":"test-controller","ReportingInstance":"test-node","Series":{"Count":2,"LastObservedTime":"2017-09-19T13:49:16.000000Z"},"Source":{"Component":"","Host":""},"Type":"Normal","name":"foo","namespace":"default"})[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:07 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/event/storage/ctest_storage_test.go:175]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestDelete (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/event/storage	3.221s	coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/core/limitrange		coverage: 0.0% of statements
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/limitrange/storage/ctest_storage_test.go:33]: get default configs: {test_fixture.json [default limitrange] limits [limitranges] 0x1400080d8c0}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[limitranges]
[DEBUG-CTEST 2026-02-09 18:48:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[limitranges], int=1)[DEBUG-CTEST 2026-02-09 18:48:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [limitranges]
[DEBUG-CTEST 2026-02-09 18:48:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:17 load all fixtures failed: requested fixture keys not found in test_fixtures.json: limitranges
FAIL	k8s.io/kubernetes/pkg/registry/core/limitrange/storage	2.501s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/namespace	0.489s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/namespace/storage	1.009s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestDropFields
--- PASS: TestCtestDropFields (0.00s)
=== RUN   TestCtestValidateUpdate
    ctest_strategy_test.go:213: 2: Unexpected non-error
--- FAIL: TestCtestValidateUpdate (0.00s)
=== RUN   TestCtestValidate
--- PASS: TestCtestValidate (0.00s)
=== RUN   TestCtestWarningOnUpdateAndCreate
E0209 18:48:17.792534   90033 ip.go:214] "GetWarningsForCIDR called on value that was not validated with IsValidCIDRForLegacyField" err="invalid CIDR address: " field="spec.podCIDRs[0]" value=""
    ctest_strategy_test.go:349: 11: Unexpected warnings count: []
    ctest_strategy_test.go:350: v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)} vs v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}
E0209 18:48:17.792722   90033 ip.go:214] "GetWarningsForCIDR called on value that was not validated with IsValidCIDRForLegacyField" err="invalid CIDR address: " field="spec.podCIDRs[0]" value=""
    ctest_strategy_test.go:357: 11: Unexpected warnings count: []
    ctest_strategy_test.go:358: v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)} vs v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}
--- FAIL: TestCtestWarningOnUpdateAndCreate (0.00s)
FAIL
coverage: 33.3% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/node	2.992s
	k8s.io/kubernetes/pkg/registry/core/node/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/node/storage	3.816s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/persistentvolume	1.636s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/persistentvolume/storage	1.319s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPrepareForCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/persistentvolumeclaim/ctest_strategy_test.go:30]: get default configs: {test_fixture.json [PVC DataSource preparation] spec [persistentvolumeclaims] {[] nil {map[] map[]}  <nil> <nil> <nil> <nil> <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:48:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumeclaims], int=1)[DEBUG-CTEST 2026-02-09 18:48:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:48:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:24 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumeclaims
FAIL	k8s.io/kubernetes/pkg/registry/core/persistentvolumeclaim	0.850s
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/persistentvolumeclaim/storage/ctest_storage_test.go:28]: get default configs: {test_fixture.json [valid pvc spec] spec [persistentvolumeclaims] {[ReadWriteOnce] nil {map[] map[storage:{{10 9} {<nil>} 10G DecimalSI}]}  <nil> <nil> <nil> <nil> <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:48:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumeclaims], int=1)[DEBUG-CTEST 2026-02-09 18:48:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:48:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:25 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumeclaims
FAIL	k8s.io/kubernetes/pkg/registry/core/persistentvolumeclaim/storage	1.877s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/pod	0.895s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/pod/rest	2.121s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/pod/storage	0.592s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/podtemplate/ctest_strategy_test.go:27]: get default configs: {test_fixture.json [default pod template restartPolicy] restartPolicy [pods deployments statefulsets daemonsets replicasets podtemplates] OnFailure}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets podtemplates]
[DEBUG-CTEST 2026-02-09 18:48:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets podtemplates], int=6)[DEBUG-CTEST 2026-02-09 18:48:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets podtemplates]
[DEBUG-CTEST 2026-02-09 18:48:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:27 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets, podtemplates
FAIL	k8s.io/kubernetes/pkg/registry/core/podtemplate	1.086s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/podtemplate/storage	1.872s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/registry/core/rangeallocation	[no test files]
=== RUN   TestCtestDeclarativeValidateForDeclarative

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[replicationcontrollers]
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[replicationcontrollers], int=1)[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [replicationcontrollers]
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:38 load all fixtures failed: requested fixture keys not found in test_fixtures.json: replicationcontrollers
FAIL	k8s.io/kubernetes/pkg/registry/core/replicationcontroller	0.836s
=== RUN   TestCtestValidateScaleForDeclarative

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/replicationcontroller/storage/ctest_scale_declarative_validation_test.go:33]: get default configs: {test_fixture.json [default scale] replicas [pods] {{ } {foo  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {1} {0 }}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "replicas" in requested fixtures
2026/02/09 18:48:38 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:48:38 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"Replicas":1},"Status":{"Replicas":0,"Selector":""},"name":"foo","namespace":"default"})[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:48:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/replicationcontroller/storage/ctest_scale_declarative_validation_test.go:42]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateScaleForDeclarative (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/replicationcontroller/storage	1.314s	coverage: 0.0% of statements
=== RUN   TestCtestResourceQuotaStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/resourcequota/ctest_strategy_test.go:29]: get default configs: {test_fixture.json [default resourcequota] status [resourcequotas] 0x14000318500}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourcequotas]
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourcequotas], int=1)[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourcequotas]
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:39 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourcequotas
FAIL	k8s.io/kubernetes/pkg/registry/core/resourcequota	2.315s
=== RUN   TestCtestCreate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:34: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestCreate (0.00s)
=== RUN   TestCtestCreateSetsFields

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:73: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestCreateSetsFields (0.00s)
=== RUN   TestCtestDelete

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:118: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestDelete (0.00s)
=== RUN   TestCtestGet

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:148: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestGet (0.00s)
=== RUN   TestCtestList

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:178: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestList (0.00s)
=== RUN   TestCtestWatch

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:208: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestWatch (0.00s)
=== RUN   TestCtestUpdateStatus

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:250: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestUpdateStatus (0.00s)
=== RUN   TestCtestShortNames

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:321: failed to generate config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestShortNames (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/resourcequota/storage	1.856s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/rest	2.777s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/secret	3.661s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/secret/storage	3.268s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service	0.489s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service/allocator	0.658s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service/allocator/storage	1.312s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtest_isNotContained
=== RUN   TestCtest_isNotContained/ipv4_not_contained_nor_overlapping
=== RUN   TestCtest_isNotContained/ipv4_not_contained_but_contains
=== RUN   TestCtest_isNotContained/ipv4_not_contained_but_matches_existing_one
=== RUN   TestCtest_isNotContained/ipv4_contained_but_matches_existing_one
=== RUN   TestCtest_isNotContained/empty_prefix_list_=>_not_contained
=== RUN   TestCtest_isNotContained/prefix_/0_with_empty_list
=== RUN   TestCtest_isNotContained/prefix_exactly_matches_larger_existing_prefix
=== RUN   TestCtest_isNotContained/ipv6_not_contained
=== RUN   TestCtest_isNotContained/ipv6_contained_by_larger_prefix
=== RUN   TestCtest_isNotContained/duplicate_prefixes_in_list
--- PASS: TestCtest_isNotContained (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_not_contained_nor_overlapping (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_not_contained_but_contains (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_not_contained_but_matches_existing_one (0.00s)
    --- PASS: TestCtest_isNotContained/ipv4_contained_but_matches_existing_one (0.00s)
    --- PASS: TestCtest_isNotContained/empty_prefix_list_=>_not_contained (0.00s)
    --- PASS: TestCtest_isNotContained/prefix_/0_with_empty_list (0.00s)
    --- PASS: TestCtest_isNotContained/prefix_exactly_matches_larger_existing_prefix (0.00s)
    --- PASS: TestCtest_isNotContained/ipv6_not_contained (0.00s)
    --- PASS: TestCtest_isNotContained/ipv6_contained_by_larger_prefix (0.00s)
    --- PASS: TestCtest_isNotContained/duplicate_prefixes_in_list (0.00s)
PASS
coverage: 1.1% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service/ipallocator	1.727s	coverage: 1.1% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service/ipallocator/controller	0.830s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestStore

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/service/ipallocator/storage/ctest_storage_test.go:70]: get default configs: {test_fixture.json [default range allocation] range [services serviceaccounts replicasets daemonsets deployments statefulsets pods configmaps secrets namespaces persistentvolumes persistentvolumeclaims resourcequotas limitranges jobs cronjobs ingressws networkpolicys roles rolebindings clusterroles clusterrolebindings storageclasses customresourcedefinitions] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} 192.168.1.0/24 []}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services serviceaccounts replicasets daemonsets deployments statefulsets pods configmaps secrets namespaces persistentvolumes persistentvolumeclaims resourcequotas limitranges jobs cronjobs ingressws networkpolicys roles rolebindings clusterroles clusterrolebindings storageclasses customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services serviceaccounts replicasets daemonsets deployments statefulsets pods configmaps secrets namespaces persistentvolumes persistentvolumeclaims resourcequotas limitranges jobs cronjobs ingressws networkpolicys roles rolebindings clusterroles clusterrolebindings storageclasses customresourcedefinitions], int=24)[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [serviceaccounts replicasets daemonsets statefulsets configmaps persistentvolumes persistentvolumeclaims resourcequotas limitranges cronjobs ingressws networkpolicys rolebindings clusterroles clusterrolebindings storageclasses customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:50 load all fixtures failed: requested fixture keys not found in test_fixtures.json: serviceaccounts, replicasets, daemonsets, statefulsets, configmaps, persistentvolumes, persistentvolumeclaims, resourcequotas, limitranges, cronjobs, ingressws, networkpolicys, rolebindings, clusterroles, clusterrolebindings, storageclasses, customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/registry/core/service/ipallocator/storage	1.274s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service/portallocator	1.967s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestRepairWithExisting

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/service/portallocator/controller/ctest_repair_test.go:32]: get default configs: {test_fixture.json [service nodeport repair with existing services] spec [services] [{{ } {one  one    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[{  <nil> 0 {0 0 } 111}] map[]  []  []   []   0 false nil [] <nil> <nil> <nil> <nil> <nil>} {{[]} []}} {{ } {two  two    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[{  <nil> 0 {0 0 } 122} {  <nil> 0 {0 0 } 133}] map[]  []  []   []   0 false nil [] <nil> <nil> <nil> <nil> <nil>} {{[]} []}} {{ } {three  three    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[{  <nil> 0 {0 0 } 201}] map[]  []  []   []   0 false nil [] <nil> <nil> <nil> <nil> <nil>} {{[]} []}} {{ } {four  four    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[{  <nil> 0 {0 0 } 0}] map[]  []  []   []   0 false nil [] <nil> <nil> <nil> <nil> <nil>} {{[]} []}} {{ } {five  five    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[{  <nil> 0 {0 0 } 111}] map[]  []  []   []   0 false nil [] <nil> <nil> <nil> <nil> <nil>} {{[]} []}} {{ } {six  six    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {[] map[]  []  []   []   144 false nil [] <nil> <nil> <nil> <nil> <nil>} {{[]} []}}]}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:48:50 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:48:50 Base JSON size: 820 bytes
2026/02/09 18:48:50 Number of external values: 23
2026/02/09 18:48:50 BASE DATA (type: []interface {}):
[
  {
    "metadata": {
      "name": "one",
      "namespace": "one"
    },
    "spec": {
      "ports": [
        {
          "nodePort": 111,
          "port": 0,
          "targetPort": 0
        }
      ]
    },
    "status": {
      "loadBalancer": {}
    }
  },
  {
    "metadata": {
      "name": "two",
      "namespace": "two"
    },
    "spec": {
      "ports": [
        {
          "nodePort": 122,
          "port": 0,
          "targetPort": 0
        },
        {
          "nodePort": 133,
          "port": 0,
          "targetPort": 0
        }
      ]
    },
    "status": {
      "loadBalancer": {}
    }
  },
  {
    "metadata": {
      "name": "three",
      "namespace": "three"
    },
    "spec": {
      "ports": [
        {
          "nodePort": 201,
          "port": 0,
          "targetPort": 0
        }
      ]
    },
    "status": {
      "loadBalancer": {}
    }
  },
  {
    "metadata": {
      "name": "four",
      "namespace": "four"
    },
    "spec": {
      "ports": [
        {
          "port": 0,
          "targetPort": 0
        }
      ]
    },
    "status": {
      "loadBalancer": {}
    }
  },
  {
    "metadata": {
      "name": "five",
      "namespace": "five"
    },
    "spec": {
      "ports": [
        {
          "nodePort": 111,
          "port": 0,
          "targetPort": 0
        }
      ]
    },
    "status": {
      "loadBalancer": {}
    }
  },
  {
    "metadata": {
      "name": "six",
      "namespace": "six"
    },
    "spec": {
      "healthCheckNodePort": 144
    },
    "status": {
      "loadBalancer": {}
    }
  }
]
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50   [UNION REPLACE] : entire array replaced
2026/02/09 18:48:50 
=== UNION COMPLETE ===
2026/02/09 18:48:50 Generated 23 result(s)
[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=[{"metadata":{"name":"one","namespace":"one"},"spec":{"ports":[{"nodePort":111,"port":0,"targetPort":0}]},"status":{"loadBalancer":{}}},{"metadata":{"name":"two","namespace":"two"},"spec":{"ports":[{"nodePort":122,"port":0,"targetPort":0},{"nodePort":133,"port":0,"targetPort":0}]},"status":{"loadBalancer":{}}},{"metadata":{"name":"three","namespace":"three"},"spec":{"ports":[{"nodePort":201,"port":0,"targetPort":0}]},"status":{"loadBalancer":{}}},{"metadata":{"name":"four","namespace":"four"},"spec":{"ports":[{"port":0,"targetPort":0}]},"status":{"loadBalancer":{}}},{"metadata":{"name":"five","namespace":"five"},"spec":{"ports":[{"nodePort":111,"port":0,"targetPort":0}]},"status":{"loadBalancer":{}}},{"metadata":{"name":"six","namespace":"six"},"spec":{"healthCheckNodePort":144},"status":{"loadBalancer":{}}}])[DEBUG-CTEST 2026-02-09 18:48:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/service/portallocator/controller/ctest_repair_test.go:37]: Failed to generate configs: %v k8s json unmarshal into []v1.Service failed: json: cannot unmarshal object into Go value of type []v1.Service
    ctest_repair_test.go:38: config generation error: k8s json unmarshal into []v1.Service failed: json: cannot unmarshal object into Go value of type []v1.Service
--- FAIL: TestCtestRepairWithExisting (0.02s)
=== RUN   TestCtestCollectServiceNodePorts

==================== CTEST START ====================
=== RUN   TestCtestCollectServiceNodePorts/no_duplicated_nodePorts
=== RUN   TestCtestCollectServiceNodePorts/duplicated_nodePort_with_TCP_protocol
=== RUN   TestCtestCollectServiceNodePorts/duplicated_nodePort_with_UDP_protocol
=== RUN   TestCtestCollectServiceNodePorts/duplicated_nodePort_with_different_protocol
=== RUN   TestCtestCollectServiceNodePorts/no_duplicated_port(with_health_check_port)
=== RUN   TestCtestCollectServiceNodePorts/nodePort_has_different_protocol_with_duplicated_health_check_port
=== RUN   TestCtestCollectServiceNodePorts/nodePort_has_same_protocol_as_duplicated_health_check_port
=== RUN   TestCtestCollectServiceNodePorts/negative_nodePort
=== RUN   TestCtestCollectServiceNodePorts/zero_nodePort
    ctest_repair_test.go:284: Invalid result
        expected: [0]
        got: []
=== RUN   TestCtestCollectServiceNodePorts/max_int_nodePort
=== RUN   TestCtestCollectServiceNodePorts/duplicate_health_check_port_without_protocol_conflict

==================== CTEST END ======================
--- FAIL: TestCtestCollectServiceNodePorts (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/no_duplicated_nodePorts (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/duplicated_nodePort_with_TCP_protocol (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/duplicated_nodePort_with_UDP_protocol (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/duplicated_nodePort_with_different_protocol (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/no_duplicated_port(with_health_check_port) (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/nodePort_has_different_protocol_with_duplicated_health_check_port (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/nodePort_has_same_protocol_as_duplicated_health_check_port (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/negative_nodePort (0.00s)
    --- FAIL: TestCtestCollectServiceNodePorts/zero_nodePort (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/max_int_nodePort (0.00s)
    --- PASS: TestCtestCollectServiceNodePorts/duplicate_health_check_port_without_protocol_conflict (0.00s)
FAIL
coverage: 20.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/core/service/portallocator/controller	1.848s
=== RUN   TestCtestAllocate
W0209 18:48:54.504702   90495 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49902", ServerName: "localhost:49902", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
W0209 18:48:54.506615   90495 logging.go:55] [core] [Channel #12 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49902", ServerName: "localhost:49902", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
W0209 18:48:54.507534   90495 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49902", ServerName: "localhost:49902", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
=== RUN   TestCtestAllocate/Allocate_base_port
=== RUN   TestCtestAllocate/Allocate_maximum_from_the_port_range
=== RUN   TestCtestAllocate/Allocate_invalid_port:_base_port_minus_1
=== RUN   TestCtestAllocate/Allocate_invalid_port:_maximum_port_from_the_port_range_plus_1
=== RUN   TestCtestAllocate/Allocate_invalid_port:_negative_value
=== RUN   TestCtestAllocate/Allocate_invalid_port:_zero
=== RUN   TestCtestAllocate/Allocate_invalid_port:_extremely_large_value
=== NAME  TestCtestAllocate
    logger.go:146: 2026-02-09T18:48:54.510-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "mux: server closed"}
    logger.go:146: 2026-02-09T18:48:54.510-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "http: Server closed"}
    logger.go:146: 2026-02-09T18:48:54.510-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49902: use of closed network connection"}
W0209 18:48:54.510565   90495 logging.go:55] [core] [Channel #12 SubChannel #14]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49902", ServerName: "localhost:49902", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49902: connect: connection refused"
W0209 18:48:54.510585   90495 logging.go:55] [core] [Channel #2 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49902", ServerName: "localhost:49902", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp [::1]:49902: connect: connection refused"
    logger.go:146: 2026-02-09T18:48:54.519-0600	ERROR	etcd-server	setting up serving from embedded etcd failed.	{"error": "accept tcp 127.0.0.1:49903: use of closed network connection"}
--- PASS: TestCtestAllocate (0.68s)
    --- PASS: TestCtestAllocate/Allocate_base_port (0.00s)
    --- PASS: TestCtestAllocate/Allocate_maximum_from_the_port_range (0.00s)
    --- PASS: TestCtestAllocate/Allocate_invalid_port:_base_port_minus_1 (0.00s)
    --- PASS: TestCtestAllocate/Allocate_invalid_port:_maximum_port_from_the_port_range_plus_1 (0.00s)
    --- PASS: TestCtestAllocate/Allocate_invalid_port:_negative_value (0.00s)
    --- PASS: TestCtestAllocate/Allocate_invalid_port:_zero (0.00s)
    --- PASS: TestCtestAllocate/Allocate_invalid_port:_extremely_large_value (0.00s)
PASS
coverage: [no statements]
ok  	k8s.io/kubernetes/pkg/registry/core/service/portallocator/storage	1.270s	coverage: [no statements]
=== RUN   TestCtest_metaTransaction
=== PAUSE TestCtest_metaTransaction
=== CONT  TestCtest_metaTransaction
=== RUN   TestCtest_metaTransaction/commit_and_revert_match
=== PAUSE TestCtest_metaTransaction/commit_and_revert_match
=== RUN   TestCtest_metaTransaction/commit_and_revert_match_multiple_times
=== PAUSE TestCtest_metaTransaction/commit_and_revert_match_multiple_times
=== RUN   TestCtest_metaTransaction/missing_revert
=== PAUSE TestCtest_metaTransaction/missing_revert
=== RUN   TestCtest_metaTransaction/missing_commit
=== PAUSE TestCtest_metaTransaction/missing_commit
=== RUN   TestCtest_metaTransaction/commit_and_revert_match_multiple_but_different_order
=== PAUSE TestCtest_metaTransaction/commit_and_revert_match_multiple_but_different_order
=== RUN   TestCtest_metaTransaction/empty_transaction
=== PAUSE TestCtest_metaTransaction/empty_transaction
=== RUN   TestCtest_metaTransaction/single_callback_with_only_commit
=== PAUSE TestCtest_metaTransaction/single_callback_with_only_commit
=== RUN   TestCtest_metaTransaction/single_callback_with_only_revert
=== PAUSE TestCtest_metaTransaction/single_callback_with_only_revert
=== CONT  TestCtest_metaTransaction/commit_and_revert_match
=== CONT  TestCtest_metaTransaction/single_callback_with_only_revert
=== CONT  TestCtest_metaTransaction/single_callback_with_only_commit
=== CONT  TestCtest_metaTransaction/empty_transaction
=== CONT  TestCtest_metaTransaction/commit_and_revert_match_multiple_but_different_order
=== CONT  TestCtest_metaTransaction/missing_commit
=== CONT  TestCtest_metaTransaction/missing_revert
=== CONT  TestCtest_metaTransaction/commit_and_revert_match_multiple_times
--- PASS: TestCtest_metaTransaction (0.00s)
    --- PASS: TestCtest_metaTransaction/commit_and_revert_match (0.00s)
    --- PASS: TestCtest_metaTransaction/single_callback_with_only_revert (0.00s)
    --- PASS: TestCtest_metaTransaction/single_callback_with_only_commit (0.00s)
    --- PASS: TestCtest_metaTransaction/empty_transaction (0.00s)
    --- PASS: TestCtest_metaTransaction/commit_and_revert_match_multiple_but_different_order (0.00s)
    --- PASS: TestCtest_metaTransaction/missing_commit (0.00s)
    --- PASS: TestCtest_metaTransaction/missing_revert (0.00s)
    --- PASS: TestCtest_metaTransaction/commit_and_revert_match_multiple_times (0.00s)
PASS
coverage: 1.2% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/service/storage	1.131s	coverage: 1.2% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/core/serviceaccount	0.583s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:48:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/core/serviceaccount/storage/ctest_storage_test.go:67]: get default configs: {test_fixture.json [default serviceaccount] secrets [serviceaccounts] 0x14000151040}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:48:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[serviceaccounts]
[DEBUG-CTEST 2026-02-09 18:48:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[serviceaccounts], int=1)[DEBUG-CTEST 2026-02-09 18:48:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:48:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [serviceaccounts]
[DEBUG-CTEST 2026-02-09 18:48:55 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:48:55 load all fixtures failed: requested fixture keys not found in test_fixtures.json: serviceaccounts
FAIL	k8s.io/kubernetes/pkg/registry/core/serviceaccount/storage	1.195s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/discovery/endpointslice	0.553s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/discovery/endpointslice/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/discovery/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/events/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/flowcontrol/ensurer	0.676s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/flowcontrol/flowschema		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/flowcontrol/flowschema/storage		coverage: 0.0% of statements
=== RUN   TestCtestPriorityLevelConfigurationValidation
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_create,_zero_value,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_create,_unset,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_create,_non-zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_zero_value,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_zero_value_without_annotation,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_non-zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_update,_zero_value,_existing_has_non-zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_update,_zero_value,_existing_has_zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_update,_non-zero_value,_existing_has_zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_update,_zero_value,_existing_has_non-zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_update,_zero_value,_existing_has_zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_update,_non-zero_value,_existing_has_zero,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1,_create,_max_int32_value,_no_error_expected
=== RUN   TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_max_int32_value,_no_error_expected
--- PASS: TestCtestPriorityLevelConfigurationValidation (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_create,_zero_value,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_create,_unset,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_create,_non-zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_zero_value,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_zero_value_without_annotation,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_non-zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_update,_zero_value,_existing_has_non-zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_update,_zero_value,_existing_has_zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_update,_non-zero_value,_existing_has_zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_update,_zero_value,_existing_has_non-zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_update,_zero_value,_existing_has_zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_update,_non-zero_value,_existing_has_zero,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1,_create,_max_int32_value,_no_error_expected (0.00s)
    --- PASS: TestCtestPriorityLevelConfigurationValidation/v1beta3,_create,_max_int32_value,_no_error_expected (0.00s)
PASS
coverage: 21.2% of statements
ok  	k8s.io/kubernetes/pkg/registry/flowcontrol/prioritylevelconfiguration	0.336s	coverage: 21.2% of statements
	k8s.io/kubernetes/pkg/registry/flowcontrol/prioritylevelconfiguration/storage		coverage: 0.0% of statements
=== RUN   TestCtestContextFromChannelAndMaxWaitDurationWithChannelClosed
[DEBUG-CTEST 2026-02-09 18:49:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/flowcontrol/rest/ctest_storage_flowcontrol_test.go:36]: Running test case: original behavior - channel closed after start
[DEBUG-CTEST 2026-02-09 18:49:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/flowcontrol/rest/ctest_storage_flowcontrol_test.go:36]: Running test case: edge: zero max wait duration
[DEBUG-CTEST 2026-02-09 18:49:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/flowcontrol/rest/ctest_storage_flowcontrol_test.go:36]: Running test case: edge: negative max wait duration
--- PASS: TestCtestContextFromChannelAndMaxWaitDurationWithChannelClosed (0.00s)
=== RUN   TestCtestContextFromChannelAndMaxWaitDurationWithMaxWaitElapsed
[DEBUG-CTEST 2026-02-09 18:49:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/flowcontrol/rest/ctest_storage_flowcontrol_test.go:87]: Running test case: original behavior - max wait 100ms
[DEBUG-CTEST 2026-02-09 18:49:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/flowcontrol/rest/ctest_storage_flowcontrol_test.go:87]: Running test case: edge: zero max wait duration
[DEBUG-CTEST 2026-02-09 18:49:01 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/flowcontrol/rest/ctest_storage_flowcontrol_test.go:87]: Running test case: edge: negative max wait duration
--- PASS: TestCtestContextFromChannelAndMaxWaitDurationWithMaxWaitElapsed (0.10s)
PASS
coverage: 6.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/flowcontrol/rest	0.958s	coverage: 6.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/networking/ingress	0.330s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCreate
  W0209 18:49:02.510241   90644 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49921", ServerName: "localhost:49921", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:49:02.511258   90644 logging.go:55] [core] [Channel #12 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49921", ServerName: "localhost:49921", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/networking/ingress/storage/ctest_storage_test.go:68]: matched config item: {test_fixture.json [valid ingress] spec [ingresses services pods deployments statefulsets daemonsets replicasets] {<nil> 0x14000c09c90 [{[foo.bar.com *.bar.com] foosecret}] [{foo.bar.com {0x140009dfc98}}]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[ingresses services pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[ingresses services pods deployments statefulsets daemonsets replicasets], int=7)[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:02 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/registry/networking/ingress/storage	1.709s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/networking/ingressclass	2.042s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/networking/ingressclass/storage		coverage: 0.0% of statements
=== RUN   TestCtestIPAddressStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/networking/ipaddress/ctest_strategy_test.go:27]: get default configs: {test_fixture.json [default ipaddress spec] parentRef [customresourcedefinitions] {0x14000236e00}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:49:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:49:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:49:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:03 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/registry/networking/ipaddress	2.535s
	k8s.io/kubernetes/pkg/registry/networking/ipaddress/storage		coverage: 0.0% of statements
=== RUN   TestCtestNetworkPolicyStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/networking/networkpolicy/ctest_strategy_test.go:28]: get default configs: {test_fixture.json [base networkpolicy] spec [networkpolicies] {{ } {foo  bar    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {{map[a:b] []} [] [] []}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[networkpolicies]
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[networkpolicies], int=1)[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [networkpolicies]
[DEBUG-CTEST 2026-02-09 18:49:02 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:02 load all fixtures failed: requested fixture keys not found in test_fixtures.json: networkpolicies
FAIL	k8s.io/kubernetes/pkg/registry/networking/networkpolicy	1.792s
=== RUN   TestCtestCreate

==================== CTEST OVERRIDE ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:49:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/networking/networkpolicy/storage/ctest_storage_test.go:36]: matched config: {test_fixture.json [valid networkpolicy] spec [networkpolicys] 0x14000aec000}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[networkpolicys]
[DEBUG-CTEST 2026-02-09 18:49:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[networkpolicys], int=1)[DEBUG-CTEST 2026-02-09 18:49:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [networkpolicys]
[DEBUG-CTEST 2026-02-09 18:49:09 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:09 load all fixtures failed: requested fixture keys not found in test_fixtures.json: networkpolicys
FAIL	k8s.io/kubernetes/pkg/registry/networking/networkpolicy/storage	0.664s
	k8s.io/kubernetes/pkg/registry/networking/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/networking/servicecidr	0.893s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/networking/servicecidr/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/node/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/node/runtimeclass		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/node/runtimeclass/storage		coverage: 0.0% of statements
=== RUN   TestCtestPodDisruptionBudgetStatusValidationByApiVersion

==================== CTEST OVERRIDE ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/policy/poddisruptionbudget/ctest_strategy_test.go:28]: get default configs: {test_fixture.json [PDB status validation by api version] spec [pods] {<nil> &LabelSelector{MatchLabels:map[string]string{a: b,},MatchExpressions:[]LabelSelectorRequirement{},} 2 <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:49:10 [DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:49:10 Mode: 1
2026/02/09 18:49:10 Base JSON size: 111 bytes
2026/02/09 18:49:10 Number of external values: 1
2026/02/09 18:49:10   [KEEP] MinAvailable: <nil> (missing in external)
2026/02/09 18:49:10   [KEEP] Selector: map[matchLabels:map[a:b]] (missing in external)
2026/02/09 18:49:10   [KEEP] MaxUnavailable: 2 (missing in external)
2026/02/09 18:49:10   [KEEP] UnhealthyPodEvictionPolicy: <nil> (missing in external)
2026/02/09 18:49:10 [DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:49:10 [DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"MaxUnavailable":2,"MinAvailable":null,"Selector":{"matchLabels":{"a":"b"}},"UnhealthyPodEvictionPolicy":null})[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:49:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/policy/poddisruptionbudget/ctest_strategy_test.go:37]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestPodDisruptionBudgetStatusValidationByApiVersion (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/policy/poddisruptionbudget	1.416s	coverage: 0.0% of statements
=== RUN   TestCtestCreate
  W0209 18:49:16.226437   90773 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49933", ServerName: "localhost:49933", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:49:16.227830   90773 logging.go:55] [core] [Channel #12 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49933", ServerName: "localhost:49933", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/policy/poddisruptionbudget/storage/ctest_storage_test.go:65]: get default configs: {test_fixture.json [default pdb] spec [pods deployments statefulsets daemonsets replicasets podbdisruptionbudgets] {{ } {foo  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[a:b] map[] [] [] []} {7 &LabelSelector{MatchLabels:map[string]string{a: b,},MatchExpressions:[]LabelSelectorRequirement{},} <nil> <nil>} {0 map[] 0 0 0 0 []}}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets podbdisruptionbudgets]
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets podbdisruptionbudgets], int=6)[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets podbdisruptionbudgets]
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:16 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets, podbdisruptionbudgets
FAIL	k8s.io/kubernetes/pkg/registry/policy/poddisruptionbudget/storage	1.678s
	k8s.io/kubernetes/pkg/registry/policy/rest		coverage: 0.0% of statements
=== RUN   TestCtestIsOnlyMutatingGCFields

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "pod" in requested fixtures
2026/02/09 18:49:15 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:49:15 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"ActiveDeadlineSeconds":null,"Affinity":null,"AutomountServiceAccountToken":null,"Containers":null,"DNSConfig":null,"DNSPolicy":"","EnableServiceLinks":null,"EphemeralContainers":null,"HostAliases":null,"Hostname":"","HostnameOverride":null,"ImagePullSecrets":null,"InitContainers":null,"NodeName":"","NodeSelector":null,"OS":null,"Overhead":null,"PreemptionPolicy":null,"Priority":null,"PriorityClassName":"","ReadinessGates":null,"ResourceClaims":null,"Resources":null,"RestartPolicy":"","RuntimeClassName":null,"SchedulerName":"","SchedulingGates":null,"SecurityContext":null,"ServiceAccountName":"","SetHostnameAsFQDN":null,"Subdomain":"","TerminationGracePeriodSeconds":null,"Tolerations":null,"TopologySpreadConstraints":null,"Volumes":null},"Status":{"Conditions":null,"ContainerStatuses":null,"EphemeralContainerStatuses":null,"ExtendedResourceClaimStatus":null,"HostIP":"","HostIPs":null,"InitContainerStatuses":null,"Message":"","NominatedNodeName":"","ObservedGeneration":0,"Phase":"","PodIPs":null,"QOSClass":"","Reason":"","Resize":"","ResourceClaimStatuses":null,"StartTime":null}})[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:49:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/rbac/ctest_helpers_test.go:88]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestIsOnlyMutatingGCFields (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/rbac	0.569s	coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrole		coverage: 0.0% of statements
=== RUN   TestCtestEscalation

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/rbac/clusterrole/policybased/ctest_storage_test.go:33]: get default configs: {test_fixture.json [default clusterrole] rules [clusterroles] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} [PolicyRule{APIGroups:[""], Resources:["pods"], Verbs:["get"]}] <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[clusterroles]
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[clusterroles], int=1)[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [clusterroles]
[DEBUG-CTEST 2026-02-09 18:49:16 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:16 load all fixtures failed: requested fixture keys not found in test_fixtures.json: clusterroles
FAIL	k8s.io/kubernetes/pkg/registry/rbac/clusterrole/policybased	1.591s
	k8s.io/kubernetes/pkg/registry/rbac/clusterrole/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrolebinding		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrolebinding/policybased		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/clusterrolebinding/storage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/rbac/rest	0.670s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/rbac/role		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/rbac/role/policybased	1.258s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/rbac/role/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/rolebinding		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/rolebinding/policybased		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/rbac/rolebinding/storage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/rbac/validation	0.968s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/registrytest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/resource		coverage: 0.0% of statements
=== RUN   TestCtestStrategyCreate

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:49:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:49:33 
=== COMPLETE: Generated 89 results ===
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"Config":null,"ExtendedResourceName":null,"Selectors":null},"generation":1,"name":"valid-class"})[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/resource/deviceclass/ctest_strategy_test.go:35]: Failed to generate simple config: <nil>
--- SKIP: TestCtestStrategyCreate (0.01s)
=== RUN   TestCtestStrategyUpdate

==================== CTEST START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:49:33 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:49:33 
=== COMPLETE: Generated 89 results ===
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"Config":null,"ExtendedResourceName":null,"Selectors":null},"generation":1,"name":"valid-class"})[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/resource/deviceclass/ctest_strategy_test.go:148]: Failed to generate simple config: <nil>
--- SKIP: TestCtestStrategyUpdate (0.01s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/resource/deviceclass	1.106s	coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/resource/deviceclass/storage	0.647s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestDeviceTaintRuleStrategy
--- PASS: TestCtestDeviceTaintRuleStrategy (0.00s)
=== RUN   TestCtestDeviceTaintRuleStrategyCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/resource/devicetaintrule/ctest_strategy_test.go:37]: Matched config item: {test_fixture.json [valid-patch] spec.taint [deviceTaintRules] {{ } {valid-patch      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {<nil> {example.com/tainted  NoExecute <nil>}}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[deviceTaintRules]
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[deviceTaintRules], int=1)[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [deviceTaintRules]
[DEBUG-CTEST 2026-02-09 18:49:33 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:33 load all fixtures failed: requested fixture keys not found in test_fixtures.json: deviceTaintRules
FAIL	k8s.io/kubernetes/pkg/registry/resource/devicetaintrule	1.585s
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/resource/devicetaintrule/storage/ctest_storage_test.go:55]: get default configs: {test_fixture.json [default device taint rule] spec [customresourcedefinitions] {{ } {default-name      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {<nil> {example.com/taint  NoExecute 2026-02-09 18:49:34 -0600 CST}}}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:49:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[customresourcedefinitions], int=1)[DEBUG-CTEST 2026-02-09 18:49:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [customresourcedefinitions]
[DEBUG-CTEST 2026-02-09 18:49:34 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:34 load all fixtures failed: requested fixture keys not found in test_fixtures.json: customresourcedefinitions
FAIL	k8s.io/kubernetes/pkg/registry/resource/devicetaintrule/storage	2.465s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/resource/resourceclaim	2.898s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/resource/resourceclaim/storage	1.894s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestClaimTemplateStrategyCreate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceclaimtemplates]
[DEBUG-CTEST 2026-02-09 18:49:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceclaimtemplates], int=1)[DEBUG-CTEST 2026-02-09 18:49:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceclaimtemplates]
[DEBUG-CTEST 2026-02-09 18:49:35 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:35 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceclaimtemplates
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceclaimtemplate	3.440s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/resource/resourceclaimtemplate/storage	4.080s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestResourceSliceStrategyCreate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceslices]
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceslices], int=1)[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceslices]
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:42 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceslices
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceslice	0.615s
=== RUN   TestCtestCreate

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/resource/resourceslice/storage/ctest_storage_test.go:53]: get default configs: {test_fixture.json [default resourceslice spec] spec [resourceslices] {cdi.example.com {worker-1 0 1} 0x1400031f1a0 <nil> <nil> [] <nil> []}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[resourceslices]
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[resourceslices], int=1)[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [resourceslices]
[DEBUG-CTEST 2026-02-09 18:49:42 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:42 load all fixtures failed: requested fixture keys not found in test_fixtures.json: resourceslices
FAIL	k8s.io/kubernetes/pkg/registry/resource/resourceslice/storage	0.908s
	k8s.io/kubernetes/pkg/registry/resource/rest		coverage: 0.0% of statements
=== RUN   TestCtestPriorityClassStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/scheduling/priorityclass/ctest_strategy_test.go:26]: Matched config: {test_fixture.json [valid priority class] value [] 0x140003b23c0}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:49:44 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:49:44 
=== COMPLETE: Generated 89 results ===
[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Description":"","GlobalDefault":false,"PreemptionPolicy":null,"Value":10,"name":"valid-class"})[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:49:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/scheduling/priorityclass/ctest_strategy_test.go:34]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestPriorityClassStrategy (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/scheduling/priorityclass	2.853s	coverage: 0.0% of statements
=== RUN   TestCtestCreate

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:288]: entry must be a struct or pointer to struct
    ctest_storage_test.go:27: failed to generate effective config: entry must be a struct or pointer to struct; got ctestglobals.HardcodedConfig
--- FAIL: TestCtestCreate (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/registry/scheduling/priorityclass/storage	1.544s
	k8s.io/kubernetes/pkg/registry/scheduling/rest		coverage: 0.0% of statements
=== RUN   TestCtestCSIDriverStrategy

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/csidriver/ctest_strategy_test.go:30]: get default configs: {test_fixture.json [valid csidriver for strategy] spec [csidrivers] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {0x140006c0128 <nil> 0x140006c0128 [] 0x140006c0128 [] 0x140006c0128 0x140006c0128 <nil>}}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[csidrivers]
[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[csidrivers], int=1)[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [csidrivers]
[DEBUG-CTEST 2026-02-09 18:49:43 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:43 load all fixtures failed: requested fixture keys not found in test_fixtures.json: csidrivers
FAIL	k8s.io/kubernetes/pkg/registry/storage/csidriver	2.019s
=== RUN   TestCtestCreate
  W0209 18:49:49.419630   91183 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49970", ServerName: "localhost:49970", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"
  W0209 18:49:49.422492   91183 logging.go:55] [core] [Channel #12 SubChannel #13]grpc: addrConn.createTransport failed to connect to {Addr: "localhost:49970", ServerName: "localhost:49970", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp: lookup localhost: operation was canceled"

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:49 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[csidrivers]
[DEBUG-CTEST 2026-02-09 18:49:49 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[csidrivers], int=1)[DEBUG-CTEST 2026-02-09 18:49:49 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:49 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [csidrivers]
[DEBUG-CTEST 2026-02-09 18:49:49 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:49 load all fixtures failed: requested fixture keys not found in test_fixtures.json: csidrivers
FAIL	k8s.io/kubernetes/pkg/registry/storage/csidriver/storage	1.704s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/storage/csinode	0.641s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/storage/csinode/storage	0.561s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestCSIStorageCapacityStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/csistoragecapacity/ctest_strategy_test.go:62]: get default configs: {test_fixture.json [default csi storage capacity] capacity [csistoragecapacities] {{ } {default  default   1 0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} &LabelSelector{MatchLabels:map[string]string{},MatchExpressions:[]LabelSelectorRequirement{LabelSelectorRequirement{Key:node,Operator:In,Values:[node1],},},} bar 1Mi <nil>}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[csistoragecapacities]
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[csistoragecapacities], int=1)[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [csistoragecapacities]
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:50 load all fixtures failed: requested fixture keys not found in test_fixtures.json: csistoragecapacities
FAIL	k8s.io/kubernetes/pkg/registry/storage/csistoragecapacity	1.379s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/storage/csistoragecapacity/storage	0.871s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/registry/storage/rest		coverage: 0.0% of statements
=== RUN   TestCtestStorageClassStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/storageclass/ctest_strategy_test.go:27]: matched config: {test_fixture.json [valid storage class] reclaimPolicy [storageclasses] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} kubernetes.io/aws-ebs map[foo:bar] 0x140000123c0 [] <nil> 0x140000123d0 []}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[storageclasses]
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[storageclasses], int=1)[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [storageclasses]
[DEBUG-CTEST 2026-02-09 18:49:50 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:50 load all fixtures failed: requested fixture keys not found in test_fixtures.json: storageclasses
FAIL	k8s.io/kubernetes/pkg/registry/storage/storageclass	1.821s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/storage/storageclass/storage	1.051s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestVolumeAttachmentStrategy

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:49:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/volumeattachment/ctest_strategy_test.go:36]: get default configs: {test_fixture.json [valid-attachment spec] spec [volumeattachments] {valid-attacher {0x1400070a020 <nil>} valid-node}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[volumeattachments]
[DEBUG-CTEST 2026-02-09 18:49:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[volumeattachments], int=1)[DEBUG-CTEST 2026-02-09 18:49:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [volumeattachments]
[DEBUG-CTEST 2026-02-09 18:49:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:56 load all fixtures failed: requested fixture keys not found in test_fixtures.json: volumeattachments
FAIL	k8s.io/kubernetes/pkg/registry/storage/volumeattachment	0.574s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/storage/volumeattachment/storage	1.600s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestVolumeAttributesClassStrategy

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/volumeattributesclass/ctest_strategy_test.go:30]: get default configs: {test_fixture.json [valid class create] driverName [pods] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} fake map[foo:bar]}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "driverName" in requested fixtures
2026/02/09 18:49:57 [DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:49:57 Mode: 1
2026/02/09 18:49:57 Base JSON size: 48 bytes
2026/02/09 18:49:57 Number of external values: 0
2026/02/09 18:49:57 [DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:49:57 [DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"DriverName":"fake","Parameters":{"foo":"bar"}})[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:49:57 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/volumeattributesclass/ctest_strategy_test.go:39]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestVolumeAttributesClassStrategy (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/registry/storage/volumeattributesclass	2.009s	coverage: 0.0% of statements
=== RUN   TestCtestCreate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:49:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/registry/storage/volumeattributesclass/storage/ctest_storage_test.go:50]: get default configs: {test_fixture.json [default volumeattributesclass] driverName [storageclasses] {{ } {      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} fake map[foo:bar]}}

==================== CTEST UNION MODE START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:49:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[storageclasses]
[DEBUG-CTEST 2026-02-09 18:49:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[storageclasses], int=1)[DEBUG-CTEST 2026-02-09 18:49:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:49:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [storageclasses]
[DEBUG-CTEST 2026-02-09 18:49:58 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:49:58 load all fixtures failed: requested fixture keys not found in test_fixtures.json: storageclasses
FAIL	k8s.io/kubernetes/pkg/registry/storage/volumeattributesclass/storage	2.494s
	k8s.io/kubernetes/pkg/registry/storagemigration/rest		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/storagemigration/storagemigration		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/storagemigration/storagemigration/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/testapigroup/carp		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/testapigroup/carp/storage		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/registry/testapigroup/rest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/routes	0.374s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler	1.513s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPluginsNames
=== RUN   TestCtestPluginsNames/empty
=== RUN   TestCtestPluginsNames/with_duplicates
=== RUN   TestCtestPluginsNames/nil_plugin_sets
=== RUN   TestCtestPluginsNames/empty_enabled_slices
=== RUN   TestCtestPluginsNames/multiple_distinct_plugins
=== RUN   TestCtestPluginsNames/nil_plugins_pointer
    ctest_types_test.go:91: plugins mismatch (-want +got):
          []string(
        - 	{},
        + 	nil,
          )
--- FAIL: TestCtestPluginsNames (0.00s)
    --- PASS: TestCtestPluginsNames/empty (0.00s)
    --- PASS: TestCtestPluginsNames/with_duplicates (0.00s)
    --- PASS: TestCtestPluginsNames/nil_plugin_sets (0.00s)
    --- PASS: TestCtestPluginsNames/empty_enabled_slices (0.00s)
    --- PASS: TestCtestPluginsNames/multiple_distinct_plugins (0.00s)
    --- FAIL: TestCtestPluginsNames/nil_plugins_pointer (0.00s)
FAIL
coverage: 2.6% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/apis/config	1.960s
	k8s.io/kubernetes/pkg/scheduler/apis/config/latest		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/apis/config/scheme	0.668s	coverage: 100.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/apis/config/testing		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/scheduler/apis/config/testing/defaults	[no test files]
=== RUN   TestCtestApplyFeatureGates
=== RUN   TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_disabled
W0209 18:50:06.195058   91394 feature_gate.go:352] Setting GA feature gate DynamicResourceAllocation=false. It will be removed in a future release.
=== RUN   TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_enabled
W0209 18:50:06.197558   91394 feature_gate.go:352] Setting GA feature gate DynamicResourceAllocation=true. It will be removed in a future release.
=== RUN   TestCtestApplyFeatureGates/No_feature_gates_set_(edge_case)
--- PASS: TestCtestApplyFeatureGates (0.00s)
    --- PASS: TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_disabled (0.00s)
    --- PASS: TestCtestApplyFeatureGates/Feature_gate_DynamicResourceAllocation_enabled (0.00s)
    --- PASS: TestCtestApplyFeatureGates/No_feature_gates_set_(edge_case) (0.00s)
=== RUN   TestCtestMergePlugins
=== RUN   TestCtestMergePlugins/AppendCustomPlugin
=== RUN   TestCtestMergePlugins/InsertAfterDefaultPlugins2
=== RUN   TestCtestMergePlugins/ApplyNilCustomPlugin
=== RUN   TestCtestMergePlugins/BothNil_(edge_case)
=== RUN   TestCtestMergePlugins/EmptyCustom_(edge_case)
=== RUN   TestCtestMergePlugins/NilFilterCustom_(edge_case)
=== RUN   TestCtestMergePlugins/CustomWithOnlyDisabled_(edge_case)
    ctest_default_plugins_test.go:256: plugins mismatch (-want +got):
          &v1.Plugins{
          	PreEnqueue: {},
          	QueueSort:  {},
          	PreFilter:  {},
          	Filter: v1.PluginSet{
        - 		Enabled:  []v1.Plugin{{Name: "DefaultPlugin1"}, {Name: "DefaultPlugin2"}},
        + 		Enabled:  nil,
          		Disabled: {{Name: "*"}},
          	},
          	PostFilter: {},
          	PreScore:   {},
          	... // 7 identical fields
          }
--- FAIL: TestCtestMergePlugins (0.00s)
    --- PASS: TestCtestMergePlugins/AppendCustomPlugin (0.00s)
    --- PASS: TestCtestMergePlugins/InsertAfterDefaultPlugins2 (0.00s)
    --- PASS: TestCtestMergePlugins/ApplyNilCustomPlugin (0.00s)
    --- PASS: TestCtestMergePlugins/BothNil_(edge_case) (0.00s)
    --- PASS: TestCtestMergePlugins/EmptyCustom_(edge_case) (0.00s)
    --- PASS: TestCtestMergePlugins/NilFilterCustom_(edge_case) (0.00s)
    --- FAIL: TestCtestMergePlugins/CustomWithOnlyDisabled_(edge_case) (0.00s)
FAIL
coverage: 7.6% of statements
FAIL	k8s.io/kubernetes/pkg/scheduler/apis/config/v1	2.668s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/apis/config/validation	0.912s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/backend/api_cache		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/backend/api_dispatcher	0.943s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/backend/cache	0.512s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/backend/cache/debugger	1.681s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/backend/cache/fake		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/backend/heap	1.162s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/backend/queue	2.217s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework	2.661s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/api_calls	3.186s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: [no statements]
ok  	k8s.io/kubernetes/pkg/scheduler/framework/autoscaler_contract	0.890s	coverage: [no statements] [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/parallelize	0.418s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultbinder	0.659s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/defaultpreemption	0.786s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/dynamicresources	2.063s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/dynamicresources/extended	1.219s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins/feature		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/helper	0.681s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/imagelocality	1.855s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/interpodaffinity	1.184s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/scheduler/framework/plugins/names	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeaffinity	2.445s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodename	1.288s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeports	0.718s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/noderesources	0.889s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodeunschedulable	1.679s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/nodevolumelimits	1.263s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/podtopologyspread	0.587s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/queuesort	0.951s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/schedulinggates	1.402s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/tainttoleration	1.798s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins/testing		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 1.7% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumebinding	2.439s	coverage: 1.7% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumebinding/metrics		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumerestrictions	2.947s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/plugins/volumezone	0.617s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/preemption	0.969s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.3% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/framework/runtime	1.733s	coverage: 0.3% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/metrics	1.171s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/metrics/resources	2.423s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/profile	2.134s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/scheduler/testing		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/scheduler/testing/framework		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/util	0.573s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/scheduler/util/assumecache	0.447s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/security	[no test files]
=== RUN   TestCtestGetProfile

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/security/apparmor/ctest_helpers_test.go:27]: get default configs: {test_fixture.json [app armor profiles] AppArmorProfile [pods] &AppArmorProfile{Type:RuntimeDefault,LocalhostProfile:nil,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "AppArmorProfile" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 25 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"type":"RuntimeDefault"})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/security/apparmor/ctest_helpers_test.go:46]: Skipping dynamic config execution. No new configurations generated.

==================== CTEST END ======================
=== RUN   TestCtestGetProfile/no_appArmor
=== RUN   TestCtestGetProfile/pod_profile
=== RUN   TestCtestGetProfile/container_profile
=== RUN   TestCtestGetProfile/annotation_profile
=== RUN   TestCtestGetProfile/invalid_annotation
=== RUN   TestCtestGetProfile/invalid_annotation_with_pod_field
=== RUN   TestCtestGetProfile/container_field_before_annotation
=== RUN   TestCtestGetProfile/container_field_before_pod_field
=== RUN   TestCtestGetProfile/annotation_before_pod_field
=== RUN   TestCtestGetProfile/all_profiles
=== RUN   TestCtestGetProfile/empty_annotation_string
=== RUN   TestCtestGetProfile/empty_container_profile
=== RUN   TestCtestGetProfile/empty_pod_profile
=== RUN   TestCtestGetProfile/very_long_invalid_annotation
--- PASS: TestCtestGetProfile (0.00s)
    --- PASS: TestCtestGetProfile/no_appArmor (0.00s)
    --- PASS: TestCtestGetProfile/pod_profile (0.00s)
    --- PASS: TestCtestGetProfile/container_profile (0.00s)
    --- PASS: TestCtestGetProfile/annotation_profile (0.00s)
    --- PASS: TestCtestGetProfile/invalid_annotation (0.00s)
    --- PASS: TestCtestGetProfile/invalid_annotation_with_pod_field (0.00s)
    --- PASS: TestCtestGetProfile/container_field_before_annotation (0.00s)
    --- PASS: TestCtestGetProfile/container_field_before_pod_field (0.00s)
    --- PASS: TestCtestGetProfile/annotation_before_pod_field (0.00s)
    --- PASS: TestCtestGetProfile/all_profiles (0.00s)
    --- PASS: TestCtestGetProfile/empty_annotation_string (0.00s)
    --- PASS: TestCtestGetProfile/empty_container_profile (0.00s)
    --- PASS: TestCtestGetProfile/empty_pod_profile (0.00s)
    --- PASS: TestCtestGetProfile/very_long_invalid_annotation (0.00s)
=== RUN   TestCtestValidateBadHost

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "annotations" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 82 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"test","resources":{}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/security/apparmor/ctest_validate_test.go:33]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestValidateBadHost (0.00s)
=== RUN   TestCtestValidateValidHost

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "annotations" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 82 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{},"spec":{"containers":[{"name":"test","resources":{}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/security/apparmor/ctest_validate_test.go:96]: Skipping single‑container pod tests. No new configurations generated.

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "annotations" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 452 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{"annotations":{"container.apparmor.security.beta.kubernetes.io/init":"localhost/foo-container","container.apparmor.security.beta.kubernetes.io/test1":"runtime/default","container.apparmor.security.beta.kubernetes.io/test2":"localhost/docker-default"}},"spec":{"containers":[{"name":"test1","resources":{}},{"name":"test2","resources":{}},{"name":"no-profile","resources":{}}],"initContainers":[{"name":"init","resources":{}}]},"status":{}})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/security/apparmor/ctest_validate_test.go:154]: Skipping multi‑container pod test. No new configurations generated.
--- PASS: TestCtestValidateValidHost (0.00s)
PASS
coverage: 30.2% of statements
ok  	k8s.io/kubernetes/pkg/security/apparmor	1.052s	coverage: 30.2% of statements
=== RUN   TestCtestAddNoNewPrivileges

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:31]: get default configs: {test_fixture.json [nil security context nil] allowPrivilegeEscalation [pods] nil}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "allowPrivilegeEscalation" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 4 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=null)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:38]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:31]: get default configs: {test_fixture.json [allowPrivilegeEscalation nil] allowPrivilegeEscalation [pods] &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "allowPrivilegeEscalation" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 2 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:38]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:31]: get default configs: {test_fixture.json [allowPrivilegeEscalation false] allowPrivilegeEscalation [pods] &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "allowPrivilegeEscalation" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 34 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"allowPrivilegeEscalation":false})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:38]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:31]: get default configs: {test_fixture.json [allowPrivilegeEscalation true] allowPrivilegeEscalation [pods] &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "allowPrivilegeEscalation" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 33 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"allowPrivilegeEscalation":true})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:38]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:31]: get default configs: {test_fixture.json [allowPrivilegeEscalation edge empty] allowPrivilegeEscalation [pods] &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "allowPrivilegeEscalation" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 34 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"allowPrivilegeEscalation":false})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:38]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:31]: get default configs: {test_fixture.json [allowPrivilegeEscalation edge random] allowPrivilegeEscalation [pods] &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "allowPrivilegeEscalation" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 33 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"allowPrivilegeEscalation":true})[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:38]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestAddNoNewPrivileges (0.01s)
=== RUN   TestCtestConvertToRuntimeMaskedPaths

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:73]: get default configs: {test_fixture.json [procMount nil] procMount [pods] <nil>}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "procMount" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 4 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=null)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:80]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:73]: get default configs: {test_fixture.json [procMount default] procMount [pods] 0x140004cb980}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "procMount" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 9 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="Default")[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:80]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:73]: get default configs: {test_fixture.json [procMount unmasked] procMount [pods] 0x140004cb990}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "procMount" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 10 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="Unmasked")[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:80]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:73]: get default configs: {test_fixture.json [procMount edge invalid] procMount [pods] 0x140004cb9a0}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "procMount" in requested fixtures
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:50:56 Mode: 1
2026/02/09 18:50:56 Base JSON size: 9 bytes
2026/02/09 18:50:56 Number of external values: 0
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:50:56 [DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="invalid")[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:50:56 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/securitycontext/ctest_util_test.go:80]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestConvertToRuntimeMaskedPaths (0.00s)
PASS
coverage: 1.3% of statements
ok  	k8s.io/kubernetes/pkg/securitycontext	0.555s	coverage: 1.3% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/serviceaccount	2.356s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/serviceaccount/externaljwt/metrics	1.321s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestExternalTokenGenerator
=== RUN   TestCtestExternalTokenGenerator/correct_token_with_correct_claims_returned
=== RUN   TestCtestExternalTokenGenerator/correct_token_with_correct_claims_signed_by_key_that's_excluded_from_OIDC
=== RUN   TestCtestExternalTokenGenerator/token_signed_with_key_that's_excluded_from_OIDC_but_validation_is_disabled
=== RUN   TestCtestExternalTokenGenerator/empty_key_ID_returned_from_signer
=== RUN   TestCtestExternalTokenGenerator/key_id_longer_than_1024_bytes_returned_from_signer
=== RUN   TestCtestExternalTokenGenerator/unsupported_alg_returned_from_signer
=== RUN   TestCtestExternalTokenGenerator/empty_alg_returned_from_signer
=== RUN   TestCtestExternalTokenGenerator/Invalid_backend_header_type
=== RUN   TestCtestExternalTokenGenerator/nil_privateClaims
=== RUN   TestCtestExternalTokenGenerator/empty_audience_list
    ctest_plugin_test.go:375: Bad claims; diff (-got +want):
          plugin.unifiedClaimsT{
          	Issuer:    "some-issuer",
          	Subject:   "subject",
        - 	Audience:  nil,
        + 	Audience:  jwt.Audience{},
          	Expiry:    nil,
          	NotBefore: nil,
          	... // 3 identical fields
          }
--- FAIL: TestCtestExternalTokenGenerator (0.01s)
    --- PASS: TestCtestExternalTokenGenerator/correct_token_with_correct_claims_returned (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/correct_token_with_correct_claims_signed_by_key_that's_excluded_from_OIDC (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/token_signed_with_key_that's_excluded_from_OIDC_but_validation_is_disabled (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/empty_key_ID_returned_from_signer (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/key_id_longer_than_1024_bytes_returned_from_signer (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/unsupported_alg_returned_from_signer (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/empty_alg_returned_from_signer (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/Invalid_backend_header_type (0.00s)
    --- PASS: TestCtestExternalTokenGenerator/nil_privateClaims (0.00s)
    --- FAIL: TestCtestExternalTokenGenerator/empty_audience_list (0.00s)
FAIL
coverage: 56.7% of statements
FAIL	k8s.io/kubernetes/pkg/serviceaccount/externaljwt/plugin	2.454s
	k8s.io/kubernetes/pkg/serviceaccount/externaljwt/plugin/testing/v1		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/async	1.457s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/pkg/util/bandwidth	[no test files]
	k8s.io/kubernetes/pkg/util/coverage		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/env	2.195s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/filesystem	2.358s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/flag	1.590s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/util/flock		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/goroutinemap	1.705s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/util/goroutinemap/exponentialbackoff		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/hash	1.704s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/util/interrupt		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/util/iptables	[no test files]
?   	k8s.io/kubernetes/pkg/util/iptables/testing	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/kernel	1.673s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/labels	1.050s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/node	1.154s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/util/oom		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/parsers	1.193s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/pod	0.719s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/util/procfs		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/removeall	0.274s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/util/rlimit		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/slice	1.068s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/tail	0.877s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/taints	1.647s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/util/tolerations	1.474s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestGetMetricsStatFS

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:27]: get default configs: {test_fixture.json [default path] path [] }

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[]
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[], int=0)[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:51:15 Mode: 1
2026/02/09 18:51:15 Base JSON size: 2 bytes
2026/02/09 18:51:15 Number of external values: 24
2026/02/09 18:51:15   [OVERRIDE] :  → /healthy
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /healthy
2026/02/09 18:51:15   [OVERRIDE] :  → /healthz
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /readyz
2026/02/09 18:51:15   [OVERRIDE] :  → /readyz
2026/02/09 18:51:15   [OVERRIDE] :  → /readyz
2026/02/09 18:51:15   [OVERRIDE] :  → /healthy
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /healthy
2026/02/09 18:51:15   [OVERRIDE] :  → /ready
2026/02/09 18:51:15   [OVERRIDE] :  → /
2026/02/09 18:51:15   [OVERRIDE] :  → /etc/kubernetes
2026/02/09 18:51:15   [OVERRIDE] :  → /var/lib/kms/
2026/02/09 18:51:15   [OVERRIDE] :  → /var/lib/kubelet/plugins/cinder.csi.openstack.org
2026/02/09 18:51:15   [OVERRIDE] :  → /var/lib/kubelet/plugins_registry/
2026/02/09 18:51:15   [OVERRIDE] :  → /var/lib/kubelet
2026/02/09 18:51:15   [OVERRIDE] :  → /dev
2026/02/09 18:51:15   [OVERRIDE] :  → /healthz
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=24)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string="")[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 1
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/healthy)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /healthy
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 2
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 3
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/healthy)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /healthy
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 4
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/healthz)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /healthz
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 5
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 6
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 7
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 8
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 9
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 10
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/readyz)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /readyz
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 11
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/readyz)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /readyz
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 12
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/readyz)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /readyz
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 13
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/healthy)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /healthy
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 14
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 15
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/healthy)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /healthy
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 16
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/ready)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /ready
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 17
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 18
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/etc/kubernetes)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /etc/kubernetes
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 19
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/var/lib/kms/)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /var/lib/kms/
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 20
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/var/lib/kubelet/plugins/cinder.csi.openstack.org)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /var/lib/kubelet/plugins/cinder.csi.openstack.org
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 21
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/var/lib/kubelet/plugins_registry/)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /var/lib/kubelet/plugins_registry/
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 22
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/var/lib/kubelet)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /var/lib/kubelet
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 23
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/dev)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /dev
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:447]: ✅ Added Result %d as unique effective object
 24
2026/02/09 18:51:15 [DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:448]:%!(EXTRA string=Successfully converted to type %T, string=/healthz)
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:449]: Result value: %+v
 /healthz
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:458]: ✅ Generated %d unique effective object(s) after filtering
 24
=== GENERATE EFFECTIVE CONFIG COMPLETE ===
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:39]: New Json Test Configs: ["/healthy","/ready","/healthy","/healthz","/ready","/ready","/ready","/ready","/ready","/readyz","/readyz","/readyz","/healthy","/ready","/healthy","/ready","/","/etc/kubernetes","/var/lib/kms/","/var/lib/kubelet/plugins/cinder.csi.openstack.org","/var/lib/kubelet/plugins_registry/","/var/lib/kubelet","/dev","/healthz"]
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:40]: Number of test cases: 24
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /healthy
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/healthy": failed to get FsInfo due to error no such file or directory
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /healthy
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/healthy": failed to get FsInfo due to error no such file or directory
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /healthz
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/healthz": failed to get FsInfo due to error no such file or directory
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 5 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 6 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 7 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 8 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 9 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /readyz
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/readyz": failed to get FsInfo due to error no such file or directory
Running 10 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /readyz
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/readyz": failed to get FsInfo due to error no such file or directory
Running 11 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /readyz
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/readyz": failed to get FsInfo due to error no such file or directory
Running 12 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /healthy
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/healthy": failed to get FsInfo due to error no such file or directory
Running 13 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 14 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /healthy
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/healthy": failed to get FsInfo due to error no such file or directory
Running 15 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /ready
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/ready": failed to get FsInfo due to error no such file or directory
Running 16 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /
Running 17 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /etc/kubernetes
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/etc/kubernetes": failed to get FsInfo due to error no such file or directory
Running 18 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /var/lib/kms/
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/var/lib/kms/": failed to get FsInfo due to error no such file or directory
Running 19 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /var/lib/kubelet/plugins/cinder.csi.openstack.org
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/var/lib/kubelet/plugins/cinder.csi.openstack.org": failed to get FsInfo due to error no such file or directory
Running 20 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /var/lib/kubelet/plugins_registry/
Running 21 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /var/lib/kubelet
Running 22 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /dev
    ctest_metrics_statfs_test.go:97: Expected Available 0 > 0 for path "/dev"
Running 23 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /healthz
    ctest_metrics_statfs_test.go:87: Unexpected error for valid path "/healthz": failed to get FsInfo due to error no such file or directory
Running 24 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /not/a/real/directory
Running 25 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/metric_statfs_test942712861
Running 26 th test case.
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/ctest_metrics_statfs_test.go:59]: Path: /                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                

==================== CTEST END ======================
--- FAIL: TestCtestGetMetricsStatFS (0.00s)
FAIL
coverage: 6.4% of statements
FAIL	k8s.io/kubernetes/pkg/volume	1.157s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/configmap	0.633s	coverage: 0.0% of statements [no tests to run]
/var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build547637365/b5073/csi.test flag redefined: v
panic: /var/folders/zx/xblrff5j56l0s_18kh8vy7vc0000gq/T/go-build547637365/b5073/csi.test flag redefined: v

goroutine 1 [running]:
flag.(*FlagSet).Var(0x1400004e0e0, {0x1032e42b0, 0x1051c7110}, {0x102a00140, 0x1}, {0x1025e0cdf, 0x22})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/flag/flag.go:1028 +0x2d4
k8s.io/klog/v2.InitFlags.func1(0x140001ffb90?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/k8s.io/klog/v2/klog.go:447 +0x3c
flag.(*FlagSet).VisitAll(0x1?, 0x14000067e68)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/flag/flag.go:458 +0x48
k8s.io/klog/v2.InitFlags(0x14000067ea8?)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/k8s.io/klog/v2/klog.go:446 +0x44
k8s.io/kubernetes/pkg/volume/csi.TestMain(0x140005e2320)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/csi/csi_util_test.go:41 +0x28
main.main()
	_testmain.go:247 +0x98
FAIL	k8s.io/kubernetes/pkg/volume/csi	1.558s
	k8s.io/kubernetes/pkg/volume/csi/fake		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/csi/nodeinfomanager	1.967s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/pkg/volume/csi/testing		coverage: 0.0% of statements
=== RUN   TestCtestIsMigratable

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/csimigration/ctest_plugin_manager_test.go:30]: get default configs: {test_fixture.json [default portworx spec] spec [persistentvolumes] {map[] {nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil &PortworxVolumeSource{VolumeID:test-volume,FSType:,ReadOnly:false,} nil nil nil nil} [] nil   [] <nil> nil <nil>}}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:51:15 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:15 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/volume/csimigration	0.963s
=== RUN   TestCtestDownwardAPI

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:51:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/downwardapi/ctest_downwardapi_test.go:85]: get default configs: {test_fixture.json [default downwardapi config] downwardapi [pods volumes] {map[key1:value1 key2:value2] map[a1:value1 a2:value2 multiline:c
b
a] map[labels:metadata.labels name_file_name:metadata.name] map[name_file_name:256]}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods volumes]
[DEBUG-CTEST 2026-02-09 18:51:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods volumes], int=2)[DEBUG-CTEST 2026-02-09 18:51:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [volumes]
[DEBUG-CTEST 2026-02-09 18:51:17 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:17 load all fixtures failed: requested fixture keys not found in test_fixtures.json: volumes
FAIL	k8s.io/kubernetes/pkg/volume/downwardapi	2.549s
	k8s.io/kubernetes/pkg/volume/emptydir		coverage: 0.0% of statements
=== RUN   TestCtestSearchDisk
=== RUN   TestCtestSearchDisk/PCI_disk_0
I0209 18:51:20.843930   92500 fc_util.go:74] fc: find disk: /dev/sda, dm: , fc path: pci-0000:41:00.0-fc-0x500a0981891b8dc5-lun-0
I0209 18:51:20.844885   92500 fc_util.go:74] fc: find disk: /dev/sda, dm: , fc path: pci-0000:41:00.0-fc-0x500a0981891b8dc5-lun-0
=== RUN   TestCtestSearchDisk/PCI_disk_1
I0209 18:51:20.844923   92500 fc_util.go:74] fc: find disk: /dev/sdb, dm: , fc path: pci-0000:41:00.0-fc-0x500a0981891b8dc5-lun-1
I0209 18:51:20.844932   92500 fc_util.go:74] fc: find disk: /dev/sdb, dm: , fc path: pci-0000:41:00.0-fc-0x500a0981891b8dc5-lun-1
=== RUN   TestCtestSearchDisk/Non_PCI_disk
I0209 18:51:20.844964   92500 fc_util.go:74] fc: find disk: /dev/sdc, dm: , fc path: fc-0x5005076810213b32-lun-2
I0209 18:51:20.845015   92500 fc_util.go:74] fc: find disk: /dev/sdc, dm: , fc path: fc-0x5005076810213b32-lun-2
=== RUN   TestCtestSearchDisk/Invalid_Storage_Controller
=== RUN   TestCtestSearchDisk/Non_existing_disk
=== RUN   TestCtestSearchDisk/Empty_WWNs
=== RUN   TestCtestSearchDisk/Nil_WWNs_slice
=== RUN   TestCtestSearchDisk/Negative_LUN
=== RUN   TestCtestSearchDisk/Non-numeric_LUN
=== RUN   TestCtestSearchDisk/Very_large_LUN
=== RUN   TestCtestSearchDisk/Multiple_matching_WWNs_but_different_LUNs
I0209 18:51:20.845418   92500 fc_util.go:74] fc: find disk: /dev/sda, dm: , fc path: pci-0000:41:00.0-fc-0x500a0981891b8dc5-lun-0
I0209 18:51:20.845471   92500 fc_util.go:74] fc: find disk: /dev/sda, dm: , fc path: pci-0000:41:00.0-fc-0x500a0981891b8dc5-lun-0
    ctest_fc_util_test.go:119: got unexpected error: no fc disk found
    ctest_fc_util_test.go:122: expected a device path but got empty string
    ctest_fc_util_test.go:125: expected disk /dev/sda, got 
--- FAIL: TestCtestSearchDisk (0.00s)
    --- PASS: TestCtestSearchDisk/PCI_disk_0 (0.00s)
    --- PASS: TestCtestSearchDisk/PCI_disk_1 (0.00s)
    --- PASS: TestCtestSearchDisk/Non_PCI_disk (0.00s)
    --- PASS: TestCtestSearchDisk/Invalid_Storage_Controller (0.00s)
    --- PASS: TestCtestSearchDisk/Non_existing_disk (0.00s)
    --- PASS: TestCtestSearchDisk/Empty_WWNs (0.00s)
    --- PASS: TestCtestSearchDisk/Nil_WWNs_slice (0.00s)
    --- PASS: TestCtestSearchDisk/Negative_LUN (0.00s)
    --- PASS: TestCtestSearchDisk/Non-numeric_LUN (0.00s)
    --- PASS: TestCtestSearchDisk/Very_large_LUN (0.00s)
    --- FAIL: TestCtestSearchDisk/Multiple_matching_WWNs_but_different_LUNs (0.00s)
=== RUN   TestCtestParsePDName
=== RUN   TestCtestParsePDName/single_WWID
=== RUN   TestCtestParsePDName/multiple_WWID
=== RUN   TestCtestParsePDName/single_WWN
=== RUN   TestCtestParsePDName/multiple_WWNs
=== RUN   TestCtestParsePDName/no_WWNs
=== RUN   TestCtestParsePDName/invalid_lun
=== RUN   TestCtestParsePDName/empty_path
    ctest_fc_util_test.go:217: expected error but got none
    ctest_fc_util_test.go:229: expected WWIDs [], got [.]
=== RUN   TestCtestParsePDName/missing_lun_segment
    ctest_fc_util_test.go:217: expected error but got none
    ctest_fc_util_test.go:229: expected WWIDs [], got [50050768030539b6]
=== RUN   TestCtestParsePDName/extra_hyphens_in_WWIDs
    ctest_fc_util_test.go:217: expected error but got none
    ctest_fc_util_test.go:229: expected WWIDs [], got [60050763008084e6e0000000000001ae  60050763008084e6e0000000000001af]
=== RUN   TestCtestParsePDName/non‑hex_characters_in_WWID
    ctest_fc_util_test.go:217: expected error but got none
    ctest_fc_util_test.go:229: expected WWIDs [], got [ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ]
=== RUN   TestCtestParsePDName/duplicate_WWNs_with_different_LUNs
--- FAIL: TestCtestParsePDName (0.00s)
    --- PASS: TestCtestParsePDName/single_WWID (0.00s)
    --- PASS: TestCtestParsePDName/multiple_WWID (0.00s)
    --- PASS: TestCtestParsePDName/single_WWN (0.00s)
    --- PASS: TestCtestParsePDName/multiple_WWNs (0.00s)
    --- PASS: TestCtestParsePDName/no_WWNs (0.00s)
    --- PASS: TestCtestParsePDName/invalid_lun (0.00s)
    --- FAIL: TestCtestParsePDName/empty_path (0.00s)
    --- FAIL: TestCtestParsePDName/missing_lun_segment (0.00s)
    --- FAIL: TestCtestParsePDName/extra_hyphens_in_WWIDs (0.00s)
    --- FAIL: TestCtestParsePDName/non‑hex_characters_in_WWID (0.00s)
    --- PASS: TestCtestParsePDName/duplicate_WWNs_with_different_LUNs (0.00s)
FAIL
coverage: 11.5% of statements
FAIL	k8s.io/kubernetes/pkg/volume/fc	0.727s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/flexvolume	0.474s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPlugin

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/git_repo/ctest_git_repo_test.go:124]: get default configs: {test_fixture.json [default git repo source] gitRepo [volumes] {https://github.com/kubernetes/kubernetes.git 2a30ce65c5ab586b98916d83385c5983edd353a1 target_dir}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[volumes]
[DEBUG-CTEST 2026-02-09 18:51:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[volumes], int=1)[DEBUG-CTEST 2026-02-09 18:51:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [volumes]
[DEBUG-CTEST 2026-02-09 18:51:24 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:24 load all fixtures failed: requested fixture keys not found in test_fixtures.json: volumes
FAIL	k8s.io/kubernetes/pkg/volume/git_repo	0.628s
=== RUN   TestCtestDeleterTempDir
=== TestDeleterTempDir: start with 7 cases
    ctest_host_path_test.go:59: Expected failure for test 'path-with-backsteps' but got nil err
=== TestDeleterTempDir: completed
--- FAIL: TestCtestDeleterTempDir (0.00s)
=== RUN   TestCtestOSFileTypeChecker
=== TestOSFileTypeChecker: start with 8 cases
=== TestOSFileTypeChecker: completed
--- PASS: TestCtestOSFileTypeChecker (0.00s)
=== RUN   TestCtestHostPathTypeCheckerInternal
=== TestHostPathTypeCheckerInternal: start with 9 cases
=== TestHostPathTypeCheckerInternal: completed
--- PASS: TestCtestHostPathTypeCheckerInternal (0.00s)
FAIL
coverage: 36.6% of statements
FAIL	k8s.io/kubernetes/pkg/volume/hostpath	1.145s
=== RUN   TestCtestCanSupport

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/image/ctest_image_test.go:27]: get default configs: {test_fixture.json [image volume can support] reference [pods deployments statefulsets daemonsets replicasets] { }}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:51:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:25 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:25 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/pkg/volume/image	1.668s
=== RUN   TestCtestPluginVolume

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/iscsi/ctest_iscsi_test.go:30]: get default configs: {test_fixture.json [iscsi volume] volumeSource [pods] {vol1 {nil nil nil nil nil nil nil &ISCSIVolumeSource{TargetPortal:127.0.0.1:3260,IQN:iqn.2014-12.server:storage.target01,Lun:0,ISCSIInterface:,FSType:ext4,ReadOnly:false,Portals:[],DiscoveryCHAPAuth:false,SecretRef:nil,SessionCHAPAuth:false,InitiatorName:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "volumeSource" in requested fixtures
2026/02/09 18:51:27 [DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:51:27 Mode: 1
2026/02/09 18:51:27 Base JSON size: 125 bytes
2026/02/09 18:51:27 Number of external values: 0
2026/02/09 18:51:27 [DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:51:27 [DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"iscsi":{"fsType":"ext4","iqn":"iqn.2014-12.server:storage.target01","lun":0,"targetPortal":"127.0.0.1:3260"},"name":"vol1"})[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/iscsi/ctest_iscsi_test.go:38]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestPluginVolume (0.01s)
=== RUN   TestCtestPluginPersistentVolume

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/iscsi/ctest_iscsi_test.go:60]: get default configs: {test_fixture.json [iscsi persistent volume] persistentVolumeSource [persistentvolumes] {{ } {vol1      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} {map[] {nil nil nil nil nil nil &ISCSIPersistentVolumeSource{TargetPortal:127.0.0.1:3260,IQN:iqn.2014-12.server:storage.target01,Lun:0,ISCSIInterface:,FSType:ext4,ReadOnly:false,Portals:[],DiscoveryCHAPAuth:false,SecretRef:nil,SessionCHAPAuth:false,InitiatorName:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil} [] nil   [] <nil> nil <nil>} {   <nil>}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:51:27 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:27 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/volume/iscsi	3.174s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/local	1.953s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/nfs	2.337s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/portworx	0.345s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/projected	0.610s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestMakePayload

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/secret/ctest_secret_test.go:40]: get default configs: {test_fixture.json [makepayload base cases] payloadTestCase [secrets] [{no overrides [] &Secret{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string][]byte{bar: [98 97 114],foo: [102 111 111],},Type:,StringData:map[string]string{},Immutable:nil,} 0x14000134a90 false map[bar:{[98 97 114] 420 <nil>} foo:{[102 111 111] 420 <nil>}] true} {basic 1 [{foo path/to/foo.txt <nil>}] &Secret{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string][]byte{bar: [98 97 114],foo: [102 111 111],},Type:,StringData:map[string]string{},Immutable:nil,} 0x14000134aa0 false map[path/to/foo.txt:{[102 111 111] 420 <nil>}] true} {non existent key (expected failure) [{zab path/to/foo.txt <nil>}] &Secret{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string][]byte{bar: [98 97 114],foo: [102 111 111],},Type:,StringData:map[string]string{},Immutable:nil,} 0x14000134ab0 false map[] false} {empty secret with mappings (expected failure) [{foo foo.txt <nil>}] &Secret{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string][]byte{},Type:,StringData:map[string]string{},Immutable:nil,} 0x14000134ab4 false map[] false} {nil secret pointer (expected failure) [{foo foo.txt <nil>}] nil 0x14000134ab8 false map[] false} {negative mode (expected failure) [] &Secret{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Data:map[string][]byte{foo: [102 111 111],},Type:,StringData:map[string]string{},Immutable:nil,} 0x14000134ac0 false map[] false}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[secrets]
[DEBUG-CTEST 2026-02-09 18:51:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[secrets], int=1)[DEBUG-CTEST 2026-02-09 18:51:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "payloadTestCase" in requested fixtures
2026/02/09 18:51:31 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:51:31 Base JSON size: 1229 bytes
2026/02/09 18:51:31 Number of external values: 0
2026/02/09 18:51:31 BASE DATA (type: []interface {}):
[
  {
    "Mappings": null,
    "Mode": 420,
    "Name": "no overrides",
    "Optional": false,
    "Payload": {
      "bar": {
        "Data": "YmFy",
        "FsUser": null,
        "Mode": 420
      },
      "foo": {
        "Data": "Zm9v",
        "FsUser": null,
        "Mode": 420
      }
    },
    "Secret": {
      "data": {
        "bar": "YmFy",
        "foo": "Zm9v"
      },
      "metadata": {}
    },
    "Success": true
  },
  {
    "Mappings": [
      {
        "key": "foo",
        "path": "path/to/foo.txt"
      }
    ],
    "Mode": 420,
    "Name": "basic 1",
    "Optional": false,
    "Payload": {
      "path/to/foo.txt": {
        "Data": "Zm9v",
        "FsUser": null,
        "Mode": 420
      }
    },
    "Secret": {
      "data": {
        "bar": "YmFy",
        "foo": "Zm9v"
      },
      "metadata": {}
    },
    "Success": true
  },
  {
    "Mappings": [
      {
        "key": "zab",
        "path": "path/to/foo.txt"
      }
    ],
    "Mode": 420,
    "Name": "non existent key (expected failure)",
    "Optional": false,
    "Payload": null,
    "Secret": {
      "data": {
        "bar": "YmFy",
        "foo": "Zm9v"
      },
      "metadata": {}
    },
    "Success": false
  },
  {
    "Mappings": [
      {
        "key": "foo",
        "path": "foo.txt"
      }
    ],
    "Mode": 420,
    "Name": "empty secret with mappings (expected failure)",
    "Optional": false,
    "Payload": null,
    "Secret": {
      "metadata": {}
    },
    "Success": false
  },
  {
    "Mappings": [
      {
        "key": "foo",
        "path": "foo.txt"
      }
    ],
    "Mode": 420,
    "Name": "nil secret pointer (expected failure)",
    "Optional": false,
    "Payload": null,
    "Secret": null,
    "Success": false
  },
  {
    "Mappings": null,
    "Mode": -1,
    "Name": "negative mode (expected failure)",
    "Optional": false,
    "Payload": null,
    "Secret": {
      "data": {
        "foo": "Zm9v"
      },
      "metadata": {}
    },
    "Success": false
  }
]
2026/02/09 18:51:31 
=== UNION COMPLETE ===
2026/02/09 18:51:31 Generated 0 result(s)
[DEBUG-CTEST 2026-02-09 18:51:31 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/secret/ctest_secret_test.go:45]: Failed to get matched fixtures: failed to unmarshal original config to type secret.payloadTestCase: json: cannot unmarshal array into Go value of type secret.payloadTestCase
    ctest_secret_test.go:46: GenerateEffectiveConfigReturnType error: failed to unmarshal original config to type secret.payloadTestCase: json: cannot unmarshal array into Go value of type secret.payloadTestCase
--- FAIL: TestCtestMakePayload (0.00s)
FAIL
coverage: 0.0% of statements
FAIL	k8s.io/kubernetes/pkg/volume/secret	1.189s
	k8s.io/kubernetes/pkg/volume/testing		coverage: 0.0% of statements
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_without_volume_spec
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_without_using_CSI_plugin
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_with_CSI_ephemeral_volume
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_with_CSI_PV
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/empty_plugin_name_with_nil_spec
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/empty_plugin_name_with_CSI_spec
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/nil_CSI_driver_in_volume_source
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/nil_CSI_driver_in_persistent_volume_source
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/spec_with_Volume_but_nil_CSI_struct
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/spec_with_PersistentVolume_but_nil_CSI_struct
=== RUN   TestCtestGetFullQualifiedPluginNameForVolume/spec_with_unrelated_fields_only
--- PASS: TestCtestGetFullQualifiedPluginNameForVolume (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_without_volume_spec (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_without_using_CSI_plugin (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_with_CSI_ephemeral_volume (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/get_full_qualified_plugin_name_with_CSI_PV (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/empty_plugin_name_with_nil_spec (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/empty_plugin_name_with_CSI_spec (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/nil_CSI_driver_in_volume_source (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/nil_CSI_driver_in_persistent_volume_source (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/spec_with_Volume_but_nil_CSI_struct (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/spec_with_PersistentVolume_but_nil_CSI_struct (0.00s)
    --- PASS: TestCtestGetFullQualifiedPluginNameForVolume/spec_with_unrelated_fields_only (0.00s)
=== RUN   TestCtestGetNestedMountpoints
[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/ctest_nested_volumes_test.go:178]: Start Union mode for volumeMount configurations
[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/ctest_nested_volumes_test.go:185]: union config item: {test_fixture.json [union default] volumeMounts [pods] {[] [] [{  [] []  [] [] [] {map[] map[] []} [] <nil> [] [{vol1 false <nil> /dir  <nil> }] [] nil nil nil nil    nil false false false}] []  <nil> <nil>  map[]   <nil>  false false false <nil> nil []   nil  [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] [] nil <nil>}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:51:32 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:51:32 Base JSON size: 95 bytes
2026/02/09 18:51:32 Number of external values: 1
2026/02/09 18:51:32 BASE DATA (type: map[string]interface {}):
{
  "containers": [
    {
      "name": "",
      "resources": {},
      "volumeMounts": [
        {
          "mountPath": "/dir",
          "name": "vol1"
        }
      ]
    }
  ]
}
2026/02/09 18:51:32   [UNION REPLACE] : entire structure replaced
2026/02/09 18:51:32 
=== UNION COMPLETE ===
2026/02/09 18:51:32 Generated 1 result(s)
[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"containers":[{"name":"","resources":{},"volumeMounts":[{"mountPath":"/dir","name":"vol1"}]}]})[DEBUG-CTEST 2026-02-09 18:51:32 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/ctest_nested_volumes_test.go:189]: GenerateEffectiveConfigReturnType error: k8s json unmarshal into v1.PodSpec failed: json: cannot unmarshal array into Go value of type v1.PodSpec
--- FAIL: TestCtestGetNestedMountpoints (0.01s)
panic: [1m[38;5;9mYour Test Panicked[0m
	[38;5;246mframework.Failf("Failed to generate effective config: %v", err)[0m
	[38;5;243m/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/ctest_nested_volumes_test.go:190[0m
	  When you, or your assertion library, calls Ginkgo's Fail(),
	  Ginkgo panics to prevent subsequent assertions from running.
	
	  Normally Ginkgo rescues this panic so you shouldn't see it.
	
	  However, if you make an assertion in a goroutine, Ginkgo can't capture the
	  panic.
	  To circumvent this, you should call
	
	  	defer GinkgoRecover()
	
	  at the top of the goroutine that caused this panic.
	
	  Alternatively, you may have made an assertion outside of a Ginkgo
	  leaf node (e.g. in a container node or some out-of-band function) - please
	  move your assertion to
	  an appropriate Ginkgo node (e.g. a BeforeSuite, BeforeEach, It, etc...).
	
	  [1mLearn more at:[0m
	  [38;5;14m[4mhttp://onsi.github.io/ginkgo/#mental-model-how-ginkgo-handles-failure[0m
	 [recovered]
	panic: [1m[38;5;9mYour Test Panicked[0m
	[38;5;246mframework.Failf("Failed to generate effective config: %v", err)[0m
	[38;5;243m/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/ctest_nested_volumes_test.go:190[0m
	  When you, or your assertion library, calls Ginkgo's Fail(),
	  Ginkgo panics to prevent subsequent assertions from running.
	
	  Normally Ginkgo rescues this panic so you shouldn't see it.
	
	  However, if you make an assertion in a goroutine, Ginkgo can't capture the
	  panic.
	  To circumvent this, you should call
	
	  	defer GinkgoRecover()
	
	  at the top of the goroutine that caused this panic.
	
	  Alternatively, you may have made an assertion outside of a Ginkgo
	  leaf node (e.g. in a container node or some out-of-band function) - please
	  move your assertion to
	  an appropriate Ginkgo node (e.g. a BeforeSuite, BeforeEach, It, etc...).
	
	  [1mLearn more at:[0m
	  [38;5;14m[4mhttp://onsi.github.io/ginkgo/#mental-model-how-ginkgo-handles-failure[0m
	

goroutine 81 [running]:
testing.tRunner.func1.2({0x102c862c0, 0x1400063ea10})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x102c862c0?, 0x1400063ea10?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
github.com/onsi/ginkgo/v2.Fail({0x14000a162d0, 0x8d}, {0x1400073dc00?, 0x1?, 0x1?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/vendor/github.com/onsi/ginkgo/v2/core_dsl.go:427 +0x168
k8s.io/kubernetes/test/e2e/framework.Failf({0x10229dcec?, 0x14000194040?}, {0x14000741d18?, 0x3?, 0x3?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/e2e/framework/log.go:39 +0x38
k8s.io/kubernetes/pkg/volume/util.TestCtestGetNestedMountpoints(0x14000589340)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/ctest_nested_volumes_test.go:190 +0xbf8
testing.tRunner(0x14000589340, 0x102f2ae38)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/pkg/volume/util	1.856s
	k8s.io/kubernetes/pkg/volume/util/fs		coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/volume/util/fsquota		coverage: 0.0% of statements
?   	k8s.io/kubernetes/pkg/volume/util/fsquota/common	[no test files]
=== RUN   TestCtestGetFileType
    ctest_hostutil_test.go:153: [0-Directory Test] unexpected error: volume/util/hostutil on this platform is not supported
--- FAIL: TestCtestGetFileType (0.00s)
FAIL
coverage: 3.6% of statements
FAIL	k8s.io/kubernetes/pkg/volume/util/hostutil	1.228s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/util/nestedpendingoperations	1.553s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/util/operationexecutor	1.009s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestRecyclerPodEdge

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/recyclerclient/ctest_recycler_client_test.go:29]: Base config item: {test_fixture.json [base pod template] metadata.name [pods] &Pod{ObjectMeta:{base-pod  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:PodSpec{Volumes:[]Volume{},Containers:[]Container{},RestartPolicy:,TerminationGracePeriodSeconds:nil,ActiveDeadlineSeconds:nil,DNSPolicy:,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:nil,ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{},HostAliases:[]HostAlias{},PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},Resources:nil,HostnameOverride:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,ResourceClaimStatuses:[]PodResourceClaimStatus{},HostIPs:[]HostIP{},ObservedGeneration:0,ExtendedResourceClaimStatus:nil,},}}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
2026/02/09 18:51:37 [DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:51:37 Mode: 1
2026/02/09 18:51:37 Base JSON size: 110 bytes
2026/02/09 18:51:37 Number of external values: 1
2026/02/09 18:51:37   [REPLACE ALL] : entire structure replaced
2026/02/09 18:51:37 [DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:51:37 [DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=1)
[DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"metadata":{"name":"base-pod","namespace":"default"},"spec":{"containers":null},"status":{"phase":"Pending"}})[DEBUG-CTEST 2026-02-09 18:51:37 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/util/recyclerclient/ctest_recycler_client_test.go:34]: Failed to generate config: k8s json unmarshal into *v1.Pod failed: json: cannot unmarshal string into Go value of type v1.Pod

==================== CTEST END ======================
--- PASS: TestCtestRecyclerPodEdge (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/util/recyclerclient	0.559s	coverage: 0.0% of statements
	k8s.io/kubernetes/pkg/volume/util/subpath		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/util/types	1.188s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/pkg/volume/util/volumepathhandler	1.866s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestValidatePersistentVolumes

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/pkg/volume/validation/ctest_pv_validation_test.go:30]: get default configs: [{test_fixture.json [valid nfs volume spec] spec [persistentvolumes] {map[storage:{{10 9} {<nil>} 10G DecimalSI}] {<nil> <nil> <nil> <nil> 0x1400068b230 <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil>} [ReadWriteOnce] <nil>   [] <nil> <nil> <nil>}} {test_fixture.json [invalid hostpath volume spec] spec [persistentvolumes] {map[storage:{{10 9} {<nil>} 10G DecimalSI}] {<nil> <nil> 0x140005102e8 <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil>} [ReadWriteOnce] <nil>   [] <nil> <nil> <nil>}} {test_fixture.json [edge zero capacity] spec [persistentvolumes] {map[storage:{{0 9} {<nil>}  DecimalSI}] {<nil> <nil> <nil> <nil> 0x1400068b2c0 <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil> <nil>} [ReadWriteOnce] <nil>   [] <nil> <nil> <nil>}}]

==================== CTEST UNION MODE START ====================
Processing config item 0: [valid nfs volume spec]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:51:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumes], int=1)[DEBUG-CTEST 2026-02-09 18:51:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumes]
[DEBUG-CTEST 2026-02-09 18:51:39 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:39 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumes
FAIL	k8s.io/kubernetes/pkg/volume/validation	2.434s
=== RUN   TestCtestAdmissionNonNilAttribute
[DEBUG-CTEST 2026-02-09 18:51:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/admit/ctest_admission_test.go:20]: Start TestCtestAdmissionNonNilAttribute
  W0209 18:51:38.611750   92723 admission.go:62] AlwaysAdmit admission controller is deprecated. Please remove this controller from your configuration files and scripts.
[DEBUG-CTEST 2026-02-09 18:51:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/admit/ctest_admission_test.go:39]: End TestCtestAdmissionNonNilAttribute
--- PASS: TestCtestAdmissionNonNilAttribute (0.00s)
=== RUN   TestCtestAdmissionNilAttribute
[DEBUG-CTEST 2026-02-09 18:51:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/admit/ctest_admission_test.go:43]: Start TestCtestAdmissionNilAttribute
  W0209 18:51:38.611788   92723 admission.go:62] AlwaysAdmit admission controller is deprecated. Please remove this controller from your configuration files and scripts.
[DEBUG-CTEST 2026-02-09 18:51:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/admit/ctest_admission_test.go:49]: End TestCtestAdmissionNilAttribute
--- PASS: TestCtestAdmissionNilAttribute (0.00s)
=== RUN   TestCtestHandles
[DEBUG-CTEST 2026-02-09 18:51:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/admit/ctest_admission_test.go:53]: Start TestCtestHandles
  W0209 18:51:38.611807   92723 admission.go:62] AlwaysAdmit admission controller is deprecated. Please remove this controller from your configuration files and scripts.
Running operation #0: CREATE
Running operation #1: CONNECT
Running operation #2: UPDATE
Running operation #3: DELETE
Running operation #4: 
    ctest_admission_test.go:68: Handles mismatch for operation : expected false
[DEBUG-CTEST 2026-02-09 18:51:38 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/admit/ctest_admission_test.go:71]: End TestCtestHandles
--- FAIL: TestCtestHandles (0.00s)
FAIL
coverage: 57.1% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/admit	1.712s
=== RUN   TestCtestOtherResources

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:51:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:40 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:40 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/plugin/pkg/admission/alwayspullimages	3.121s
=== RUN   TestCtestInterPodAffinityAdmission

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/antiaffinity/ctest_admission_test.go:27]: get default configs: {test_fixture.json [inter pod affinity test cases] affinity [pods deployments statefulsets daemonsets replicasets] {<nil> <nil> <nil>}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:51:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:44 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:44 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/plugin/pkg/admission/antiaffinity	1.239s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/certificates/approval	0.732s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPluginValidate

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:326]: Total test cases: 18
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: wrong type on create
=== RUN   TestCtestPluginValidate/wrong_type_on_create
Running 1 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: wrong type on update
=== RUN   TestCtestPluginValidate/wrong_type_on_update
Running 2 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: reject requests if looking up permissions fails
=== RUN   TestCtestPluginValidate/reject_requests_if_looking_up_permissions_fails
Running 3 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should allow create if no signer name is specified
=== RUN   TestCtestPluginValidate/should_allow_create_if_no_signer_name_is_specified
Running 4 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should allow update if no signer name is specified
=== RUN   TestCtestPluginValidate/should_allow_update_if_no_signer_name_is_specified
Running 5 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should allow create if user is authorized for specific signerName
=== RUN   TestCtestPluginValidate/should_allow_create_if_user_is_authorized_for_specific_signerName
Running 6 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should allow update if user is authorized for specific signerName
=== RUN   TestCtestPluginValidate/should_allow_update_if_user_is_authorized_for_specific_signerName
Running 7 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should allow create if user is authorized with wildcard
=== RUN   TestCtestPluginValidate/should_allow_create_if_user_is_authorized_with_wildcard
Running 8 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should allow update if user is authorized with wildcard
=== RUN   TestCtestPluginValidate/should_allow_update_if_user_is_authorized_with_wildcard
Running 9 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should deny create if user does not have permission for this signerName
=== RUN   TestCtestPluginValidate/should_deny_create_if_user_does_not_have_permission_for_this_signerName
Running 10 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should deny update if user does not have permission for this signerName
=== RUN   TestCtestPluginValidate/should_deny_update_if_user_does_not_have_permission_for_this_signerName
Running 11 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should always allow no-op update
=== RUN   TestCtestPluginValidate/should_always_allow_no-op_update
Running 12 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: should always allow finalizer update
=== RUN   TestCtestPluginValidate/should_always_allow_finalizer_update
Running 13 th test case.
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:330]: description: nil attributes should be rejected
=== RUN   TestCtestPluginValidate/nil_attributes_should_be_rejected
--- FAIL: TestCtestPluginValidate (0.00s)
    --- PASS: TestCtestPluginValidate/wrong_type_on_create (0.00s)
    --- PASS: TestCtestPluginValidate/wrong_type_on_update (0.00s)
    --- PASS: TestCtestPluginValidate/reject_requests_if_looking_up_permissions_fails (0.00s)
    --- PASS: TestCtestPluginValidate/should_allow_create_if_no_signer_name_is_specified (0.00s)
    --- PASS: TestCtestPluginValidate/should_allow_update_if_no_signer_name_is_specified (0.00s)
    --- PASS: TestCtestPluginValidate/should_allow_create_if_user_is_authorized_for_specific_signerName (0.00s)
    --- PASS: TestCtestPluginValidate/should_allow_update_if_user_is_authorized_for_specific_signerName (0.00s)
    --- PASS: TestCtestPluginValidate/should_allow_create_if_user_is_authorized_with_wildcard (0.00s)
    --- PASS: TestCtestPluginValidate/should_allow_update_if_user_is_authorized_with_wildcard (0.00s)
    --- PASS: TestCtestPluginValidate/should_deny_create_if_user_does_not_have_permission_for_this_signerName (0.00s)
    --- PASS: TestCtestPluginValidate/should_deny_update_if_user_does_not_have_permission_for_this_signerName (0.00s)
    --- PASS: TestCtestPluginValidate/should_always_allow_no-op_update (0.00s)
    --- PASS: TestCtestPluginValidate/should_always_allow_finalizer_update (0.00s)
    --- FAIL: TestCtestPluginValidate/nil_attributes_should_be_rejected (0.00s)
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x68 pc=0x10238ea38]

goroutine 93 [running]:
testing.tRunner.func1.2({0x102a8c860, 0x104485660})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x102a8c860?, 0x104485660?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/plugin/pkg/admission/certificates/ctbattest.(*Plugin).Validate(0x14000617ed8, {0x102ece6c8, 0x104512700}, {0x0, 0x0}, {0x12?, 0x1622a1e01?})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/admission.go:96 +0xf8
k8s.io/kubernetes/plugin/pkg/admission/certificates/ctbattest.TestCtestPluginValidate.func1(0x140005828c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/ctbattest/ctest_admission_test.go:346 +0x1ac
testing.tRunner(0x140005828c0, 0x140005e7260)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 47
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/ctbattest	1.035s
=== RUN   TestCtestPlugin_Validate
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/signing/ctest_admission_test.go:261]: Starting TestCtestPlugin_Validate
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/signing/ctest_admission_test.go:262]: Original test case count: 8
[DEBUG-CTEST 2026-02-09 18:51:45 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/signing/ctest_admission_test.go:263]: Edge test case count: 4
Running test case: should deny request if user does not have permission for this signerName
Running test case: should deny request if user attempts to update signerName to a new value they *do* have permission to sign for
Running test case: wrong type
Running test case: allowed if the 'certificate' and conditions field has not changed
Running test case: deny request if authz lookup fails on certificate change
Running test case: deny request if authz lookup fails on condition change
Running test case: allow request if user is authorized for specific signerName
Running test case: allow request if user is authorized with wildcard
Running test case: nil old object
Running test case: unsupported operation
    ctest_admission_test.go:279: Test "unsupported operation": expected rejection but got allowed
Running test case: empty signerName with permission
    ctest_admission_test.go:279: Test "empty signerName with permission": expected rejection but got allowed
Running test case: invalid resource group
    ctest_admission_test.go:279: Test "invalid resource group": expected rejection but got allowed
--- FAIL: TestCtestPlugin_Validate (0.00s)
FAIL
coverage: 61.9% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/signing	1.503s
=== RUN   TestCtestPlugin_Validate
[DEBUG-CTEST 2026-02-09 18:51:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/subjectrestriction/ctest_admission_test.go:23]: Start TestCtestPlugin_Validate
Running 0 th test case: ignored resource
=== RUN   TestCtestPlugin_Validate/ignored_resource
Running 1 th test case: ignored subresource
=== RUN   TestCtestPlugin_Validate/ignored_subresource
Running 2 th test case: wrong type
=== RUN   TestCtestPlugin_Validate/wrong_type
Running 3 th test case: some other signer
=== RUN   TestCtestPlugin_Validate/some_other_signer
Running 4 th test case: invalid request
=== RUN   TestCtestPlugin_Validate/invalid_request
Running 5 th test case: some other group
=== RUN   TestCtestPlugin_Validate/some_other_group
Running 6 th test case: request for system:masters
=== RUN   TestCtestPlugin_Validate/request_for_system:masters
Running 7 th test case: nil object
=== RUN   TestCtestPlugin_Validate/nil_object
Running 8 th test case: empty request bytes
=== RUN   TestCtestPlugin_Validate/empty_request_bytes
Running 9 th test case: invalid signer name
=== RUN   TestCtestPlugin_Validate/invalid_signer_name
    ctest_admission_test.go:164: Validate() error = <nil>, wantErr certificatesigningrequests.certificates.k8s.io "badsigner" is forbidden: unknown signer name "invalid/signer"
Running 10 th test case: excessively large request
=== RUN   TestCtestPlugin_Validate/excessively_large_request
[DEBUG-CTEST 2026-02-09 18:51:46 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/certificates/subjectrestriction/ctest_admission_test.go:168]: End TestCtestPlugin_Validate
--- FAIL: TestCtestPlugin_Validate (0.00s)
    --- PASS: TestCtestPlugin_Validate/ignored_resource (0.00s)
    --- PASS: TestCtestPlugin_Validate/ignored_subresource (0.00s)
    --- PASS: TestCtestPlugin_Validate/wrong_type (0.00s)
    --- PASS: TestCtestPlugin_Validate/some_other_signer (0.00s)
    --- PASS: TestCtestPlugin_Validate/invalid_request (0.00s)
    --- PASS: TestCtestPlugin_Validate/some_other_group (0.00s)
    --- PASS: TestCtestPlugin_Validate/request_for_system:masters (0.00s)
    --- PASS: TestCtestPlugin_Validate/nil_object (0.00s)
    --- PASS: TestCtestPlugin_Validate/empty_request_bytes (0.00s)
    --- FAIL: TestCtestPlugin_Validate/invalid_signer_name (0.00s)
    --- PASS: TestCtestPlugin_Validate/excessively_large_request (0.00s)
FAIL
coverage: 78.9% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/certificates/subjectrestriction	1.249s
=== RUN   TestCtestForgivenessAdmission

==================== CTEST EXTEND ONLY START ====================
[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/defaulttolerationseconds/ctest_admission_test.go:36]: get default configs: {test_fixture.json [default tolerations] tolerations [pods] [{node.kubernetes.io/not-ready Exists  NoExecute 0x140004086e0} {node.kubernetes.io/unreachable Exists  NoExecute 0x140004086e8}]}
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "tolerations" in requested fixtures
2026/02/09 18:51:47 === EXTEND ONLY (RECURSIVE MERGE) ===
2026/02/09 18:51:47 
=== COMPLETE: Generated 0 results ===
[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string=[{"Effect":"NoExecute","Key":"node.kubernetes.io/not-ready","Operator":"Exists","TolerationSeconds":300,"Value":""},{"Effect":"NoExecute","Key":"node.kubernetes.io/unreachable","Operator":"Exists","TolerationSeconds":300,"Value":""}])[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:51:47 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/defaulttolerationseconds/ctest_admission_test.go:43]: Skipping test execution. No new configurations generated.
--- PASS: TestCtestForgivenessAdmission (0.00s)
PASS
coverage: 18.8% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/defaulttolerationseconds	2.234s	coverage: 18.8% of statements
=== RUN   TestCtestAdmission

==================== CTEST START ====================
  W0209 18:51:46.725320   92849 admission.go:64] AlwaysDeny admission controller is deprecated. Please remove this controller from your configuration files and scripts.
Running edge case 0 for TestAdmission
Running edge case 1 for TestAdmission
Running edge case 2 for TestAdmission
Running edge case 3 for TestAdmission

==================== CTEST END ======================
--- PASS: TestCtestAdmission (0.00s)
=== RUN   TestCtestHandles

==================== CTEST START ====================
  W0209 18:51:46.725670   92849 admission.go:64] AlwaysDeny admission controller is deprecated. Please remove this controller from your configuration files and scripts.
Running edge case 0 for TestHandles (operation=CREATE)
Running edge case 1 for TestHandles (operation=CONNECT)
Running edge case 2 for TestHandles (operation=UPDATE)
Running edge case 3 for TestHandles (operation=DELETE)
Running edge case 4 for TestHandles (operation=UnknownOp)

==================== CTEST END ======================
--- PASS: TestCtestHandles (0.00s)
PASS
coverage: 57.1% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/deny	1.747s	coverage: 57.1% of statements
=== RUN   TestCtestEventRateLimiting

==================== CTEST EXTEND ONLY START ====================
=== RUN   TestCtestEventRateLimiting/event_not_blocked_when_tokens_available
=== RUN   TestCtestEventRateLimiting/non-event_not_blocked
=== RUN   TestCtestEventRateLimiting/event_blocked_after_tokens_exhausted
=== RUN   TestCtestEventRateLimiting/event_not_blocked_by_dry-run_requests
=== RUN   TestCtestEventRateLimiting/non-event_not_blocked_after_tokens_exhausted
=== RUN   TestCtestEventRateLimiting/non-events_should_not_count_against_limit
=== RUN   TestCtestEventRateLimiting/event_accepted_after_token_refill
=== RUN   TestCtestEventRateLimiting/event_blocked_by_namespace_limits
=== RUN   TestCtestEventRateLimiting/event_from_other_namespace_not_blocked
=== RUN   TestCtestEventRateLimiting/events_from_other_namespaces_should_not_count_against_limit
=== RUN   TestCtestEventRateLimiting/event_accepted_after_namespace_token_refill
=== RUN   TestCtestEventRateLimiting/event_from_other_namespaces_should_not_clear_namespace_limits
=== RUN   TestCtestEventRateLimiting/namespace_limits_from_lru_namespace_should_clear_when_cache_size_exceeded
=== RUN   TestCtestEventRateLimiting/event_blocked_by_source+object_limits
=== RUN   TestCtestEventRateLimiting/event_from_other_source+object_not_blocked
=== RUN   TestCtestEventRateLimiting/events_from_other_source+object_should_not_count_against_limit
=== RUN   TestCtestEventRateLimiting/event_accepted_after_source+object_token_refill
=== RUN   TestCtestEventRateLimiting/event_from_other_source+object_should_not_clear_source+object_limits
=== RUN   TestCtestEventRateLimiting/source+object_limits_from_lru_source+object_should_clear_when_cache_size_exceeded
=== RUN   TestCtestEventRateLimiting/source_host_should_be_included_in_source+object_key
=== RUN   TestCtestEventRateLimiting/involved_object_kind_should_be_included_in_source+object_key
=== RUN   TestCtestEventRateLimiting/involved_object_namespace_should_be_included_in_source+object_key
=== RUN   TestCtestEventRateLimiting/involved_object_name_should_be_included_in_source+object_key
=== RUN   TestCtestEventRateLimiting/involved_object_UID_should_be_included_in_source+object_key
=== RUN   TestCtestEventRateLimiting/involved_object_APIVersion_should_be_included_in_source+object_key
=== RUN   TestCtestEventRateLimiting/event_blocked_by_user_limits
=== RUN   TestCtestEventRateLimiting/event_from_other_user_not_blocked
=== RUN   TestCtestEventRateLimiting/events_from_other_user_should_not_count_against_limit
=== RUN   TestCtestEventRateLimiting/zero_server_burst_(no_limit)
=== RUN   TestCtestEventRateLimiting/zero_namespace_burst_(no_limit)
=== RUN   TestCtestEventRateLimiting/zero_user_burst_(no_limit)
=== RUN   TestCtestEventRateLimiting/zero_source+object_burst_(no_limit)
=== RUN   TestCtestEventRateLimiting/negative_server_burst_(treated_as_zero_limit)

==================== CTEST END ======================
--- PASS: TestCtestEventRateLimiting (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_not_blocked_when_tokens_available (0.00s)
    --- PASS: TestCtestEventRateLimiting/non-event_not_blocked (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_blocked_after_tokens_exhausted (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_not_blocked_by_dry-run_requests (0.00s)
    --- PASS: TestCtestEventRateLimiting/non-event_not_blocked_after_tokens_exhausted (0.00s)
    --- PASS: TestCtestEventRateLimiting/non-events_should_not_count_against_limit (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_accepted_after_token_refill (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_blocked_by_namespace_limits (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_from_other_namespace_not_blocked (0.00s)
    --- PASS: TestCtestEventRateLimiting/events_from_other_namespaces_should_not_count_against_limit (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_accepted_after_namespace_token_refill (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_from_other_namespaces_should_not_clear_namespace_limits (0.00s)
    --- PASS: TestCtestEventRateLimiting/namespace_limits_from_lru_namespace_should_clear_when_cache_size_exceeded (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_blocked_by_source+object_limits (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_from_other_source+object_not_blocked (0.00s)
    --- PASS: TestCtestEventRateLimiting/events_from_other_source+object_should_not_count_against_limit (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_accepted_after_source+object_token_refill (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_from_other_source+object_should_not_clear_source+object_limits (0.00s)
    --- PASS: TestCtestEventRateLimiting/source+object_limits_from_lru_source+object_should_clear_when_cache_size_exceeded (0.00s)
    --- PASS: TestCtestEventRateLimiting/source_host_should_be_included_in_source+object_key (0.00s)
    --- PASS: TestCtestEventRateLimiting/involved_object_kind_should_be_included_in_source+object_key (0.00s)
    --- PASS: TestCtestEventRateLimiting/involved_object_namespace_should_be_included_in_source+object_key (0.00s)
    --- PASS: TestCtestEventRateLimiting/involved_object_name_should_be_included_in_source+object_key (0.00s)
    --- PASS: TestCtestEventRateLimiting/involved_object_UID_should_be_included_in_source+object_key (0.00s)
    --- PASS: TestCtestEventRateLimiting/involved_object_APIVersion_should_be_included_in_source+object_key (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_blocked_by_user_limits (0.00s)
    --- PASS: TestCtestEventRateLimiting/event_from_other_user_not_blocked (0.00s)
    --- PASS: TestCtestEventRateLimiting/events_from_other_user_should_not_count_against_limit (0.00s)
    --- PASS: TestCtestEventRateLimiting/zero_server_burst_(no_limit) (0.00s)
    --- PASS: TestCtestEventRateLimiting/zero_namespace_burst_(no_limit) (0.00s)
    --- PASS: TestCtestEventRateLimiting/zero_user_burst_(no_limit) (0.00s)
    --- PASS: TestCtestEventRateLimiting/zero_source+object_burst_(no_limit) (0.00s)
    --- PASS: TestCtestEventRateLimiting/negative_server_burst_(treated_as_zero_limit) (0.00s)
=== RUN   TestCtestingleCache
--- PASS: TestCtestingleCache (0.00s)
=== RUN   TestCtestLRUCache
--- FAIL: TestCtestLRUCache (0.00s)
panic: runtime error: index out of range [4] with length 4 [recovered]
	panic: runtime error: index out of range [4] with length 4

goroutine 109 [running]:
testing.tRunner.func1.2({0x104325080, 0x1400082c060})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1734 +0x1ac
testing.tRunner.func1()
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1737 +0x334
panic({0x104325080?, 0x1400082c060?})
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/runtime/panic.go:787 +0x124
k8s.io/kubernetes/plugin/pkg/admission/eventratelimit.TestCtestLRUCache.func1()
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/eventratelimit/ctest_cache_test.go:41 +0x5c
k8s.io/kubernetes/plugin/pkg/admission/eventratelimit.(*lruCache).get(0x14000019ca0, {0x103f6e7e0, 0x103de5a50})
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/eventratelimit/cache.go:51 +0x158
k8s.io/kubernetes/plugin/pkg/admission/eventratelimit.TestCtestLRUCache(0x140006056c0)
	/Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/eventratelimit/ctest_cache_test.go:113 +0x784
testing.tRunner(0x140006056c0, 0x1044e71d8)
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1792 +0xe4
created by testing.(*T).Run in goroutine 1
	/Users/yuezhang/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.24.0.darwin-arm64/src/testing/testing.go:1851 +0x374
FAIL	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit	0.652s
	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/install		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestValidateConfiguration
[DEBUG-CTEST 2026-02-09 18:51:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/eventratelimit/apis/eventratelimit/validation/ctest_validation_test.go:12]: Starting TestCtestValidateConfiguration
Running test case 0: valid server
Running test case 1: valid namespace
Running test case 2: valid user
Running test case 3: valid source+object
Running test case 4: valid multiple
Running test case 5: missing limits
Running test case 6: missing type
Running test case 7: invalid type
Running test case 8: missing burst
Running test case 9: missing qps
Running test case 10: negative cache size
Running test case 11: empty limits slice
Running test case 12: negative burst
Running test case 13: zero qps
Running test case 14: excessively large burst and qps
Running test case 15: namespace missing cache size
    ctest_validation_test.go:223: namespace missing cache size: expected failure, got success
Running test case 16: empty type string
[DEBUG-CTEST 2026-02-09 18:51:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/eventratelimit/apis/eventratelimit/validation/ctest_validation_test.go:227]: Completed TestCtestValidateConfiguration
--- FAIL: TestCtestValidateConfiguration (0.00s)
FAIL
coverage: 100.0% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/eventratelimit/apis/eventratelimit/validation	0.624s
=== RUN   TestCtestAdmit
[DEBUG-CTEST 2026-02-09 18:51:51 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/extendedresourcetoleration/ctest_admission_test.go:384]: Starting TestCtestAdmit with 12 test cases
    ctest_admission_test.go:395: [10: pod with container requesting zero extended resource, expect no toleration added] expected []core.Toleration(nil) got []core.Toleration{core.Toleration{Key:"example.com/device-ek", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}
--- FAIL: TestCtestAdmit (0.00s)
FAIL
coverage: 80.0% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/extendedresourcetoleration	1.024s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/gc	1.680s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestContainerCombinations
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/imagepolicy/ctest_admission_test.go:124]: Start dynamic pod spec generation
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/imagepolicy/ctest_admission_test.go:126]: Loaded hardcoded config entries: 13
=== RUN   TestCtestContainerCombinations/Single_container_allowed
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/imagepolicy/ctest_admission_test.go:139]: Matched config for test: Single container allowed

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods deployments statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods deployments statefulsets daemonsets replicasets], int=5)[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [statefulsets daemonsets replicasets]
[DEBUG-CTEST 2026-02-09 18:51:52 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:51:52 load all fixtures failed: requested fixture keys not found in test_fixtures.json: statefulsets, daemonsets, replicasets
FAIL	k8s.io/kubernetes/plugin/pkg/admission/imagepolicy	0.868s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/limitranger	1.139s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/namespace/autoprovision	0.706s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/namespace/exists	1.964s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestAdmission

==================== CTEST START ====================
=== RUN   TestCtestAdmission/no_default,_no_modification_of_Ingress
Running 0 th test case.
[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/network/defaultingressclass/ctest_admission_test.go:124]: TestInfo: no-default-classes
[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/network/defaultingressclass/ctest_admission_test.go:132]: get default configs: {test_fixture.json [no-default-classes] annotations [ingressclasses] [{{ } {nondefault-false      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[ingressclass.kubernetes.io/is-default-class:false] [] [] []} { nil}} {{ } {nondefault-noanno      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} { nil}} {{ } {nondefault-empty      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[ingressclass.kubernetes.io/is-default-class:] [] [] []} { nil}}]}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[ingressclasses]
[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[ingressclasses], int=1)[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [ingressclasses]
[DEBUG-CTEST 2026-02-09 18:52:04 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:52:04 load all fixtures failed: requested fixture keys not found in test_fixtures.json: ingressclasses
FAIL	k8s.io/kubernetes/plugin/pkg/admission/network/defaultingressclass	3.239s
=== RUN   TestCtestAdmission

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/network/denyserviceexternalips/ctest_admission_test.go:28]: matched config item: {test_fixture.json [default service externalIPs] externalIPs [services] {{ } {test-svc  test-ns    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []} { [] map[]  [] [] <nil>  [1.1.1.1]   <nil> []  0 false <nil> <nil> <nil> <nil>} {{[]} []}}}

==================== CTEST OVERRIDE ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[services]
[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[services], int=1)[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "externalIPs" in requested fixtures
2026/02/09 18:52:03 [DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:685]: === OVERRIDE ONLY FUNCTION START ===
2026/02/09 18:52:03 Mode: 1
2026/02/09 18:52:03 Base JSON size: 571 bytes
2026/02/09 18:52:03 Number of external values: 0
2026/02/09 18:52:03 [DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:746]:%!(EXTRA string=
=== OVERRIDE ONLY COMPLETE ===)
2026/02/09 18:52:03 [DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:747]:%!(EXTRA string=Generated %d valid result(s), int=0)
[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Spec":{"AllocateLoadBalancerNodePorts":null,"ClusterIP":"","ClusterIPs":null,"ExternalIPs":["1.1.1.1"],"ExternalName":"","ExternalTrafficPolicy":"","HealthCheckNodePort":0,"IPFamilies":null,"IPFamilyPolicy":null,"InternalTrafficPolicy":null,"LoadBalancerClass":null,"LoadBalancerIP":"","LoadBalancerSourceRanges":null,"Ports":null,"PublishNotReadyAddresses":false,"Selector":null,"SessionAffinity":"","SessionAffinityConfig":null,"TrafficDistribution":null,"Type":""},"Status":{"Conditions":null,"LoadBalancer":{"Ingress":null}},"name":"test-svc","namespace":"test-ns"})[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:52:03 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/network/denyserviceexternalips/ctest_admission_test.go:36]: Skipping test execution. No new configurations generated.

==================== CTEST END ======================
--- PASS: TestCtestAdmission (0.00s)
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/network/denyserviceexternalips	2.776s	coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/noderestriction	3.509s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/nodetaint	2.286s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/podnodeselector	0.449s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.9% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction	0.561s	coverage: 0.9% of statements [no tests to run]
	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/install		coverage: 0.0% of statements
	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/v1alpha1		coverage: 0.0% of statements
=== RUN   TestCtestValidateConfiguration

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:20]: Loaded hard‑coded config entries: 3
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:24]: Processing fixture: [Valid cases]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "default" in requested fixtures
2026/02/09 18:52:10 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:52:10 Base JSON size: 563 bytes
2026/02/09 18:52:10 Number of external values: 0
2026/02/09 18:52:10 BASE DATA (type: map[string]interface {}):
{
  "Default": [
    {
      "Effect": "NoExecute",
      "Key": "foo",
      "Operator": "Exists",
      "TolerationSeconds": 60,
      "Value": ""
    },
    {
      "Effect": "NoExecute",
      "Key": "foo",
      "Operator": "Equal",
      "TolerationSeconds": 60,
      "Value": "bar"
    },
    {
      "Effect": "NoSchedule",
      "Key": "foo",
      "Operator": "Equal",
      "TolerationSeconds": null,
      "Value": "bar"
    },
    {
      "Effect": "NoSchedule",
      "Key": "",
      "Operator": "Exists",
      "TolerationSeconds": null,
      "Value": ""
    }
  ],
  "Whitelist": [
    {
      "Effect": "NoSchedule",
      "Key": "foo",
      "Operator": "",
      "TolerationSeconds": null,
      "Value": "bar"
    },
    {
      "Effect": "",
      "Key": "foo",
      "Operator": "Equal",
      "TolerationSeconds": null,
      "Value": "bar"
    }
  ]
}
2026/02/09 18:52:10 
=== UNION COMPLETE ===
2026/02/09 18:52:10 Generated 0 result(s)
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Default":[{"Effect":"NoExecute","Key":"foo","Operator":"Exists","TolerationSeconds":60,"Value":""},{"Effect":"NoExecute","Key":"foo","Operator":"Equal","TolerationSeconds":60,"Value":"bar"},{"Effect":"NoSchedule","Key":"foo","Operator":"Equal","TolerationSeconds":null,"Value":"bar"},{"Effect":"NoSchedule","Key":"","Operator":"Exists","TolerationSeconds":null,"Value":""}],"Whitelist":[{"Effect":"NoSchedule","Key":"foo","Operator":"","TolerationSeconds":null,"Value":"bar"},{"Effect":"","Key":"foo","Operator":"Equal","TolerationSeconds":null,"Value":"bar"}]})[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:30]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:24]: Processing fixture: [Invalid case (whitelist only)]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "whitelist" in requested fixtures
2026/02/09 18:52:10 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:52:10 Base JSON size: 125 bytes
2026/02/09 18:52:10 Number of external values: 0
2026/02/09 18:52:10 BASE DATA (type: map[string]interface {}):
{
  "Default": null,
  "Whitelist": [
    {
      "Effect": "NoSchedule",
      "Key": "foo",
      "Operator": "Exists",
      "TolerationSeconds": null,
      "Value": "bar"
    }
  ]
}
2026/02/09 18:52:10 
=== UNION COMPLETE ===
2026/02/09 18:52:10 Generated 0 result(s)
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Default":null,"Whitelist":[{"Effect":"NoSchedule","Key":"foo","Operator":"Exists","TolerationSeconds":null,"Value":"bar"}]})[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:30]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:24]: Processing fixture: [Invalid case (default only)]
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[pods]
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[pods], int=1)[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:369]: err: no values found for field "default" in requested fixtures
2026/02/09 18:52:10 === UNION FUNCTION START (OVERRIDE + EXTEND) ===
2026/02/09 18:52:10 Base JSON size: 121 bytes
2026/02/09 18:52:10 Number of external values: 0
2026/02/09 18:52:10 BASE DATA (type: map[string]interface {}):
{
  "Default": [
    {
      "Effect": "NoSchedule",
      "Key": "",
      "Operator": "Equal",
      "TolerationSeconds": null,
      "Value": "bar"
    }
  ],
  "Whitelist": null
}
2026/02/09 18:52:10 
=== UNION COMPLETE ===
2026/02/09 18:52:10 Generated 0 result(s)
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:411]:%!(EXTRA string=Normalized original JSON: %s
, string={"Default":[{"Effect":"NoSchedule","Key":"","Operator":"Equal","TolerationSeconds":null,"Value":"bar"}],"Whitelist":null})[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:454]: ⚠️  All results were identical to original hardcoded config, returning nil
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:30]: Skipping test execution. No new configurations generated.
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:110]: Running edge case: Edge: minimal Exists toleration
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:110]: Running edge case: Edge: invalid operator
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:110]: Running edge case: Edge: negative tolerationSeconds
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:110]: Running edge case: Edge: empty key with Exists
[DEBUG-CTEST 2026-02-09 18:52:10 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation/ctest_validation_test.go:110]: Running edge case: Edge: completely empty configuration

==================== CTEST END ======================
--- PASS: TestCtestValidateConfiguration (0.01s)
PASS
coverage: 100.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/podtolerationrestriction/apis/podtolerationrestriction/validation	0.701s	coverage: 100.0% of statements
=== RUN   TestCtestPodTopology

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[nodes]
[DEBUG-CTEST 2026-02-09 18:52:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[nodes], int=1)[DEBUG-CTEST 2026-02-09 18:52:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [nodes]
[DEBUG-CTEST 2026-02-09 18:52:12 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:52:12 load all fixtures failed: requested fixture keys not found in test_fixtures.json: nodes
FAIL	k8s.io/kubernetes/plugin/pkg/admission/podtopologylabels	1.759s
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/priority	0.673s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: [no statements]
ok  	k8s.io/kubernetes/plugin/pkg/admission/resourcequota	2.709s	coverage: [no statements] [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/runtimeclass	1.760s	coverage: 0.0% of statements [no tests to run]
?   	k8s.io/kubernetes/plugin/pkg/admission/security	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/security/podsecurity	2.180s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/admission/serviceaccount	1.260s	coverage: 0.0% of statements [no tests to run]
=== RUN   TestCtestPVCResizeAdmission
    ctest_admission_test.go:300: resize to zero size: unexpected err: <nil>
    ctest_admission_test.go:300: resize to negative size: unexpected err: <nil>
--- FAIL: TestCtestPVCResizeAdmission (0.00s)
FAIL
coverage: 74.4% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/storage/persistentvolume/resize	0.373s
=== RUN   TestCtestAdmission
I0209 18:52:15.955925   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956001   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956018   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956029   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956046   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956061   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956077   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956092   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956103   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956114   93234 ctest_admission_test.go:333] Got <nil>
I0209 18:52:15.956123   93234 ctest_admission_test.go:333] Got <nil>
    ctest_admission_test.go:338: Test "PVC with nil namespace (should error)": expected error and no error received
    ctest_admission_test.go:349: Test "PVC with nil namespace (should error)": expected class name "", got "default1"
I0209 18:52:15.956165   93234 ctest_admission_test.go:333] Got <nil>
    ctest_admission_test.go:338: Test "PVC with empty name (should error)": expected error and no error received
    ctest_admission_test.go:349: Test "PVC with empty name (should error)": expected class name "", got "default1"
I0209 18:52:15.956186   93234 ctest_admission_test.go:333] Got <nil>
--- FAIL: TestCtestAdmission (0.00s)
FAIL
coverage: 64.3% of statements
FAIL	k8s.io/kubernetes/plugin/pkg/admission/storage/storageclass/setdefault	0.773s
=== RUN   TestCtestAdmit

==================== CTEST START ====================
[DEBUG-CTEST 2026-02-09 18:52:18 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/plugin/pkg/admission/storage/storageobjectinuseprotection/ctest_admission_test.go:35]: PVC config item: {test_fixture.json [persistentvolumeclaims default] finalizers [persistentvolumeclaims] 0x14000594000}

==================== CTEST EXTEND ONLY START ====================
=== GENERATE EFFECTIVE CONFIG START ===
[DEBUG-CTEST 2026-02-09 18:52:18 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:320]: K8sObjects: 
[persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:52:18 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:356]:%!(EXTRA string=[DEBUG] Loading fixtures for types: %v (count: %d)
, []string=[persistentvolumeclaims], int=1)[DEBUG-CTEST 2026-02-09 18:52:18 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:25]: Loading embedded fixture file: test_fixtures.json
[DEBUG-CTEST 2026-02-09 18:52:18 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/fixtures/helper_getjson.go:77]: Missing requested fixture keys: [persistentvolumeclaims]
[DEBUG-CTEST 2026-02-09 18:52:18 /Users/yuezhang/research/k8s-config-test/kubernetes-1.34.2/test/ctest/generate_new_config.go:364]: load all fixtures failed
2026/02/09 18:52:18 load all fixtures failed: requested fixture keys not found in test_fixtures.json: persistentvolumeclaims
FAIL	k8s.io/kubernetes/plugin/pkg/admission/storage/storageobjectinuseprotection	1.753s
?   	k8s.io/kubernetes/plugin/pkg/auth	[no test files]
=== RUN   TestCtestTokenAuthenticator
[DEBUG] Starting TestCtestTokenAuthenticator
[DEBUG] Running sub‑test: valid token
[DEBUG] Running sub‑test: valid token with extra group
[DEBUG] Running sub‑test: invalid group
[DEBUG] Running sub‑test: invalid secret name
[DEBUG] Running sub‑test: no usage
[DEBUG] Running sub‑test: wrong token
[DEBUG] Running sub‑test: deleted token
[DEBUG] Running sub‑test: expired token
[DEBUG] Running sub‑test: not expired token
[DEBUG] Running sub‑test: token id wrong length
[DEBUG] Running sub‑test: empty token
[DEBUG] Running sub‑test: missing secret part
[DEBUG] Running sub‑test: extra delimiter
[DEBUG] Running sub‑test: secret with non‑base64 characters in token secret
[DEBUG] Completed TestCtestTokenAuthenticator
--- PASS: TestCtestTokenAuthenticator (0.00s)
PASS
coverage: 81.1% of statements
ok  	k8s.io/kubernetes/plugin/pkg/auth/authenticator/token/bootstrap	1.233s	coverage: 81.1% of statements
?   	k8s.io/kubernetes/plugin/pkg/auth/authorizer	[no test files]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/auth/authorizer/node	0.382s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac	0.634s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 22.4% of statements
ok  	k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac/bootstrappolicy	0.519s	coverage: 22.4% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/third_party/forked/golang/expansion	1.020s	coverage: 0.0% of statements [no tests to run]
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/third_party/forked/golang/net	0.824s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/third_party/forked/gonum/graph		coverage: 0.0% of statements
	k8s.io/kubernetes/third_party/forked/gonum/graph/internal/linear		coverage: 0.0% of statements
testing: warning: no tests to run
PASS
coverage: 0.0% of statements
ok  	k8s.io/kubernetes/third_party/forked/gonum/graph/simple	0.660s	coverage: 0.0% of statements [no tests to run]
	k8s.io/kubernetes/third_party/forked/gonum/graph/traverse		coverage: 0.0% of statements
?   	k8s.io/kubernetes/third_party/forked/gotestsum/junitxml	[no test files]
	k8s.io/kubernetes/third_party/forked/libcontainer/apparmor		coverage: 0.0% of statements
	k8s.io/kubernetes/third_party/forked/libcontainer/utils		coverage: 0.0% of statements
	k8s.io/kubernetes/third_party/forked/vishhstress		coverage: 0.0% of statements
FAIL
